{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28e59f61-bd10-4de9-908a-2ba3de94a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertForMaskedLM, DistilBertTokenizerFast\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ccec89-129b-4ff3-b666-329bae561cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator_BERT:\n",
    "    def __init__(self, tokenizer, size_seq_max, need_masked=0.3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size_seq_max = size_seq_max\n",
    "        self.need_masked = need_masked\n",
    "        \n",
    "        index_special_token_2_id = {k:v for k,v in zip(tokenizer.all_special_tokens,tokenizer.all_special_ids)}\n",
    "        \n",
    "        self.id_pad = index_special_token_2_id['[PAD]']\n",
    "        self.id_mask = index_special_token_2_id['[MASK]']\n",
    "        self.id_cls = index_special_token_2_id['[CLS]']\n",
    "        self.id_sep = index_special_token_2_id['[SEP]']\n",
    "        self.id_unk = index_special_token_2_id['[UNK]']\n",
    "        \n",
    "        self.regex_special_token = re.compile(r'\\[(PAD|MASK|CLS|SEP|EOL|UNK)\\]')\n",
    "    # end\n",
    "    \n",
    "    def _preprocess(self, line):\n",
    "        line = re.sub(self.regex_special_token, r'<\\1>', line)\n",
    "        line = re.sub(r'''('|\"|`){2}''', '', line)\n",
    "        line = re.sub(r'\\.{2,3}', '', line)\n",
    "        line = re.sub(r' {2,}', ' ', line)\n",
    "        line = line.lstrip().rstrip()\n",
    "        return line\n",
    "    # end\n",
    "    \n",
    "\n",
    "    def __call__(self, list_sequence_batch):\n",
    "        list_sequence_batch = [self._preprocess(sequence) for sequence in list_sequence_batch]   # remove special tokens\n",
    "        \n",
    "        list_sequence_tokenized = self.tokenizer.batch_encode_plus(list_sequence_batch, add_special_tokens=False)['input_ids']\n",
    "        \n",
    "        # Process I. \n",
    "        list_list_tokenized = []\n",
    "        \n",
    "        # batch initialized condition\n",
    "        list_tokenized_cache = []\n",
    "        len_tokenized_accumulated = 2 # add cls and sep\n",
    "        \n",
    "        while list_sequence_tokenized:\n",
    "            tokenized_poped = list_sequence_tokenized.pop(0)\n",
    "            len_tokenized_current = len(tokenized_poped)\n",
    "            \n",
    "            if len_tokenized_accumulated + len_tokenized_current > self.size_seq_max:\n",
    "                if list_tokenized_cache:\n",
    "                    list_list_tokenized.append(list_tokenized_cache)\n",
    "                \n",
    "                    # clear\n",
    "                    list_tokenized_cache = []\n",
    "                    len_tokenized_accumulated = 2\n",
    "                # end\n",
    "            # end\n",
    "\n",
    "            list_tokenized_cache.append(tokenized_poped)\n",
    "            len_tokenized_accumulated += len_tokenized_current\n",
    "        # end\n",
    "        \n",
    "        list_list_tokenized.append(list_tokenized_cache)\n",
    "        \n",
    "        \n",
    "        # Process II. Merge list_tokenized\n",
    "        list_tokenized_merged = []\n",
    "        \n",
    "        for list_tokenized in list_list_tokenized:\n",
    "            # tokenized_merged = [token for tokenized_padded in [tokenized + [self.id_eol] for tokenized in list_tokenized] for token in tokenized_padded]\n",
    "            tokenized_merged = [token for tokenized in list_tokenized for token in tokenized][:self.size_seq_max-2]\n",
    "            list_tokenized_merged.append(tokenized_merged)\n",
    "        # end\n",
    "        \n",
    "        \n",
    "        # Process III. Add begin and stop special token, same as jinyuj_transformers_quora.ipynb\n",
    "        tokens_input_encoder = []\n",
    "        tokens_input_decoder = []\n",
    "        tokens_label_decoder = []\n",
    "        \n",
    "        for tokenized_merged in list_tokenized_merged:\n",
    "            tokens_input_encoder.append([self.id_cls] + tokenized_merged + [self.id_sep])\n",
    "            tokens_input_decoder.append([self.id_cls] + tokenized_merged)\n",
    "            tokens_label_decoder.append(tokenized_merged + [self.id_sep])\n",
    "        # end\n",
    "        \n",
    "        inputs_encoder, masks_encoder, segments_encoder, labels_encoder = self.pad_sequences(tokens_input_encoder, self.size_seq_max, need_masked=self.need_masked)\n",
    "        inputs_decoder, masks_decoder, segments_decoder, _ = self.pad_sequences(tokens_input_decoder, self.size_seq_max, need_diagonal=True)\n",
    "        labels_decoder, masks_label, segments_label, _ = self.pad_sequences(tokens_label_decoder, self.size_seq_max)\n",
    "\n",
    "        \n",
    "        return Batch(\n",
    "            input_ids=inputs_encoder,  # contains [mask]s\n",
    "            attention_mask=masks_encoder,\n",
    "            labels=labels_encoder,  # doesn't contain [mask]\n",
    "            output_hidden_states=False,\n",
    "            segments=segments_encoder\n",
    "        )\n",
    "\n",
    "    # end\n",
    "\n",
    "\n",
    "    # return masks_attention?, return masks_segment?\n",
    "    def pad_sequences(self, sequences, size_seq_max, need_diagonal=False, need_masked=0): # need_diagonal and need_masked cannot both set, one for bert seq one for s2s seq\n",
    "        id_pad = self.id_pad\n",
    "        id_mask = self.id_mask\n",
    "\n",
    "        sequences_padded = []\n",
    "        sequences_masked_padded = []\n",
    "\n",
    "        for sequence in sequences:\n",
    "            len_seq = len(sequence)\n",
    "\n",
    "            count_pad = size_seq_max - len_seq\n",
    "\n",
    "            sequence = torch.LongTensor(sequence)\n",
    "            sequence_padded = torch.cat((sequence, torch.LongTensor([id_pad] * count_pad)))\n",
    "            # print(sequence_padded.shape)\n",
    "            sequences_padded.append(sequence_padded)\n",
    "\n",
    "            if need_masked:\n",
    "                index_masked = list(range(1, len_seq-1))\n",
    "                random.shuffle(index_masked)\n",
    "                index_masked = torch.LongTensor(index_masked[:int(need_masked * (len_seq-2))])\n",
    "\n",
    "                sequence_masked = sequence.detach().clone()\n",
    "                sequence_masked.index_fill_(0, index_masked, id_mask)\n",
    "                sequence_masked_padded = torch.cat((sequence_masked, torch.LongTensor([id_pad] * count_pad)))\n",
    "                \n",
    "                sequences_masked_padded.append(sequence_masked_padded)\n",
    "            # end\n",
    "    #   # end for\n",
    "\n",
    "        inputs = torch.stack(sequences_padded)  # (batch, size_seq_max)\n",
    "        if need_masked:\n",
    "            inputs_masked_padded = torch.stack(sequences_masked_padded)\n",
    "        # end\n",
    "\n",
    "        masks_segment = (inputs != self.id_pad).unsqueeze(-2)    #(nbatch, 1, seq)\n",
    "        masks_attention = self.make_std_mask(inputs, self.id_pad) if need_diagonal else masks_segment\n",
    "\n",
    "        if need_masked:\n",
    "            masks_masked = (inputs_masked_padded != id_mask).unsqueeze(-2)\n",
    "            masks_attention = masks_attention & masks_masked\n",
    "            return inputs_masked_padded, masks_attention, masks_segment, inputs # (inputs, masks_attention, masks_segment, labels)\n",
    "        else:\n",
    "            return inputs, masks_attention, masks_segment, None\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "\n",
    "    def subsequent_mask(self, size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "            torch.uint8\n",
    "        )\n",
    "        return subsequent_mask == 0\n",
    "    # end\n",
    "\n",
    "    \n",
    "    def make_std_mask(self, tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & self.subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10b396d-c234-45fe-b1bf-0db36ef4a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BookCorpus2000(split=0.1):\n",
    "    filename = 'bookcorpus_2000.json'\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        list_corpus = json.load(file)\n",
    "    # end\n",
    "    \n",
    "    indexs_all = list(range(len(list_corpus)))\n",
    "    random.shuffle(indexs_all)\n",
    "    \n",
    "    index_split = int(split * len(list_corpus))\n",
    "    \n",
    "    indexs_eval = indexs_all[:index_split]\n",
    "    indexs_train = indexs_all[index_split:]\n",
    "    \n",
    "    list_corpus_eval = [list_corpus[i_e] for i_e in indexs_eval]\n",
    "    list_corpus_train = [list_corpus[i_t] for i_t in indexs_train]\n",
    "    \n",
    "    return list_corpus_train, list_corpus_eval, None\n",
    "# end\n",
    "\n",
    "\n",
    "def BookCorpus(split=0.0001, used=-1):\n",
    "    import datasets\n",
    "    \n",
    "    list_corpus = datasets.load_dataset('bookcorpus')['train']['text'][:used]   # 70,000,000, 70 Million\n",
    "    \n",
    "    indexs_all = list(range(len(list_corpus)))\n",
    "    random.shuffle(indexs_all)\n",
    "    \n",
    "    index_split = int(split * len(list_corpus))\n",
    "    \n",
    "    indexs_eval = indexs_all[:index_split]\n",
    "    indexs_train = indexs_all[index_split:]\n",
    "    \n",
    "    list_corpus_eval = [list_corpus[i_e] for i_e in indexs_eval]\n",
    "    list_corpus_train = [list_corpus[i_t] for i_t in indexs_train]\n",
    "    \n",
    "    return list_corpus_train, list_corpus_eval, None\n",
    "# end\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    DEVICE = 'cuda'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if v is not None and type(v) is not bool:\n",
    "                self.kwargs[k] = v.to(Batch.DEVICE)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.kwargs\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class Dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        for k, v in self.items():\n",
    "            if k in other and other[k]:\n",
    "                self[k] += other[k]\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        return self\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "def get_info_accuracy_template_mlm():\n",
    "    return Dotdict({\n",
    "        'corrects_segmented': 0,\n",
    "        'corrects_masked': 0,\n",
    "        'num_segmented': 0,\n",
    "        'num_masked': 0 \n",
    "    })    \n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c6db9c-0782-42eb-a538-a750eb331245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "gpu = 1\n",
    "torch.cuda.set_device(gpu)\n",
    "#### @jingdi start\n",
    "dim = 768\n",
    "seq_max = 512\n",
    "dropout=0.1\n",
    "n_layers=6\n",
    "n_heads=12\n",
    "batch_size=16\n",
    "epoch = 1\n",
    "############ ends\n",
    "\n",
    "# config_pretrained = PretrainedConfig(\n",
    "#     vocab_size=tokenizer.vocab_size,\n",
    "#     dim=dim,\n",
    "#     dropout=dropout,\n",
    "#     max_position_embeddings=seq_max,\n",
    "#     attention_dropout=dropout,\n",
    "#     n_layers=n_layers,\n",
    "#     num_hidden_layers=n_layers,\n",
    "#     n_heads=n_heads,\n",
    "#     hidden_dim=dim,\n",
    "#     activation='relu',\n",
    "#     initializer_range=0.02,\n",
    "#     sinusoidal_pos_embds=True,\n",
    "# )\n",
    "\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased').to('cuda')\n",
    "# model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to('cuda')\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "#### @jingdi start\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "# decayRate = 0.96\n",
    "# # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "# lr_scheduler = transformers.get_scheduler(\n",
    "#     name=\"cosine_with_restarts\", optimizer=optimizer, num_warmup_steps=1000000, num_training_steps=624375 * 10\n",
    "# )\n",
    "############ ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c624c5-8e83-48ac-8591-b0ca97edf4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator_BERT(tokenizer, seq_max)\n",
    "source_train, source_eval, _  = BookCorpus2000()  # @jingdi\n",
    "dataloader_train = DataLoader(source_train, batch_size, shuffle=False, collate_fn=collator)\n",
    "dataloader_eval = DataLoader(source_eval, 1, shuffle=False, collate_fn=collator)\n",
    "func_loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2fca36d-eb67-46c0-a451-a3b698b013e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/113 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "info_acc_train = get_info_accuracy_template_mlm()\n",
    "info_acc_epoch = get_info_accuracy_template_mlm()\n",
    "losss_train = []\n",
    "\n",
    "\n",
    "# train phase\n",
    "model.eval()\n",
    "for b, batch in enumerate(tqdm(dataloader_train)):\n",
    "    info_batch = batch()\n",
    "\n",
    "    segments_encoder = info_batch['segments']\n",
    "    masks_encoder = info_batch['attention_mask']\n",
    "    labels_mlm = info_batch['labels']\n",
    "    del info_batch['segments']\n",
    "    del info_batch['labels']\n",
    "\n",
    "    output_mlm = model(**info_batch).logits\n",
    "    # output_mlm = model(**info_batch)['last_hidden_state']\n",
    "\n",
    "    info_acc = get_info_accuracy_template_mlm()\n",
    "\n",
    "    segments_encoder_2d = segments_encoder.transpose(-1,-2)[:,:,0]\n",
    "    hidden_mlm_segmented = output_mlm.masked_select(segments_encoder_2d.unsqueeze(-1)).reshape(-1, output_mlm.shape[-1]) # should be (segmented_all_batchs, size_vocab)\n",
    "\n",
    "    info_acc.corrects_segmented = torch.sum(hidden_mlm_segmented.argmax(-1) == labels_mlm.masked_select(segments_encoder_2d)).cpu().item()\n",
    "    info_acc.num_segmented = hidden_mlm_segmented.shape[0]\n",
    "\n",
    "    masks_masked = torch.logical_xor(masks_encoder, segments_encoder) & segments_encoder # True is masked\n",
    "    masks_masked_perbatch = masks_masked[:,0,:]\n",
    "    hidden_mlm_masked = output_mlm.masked_select(masks_masked_perbatch.unsqueeze(-1)).reshape(-1, output_mlm.shape[-1])\n",
    "\n",
    "    info_acc.corrects_masked = torch.sum(hidden_mlm_masked.argmax(-1) == labels_mlm.masked_select(masks_masked_perbatch)).cpu().item()\n",
    "    info_acc.num_masked = hidden_mlm_masked.shape[0]\n",
    "\n",
    "    info_acc_epoch += info_acc\n",
    "\n",
    "\n",
    "# lr_scheduler.step()  # scheduler step per epoch, @jingdi\n",
    "# loss_average_per_e = sum(losss_per_e) / len(losss_per_e)\n",
    "info_acc_train += info_acc_epoch\n",
    "# losss_train += losss_per_e\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84deccd-f81e-4d8a-8661-c7d5c133d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_mlm_segmented.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bfe83ea-4cf2-44fa-a172-1527a0ec5388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 30522])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**info_batch).logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b43d190-50ad-4ce4-814e-f852a2f170c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mlm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f54c7b-db28-4188-bb89-e61f038d3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_encoder_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11690058-433e-425a-bbcd-e10519627f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mlm.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c39363-9c71-416a-9d98-9ee0d54a731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mlm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a1a91-db4c-4042-aec0-df995eaa1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mlm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88187c06-0e72-46af-a078-16901773a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b234e-6dee-41ea-a40f-7eec341008be",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mlm.masked_select(segments_encoder_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395e4dd-6fd0-4587-9a57-7ec5dc7250e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_encoder_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72d952-ed6a-49a2-838b-4ced0b2de589",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4f02f3-f629-49ab-9e96-9d227445d084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042abedd-83f4-4447-a443-4f5bcafbe1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# info_acc_eval = get_info_accuracy_template_mlm()\n",
    "\n",
    "# for e in range(1):\n",
    "#     info_acc_epoch = get_info_accuracy_template_mlm()\n",
    "    \n",
    "#     for b, batch in enumerate(tqdm(dataloader_eval)):\n",
    "#         info_batch = batch()\n",
    "        \n",
    "#         segments_encoder = info_batch['segments']\n",
    "#         masks_encoder = info_batch['attention_mask']\n",
    "#         labels_mlm = info_batch['labels']\n",
    "#         del info_batch['segments']\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             output_mlm = model(**info_batch).logits\n",
    "#         # end\n",
    "\n",
    "#         info_acc = get_info_accuracy_template_mlm()\n",
    "        \n",
    "#         segments_encoder_2d = segments_encoder.transpose(-1,-2)[:,:,0]\n",
    "#         hidden_mlm_segmented = output_mlm.masked_select(segments_encoder_2d.unsqueeze(-1)).reshape(-1, output_mlm.shape[-1]) # should be (segmented_all_batchs, size_vocab)\n",
    "        \n",
    "#         info_acc.corrects_segmented = torch.sum(hidden_mlm_segmented.argmax(-1) == labels_mlm.masked_select(segments_encoder_2d)).cpu().item()\n",
    "#         info_acc.num_segmented = hidden_mlm_segmented.shape[0]\n",
    "        \n",
    "#         masks_masked = torch.logical_xor(masks_encoder, segments_encoder) & segments_encoder # True is masked\n",
    "#         masks_masked_perbatch = masks_masked[:,0,:]\n",
    "#         hidden_mlm_masked = output_mlm.masked_select(masks_masked_perbatch.unsqueeze(-1)).reshape(-1, output_mlm.shape[-1])\n",
    "        \n",
    "#         info_acc.corrects_masked = torch.sum(hidden_mlm_masked.argmax(-1) == labels_mlm.masked_select(masks_masked_perbatch)).cpu().item()\n",
    "#         info_acc.num_masked = hidden_mlm_masked.shape[0]\n",
    "        \n",
    "#         info_acc_epoch += info_acc\n",
    "#     # end\n",
    "    \n",
    "#     info_acc_eval += info_acc_epoch\n",
    "\n",
    "# # end\n",
    "\n",
    "# print(\n",
    "#     'Eval ends. Result:  acc_mlm: {}'.format(\n",
    "#         info_acc_eval.corrects_masked / info_acc_eval.num_masked\n",
    "#     )\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
