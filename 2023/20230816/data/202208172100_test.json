[{"id": 5469, "name": "log-3509", "raw": "2022-08-15 08:13:15,015 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-08-15 08:13:15,015 | TASK [vmxnet3_network_device_ops][Try to ping IP] **********\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops try to ping ip task path home worker workspace ansible ubuntu lts server iso nvme vmxnet number efi ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "retry", "target": "targetvm", "version": 202208172100}, {"id": 5470, "name": "log-3519", "raw": "2022-08-15 14:03:22,015 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2022-08-15 14:03:22,015 | TASK [deploy_vm_efi_nvme_vmxnet3][Configure VM CDROM to 'client'] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nexception in /vmware.py when connect_to_api in /ssl.py when do_handshake\nfatal: [localhost]: FAILED! => Unable to connect to vCenter or ESXi API at 10.191.149.181 on TCP/443: EOF occurred in violation of protocol (_ssl.c:1123)\n2022-08-15 14:04:02,015 | TASK [deploy_vm_efi_nvme_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_nvme_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task deploy vm efi nvme vmxnet number configure vm cdrom to client task path home worker workspace ansible ubuntu lts server iso nvme vmxnet number efi ansible vsphere gos validation common vm configure cdrom yml number exception in vmware python when connect to api in ssl python when do handshake fatal localhost failed unable to connect to vcenter or esxi api at ip address on tcp number eof occurred in violation of protocol ssl c number timestamp task deploy vm efi nvme vmxnet number testing exit due to failure task path home worker workspace ansible ubuntu lts server iso nvme vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi nvme vmxnet number", "solution": "retry", "target": "nimbus", "version": 202208172100}, {"id": 5471, "name": "log-3520", "raw": "2022-08-15 14:38:00,015 | Failed at Play [deploy_vm_bios_lsilogicsas_e1000e] *********\n2022-08-15 14:38:00,015 | TASK [deploy_vm_bios_lsilogicsas_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20220815132317.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-08-15 14:38:13,015 | TASK [deploy_vm_bios_lsilogicsas_e1000e][Fetch file /tmp/cloud-init_2022-08-15-14-38-04.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "", "processed": "timestamp failed at play deploy vm bios lsi logic sas timestamp task deploy vm bios lsi logic sas wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible ubuntu lts server iso lsi logic sas bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm bios lsi logic sas fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible ubuntu lts server iso lsi logic sas bios ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5472, "name": "log-3521", "raw": "2022-08-15 14:43:58,015 | Failed at Play [deploy_vm_bios_sata_e1000e] ****************\n2022-08-15 14:43:58,015 | TASK [deploy_vm_bios_sata_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20220815132730.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-08-15 14:44:12,015 | TASK [deploy_vm_bios_sata_e1000e][Fetch file /tmp/cloud-init_2022-08-15-14-44-03.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "", "processed": "timestamp failed at play deploy vm bios sata timestamp task deploy vm bios sata wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible ubuntu lts server iso number ga sata bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm bios sata fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible ubuntu lts server iso number ga sata bios ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5450, "name": "log-3364", "raw": "2022-08-03 13:09:49,003 | Failed at Play [secureboot_enable_disable] *****************\n2022-08-03 13:09:49,003 | TASK [secureboot_enable_disable][Check secure boot enable and disable test results] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_OVA/ansible-vsphere-gos-validation/linux/secureboot_enable_disable/secureboot_enable_disable.yml:96\nfatal: [localhost]: FAILED! => Enable secureboot result: False, disable secureboot result: True", "category": "", "processed": "timestamp failed at play secure boot enable disable timestamp task secure boot enable disable check secure boot enable and disable test results task path home worker workspace ansible regression photon number ova ansible vsphere gos validation linux secure boot enable disable secure boot enable disable yml number fatal localhost failed enable secure boot result false disable secure boot result true", "solution": "deepdive", "target": "product", "version": 202208172100}, {"id": 5451, "name": "log-3369", "raw": "2022-08-04 23:07:54,004 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-08-04 23:07:54,004 | TASK [testbed_deploy_nimbus][Get ESXi datastores info] *****\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/esxi_get_datastores.yml:5\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.206.190.129:443 : [Errno 113] No route to host", "category": "", "processed": "timestamp failed at play testbed deploy nimbus timestamp task testbed deploy nimbus get esxi data stores info task path home worker workspace ansible cycle windows number ansible vsphere gos validation common esxi get data stores yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number no route to host", "solution": "retry", "target": "nimbus", "version": 202208172100}, {"id": 5447, "name": "log-3294", "raw": "2022-08-02 02:11:43,002 | Failed at Play [wintools_complete_install_verify] **********\n2022-08-02 02:11:43,002 | TASK [Execute powershell command 'get-service -Name VMTools | foreach {$_.Status}'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.185.13.217]: FAILED! => non-zero return code when get-service", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task execute powershell command get service name vm tools for each status task path home worker workspace ansible windows server lts c main para virtual vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when get service", "solution": "retry", "target": "targetvm", "version": 202208172100}, {"id": 5467, "name": "log-3495", "raw": "2022-08-15 07:56:43,015 | Failed at Play [e1000e_network_device_ops] *****************\n2022-08-15 07:56:43,015 | TASK [e1000e_network_device_ops][Try to ping IP] ***********\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "", "processed": "timestamp failed at play network device ops timestamp task network device ops try to ping ip task path home worker workspace ansible ubuntu lts server iso main nvme bios ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "retry", "target": "targetvm", "version": 202208172100}, {"id": 5446, "name": "log-3288", "raw": "2022-08-01 08:28:53,001 | Failed at Play [env_setup] *********************************\n2022-08-01 08:28:53,001 | TASK [Get all registerd VMs and templates on 10.117.17.83] *\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nAttributeError: 'NoneType' object has no attribute 'datastoreUrl'\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "", "processed": "timestamp failed at play environment setup timestamp task get all register dv ms and templates on ip address task path home worker workspace ansible rhel number x para virtual bios ansible vsphere gos validation common vm check exist yml number attribute error none type object has no attribute data store url fatal localhost failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testbed", "version": 202208172100}, {"id": 5448, "name": "log-3319", "raw": "2022-08-02 07:16:33,002 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2022-08-02 07:16:33,002 | TASK [Add IDE boot disk] ***********************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_create_with_ide_disk.yml:49\nfatal: [localhost -> 10.78.89.80]: FAILED! => non-zero return code when vim-cmd\n2022-08-02 07:16:34,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi ide timestamp task add ide boot disk task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common vm create with ide disk yml number fatal localhost ip address failed nonzero return code when vim command timestamp task testing exit due to failure task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5449, "name": "log-3363", "raw": "2022-08-03 12:22:06,003 | Failed at Play [ovt_verify_install] ************************\n2022-08-03 12:22:06,003 | TASK [ovt_verify_install][Wait for port 22 to become stopped] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/linux/utils/shutdown.yml:13\nfatal: [localhost]: FAILED! => Timeout when waiting for 10.170.67.119:22 to stop.\n2022-08-03 12:22:20,003 | TASK [ovt_verify_install][Testing exit due to failure] *****\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install", "category": "", "processed": "timestamp failed at play ovt verify install timestamp task ovt verify install wait for port number to become stopped task path home worker workspace ansible regression photon number ova ansible vsphere gos validation linux utilities shutdown yml number fatal localhost failed timeout when waiting for ip address to stop timestamp task ovt verify install testing exit due to failure task path home worker workspace ansible regression photon number ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case ovt verify install", "solution": "retry", "target": "targetvm", "version": 202208172100}, {"id": 5452, "name": "log-3381", "raw": "2022-08-05 03:57:53,005 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2022-08-05 03:57:53,005 | TASK [deploy_vm][Datastore file operation] *****************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Other/Windows/Windows10/v22H2/19045.1826/Windows10_InsiderPreview_EnterpriseVL_x32_en-us_19045.1826.iso' is absent, cannot continue\n2022-08-05 03:57:54,005 | TASK [deploy_vm][Testing exit due to failure] **************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual timestamp task deploy vm data store file operation task path home worker workspace ansible cycle windows number ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed file os other windows windows number v number h number windows number insider preview enterprise vl en us number i so is absent can not continue timestamp task deploy vm testing exit due to failure task path home worker workspace ansible cycle windows number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual e number e", "solution": "deepdive", "target": "testbed", "version": 202208172100}, {"id": 5453, "name": "log-3413", "raw": "2022-08-09 05:38:57,009 | Failed at Play [check_inbox_driver] ************************\n2022-08-09 05:38:57,009 | TASK [check_inbox_driver][Check inbox driver's filename is valid] \ntask path: /home/worker/workspace/Ansible_Regression_SLES_15.x/ansible-vsphere-gos-validation/linux/check_inbox_driver/get_inbox_drivers.yml:123\nfailed: [localhost] => (item=vmxnet3 is /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst) => Invalid inbox driver vmxnet3 filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst and vmxnet3 is /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst\nfailed: [localhost] => (item=vmw_vmci is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst) => Invalid inbox driver vmw_vmci filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst and vmw_vmci is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst\nfailed: [localhost] => (item=vsock is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst) => Invalid inbox driver vsock filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst and vsock is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst\nfailed: [localhost] => (item=vmw_vsock_vmci_transport is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst) => Invalid inbox driver vmw_vsock_vmci_transport filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst and vmw_vsock_vmci_transport is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst\nfailed: [localhost] => (item=vmw_vsock_virtio_transport_common is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst) => Invalid inbox driver vmw_vsock_virtio_transport_common filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst and vmw_vsock_virtio_transport_common is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst\nfailed: [localhost] => (item=vmw_pvscsi is /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst) => Invalid inbox driver vmw_pvscsi filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst and vmw_pvscsi is /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst\nfailed: [localhost] => (item=vmw_balloon is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst) => Invalid inbox driver vmw_balloon filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst and vmw_balloon is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst\nfailed: [localhost] => (item=vmwgfx is /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst) => Invalid inbox driver vmwgfx filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst and vmwgfx is /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst\nfailed: [localhost] => (item=vmw_pvrdma is /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst) => Invalid inbox driver vmw_pvrdma filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst and vmw_pvrdma is /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst\nfailed: [localhost] => (item=ptp_vmw is /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst) => Invalid inbox driver ptp_vmw filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst and ptp_vmw is /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver check inbox driver s filename is valid task path home worker workspace ansible regression sles number x ansible vsphere gos validation linux check inbox driver get inbox drivers yml number failed localhost item vmxnet number is library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st invalid inbox driver vmxnet number filename library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st and vmxnet number is library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st failed localhost item vmware vmci is library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st invalid inbox driver vmware vmci filename library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st and vmware vmci is library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st failed localhost item vsock is library modules version id number default kernel net vmware vsock vsock ko z st invalid inbox driver vsock filename library modules version id number default kernel net vmware vsock vsock ko z st and vsock is library modules version id number default kernel net vmware vsock vsock ko z st failed localhost item vmware vsock vmci transport is library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st invalid inbox driver vmware vsock vmci transport filename library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st and vmware vsock vmci transport is library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st failed localhost item vmware vsock virtio transport common is library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st invalid inbox driver vmware vsock virtio transport common filename library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st and vmware vsock virtio transport common is library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st failed localhost item vmware pvscsi is library modules version id number default kernel drivers scsi vmware pvscsi ko z st invalid inbox driver vmware pvscsi filename library modules version id number default kernel drivers scsi vmware pvscsi ko z st and vmware pvscsi is library modules version id number default kernel drivers scsi vmware pvscsi ko z st failed localhost item vmware balloon is library modules version id number default kernel drivers misc vmware balloon ko z st invalid inbox driver vmware balloon filename library modules version id number default kernel drivers misc vmware balloon ko z st and vmware balloon is library modules version id number default kernel drivers misc vmware balloon ko z st failed localhost item vmware gfx is library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st invalid inbox driver vmware gfx filename library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st and vmware gfx is library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st failed localhost item vmware pvrdma is library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st invalid inbox driver vmware pvrdma filename library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st and vmware pvrdma is library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st failed localhost item p tp vmware is library modules version id number default kernel drivers p tp p tp vmware ko z st invalid inbox driver p tp vmware filename library modules version id number default kernel drivers p tp p tp vmware ko z st and p tp vmware is library modules version id number default kernel drivers p tp p tp vmware ko z st", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5454, "name": "log-3414", "raw": "2022-08-09 05:42:22,009 | Failed at Play [check_inbox_driver] ************************\n2022-08-09 05:42:22,009 | TASK [check_inbox_driver][Check inbox driver's filename is valid] \ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/check_inbox_driver/get_inbox_drivers.yml:123\nfailed: [localhost] => (item=vmxnet3 is /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst) => Invalid inbox driver vmxnet3 filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst and vmxnet3 is /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst\nfailed: [localhost] => (item=vmw_vmci is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst) => Invalid inbox driver vmw_vmci filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst and vmw_vmci is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst\nfailed: [localhost] => (item=vsock is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst) => Invalid inbox driver vsock filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst and vsock is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst\nfailed: [localhost] => (item=vmw_vsock_vmci_transport is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst) => Invalid inbox driver vmw_vsock_vmci_transport filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst and vmw_vsock_vmci_transport is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst\nfailed: [localhost] => (item=vmw_vsock_virtio_transport_common is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst) => Invalid inbox driver vmw_vsock_virtio_transport_common filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst and vmw_vsock_virtio_transport_common is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst\nfailed: [localhost] => (item=vmw_pvscsi is /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst) => Invalid inbox driver vmw_pvscsi filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst and vmw_pvscsi is /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst\nfailed: [localhost] => (item=vmw_balloon is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst) => Invalid inbox driver vmw_balloon filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst and vmw_balloon is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst\nfailed: [localhost] => (item=vmwgfx is /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst) => Invalid inbox driver vmwgfx filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst and vmwgfx is /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst\nfailed: [localhost] => (item=vmw_pvrdma is /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst) => Invalid inbox driver vmw_pvrdma filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst and vmw_pvrdma is /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst\nfailed: [localhost] => (item=ptp_vmw is /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst) => Invalid inbox driver ptp_vmw filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst and ptp_vmw is /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver check inbox driver s filename is valid task path home worker workspace ansible regression sled number x ansible vsphere gos validation linux check inbox driver get inbox drivers yml number failed localhost item vmxnet number is library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st invalid inbox driver vmxnet number filename library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st and vmxnet number is library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st failed localhost item vmware vmci is library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st invalid inbox driver vmware vmci filename library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st and vmware vmci is library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st failed localhost item vsock is library modules version id number default kernel net vmware vsock vsock ko z st invalid inbox driver vsock filename library modules version id number default kernel net vmware vsock vsock ko z st and vsock is library modules version id number default kernel net vmware vsock vsock ko z st failed localhost item vmware vsock vmci transport is library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st invalid inbox driver vmware vsock vmci transport filename library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st and vmware vsock vmci transport is library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st failed localhost item vmware vsock virtio transport common is library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st invalid inbox driver vmware vsock virtio transport common filename library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st and vmware vsock virtio transport common is library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st failed localhost item vmware pvscsi is library modules version id number default kernel drivers scsi vmware pvscsi ko z st invalid inbox driver vmware pvscsi filename library modules version id number default kernel drivers scsi vmware pvscsi ko z st and vmware pvscsi is library modules version id number default kernel drivers scsi vmware pvscsi ko z st failed localhost item vmware balloon is library modules version id number default kernel drivers misc vmware balloon ko z st invalid inbox driver vmware balloon filename library modules version id number default kernel drivers misc vmware balloon ko z st and vmware balloon is library modules version id number default kernel drivers misc vmware balloon ko z st failed localhost item vmware gfx is library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st invalid inbox driver vmware gfx filename library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st and vmware gfx is library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st failed localhost item vmware pvrdma is library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st invalid inbox driver vmware pvrdma filename library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st and vmware pvrdma is library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st failed localhost item p tp vmware is library modules version id number default kernel drivers p tp p tp vmware ko z st invalid inbox driver p tp vmware filename library modules version id number default kernel drivers p tp p tp vmware ko z st and p tp vmware is library modules version id number default kernel drivers p tp p tp vmware ko z st", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5455, "name": "log-3435", "raw": "2022-08-10 06:16:36,010 | Failed at Play [secureboot_enable_disable] *****************\n2022-08-10 06:16:36,010 | TASK [secureboot_enable_disable][Check VM 'test_vm' IP address] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play secure boot enable disable timestamp task secure boot enable disable check vm test vm ip address task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5456, "name": "log-3441", "raw": "2022-08-11 02:57:18,011 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-08-11 02:57:18,011 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Wait for getting VM 'dw_test_winsrv_ansible' IP address on ESXi 'pek2-hs1-a0410.eng.vmware.com'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip_esxcli.yml:51\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect password: Warning: Permanently added 'pek2-hs1-a0410.eng.vmware.com' (ECDSA) to the list of known hosts.", "category": "", "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task deploy vm efi lsi logic sas wait for getting vm dw test win srv ansible ip address on esxi pe k number hs number eng vmware com task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation common vm get ip esx cli yml number fatal localhost un reachable invalid incorrect password warning permanently added pe k number hs number eng vmware com ecdsa to the list of known hosts", "solution": "retry", "target": "nimbus", "version": 202208172100}, {"id": 5457, "name": "log-3442", "raw": "2022-08-11 04:22:33,011 | Failed at Play [vbs_enable_disable] ************************\n2022-08-11 04:22:33,011 | TASK [vbs_enable_disable][Check VM 'dw_test_winsrv_ansible' IP address] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'dw_test_winsrv_ansible' IP Address", "category": "", "processed": "timestamp failed at play vbs enable disable timestamp task vbs enable disable check vm dw test win srv ansible ip address task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm dw test win srv ansible ip address", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5458, "name": "log-3444", "raw": "2022-08-11 04:33:38,011 | Failed at Play [check_inbox_driver] ************************\n2022-08-11 04:33:38,011 | TASK [check_inbox_driver][Check all mandatory inbox drivers exists in Ubuntu] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_OVA/ansible-vsphere-gos-validation/linux/check_inbox_driver/get_inbox_drivers.yml:181\nfailed: [localhost] => (item=None) => Unexpected templating type error occurred on ({{ mandatory_drivers - ['vmw_balloon']}}): unsupported operand type(s) for -: 'list' and 'list'", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver check all mandatory inbox drivers exists in ubuntu task path home worker workspace ansible regression ubuntu number server ova ansible vsphere gos validation linux check inbox driver get inbox drivers yml number failed localhost item none unexpected templating type error occurred on mandatory drivers vmware balloon un supported operand type s for list and list", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5459, "name": "log-3448", "raw": "2022-08-11 04:39:41,011 | Failed at Play [check_inbox_driver] ************************\n2022-08-11 04:39:41,011 | TASK [check_inbox_driver][Check all mandatory inbox drivers exists in Ubuntu] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_ISO/ansible-vsphere-gos-validation/linux/check_inbox_driver/get_inbox_drivers.yml:181\nfailed: [localhost] => (item=None) => Unexpected templating type error occurred on ({{ mandatory_drivers - ['vmw_balloon']}}): unsupported operand type(s) for -: 'list' and 'list'", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver check all mandatory inbox drivers exists in ubuntu task path home worker workspace ansible regression ubuntu number server iso ansible vsphere gos validation linux check inbox driver get inbox drivers yml number failed localhost item none unexpected templating type error occurred on mandatory drivers vmware balloon un supported operand type s for list and list", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5460, "name": "log-3461", "raw": "2022-08-11 04:58:51,011 | Failed at Play [check_inbox_driver] ************************\n2022-08-11 04:58:51,011 | TASK [check_inbox_driver][Check all mandatory inbox drivers exists in OracleLinux] \ntask path: /home/worker/workspace/Ansible_Regression_OracleLinux_8.x/ansible-vsphere-gos-validation/linux/check_inbox_driver/get_inbox_drivers.yml:181\nfailed: [localhost] => (item=None) => Unexpected templating type error occurred on ({{ mandatory_drivers - ['vmw_balloon']}}): unsupported operand type(s) for -: 'list' and 'list'", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver check all mandatory inbox drivers exists in oracle linux task path home worker workspace ansible regression oracle linux number x ansible vsphere gos validation linux check inbox driver get inbox drivers yml number failed localhost item none unexpected templating type error occurred on mandatory drivers vmware balloon un supported operand type s for list and list", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5468, "name": "log-3496", "raw": "2022-08-15 08:36:18,015 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-08-15 08:36:18,015 | TASK [vmxnet3_network_device_ops][Try to ping IP] **********\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops try to ping ip task path home worker workspace ansible ubuntu lts server iso main nvme bios ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "retry", "target": "targetvm", "version": 202208172100}, {"id": 5461, "name": "log-3464", "raw": "2022-08-11 06:25:20,011 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-08-11 06:25:20,011 | TASK [deploy_vm][Datastore file operation] *****************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Linux/Photon/3.0/Rev3U1/photon-3.0-913b49438.iso' is absent, cannot continue\n2022-08-11 06:25:21,011 | TASK [deploy_vm][Get specified property info for VM 'test_photon3_iso_1660198989405'] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_photon3_iso_1660198989405", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm data store file operation task path home worker workspace ansible regression photon number iso ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed file os linux photon number photon number b number i so is absent can not continue timestamp task deploy vm get specified property info for vm test photon number iso number task path home worker workspace ansible regression photon number iso ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test photon number iso number", "solution": "deepdive", "target": "testbed", "version": 202208172100}, {"id": 5462, "name": "log-3471", "raw": "2022-08-11 10:38:54,011 | Failed at Play [check_quiesce_snapshot] ********************\n2022-08-11 10:38:54,011 | TASK [check_quiesce_snapshot][Check pre-freeze script executed after snapshot take] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/windows/check_quiesce_snapshot/check_custom_scripts_executed.yml:46\nfatal: [localhost]: FAILED! => Assertion failed", "category": "", "processed": "timestamp failed at play check quiesce snapshot timestamp task check quiesce snapshot check pre freeze script executed after snapshot take task path home worker workspace ansible windows server lts c main nvme bios ansible vsphere gos validation windows check quiesce snapshot check custom scripts executed yml number fatal localhost failed assertion failed", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5463, "name": "log-3480", "raw": "2022-08-15 03:40:05,015 | Failed at Play [secureboot_enable_disable] *****************\n2022-08-15 03:40:05,015 | TASK [secureboot_enable_disable][Check VM 'test_vm' IP address] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play secure boot enable disable timestamp task secure boot enable disable check vm test vm ip address task path home worker workspace ansible windows number b it main nvme efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5464, "name": "log-3493", "raw": "2022-08-15 07:41:17,015 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-08-15 07:41:17,015 | TASK [deploy_ubuntu_ova][Configure VM CDROM to 'client'] ***\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_67GA/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nfatal: [localhost]: FAILED! => Connection control operation failed for disk 'sata0:0'.\n2022-08-15 07:41:41,015 | TASK [deploy_ubuntu_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_67GA/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova", "category": "", "processed": "timestamp failed at play deploy ubuntu ova timestamp task deploy ubuntu ova configure vm cdrom to client task path home worker workspace ansible ubuntu lts server ova number ga ansible vsphere gos validation common vm configure cdrom yml number fatal localhost failed connection control operation failed for disk sata number timestamp task deploy ubuntu ova testing exit due to failure task path home worker workspace ansible ubuntu lts server ova number ga ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ubuntu ova", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5465, "name": "log-3507", "raw": "2022-08-15 08:48:56,015 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-08-15 08:48:56,015 | TASK [deploy_ubuntu_ova][Configure VM CDROM to 'client'] ***\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_MAIN/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nfatal: [localhost]: FAILED! => Connection control operation failed for disk 'sata0:0'.\n2022-08-15 08:49:18,015 | TASK [deploy_ubuntu_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_MAIN/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova", "category": "", "processed": "timestamp failed at play deploy ubuntu ova timestamp task deploy ubuntu ova configure vm cdrom to client task path home worker workspace ansible ubuntu lts server ova main ansible vsphere gos validation common vm configure cdrom yml number fatal localhost failed connection control operation failed for disk sata number timestamp task deploy ubuntu ova testing exit due to failure task path home worker workspace ansible ubuntu lts server ova main ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ubuntu ova", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5466, "name": "log-3488", "raw": "2022-08-15 07:30:33,015 | Failed at Play [deploy_vm_efi_lsilogic_e1000e] *************\n2022-08-15 07:30:33,015 | TASK [deploy_vm_efi_lsilogic_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20220815061646.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_MAIN_LSILOGIC_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-08-15 07:30:46,015 | TASK [deploy_vm_efi_lsilogic_e1000e][Fetch file /tmp/cloud-init_2022-08-15-07-30-37.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_MAIN_LSILOGIC_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "", "processed": "timestamp failed at play deploy vm efi lsi logic timestamp task deploy vm efi lsi logic wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible ubuntu lts server iso main lsi logic efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm efi lsi logic fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible ubuntu lts server iso main lsi logic efi ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5473, "name": "log-3524", "raw": "2022-08-16 07:38:39,016 | Failed at Play [env_setup] *********************************\n2022-08-16 07:38:39,016 | TASK [env_setup][Get all registerd VMs and templates on gosv-vc-70update.eng.vmware.com] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nAttributeError: 'NoneType' object has no attribute 'datastoreUrl'\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on go sv vc number update eng vmware com task path home worker workspace ansible windows server lts c lsi logic sas bios ansible vsphere gos validation common vm check exist yml number attribute error none type object has no attribute data store url fatal localhost failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testbed", "version": 202208172100}, {"id": 5474, "name": "log-3526", "raw": "2022-08-16 08:35:19,016 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-08-16 08:35:19,016 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Create a new VM 'test_vm' on server 'gosv-vc-70update.eng.vmware.com'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : Invalid configuration for device '4'.\n2022-08-16 08:35:20,016 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task deploy vm efi lsi logic sas create a new vm test vm on server go sv vc number update eng vmware com task path home worker workspace ansible windows server lts c lsi logic sas bios ansible vsphere gos validation common vm create yml number fatal localhost failed failed to create a virtual machine invalid configuration for device number timestamp task deploy vm efi lsi logic sas testing exit due to failure task path home worker workspace ansible windows server lts c lsi logic sas bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e", "solution": "deepdive", "target": "usererror", "version": 202208172100}]