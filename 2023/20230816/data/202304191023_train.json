[{"id": 1058, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Regression_RHEL_9_x-69_failed_tasks_log.0", "raw": "2022-03-25 12:20:53,025 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-03-25 12:20:53,025 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:108\nfatal: [localhost]: FAILED! => [\"VM static IPv4 address is '192.168.192.169', expected IPv4 address is 192.168.192.101\", \"VM static IPv4 gateway is '192.168.1.1', expected IPv4 gateway is 192.168.192.1\"]", "category": "ip_not_expect_0509", "processed": "timestamp failed at play go sc cloud init static ip timestamp task gos customization failed fatal localhost failed vm static ip vnumber address is ip address expected ip vnumber address is ip address vm static ip vnumber gateway is ip address expected ip vnumber gateway is ip address", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1150, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_SLES_15SP4_70U3_PARAVIRTUAL_VMXNET3_BIOS-26_failed_tasks_log.0", "raw": "2022-04-12 12:22:05,012 | Failed at Play [env_setup] *********************************\n2022-04-12 12:22:05,012 | TASK [Deploy VM from ovf template] *************************\ntask path: /home/worker/workspace/Ansible_SLES_15SP4_70U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nexception in /AnsiballZ_vmware_deploy_ovf.py when _ansiballz_main in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_0509", "processed": "timestamp failed at play environment setup timestamp task deploy vm from ovf template exception in ansi ball z vmware deploy ovf python when ansi ball z main in soap adapter python when invoke method fatal localhost failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1171, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Windows_Server_LTSC_vNext-1_logs_failed_tasks_log.1", "raw": "2022-01-26 06:04:10,026 | Failed at Play [check_quiesce_snapshot] ********************\n2022-01-26 06:04:10,026 | TASK [Check specified file status until it exists in Windows guest] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_wait_file_exist.yml:8\nfatal: [localhost -> 10.185.226.50]: FAILED! => {\n    \"attempts\": 40,\n    \"changed\": false,\n    \"invocation\": {\n        \"module_args\": {\n            \"checksum_algorithm\": \"sha1\",\n            \"follow\": false,\n            \"get_checksum\": true,\n            \"path\": \"C:\\\\test_pre_freeze.txt\"\n        }\n    },\n    \"stat\": {\n        \"exists\": false\n    }\n}", "category": "test_pre_freeze_0509", "processed": "timestamp failed at play check quiesce snapshot timestamp task check specified file status until it exists in windows guest fatal localhost ip address failed", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1059, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Regression_RHEL_7_x-55_failed_tasks_log.0", "raw": "2022-02-21 03:41:20,021 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-02-21 03:41:20,021 | TASK [Create snapshot 'BaseSnapshot' on 'test_rhel7'] ******\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_7.x/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play nvme v hba device ops timestamp task create snapshot base snapshot on test rhel number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1172, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Ubuntu_LTS_Server_OVA_67GA-4_failed_tasks_log.0", "raw": "2022-02-24 02:45:37,024 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-02-24 02:45:37,024 | TASK [Remove a serial port using output file] **************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_67GA/ansible-vsphere-gos-validation/common/vm_remove_serial_port.yml:16\nfatal: [localhost]: FAILED! => A serial device cannot be added to a VM in the current state(poweredOn). Please use the vmware_guest_powerstate module to power off the VM\n2022-02-24 02:46:34,024 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_67GA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_ubuntu_ova", "category": "device_cannot_be_added_0509", "processed": "timestamp failed at play deploy ubuntu ova timestamp task remove a serial port using output file fatal localhost failed a serial device can not be added to a vm in the current state powered on please use the vmware guest power state module to power off the vm timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy ubuntu ova", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1173, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Regression_Windows_10_32-42_failed_tasks_log.0", "raw": "2022-02-21 07:58:48,021 | Failed at Play [deploy_vm_efi_nvme_e1000e] *****************\n2022-02-21 07:58:48,021 | TASK [Wait for getting VM 'test_windows10_32_1645428736027' IP address on ESXi 'pek2-hs1-a0409.eng.vmware.com'] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_32/ansible-vsphere-gos-validation/common/vm_get_ip_esxcli.yml:51\nfatal: [localhost]: UNREACHABLE! => Data could not be sent to remote host \"pek2-hs1-a0409.eng.vmware.com\". Make sure this host can be reached over ssh: ssh: connect to host pek2-hs1-a0409.eng.vmware.com port 22: Connection timed out", "category": "data_not_sent_to_remote_0509", "processed": "timestamp failed at play deploy vm efi nvme timestamp task wait for getting vm test windows number ip address on esxi pe knumber hsnumber eng vmware com fatal localhost un reachable data could not be sent to remote host pe knumber hsnumber eng vmware com make sure this host can be reached over ssh ssh connect to host pe knumber hsnumber eng vmware com port number connection timed out", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1073, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.5", "raw": "2022-02-16 10:46:35,016 | Failed at Play [stat_hosttime] *****************************\n2022-02-16 10:46:35,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play stat host time timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1174, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Regression_Windows_10_64-46_failed_tasks_log.0", "raw": "2022-02-21 07:58:49,021 | Failed at Play [deploy_vm_efi_nvme_e1000e] *****************\n2022-02-21 07:58:49,021 | TASK [Wait for getting VM 'test_windows10_64_1645428722003' IP address on ESXi 'pek2-hs1-a0409.eng.vmware.com'] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_64/ansible-vsphere-gos-validation/common/vm_get_ip_esxcli.yml:51\nfatal: [localhost]: UNREACHABLE! => Data could not be sent to remote host \"pek2-hs1-a0409.eng.vmware.com\". Make sure this host can be reached over ssh: ssh: connect to host pek2-hs1-a0409.eng.vmware.com port 22: Connection timed out", "category": "data_not_sent_to_remote_0509", "processed": "timestamp failed at play deploy vm efi nvme timestamp task wait for getting vm test windows number ip address on esxi pe knumber hsnumber eng vmware com fatal localhost un reachable data could not be sent to remote host pe knumber hsnumber eng vmware com make sure this host can be reached over ssh ssh connect to host pe knumber hsnumber eng vmware com port number connection timed out", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1175, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Regression_Windows_Server_2022-55_failed_tasks_log.0", "raw": "2022-02-21 07:58:34,021 | Failed at Play [deploy_vm_efi_nvme_e1000e] *****************\n2022-02-21 07:58:34,021 | TASK [Wait for getting VM 'test_windows_server_ltsc_22538_1645428724556' IP address on ESXi 'pek2-hs1-a0409.eng.vmware.com'] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_2022/ansible-vsphere-gos-validation/common/vm_get_ip_esxcli.yml:51\nfatal: [localhost]: UNREACHABLE! => Data could not be sent to remote host \"pek2-hs1-a0409.eng.vmware.com\". Make sure this host can be reached over ssh: ssh: connect to host pek2-hs1-a0409.eng.vmware.com port 22: Connection timed out", "category": "data_not_sent_to_remote_0509", "processed": "timestamp failed at play deploy vm efi nvme timestamp task wait for getting vm test windows server lts c number ip address on esxi pe knumber hsnumber eng vmware com fatal localhost un reachable data could not be sent to remote host pe knumber hsnumber eng vmware com make sure this host can be reached over ssh ssh connect to host pe knumber hsnumber eng vmware com port number connection timed out", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1176, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_ISO_67GA_LSILOGIC_VMXNET3_BIOS-41_failed_tasks_log.1", "raw": "2022-03-28 04:34:53,028 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-03-28 04:34:53,028 | TASK [Wait for device list changed] ************************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_67GA_LSILOGIC_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:80\nfatal: [localhost]: FAILED! => Failed to set execute bit on remote files (rc: 1, err: chmod: changing permissions of '/tmp/ansible-tmp-1648442093.7594314-3680-240718087425637/': Read-only file system\nchmod: changing permissions of '/tmp/ansible-tmp-1648442093.7594314-3680-240718087425637/AnsiballZ_command.py': Read-only file system\n)", "category": "chmod_readonly_0509", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task wait for device list changed fatal localhost failed failed to set execute bit on remote files rc number error chmod changing permissions of tmp ansible tmp hex id number timestamp read only file system chmod changing permissions of tmp ansible tmp hex id number timestamp ansi ball z command python read only file system", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1177, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS-27_failed_tasks_log.0", "raw": "2022-03-25 12:20:23,025 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-03-25 12:20:23,025 | TASK [Wait for device list changed] ************************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:97\nfatal: [localhost]: FAILED! => Failed to set execute bit on remote files (rc: 1, err: chmod: changing permissions of '/tmp/ansible-tmp-1648210822.8840117-3383-200670728620988/': Read-only file system\nchmod: changing permissions of '/tmp/ansible-tmp-1648210822.8840117-3383-200670728620988/AnsiballZ_command.py': Read-only file system\n)", "category": "chmod_readonly_0509", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task wait for device list changed fatal localhost failed failed to set execute bit on remote files rc number error chmod changing permissions of tmp ansible tmp hex id number timestamp read only file system chmod changing permissions of tmp ansible tmp hex id number timestamp ansi ball z command python read only file system", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1178, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_Windows_10_32-57_failed_tasks_log.1", "raw": "2022-02-12 03:50:30,012 | Failed at Play [memory_hot_add_basic] **********************\n2022-02-12 03:50:30,012 | TASK [Set VM power state to 'shutdown-guest'] **************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:34\nfatal: [localhost]: FAILED! => VMware tools should be installed for guest shutdown/reboot", "category": "vmtool_not_installed_0509", "processed": "timestamp failed at play memory hot add basic timestamp task set vm power state to shutdown guest fatal localhost failed vmware tools should be installed for guest shutdown reboot", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1186, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_SLED_15_x-27_failed_tasks_log.1", "raw": "2022-02-21 07:10:21,021 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-02-21 07:10:21,021 | TASK [Check disk controller facts are same as before hot adding tests] \ntask path: /home/worker/workspace/Ansible_Cycle_SLED_15.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/vhba_device_hot_add_remove.yml:109\nfatal: [localhost]: FAILED! => After tests VM disk controller fact is: {'scsi': {'0': {'controller_summary': 'VMware paravirtual SCSI', 'controller_label': 'SCSI controller 0', 'controller_busnumber': 0, 'controller_controllerkey': 100, 'controller_devicekey': 1000, 'controller_unitnumber': 3, 'controller_disks_devicekey': [2000], 'controller_bus_sharing': 'noSharing'}, '1': {'controller_summary': 'VMware paravirtual SCSI', 'controller_label': 'SCSI controller 1', 'controller_busnumber': 1, 'controller_controllerkey': 100, 'controller_devicekey': 1001, 'controller_unitnumber': 4, 'controller_disks_devicekey': [], 'controller_bus_sharing': 'noSharing'}}, 'sata': {}, 'nvme': {}, 'usb2': {}, 'usb3': {}}; before tests it's: {'scsi': {'0': {'controller_summary': 'VMware paravirtual SCSI', 'controller_label': 'SCSI controller 0', 'controller_busnumber': 0, 'controller_controllerkey': 100, 'controller_devicekey': 1000, 'controller_unitnumber': 3, 'controller_disks_devicekey': [2000], 'controller_bus_sharing': 'noSharing'}}, 'sata': {}, 'nvme': {}, 'usb2': {}, 'usb3': {}}", "category": "dick_controller_fact_is_0509", "processed": "timestamp failed at play para virtual v hba device ops timestamp task check disk controller facts are same as before hot adding tests fatal localhost failed after tests vm disk controller fact is scsi number controller summary vmware para virtual scsi controller label scsi controller number controller bus number controller controller key number controller device key number controller unit number controller disks device key number controller bus sharing no sharing number controller summary vmware para virtual scsi controller label scsi controller number controller bus number controller controller key number controller device key number controller unit number controller disks device key controller bus sharing no sharing sata nvme usb number usb number before tests it s scsi number controller summary vmware para virtual scsi controller label scsi controller number controller bus number controller controller key number controller device key number controller unit number controller disks device key number controller bus sharing no sharing sata nvme usb number usb number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1072, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.3", "raw": "2022-02-16 10:45:35,016 | Failed at Play [check_ip_address] **************************\n2022-02-16 10:45:35,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check ip address timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1187, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_Ubuntu_21_10_ISO-35_failed_tasks_log.0", "raw": "2022-02-22 10:15:16,022 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-02-22 10:15:16,022 | TASK [Check disk controller facts are same as before hot adding tests] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/vhba_device_hot_add_remove.yml:109\nfatal: [localhost]: FAILED! => After tests VM disk controller fact is: {'scsi': {'0': {'controller_summary': 'VMware paravirtual SCSI', 'controller_label': 'SCSI controller 0', 'controller_busnumber': 0, 'controller_controllerkey': 100, 'controller_devicekey': 1000, 'controller_unitnumber': 3, 'controller_disks_devicekey': [2000], 'controller_bus_sharing': 'noSharing'}}, 'sata': {}, 'nvme': {'0': {'controller_summary': 'NVME controller 0', 'controller_label': 'NVME controller 0', 'controller_busnumber': 0, 'controller_controllerkey': 100, 'controller_devicekey': 31000, 'controller_unitnumber': 30, 'controller_disks_devicekey': [32000]}}, 'usb2': {}, 'usb3': {}}; before tests it's: {'scsi': {'0': {'controller_summary': 'VMware paravirtual SCSI', 'controller_label': 'SCSI controller 0', 'controller_busnumber': 0, 'controller_controllerkey': 100, 'controller_devicekey': 1000, 'controller_unitnumber': 3, 'controller_disks_devicekey': [2000], 'controller_bus_sharing': 'noSharing'}}, 'sata': {}, 'nvme': {}, 'usb2': {}, 'usb3': {}}", "category": "dick_controller_fact_is_0509", "processed": "timestamp failed at play nvme v hba device ops timestamp task check disk controller facts are same as before hot adding tests fatal localhost failed after tests vm disk controller fact is scsi number controller summary vmware para virtual scsi controller label scsi controller number controller bus number controller controller key number controller device key number controller unit number controller disks device key number controller bus sharing no sharing sata nvme number controller summary nvme controller number controller label nvme controller number controller bus number controller controller key number controller device key number controller unit number controller disks device key number usb number usb number before tests it s scsi number controller summary vmware para virtual scsi controller label scsi controller number controller bus number controller controller key number controller device key number controller unit number controller disks device key number controller bus sharing no sharing sata nvme usb number usb number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1188, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_Server_LTSC_70GA_IDE_E1000E_EFI-13_failed_tasks_log.10", "raw": "2022-04-28 07:23:42,028 | Failed at Play [gosc_sanity_dhcp] **************************\n2022-04-28 07:23:42,028 | TASK [Check guest customization state] *********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_gosc_completed.yml:35\nfatal: [localhost]: FAILED! => Not found 'ToolsDeployPkgPublishState: state=5, code=0' in vmware.log\n2022-04-28 07:39:08,028 | TASK [Check if specified file exists in Windows guest OS] **\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_check_file_exist.yml:14\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: HTTPSConnectionPool(host='10.186.81.255', port=5986): Max retries exceeded with url: /wsman (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2aabab5040>: Failed to establish a new connection: [Errno 110] Connection timed out'))", "category": "failed_establish_new_connect_0509", "processed": "timestamp failed at play go sc sanity dhcp timestamp task check guest customization state fatal localhost failed not found tools deploy package publish state state number code number in vmware log timestamp task check if specified file exists in windows guest os fatal localhost un reachable failed to connect to the host via psrp https connection pool host ip address port number max retries exceeded with url wsman caused by new connection error url library number connection https connection object at hex id failed to establish a new connection error number connection timed out", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1191, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_AlmaLinux_8_x-56_failed_tasks_log.1", "raw": "2022-02-21 07:10:36,021 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-02-21 07:10:36,021 | TASK [Check CPU cores per socket is set correctly] *********\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_set_cpu_number.yml:31\nfatal: [localhost]: FAILED! => Failed to set CPU cores per socket to '12'\n2022-02-21 07:10:40,021 | TASK [Check VM snapshot is taken and is the current one] ***\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:26\nfatal: [localhost]: FAILED! => Failed to take snapshot 'fail-cpu_multicores_per_socket-2022-02-21-07-10-37' on VM 'test_vm'.", "category": "fail_to_take_snapshot_0509", "processed": "timestamp failed at play cpu multi cores per socket timestamp task check cpu cores per socket is set correctly fatal localhost failed failed to set cpu cores per socket to number timestamp task check vm snapshot is taken and is the current one fatal localhost failed failed to take snapshot fail cpu multi cores per socket timestamp number on vm test vm", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1192, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI-18_failed_tasks_log.0", "raw": "2022-03-31 14:17:34,031 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-03-31 14:17:34,031 | TASK [Add virtual TPM device to VM] ************************\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_add_remove_vtpm.yml:14\nexception in /vmware_guest_tpm.py when vtpm_operation in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => Failed to configure vTPM device on virtual machine due to '('RuntimeFault.summary', None)'\n2022-03-31 14:17:41,031 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "vtpm_device_on_vm_0509", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task add virtual tpm device to vm exception in vmware guest tpm python when vtpm operation in vmware python when wait for task fatal localhost failed failed to configure vtpm device on virtual machine due to runtime fault summary none timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1193, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_Server_LTSC_70GA_IDE_E1000E_EFI-13_failed_tasks_log.0", "raw": "2022-04-28 06:09:19,028 | Failed at Play [secureboot_enable_disable] *****************\n2022-04-28 06:09:19,028 | TASK [Check secure boot is enabled in guest OS] ************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/windows/secureboot_enable_disable/change_secureboot_config.yml:34\nfatal: [localhost]: FAILED! => Secure boot enabled status in guest OS: False", "category": "secure_boot_status_off_0509", "processed": "timestamp failed at play secure boot enable disable timestamp task check secure boot is enabled in guest os fatal localhost failed secure boot enabled status in guest os false", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1194, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_70U1_LSILOGICSAS_VMXNET3_BIOS-5_failed_tasks_log.0", "raw": "2022-02-10 05:00:07,010 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-02-10 05:00:07,010 | TASK [Rescan all scsi devices] *****************************\ntask path: /home/worker/workspace/Ansible_SLES_15SP4_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:30\nFileNotFoundError: [Errno 2] No usable temporary directory found in ['/tmp', '/var/tmp', '/usr/tmp', '/root']\nfatal: [localhost -> 10.184.87.34]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "no_usable_temp_dir_found_in_0509", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task re scan all scsi devices file not found error error number no usable temporary directory found in tmp var tmp user tmp root fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1151, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS-42_failed_tasks_log.0", "raw": "2022-04-07 13:22:46,007 | Failed at Play [deploy_vm_bios_lsilogicsas_e1000e] *********\n2022-04-07 13:22:46,007 | TASK [Datastore file operation] ****************************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Linux/Ubuntu/22.04/20220406/jammy-live-server-amd64.iso' is absent, cannot continue\n2022-04-07 13:23:29,007 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_lsilogicsas_e1000e", "category": "absent_cannot_continue_0509", "processed": "timestamp failed at play deploy vm bios lsi logic sas timestamp task data store file operation fatal localhost failed file os linux ubuntu number hex id jammy live server amd number i so is absent can not continue timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios lsi logic sas enumbere", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1152, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS-7_failed_tasks_log.0", "raw": "2022-03-07 09:48:42,007 | Failed at Play [deploy_vm_bios_lsilogicsas_e1000e] *********\n2022-03-07 09:48:42,007 | TASK [Datastore file operation] ****************************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Linux/Ubuntu/22.04/jammy-live-server-amd64.iso' is absent, cannot continue\n2022-03-07 09:49:14,007 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_lsilogicsas_e1000e", "category": "absent_cannot_continue_0509", "processed": "timestamp failed at play deploy vm bios lsi logic sas timestamp task data store file operation fatal localhost failed file os linux ubuntu number jammy live server amd number i so is absent can not continue timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios lsi logic sas enumbere", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1153, "name": "ansible_gosv_logs_FAILURE_Ansible_Photon_4_x_ISO_70GA_SATA-1_logs_failed_tasks_log.0", "raw": "2022-01-28 01:36:29,028 | Failed at Play [deploy_vm_bios_sata_e1000e] ****************\n2022-01-28 01:36:29,028 | TASK [Datastore file operation] ****************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_ISO_70GA_SATA/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Linux/Photon/4.0/Rev2/photon-4.0-c001795b8.iso' is absent, cannot continue\n2022-01-28 01:37:11,028 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Photon_4.x_ISO_70GA_SATA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_sata_e1000e", "category": "absent_cannot_continue_0509", "processed": "timestamp failed at play deploy vm bios sata timestamp task data store file operation fatal localhost failed file os linux photon number rev number photon number iso is absent can not continue timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios sata enumbere", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1154, "name": "ansible_gosv_logs_FAILURE_Ansible_Photon_4_x_ISO_70U1_LsiLogicSAS-1_logs_failed_tasks_log.0", "raw": "2022-01-28 01:39:09,028 | Failed at Play [deploy_vm_bios_lsilogicsas_vmxnet3] ********\n2022-01-28 01:39:09,028 | TASK [Datastore file operation] ****************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_ISO_70U1_LsiLogicSAS/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Linux/Photon/4.0/Rev2/photon-4.0-c001795b8.iso' is absent, cannot continue\n2022-01-28 01:39:51,028 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Photon_4.x_ISO_70U1_LsiLogicSAS/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_lsilogicsas_vmxnet3", "category": "absent_cannot_continue_0509", "processed": "timestamp failed at play deploy vm bios lsi logic sas vmxnet number timestamp task data store file operation fatal localhost failed file os linux photon number rev number photon number iso is absent can not continue timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios lsi logic sas vmxnet number", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1165, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Ubuntu_LTS_Server_OVA_MAIN-21_failed_tasks_log.0", "raw": "2022-04-25 08:09:01,025 | Failed at Play [env_setup] *********************************\n2022-04-25 08:09:01,025 | TASK [Check variables for new VM settings] *****************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_MAIN/ansible-vsphere-gos-validation/env_setup/check_testing_vars.yml:57\nfatal: [localhost]: FAILED! => Invalid variables for new VM settings", "category": "invalid_variable_0509", "processed": "timestamp failed at play environment setup timestamp task check variables for new vm settings fatal localhost failed invalid variables for new vm settings", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1195, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_RHEL_9_x_67U2_PARAVIRTUAL_E1000E_BIOS-2_failed_tasks_log.0", "raw": "2022-02-16 23:54:02,016 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-02-16 23:54:02,016 | TASK [Parameter error] *************************************\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_67U2_PARAVIRTUAL_E1000E_BIOS/newgos_testing_internal/testbed_deploy/testbed_deploy_nimbus.yml:84\nfatal: [localhost]: FAILED! => Parameter 'testbed_version' is not valid: '67U2', valid values: ['main', 'cycle', '70U3', '70U2', '70U1', '70GA', '67U3', '67U1', '67GA', '65U3', '65GA']", "category": "param_testbed_not_valid_0509", "processed": "timestamp failed at play testbed deploy nimbus timestamp task parameter error fatal localhost failed parameter testbed version is not valid valid values main cycle numberga numberga numberga", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1184, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_Ubuntu_20_04_3_ISO-49_failed_tasks_log.24", "raw": "2022-02-11 17:43:12,011 | Failed at Play [sata_vhba_device_ops] **********************\n2022-02-11 17:43:12,011 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.3_ISO/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.191.142.209:443 : [Errno 113] No route to host", "category": "unknown_error_no_route_0509", "processed": "timestamp failed at play sata v hba device ops timestamp task get specified property info for vm test vm exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number no route to host", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1162, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_ISO_70GA_LSILOGICSAS_VMXNET3_EFI-39_failed_tasks_log.0", "raw": "2022-03-28 03:38:45,028 | Failed at Play [deploy_vm_efi_lsilogicsas_vmxnet3] *********\n2022-03-28 03:38:45,028 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70GA_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:30\nfatal: [localhost]: FAILED! => IP 10.185.244.81 is not pingable\n2022-03-28 03:39:48,028 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70GA_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_lsilogicsas_vmxnet3", "category": "failed_not_pingable_0509", "processed": "timestamp failed at play deploy vm efi lsi logic sas vmxnet number timestamp task fail fatal localhost failed ip ip address is not pingable timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi lsi logic sas vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1189, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_SLES_15SP4_67GA_LSILOGIC_E1000E_BIOS-30_failed_tasks_log.1", "raw": "2022-04-13 17:43:04,013 | Failed at Play [stat_hosttime] *****************************\n2022-04-13 17:43:04,013 | TASK [Check difference between stat host time and the real host time is less than 15s] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67GA_LSILOGIC_E1000E_BIOS/ansible-vsphere-gos-validation/linux/stat_hosttime/stat_hosttime.yml:47\nfatal: [localhost]: FAILED! => Difference between stat host time and real host time is larger than 15s: 33.0s", "category": "diff_host_time_real_time_0509", "processed": "timestamp failed at play stat host time timestamp task check difference between stat host time and the real host time is less than fatal localhost failed difference between stat host time and real host time is larger than number numbers", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1065, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.18", "raw": "2022-02-16 10:53:24,016 | Failed at Play [gosc_perl_staticip] ************************\n2022-02-16 10:53:24,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc perl static ip timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1066, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.19", "raw": "2022-02-16 10:53:55,016 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-02-16 10:53:55,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1067, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.2", "raw": "2022-02-16 10:45:06,016 | Failed at Play [vgauth_check_service] **********************\n2022-02-16 10:45:06,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play vg auth check service timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1068, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.20", "raw": "2022-02-16 10:54:24,016 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-02-16 10:54:24,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc cloud init static ip timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1069, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.21", "raw": "2022-02-16 10:54:54,016 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-02-16 10:54:54,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play para virtual v hba device ops timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1070, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.24", "raw": "2022-02-16 10:56:24,016 | Failed at Play [sata_vhba_device_ops] **********************\n2022-02-16 10:56:24,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play sata v hba device ops timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1071, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.26", "raw": "2022-02-16 10:57:29,016 | Failed at Play [ovt_verify_uninstall] **********************\n2022-02-16 10:57:29,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify un install timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1074, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.7", "raw": "2022-02-16 10:47:35,016 | Failed at Play [check_os_fullname] *************************\n2022-02-16 10:47:35,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check os full name timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1075, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.8", "raw": "2022-02-16 10:48:06,016 | Failed at Play [check_efi_firmware] ************************\n2022-02-16 10:48:06,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check efi firmware timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1076, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.9", "raw": "2022-02-16 10:48:38,016 | Failed at Play [cpu_hot_add_basic] *************************\n2022-02-16 10:48:38,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play cpu hot add basic timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1077, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.0", "raw": "2022-02-16 10:45:51,016 | Failed at Play [ovt_verify_install] ************************\n2022-02-16 10:45:51,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify install timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1078, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.1", "raw": "2022-02-16 10:46:22,016 | Failed at Play [ovt_verify_status] *************************\n2022-02-16 10:46:22,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify status timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1079, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.11", "raw": "2022-02-16 10:51:35,016 | Failed at Play [memory_hot_add_basic] **********************\n2022-02-16 10:51:35,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play memory hot add basic timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1080, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.13", "raw": "2022-02-16 10:52:36,016 | Failed at Play [secureboot_enable_disable] *****************\n2022-02-16 10:52:36,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play secure boot enable disable timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1081, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.17", "raw": "2022-02-16 10:54:43,016 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-02-16 10:54:43,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc perl dhcp timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1082, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.18", "raw": "2022-02-16 10:55:13,016 | Failed at Play [gosc_perl_staticip] ************************\n2022-02-16 10:55:13,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc perl static ip timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1083, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.19", "raw": "2022-02-16 10:55:44,016 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-02-16 10:55:44,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1084, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.2", "raw": "2022-02-16 10:46:53,016 | Failed at Play [vgauth_check_service] **********************\n2022-02-16 10:46:53,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play vg auth check service timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1085, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.20", "raw": "2022-02-16 10:56:15,016 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-02-16 10:56:15,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc cloud init static ip timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1086, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.26", "raw": "2022-02-16 10:59:20,016 | Failed at Play [ovt_verify_uninstall] **********************\n2022-02-16 10:59:20,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify un install timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1087, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.3", "raw": "2022-02-16 10:47:23,016 | Failed at Play [check_ip_address] **************************\n2022-02-16 10:47:23,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check ip address timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1088, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.5", "raw": "2022-02-16 10:48:24,016 | Failed at Play [stat_hosttime] *****************************\n2022-02-16 10:48:24,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play stat host time timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1089, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.6", "raw": "2022-02-16 10:48:56,016 | Failed at Play [check_inbox_driver] ************************\n2022-02-16 10:48:56,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check inbox driver timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1090, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.7", "raw": "2022-02-16 10:49:31,016 | Failed at Play [check_os_fullname] *************************\n2022-02-16 10:49:31,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check os full name timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1091, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.8", "raw": "2022-02-16 10:50:01,016 | Failed at Play [check_efi_firmware] ************************\n2022-02-16 10:50:01,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check efi firmware timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1092, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.9", "raw": "2022-02-16 10:50:33,016 | Failed at Play [cpu_hot_add_basic] *************************\n2022-02-16 10:50:33,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006357061'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67GA_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play cpu hot add basic timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1093, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.0", "raw": "2022-02-16 10:46:59,016 | Failed at Play [ovt_verify_install] ************************\n2022-02-16 10:46:59,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify install timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1094, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.1", "raw": "2022-02-16 10:47:32,016 | Failed at Play [ovt_verify_status] *************************\n2022-02-16 10:47:32,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify status timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1096, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.13", "raw": "2022-02-16 10:54:14,016 | Failed at Play [secureboot_enable_disable] *****************\n2022-02-16 10:54:14,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play secure boot enable disable timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1097, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.17", "raw": "2022-02-16 10:56:24,016 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-02-16 10:56:24,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc perl dhcp timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1098, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.18", "raw": "2022-02-16 10:56:56,016 | Failed at Play [gosc_perl_staticip] ************************\n2022-02-16 10:56:56,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc perl static ip timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1099, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.19", "raw": "2022-02-16 10:57:28,016 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-02-16 10:57:28,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1100, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.2", "raw": "2022-02-16 10:48:03,016 | Failed at Play [vgauth_check_service] **********************\n2022-02-16 10:48:03,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play vg auth check service timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1144, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Photon_4_x_OVA_70GA-19_failed_tasks_log.0", "raw": "2022-03-31 16:20:29,031 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-03-31 16:20:29,031 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_OVA_70GA/newgos_testing_internal/testbed_deploy/testbed_deploy_nimbus.yml:195\nfatal: [localhost]: FAILED! => Get deployment result in testbedInfo.json is not PASS: deployment_result=INVALID", "category": "deploy_result_not_pass_0509", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail fatal localhost failed get deployment result in testbed info json is not pass deployment result invalid", "solution": "retry", "target": "nimbus", "version": 202205240000}, {"id": 1145, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_OVA_70GA-25_failed_tasks_log.0", "raw": "2022-03-24 06:10:24,024 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-03-24 06:10:24,024 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_70GA/newgos_testing_internal/testbed_deploy/testbed_deploy_nimbus.yml:189\nfatal: [localhost]: FAILED! => Get deployment result in testbedInfo.json is not PASS", "category": "deploy_result_not_pass_0509", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail fatal localhost failed get deployment result in testbed info json is not pass", "solution": "retry", "target": "nimbus", "version": 202205240000}, {"id": 1141, "name": "ansible_gosv_logs_FAILURE_Ansible_AmazonLinux_2_70GA-3_logs_failed_tasks_log.0", "raw": "2022-01-06 05:52:29,006 | Failed at Play [ovt_verify_install] ************************\n2022-01-06 05:52:29,006 | TASK [Collect filtered guest information for '10.187.128.65'] \ntask path: /home/worker/workspace/Ansible_AmazonLinux_2_70GA/ansible-vsphere-gos-validation/common/get_system_info.yml:22\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via ssh: Warning: Permanently added '10.187.128.65' (ED25519) to the list of known hosts.\nroot@10.187.128.65: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).", "category": "permission_deny", "processed": "timestamp failed at play ovt verify install timestamp task collect filtered guest information for ip address fatal localhost un reachable failed to connect to the host via ssh warning permanently added ip address to the list of known hosts root ip address permission denied public key gss api k eye x gss api with mic", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1142, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Flatcar_OVA-20_logs_failed_tasks_log.0", "raw": "2022-01-05 07:24:06,005 | Failed at Play [env_setup] *********************************\n2022-01-05 07:24:06,005 | TASK [Check supershell] ************************************\ntask path: /home/worker/workspace/Ansible_Regression_Flatcar_OVA/ansible-vsphere-gos-validation/env_setup/env_setup.yml:53\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via ssh: root@10.78.233.66: Permission denied (publickey,keyboard-interactive).", "category": "permission_deny", "processed": "timestamp failed at play environment setup timestamp task check super shell fatal localhost un reachable failed to connect to the host via ssh root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1143, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_3_0_ISO-23_logs_failed_tasks_log.0", "raw": "2022-01-05 07:48:03,005 | Failed at Play [env_setup] *********************************\n2022-01-05 07:48:03,005 | TASK [Get shell executable on ESXi server] *****************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/env_setup/env_setup.yml:53\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via ssh: root@10.168.212.31: Permission denied (publickey,keyboard-interactive).", "category": "permission_deny", "processed": "timestamp failed at play environment setup timestamp task get shell executable on esxi server fatal localhost un reachable failed to connect to the host via ssh root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1138, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.7", "raw": "2022-02-16 10:42:01,016 | Failed at Play [check_os_fullname] *************************\n2022-02-16 10:42:01,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check os full name timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1101, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.20", "raw": "2022-02-16 10:58:03,016 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-02-16 10:58:03,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc cloud init static ip timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1102, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.26", "raw": "2022-02-16 11:01:26,016 | Failed at Play [ovt_verify_uninstall] **********************\n2022-02-16 11:01:26,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify un install timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1103, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.3", "raw": "2022-02-16 10:48:36,016 | Failed at Play [check_ip_address] **************************\n2022-02-16 10:48:36,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check ip address timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1104, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.5", "raw": "2022-02-16 10:49:46,016 | Failed at Play [stat_hosttime] *****************************\n2022-02-16 10:49:46,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play stat host time timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1148, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_Windows_10_32-27_failed_tasks_log.0", "raw": "2022-02-07 21:01:35,007 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-02-07 21:01:35,007 | TASK [Transfer nimbus testbed deploy spec to dbc server] ***\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/newgos_testing_internal/testbed_deploy/deploy_from_dbc_prepare.yml:52\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 3 retries to prevent account lockout: Warning: Permanently added 'wdc-dbc2108.eng.vmware.com' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nsvc.gosv-automation@wdc-dbc2108.eng.vmware.com: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).", "category": "invalid_password_0509", "processed": "timestamp failed at play testbed deploy nimbus timestamp task transfer nimbus testbed deploy spec to database c server fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added wdc eng vmware com to the list of known hosts permission denied please try again permission denied please try again svc go sv automation wdc eng vmware com permission denied public key gss api k eye x gss api with mic password", "solution": "deepdive", "target": "infra", "version": 202205240000}, {"id": 1149, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Cycle_CentOS_8_x-62_failed_tasks_log.0", "raw": "2022-03-01 14:14:48,001 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-03-01 14:14:48,001 | TASK [Transfer nimbus testbed deploy spec to dbc server] ***\ntask path: /home/worker/workspace/Ansible_Cycle_CentOS_8.x/newgos_testing_internal/testbed_deploy/deploy_from_dbc_prepare.yml:52\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 4 retries to prevent account lockout: Warning: Permanently added 'wdc-dbc2108.eng.vmware.com' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nsvc.gosv-automation@wdc-dbc2108.eng.vmware.com: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).", "category": "invalid_password_0509", "processed": "timestamp failed at play testbed deploy nimbus timestamp task transfer nimbus testbed deploy spec to database c server fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added wdc eng vmware com to the list of known hosts permission denied please try again permission denied please try again svc go sv automation wdc eng vmware com permission denied public key gss api k eye x gss api with mic password", "solution": "deepdive", "target": "infra", "version": 202205240000}, {"id": 1156, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_SLES_15SP4_MAIN_NVME_E1000E_BIOS-42_failed_tasks_log.0", "raw": "2022-04-29 04:02:31,029 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-04-29 04:02:31,029 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_SLES_15SP4_MAIN_NVME_E1000E_BIOS/newgos_testing_internal/testbed_deploy/parse_testbed_info.yml:46\nfatal: [localhost]: FAILED! => There is no VMFS datastore on ESXi server 10.182.178.193. Nimbus testbed deployment failed.", "category": "no_vmfs_datastore_0509", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail fatal localhost failed there is no vmfs data store on esxi server ip address nimbus testbed deployment failed", "solution": "deepdive", "target": "nimbus", "version": 202205240000}, {"id": 1155, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS-18_failed_tasks_log.0", "raw": "2022-04-21 15:28:33,021 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-04-21 15:28:33,021 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/testbed_result/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS-18/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "failed_to_fetch_nimbus_testbed_0509", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus testbed result ansible windows server lts c lsi logic sas bios number testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202205240000}, {"id": 1157, "name": "ansible_gosv_logs_202202_ABORTED_Ansible_Cycle_CentOS_8_x-16_failed_tasks_log.3", "raw": "2022-02-06 14:33:40,006 | Failed at Play [memory_hot_add_basic] **********************\n2022-02-06 14:33:40,006 | TASK [Get ESXi host specified property] ********************\ntask path: /home/worker/workspace/Ansible_Cycle_CentOS_8.x/ansible-vsphere-gos-validation/common/esxi_get_property.yml:8\nexception in /vmware.py when connect_to_api in /ssl.py when do_handshake\nfatal: [localhost]: FAILED! => Unable to connect to vCenter or ESXi API at 10.187.157.136 on TCP/443: EOF occurred in violation of protocol (_ssl.c:1123)", "category": "unable_to_vc_or_esx_0509", "processed": "timestamp failed at play memory hot add basic timestamp task get esxi host specified property exception in vmware python when connect to api in ssl python when do handshake fatal localhost failed unable to connect to vcenter or esxi api at ip address on tcp number eof occurred in violation of protocol ssl c number", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1105, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.6", "raw": "2022-02-16 10:50:19,016 | Failed at Play [check_inbox_driver] ************************\n2022-02-16 10:50:19,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check inbox driver timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1160, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_3_0_ISO-31_logs_failed_tasks_log.0", "raw": "2022-01-18 16:36:53,018 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-01-18 16:36:53,018 | TASK [Run iozone test on new added disk] *******************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/run_iozone_test.yml:14\nfatal: [localhost -> 10.187.152.164]: FAILED! => non-zero return code", "category": "non_zero_return", "processed": "timestamp failed at play para virtual v hba device ops timestamp task run io zone test on new added disk fatal localhost ip address failed non zero return code", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1106, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.7", "raw": "2022-02-16 10:50:51,016 | Failed at Play [check_os_fullname] *************************\n2022-02-16 10:50:51,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check os full name timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1163, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_CentOS_8_x_70GA_IDE_VMXNET3_BIOS-5_failed_tasks_log.0", "raw": "2022-04-16 19:49:56,016 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-04-16 19:49:56,016 | TASK [Set VM CPU number and/or cores per socket number] ****\ntask path: /home/worker/workspace/Ansible_CentOS_8.x_70GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_set_cpu_number.yml:9\nfatal: [localhost]: FAILED! => An error occurred while communicating with the remote host.", "category": "error_occur_while_commu_0509", "processed": "timestamp failed at play cpu multi cores per socket timestamp task set vm cpu number and or cores per socket number fatal localhost failed an error occurred while communicating with the remote host", "solution": "deepdive", "target": "testbed", "version": 202205240000}, {"id": 1164, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_SLED_15SP4_70U3_NVME_E1000E_EFI-20_failed_tasks_log.1", "raw": "2022-04-12 10:30:39,012 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-04-12 10:30:39,012 | TASK [absent disk to VM] ***********************************\ntask path: /home/worker/workspace/Ansible_SLED_15SP4_70U3_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_hot_add_remove_disk.yml:44\nexception in /vmware_guest_disk.py when main in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => Failed to manage disks for virtual machine 'test_vm' with exception : ('An error occurred while communicating with the remote host.', None)", "category": "error_occur_while_commu_0509", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task absent disk to vm exception in vmware guest disk python when main in vmware python when wait for task fatal localhost failed failed to manage disks for virtual machine test vm with exception an error occurred while communicating with the remote host none", "solution": "deepdive", "target": "testbed", "version": 202205240000}, {"id": 1166, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Photon_4_x_ISO_MAIN_PARAVIRTUAL_VMXNET3_BIOS-4_failed_tasks_log.0", "raw": "2022-02-21 07:30:07,021 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-02-21 07:30:07,021 | TASK [present disk to VM] **********************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_ISO_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_hot_add_remove_disk.yml:44\nexception in /vmware_guest_disk.py when main in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => Failed to manage disks for virtual machine 'test_vm' with exception : ('Unable to communicate with the remote host, since it is disconnected.', None)", "category": "unable_to_commu_as_disconnect_0509", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task present disk to vm exception in vmware guest disk python when main in vmware python when wait for task fatal localhost failed failed to manage disks for virtual machine test vm with exception unable to communicate with the remote host since it is disconnected none", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1107, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.8", "raw": "2022-02-16 10:51:24,016 | Failed at Play [check_efi_firmware] ************************\n2022-02-16 10:51:24,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check efi firmware timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1108, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.1", "raw": "2022-02-16 10:40:47,016 | Failed at Play [ovt_verify_status] *************************\n2022-02-16 10:40:47,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify status timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1169, "name": "ansible_gosv_logs_202203_FAILURE_zyh_ansible_ubuntu_existing_testbed_3_1-4_failed_tasks_log.0", "raw": "2022-03-02 06:31:40,002 | Failed at Play [env_setup] *********************************\n2022-03-02 06:31:40,002 | TASK [Add a new 'e1000' adapter in 'vSwitch2022-03-02-06-30-45_PG' to VM 'Ansible_Router_VM_2022-03-02-06-30-45'] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/common/vm_add_network_adapter.yml:4\nexception in /vmware_guest_network.py when _nic_present in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => ('Unable to communicate with the remote host, since it is disconnected.', None)", "category": "unable_to_commu_as_disconnect_0509", "processed": "timestamp failed at play environment setup timestamp task add a new adapter in vswitch timestamp number page to vm ansible router vm timestamp number exception in vmware guest network python when nic present in vmware python when wait for task fatal localhost failed unable to communicate with the remote host since it is disconnected none", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1170, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Cycle_Windows_10_64-136_failed_tasks_log.7", "raw": "2022-05-04 10:24:20,004 | Failed at Play [e1000e_network_device_ops] *****************\n2022-05-04 10:24:20,004 | TASK [Add a standard vSwitch vSwitch2022-05-04-09-12-56 to ESXi host] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/esxi_add_vswitch.yml:4\nexception in /vmware_vswitch.py when state_create_vswitch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Failed to add vSwitch 'vSwitch2022-05-04-09-12-56' due to generic exception : (vmodl.fault.HostNotConnected) {\n   dynamicType = <unset>,\n   dynamicProperty = (vmodl.DynamicProperty) [],\n   msg = 'Unable to communicate with the remote host, since it is disconnected.',\n   faultCause = <unset>,\n   faultMessage = (vmodl.LocalizableMessage) []\n}", "category": "unable_to_commu_as_disconnect_0509", "processed": "timestamp failed at play network device ops timestamp task add a standard vswitch vswitch timestamp number to esxi host exception in vmware vswitch python when state create vswitch in soap adapter python when invoke method fatal localhost failed failed to add vswitch vswitch timestamp number due to generic exception vmodl fault host not connected dynamic property vmodl dynamic property message unable to communicate with the remote host since it is disconnected fault message vmodl localizable message", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1109, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.11", "raw": "2022-02-16 10:46:17,016 | Failed at Play [memory_hot_add_basic] **********************\n2022-02-16 10:46:17,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play memory hot add basic timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1110, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.13", "raw": "2022-02-16 10:47:21,016 | Failed at Play [secureboot_enable_disable] *****************\n2022-02-16 10:47:21,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play secure boot enable disable timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1219, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Cycle_Windows_10_64-108_failed_tasks_log.0", "raw": "2022-03-15 17:28:28,015 | Failed at Play [check_inbox_driver] ************************\n2022-03-15 17:28:28,015 | TASK [Collect filtered guest information for '10.186.237.131'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/get_system_info.yml:22\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play check inbox driver timestamp task collect filtered guest information for ip address fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1179, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_Ubuntu_20_04_3_ISO-49_failed_tasks_log.15", "raw": "2022-02-11 17:41:48,011 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-02-11 17:41:48,011 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.3_ISO/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.191.142.209:443 : [Errno 113] No route to host", "category": "unknown_error_no_route_0509", "processed": "timestamp failed at play vmxnet number network device ops timestamp task get specified property info for vm test vm exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number no route to host", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1180, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_Ubuntu_20_04_3_ISO-49_failed_tasks_log.19", "raw": "2022-02-11 17:42:26,011 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-02-11 17:42:26,011 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.3_ISO/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.191.142.209:443 : [Errno 113] No route to host", "category": "unknown_error_no_route_0509", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task get specified property info for vm test vm exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number no route to host", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1181, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_Ubuntu_20_04_3_ISO-49_failed_tasks_log.20", "raw": "2022-02-11 17:42:33,011 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-02-11 17:42:33,011 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.3_ISO/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.191.142.209:443 : [Errno 113] No route to host", "category": "unknown_error_no_route_0509", "processed": "timestamp failed at play go sc cloud init static ip timestamp task get specified property info for vm test vm exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number no route to host", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1111, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.17", "raw": "2022-02-16 10:49:33,016 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-02-16 10:49:33,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc perl dhcp timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1112, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.18", "raw": "2022-02-16 10:50:05,016 | Failed at Play [gosc_perl_staticip] ************************\n2022-02-16 10:50:05,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc perl static ip timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1185, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_ISO-93_logs_failed_tasks_log.0", "raw": "2022-01-18 10:26:45,018 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2022-01-18 10:26:45,018 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log bundle serial-20220118090529.log] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/common/vm_wait_logbundle_msg.yml:27\nfatal: [localhost]: FAILED! => OK (54 bytes)\n2022-01-18 10:57:21,018 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "failed_ok_54_bytes_0509", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task wait for message auto install is completed appear in vm log bundle serial timestamp log fatal localhost failed ok number bytes timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1182, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_Ubuntu_20_04_3_ISO-49_failed_tasks_log.21", "raw": "2022-02-11 17:42:43,011 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-02-11 17:42:43,011 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.3_ISO/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.191.142.209:443 : [Errno 113] No route to host", "category": "unknown_error_no_route_0509", "processed": "timestamp failed at play para virtual v hba device ops timestamp task get specified property info for vm test vm exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number no route to host", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1183, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_Ubuntu_20_04_3_ISO-49_failed_tasks_log.22", "raw": "2022-02-11 17:42:52,011 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-02-11 17:42:52,011 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.3_ISO/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.191.142.209:443 : [Errno 113] No route to host", "category": "unknown_error_no_route_0509", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task get specified property info for vm test vm exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number no route to host", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1190, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Cycle_Windows_10_64-120_failed_tasks_log.0", "raw": "2022-03-31 03:16:29,031 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2022-03-31 03:16:29,031 | TASK [Configure VM CDROM to 'client'] **********************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.78.212.146:443 : [Errno 110] Connection timed out\n2022-03-31 03:20:46,031 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "unknown_error_connect_timeout_0509", "processed": "timestamp failed at play deploy vm efi para virtual timestamp task configure vm cdrom to client exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1113, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.19", "raw": "2022-02-16 10:50:40,016 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-02-16 10:50:40,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1114, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.2", "raw": "2022-02-16 10:41:22,016 | Failed at Play [vgauth_check_service] **********************\n2022-02-16 10:41:22,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play vg auth check service timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1313, "name": "__Ansible_Regression_Photon_4_0_OVA-6_logs_failed_tasks_log.0", "raw": "2021-12-02 09:48:55,002 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-02 09:48:55,002 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play go sc perl dhcp timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1115, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.20", "raw": "2022-02-16 10:51:12,016 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-02-16 10:51:12,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc cloud init static ip timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1116, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.21", "raw": "2022-02-16 10:51:44,016 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-02-16 10:51:44,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play para virtual v hba device ops timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1117, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.24", "raw": "2022-02-16 10:53:20,016 | Failed at Play [sata_vhba_device_ops] **********************\n2022-02-16 10:53:20,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play sata v hba device ops timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1118, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.26", "raw": "2022-02-16 10:54:28,016 | Failed at Play [ovt_verify_uninstall] **********************\n2022-02-16 10:54:28,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify un install timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1119, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.3", "raw": "2022-02-16 10:41:54,016 | Failed at Play [check_ip_address] **************************\n2022-02-16 10:41:54,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check ip address timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1120, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.4", "raw": "2022-02-16 10:42:26,016 | Failed at Play [stat_balloon] ******************************\n2022-02-16 10:42:26,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play stat balloon timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1260, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Cycle_Ubuntu_20_04_4_ISO-118_failed_tasks_log.0", "raw": "2022-05-05 07:01:09,005 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-05-05 07:01:09,005 | TASK [Try to ping IP] **************************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.4_ISO/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "ping_issue", "processed": "timestamp failed at play vmxnet number network device ops timestamp task try to ping ip fatal localhost failed non zero return code when ping", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1212, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_RHEL_8_x_67GA_PARAVIRTUAL_E1000E_EFI-9_failed_tasks_log.2", "raw": "2022-04-20 07:44:08,020 | Failed at Play [gosc_perl_staticip] ************************\n2022-04-20 07:44:08,020 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_67GA_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:108\nfatal: [localhost]: FAILED! => [\"VM DNS domain search domains are  [] not expected search domains ['test.com', 'gosc.test.com']\"]", "category": "domain_not_expect", "processed": "timestamp failed at play go sc perl static ip timestamp task gos customization failed fatal localhost failed vm dns domain search domains are not expected search domains test com go sc test com", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1213, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Regression_Windows_10_32-35_failed_tasks_log.1", "raw": "2022-02-17 05:34:03,017 | Failed at Play [sata_vhba_device_ops] **********************\n2022-02-17 05:34:03,017 | TASK [Create partition of the raw disk in Windows guest OS] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_create_disk_partition.yml:8\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play sata v hba device ops timestamp task create partition of the raw disk in windows guest os connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1214, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Regression_Windows_10_32-35_failed_tasks_log.2", "raw": "2022-02-17 05:38:04,017 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-02-17 05:38:04,017 | TASK [Create partition of the raw disk in Windows guest OS] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_create_disk_partition.yml:8\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play nvme v hba device ops timestamp task create partition of the raw disk in windows guest os connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1215, "name": "ansible_gosv_logs_202203_ABORTED_Ansible_Cycle_Windows_10_64-101_failed_tasks_log.7", "raw": "2022-03-08 06:33:40,008 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-03-08 06:33:40,008 | TASK [Get VMware tools version and build number in guest OS] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_get_vmtools_version_build.yml:13\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task get vmware tools version and build number in guest os connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1216, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Cycle_Windows_10_32-100_failed_tasks_log.0", "raw": "2022-03-10 03:41:45,010 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-03-10 03:41:45,010 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nsocket.timeout: timed out\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x7f58e38d3fa0>, 'Connection to 10.168.188.211 timed out. (connect timeout=30)')\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='10.168.188.211', port=5986): Max retries exceeded with url: /wsman (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f58e38d3fa0>, 'Connection to 10.168.188.211 timed out. (connect timeout=30)'))\nrequests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='10.168.188.211', port=5986): Max retries exceeded with url: /wsman (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f58e38d3fa0>, 'Connection to 10.168.188.211 timed out. (connect timeout=30)'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play nvme v hba device ops timestamp task execute powershell command in windows guest connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer url library number exceptions connect timeout error url library number connection https connection object at hex id connection to ip address timed out connect timeout number url library number exceptions max retry error https connection pool host ip address port number max retries exceeded with url wsman caused by connect timeout error url library number connection https connection object at hex id connection to ip address timed out connect timeout number requests exceptions connect timeout https connection pool host ip address port number max retries exceeded with url wsman caused by connect timeout error url library number connection https connection object at hex id connection to ip address timed out connect timeout number fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1217, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Cycle_Windows_10_64-100_failed_tasks_log.0", "raw": "2022-03-08 03:15:41,008 | Failed at Play [check_efi_firmware] ************************\n2022-03-08 03:15:41,008 | TASK [Get firmware type in Windows guest OS] ***************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_get_firmware.yml:5\nfatal: [localhost]: UNREACHABLE! => basic: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play check efi firmware timestamp task get firmware type in windows guest os fatal localhost un reachable basic connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1218, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Windows_11_70GA_IDE_E1000E_EFI-5_failed_tasks_log.1", "raw": "2022-03-08 15:46:14,008 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-03-08 15:46:14,008 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play para virtual v hba device ops timestamp task execute powershell command in windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1220, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI-23_failed_tasks_log.0", "raw": "2022-05-05 10:17:13,005 | Failed at Play [sata_vhba_device_ops] **********************\n2022-05-05 10:17:13,005 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play sata v hba device ops timestamp task execute powershell command in windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1221, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Windows_11_70U1_SATA_E1000E_EFI-7_failed_tasks_log.0", "raw": "2022-03-30 15:28:03,030 | Failed at Play [cpu_hot_add_basic] *************************\n2022-03-30 15:28:03,030 | TASK [Shutdown guest OS inside OS] *************************\ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_shutdown_restart.yml:8\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play cpu hot add basic timestamp task shutdown guest os inside os fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1222, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Windows_Server_LTSC_70GA_SATA_E1000E_BIOS-10_failed_tasks_log.0", "raw": "2022-03-08 03:34:21,008 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-03-08 03:34:21,008 | TASK [Shutdown guest OS inside OS] *************************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_shutdown_restart.yml:8\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play cpu multi cores per socket timestamp task shutdown guest os inside os fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1223, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_11_70U3_LSILOGICSAS_E1000E_EFI-15_failed_tasks_log.3", "raw": "2022-04-14 14:55:36,014 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-04-14 14:55:36,014 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play nvme v hba device ops timestamp task execute powershell command in windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1224, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Cycle_Windows_10_64-124_failed_tasks_log.1", "raw": "2022-04-11 07:57:49,011 | Failed at Play [wintools_uninstall_verify] *****************\n2022-04-11 07:57:49,011 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play win tools un install verify timestamp task execute powershell command in windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1225, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Debian_10_x_32bit_MAIN_NVME_VMXNET3_EFI-24_failed_tasks_log.0", "raw": "2022-03-08 14:21:32,008 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2022-03-08 14:21:32,008 | TASK [Upload local file to ESXi datastore] *****************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_32bit_MAIN_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/esxi_upload_datastore_file.yml:11\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib.error.URLError: <urlopen error [Errno 104] Connection reset by peer>\nTypeError: 'URLError' object is not subscriptable\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error\n2022-03-08 14:21:32,008 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Debian_10.x_32bit_MAIN_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "connectionreset", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task upload local file to esxi data store connection reset error error number connection reset by peer url library error url error url open error error number connection reset by peer type error url error object is not sub scriptable fatal localhost failed module failure see stdout stderr for the exact error timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1226, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Regression_Windows_Server_2022-58_failed_tasks_log.2", "raw": "2022-03-10 06:04:23,010 | Failed at Play [sata_vhba_device_ops] **********************\n2022-03-10 06:04:23,010 | TASK [Create partition of the raw disk in Windows guest OS] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_2022/ansible-vsphere-gos-validation/windows/utils/win_create_disk_partition.yml:8\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play sata v hba device ops timestamp task create partition of the raw disk in windows guest os fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1227, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Cycle_Windows_11_64-93_failed_tasks_log.0", "raw": "2022-04-02 10:57:04,002 | Failed at Play [check_quiesce_snapshot] ********************\n2022-04-02 10:57:04,002 | TASK [Copy file from local to Windows guest] ***************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_copy_file_from_local.yml:9\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play check quiesce snapshot timestamp task copy file from local to windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1228, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Windows_11_MAIN_NVME_E1000E_EFI-3_failed_tasks_log.0", "raw": "2022-03-10 06:28:51,010 | Failed at Play [check_quiesce_snapshot] ********************\n2022-03-10 06:28:51,010 | TASK [Check if specified file exists in Windows guest OS] **\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_check_file_exist.yml:14\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play check quiesce snapshot timestamp task check if specified file exists in windows guest os fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1229, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI-8_failed_tasks_log.0", "raw": "2022-04-14 16:47:50,014 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-04-14 16:47:50,014 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task execute powershell command in windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1230, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Cycle_Windows_11_64-99_failed_tasks_log.0", "raw": "2022-04-15 06:01:01,015 | Failed at Play [stat_balloon] ******************************\n2022-04-15 06:01:01,015 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play stat balloon timestamp task execute powershell command in windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1231, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Cycle_Windows_11_64-98_failed_tasks_log.0", "raw": "2022-04-15 03:16:48,015 | Failed at Play [mouse_driver_vmtools] **********************\n2022-04-15 03:16:48,015 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play mouse driver vm tools timestamp task execute powershell command in windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1232, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Cycle_Windows_11_64-106_failed_tasks_log.1", "raw": "2022-05-04 11:20:25,004 | Failed at Play [gosc_sanity_dhcp] **************************\n2022-05-04 11:20:25,004 | TASK [Get the absolute path in Windows guest] **************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_get_path.yml:10\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play go sc sanity dhcp timestamp task get the absolute path in windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1233, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_11_70U1_NVME_E1000E_EFI-6_failed_tasks_log.1", "raw": "2022-04-14 11:45:28,014 | Failed at Play [e1000e_network_device_ops] *****************\n2022-04-14 11:45:28,014 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play network device ops timestamp task execute powershell command in windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1234, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Cycle_Windows_Server_2022_64-124_failed_tasks_log.0", "raw": "2022-05-04 05:51:28,004 | Failed at Play [vgauth_check_service] **********************\n2022-05-04 05:51:28,004 | TASK [Check specified service 'VGAuthService' status in Windows] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_get_service_status.yml:15\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play vg auth check service timestamp task check specified service vg auth service status in windows fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1235, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_Server_LTSC_70U3_NVME_E1000E_EFI-23_failed_tasks_log.1", "raw": "2022-04-28 06:00:06,028 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-04-28 06:00:06,028 | TASK [Get number of logical processors in Windows guest OS] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_get_cpu_cores_sockets.yml:10\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play cpu multi cores per socket timestamp task get number of logical processors in windows guest os fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1236, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_Server_LTSC_70U3_PARAVIRTUAL_E1000E_EFI-26_failed_tasks_log.0", "raw": "2022-04-25 08:25:10,025 | Failed at Play [check_efi_firmware] ************************\n2022-04-25 08:25:10,025 | TASK [Get firmware type in Windows guest OS] ***************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_get_firmware.yml:5\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play check efi firmware timestamp task get firmware type in windows guest os fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1237, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Windows_11_70GA_IDE_E1000E_EFI-27_failed_tasks_log.0", "raw": "2022-05-06 06:08:34,006 | Failed at Play [memory_hot_add_basic] **********************\n2022-05-06 06:08:34,006 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play memory hot add basic timestamp task execute powershell command in windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1238, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Cycle_Windows_10_64-135_failed_tasks_log.0", "raw": "2022-05-04 10:05:15,004 | Failed at Play [sata_vhba_device_ops] **********************\n2022-05-04 10:05:15,004 | TASK [Create an empty file in Windows guest OS] ************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_create_file.yml:8\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play sata v hba device ops timestamp task create an empty file in windows guest os fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1239, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Cycle_Windows_11_64-111_failed_tasks_log.0", "raw": "2022-05-05 09:05:53,005 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-05-05 09:05:53,005 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play vmxnet number network device ops timestamp task execute powershell command in windows guest fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1240, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_Windows_Server_LTSC_vNext-5_logs_failed_tasks_log.1", "raw": "2021-11-30 08:22:24,030 | Failed at Play [check_ip_address] **************************\n2021-11-30 08:22:24,030 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => basic: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play check ip address timestamp task execute powershell command in windows guest fatal localhost un reachable basic connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1241, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_RHEL_9_x-25_logs_failed_tasks_log.0", "raw": "2021-12-21 13:05:27,021 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2021-12-21 13:05:27,021 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nConnectionResetError: [Errno 104] Connection reset by peer\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error\n2021-12-21 13:06:58,021 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "connectionreset", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task wait for port number to become open or contain specific keyword connection reset error error number connection reset by peer fatal localhost failed module failure see stdout stderr for the exact error timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1242, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Cycle_RHEL_9_x-22_failed_tasks_log.0", "raw": "2022-02-11 21:56:40,011 | Failed at Play [ovt_verify_install] ************************\n2022-02-11 21:56:40,011 | TASK [Check VM 'test_vm_1644612776118' IP address] *********\ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_9.x/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1644612776118' IP Address", "category": "ip_address", "processed": "timestamp failed at play ovt verify install timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1243, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Regression_Windows_Server_2022-52_failed_tasks_log.0", "raw": "2022-02-21 04:05:36,021 | Failed at Play [deploy_vm_efi_nvme_e1000e] *****************\n2022-02-21 04:05:36,021 | TASK [Check VM 'test_windows_server_ltsc_22538' IP address] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_2022/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_windows_server_ltsc_22538' IP Address\n2022-02-21 04:05:42,021 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_2022/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "ip_address", "processed": "timestamp failed at play deploy vm efi nvme timestamp task check vm test windows server lts c number ip address fatal localhost failed failed to get vm test windows server lts c number ip address timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1244, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_RHEL_9_x-21_logs_failed_tasks_log.0", "raw": "2021-12-10 17:00:47,010 | Failed at Play [ovt_verify_install] ************************\n2021-12-10 17:00:47,010 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "ip_address", "processed": "timestamp failed at play ovt verify install timestamp task check vm test vm ip address fatal localhost failed failed to get vm test vm ip address", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1245, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Regression_RHEL_8_x-48_failed_tasks_log.0", "raw": "2022-03-25 06:44:30,025 | Failed at Play [ovt_verify_uninstall] **********************\n2022-03-25 06:44:30,025 | TASK [Check VM 'test_rhel8' IP address] ********************\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_rhel8' IP Address", "category": "ip_address", "processed": "timestamp failed at play ovt verify un install timestamp task check vm test rhel number ip address fatal localhost failed failed to get vm test rhel number ip address", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1246, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI-55_failed_tasks_log.0", "raw": "2022-04-25 03:30:36,025 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-04-25 03:30:36,025 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-04-25 03:31:04,025 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:42\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": "ip_address", "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task check vm test vm ip address fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas enumbere", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1247, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_Server_LTSC_70GA_SATA_E1000E_BIOS-17_failed_tasks_log.0", "raw": "2022-04-22 02:42:45,022 | Failed at Play [deploy_vm_bios_sata_e1000e] ****************\n2022-04-22 02:42:45,022 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-04-22 02:43:22,022 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:42\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_e1000e", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios sata timestamp task check vm test vm ip address fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios sata enumbere", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1248, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_Server_LTSC_70GA_SATA_E1000E_BIOS-16_failed_tasks_log.0", "raw": "2022-04-21 17:13:19,021 | Failed at Play [deploy_vm_bios_sata_e1000e] ****************\n2022-04-21 17:13:19,021 | TASK [Check VM 'test_vm_1650556495367' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1650556495367' IP Address\n2022-04-21 17:13:55,021 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:42\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_e1000e", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios sata timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address timestamp task testing exit due to failure fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios sata enumbere", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1249, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Windows_Server_LTSC_70U1_IDE_E1000E_BIOS-22_failed_tasks_log.0", "raw": "2022-04-22 02:50:26,022 | Failed at Play [deploy_vm_bios_ide_e1000e] *****************\n2022-04-22 02:50:26,022 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U1_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-04-22 02:50:57,022 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U1_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:42\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_e1000e", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios ide timestamp task check vm test vm ip address fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios ide enumbere", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1250, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Cycle_Windows_10_64-136_failed_tasks_log.13", "raw": "2022-05-04 10:46:43,004 | Failed at Play [wintools_uninstall_verify] *****************\n2022-05-04 10:46:43,004 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "ip_address", "processed": "timestamp failed at play win tools un install verify timestamp task check vm test vm ip address fatal localhost failed failed to get vm test vm ip address", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1298, "name": "__Ansible_Regression_Ubuntu_20_04_Server_ISO-20_logs_failed_tasks_log.0", "raw": "2021-11-26 07:50:43,026 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-11-26 07:50:43,026 | TASK [Assert cloud-init guest customization checks all PASS] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_20.04.3_Server_ISO/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:41\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "domain_not_expect", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task assert cloud init guest customization checks all pass failed localhost item dns suffix success is false vm dns domain search domains are eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1251, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Windows_11_70U3_LSILOGICSAS_VMXNET3_EFI-14_failed_tasks_log.0", "raw": "2022-05-04 03:25:16,004 | Failed at Play [deploy_vm_efi_lsilogicsas_vmxnet3] *********\n2022-05-04 03:25:16,004 | TASK [Check VM 'test_vm_1651630204141' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1651630204141' IP Address\n2022-05-04 03:25:48,004 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_vmxnet3", "category": "ip_address", "processed": "timestamp failed at play deploy vm efi lsi logic sas vmxnet number timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address timestamp task testing exit due to failure fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1252, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_70GA-2_logs_failed_tasks_log.0", "raw": "2021-12-10 17:46:56,010 | Failed at Play [deploy_vm_bios_paravirtual_vmxnet3] ********\n2021-12-10 17:46:56,010 | TASK [Check VM 'test_vm_1639153639529' IP address] *********\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70GA/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1639153639529' IP Address\n2021-12-10 17:47:58,010 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70GA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_paravirtual_vmxnet3", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios para virtual vmxnet number timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1253, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_70U1-11_logs_failed_tasks_log.0", "raw": "2021-12-22 07:19:08,022 | Failed at Play [deploy_vm_bios_paravirtual_vmxnet3] ********\n2021-12-22 07:19:08,022 | TASK [Check VM 'test_vm_1640152973589' IP address] *********\ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_70U1/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1640152973589' IP Address\n2021-12-22 07:20:09,022 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_70U1/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_paravirtual_vmxnet3", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios para virtual vmxnet number timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1254, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_70U1-5_logs_failed_tasks_log.0", "raw": "2021-12-13 05:13:19,013 | Failed at Play [deploy_vm_bios_paravirtual_vmxnet3] ********\n2021-12-13 05:13:19,013 | TASK [Check VM 'test_vm_1639366994091' IP address] *********\ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_70U1/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1639366994091' IP Address\n2021-12-13 05:14:21,013 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_70U1/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_paravirtual_vmxnet3", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios para virtual vmxnet number timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1255, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_Server_LTSC_64bit_70U3_LSILogicSAS_BIOS-7_logs_failed_tasks_log.0", "raw": "2021-12-13 15:09:24,013 | Failed at Play [deploy_vm_bios_lsilogicsas_e1000e] *********\n2021-12-13 15:09:24,013 | TASK [Check VM 'test_vm_1639402871391' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_70U3_LSILogicSAS_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1639402871391' IP Address\n2021-12-13 15:10:17,013 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_70U3_LSILogicSAS_BIOS/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios lsi logic sas timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1256, "name": "ansible_gosv_logs_FAILURE_Ansible_exclude_Regression_SLED_15_x-11_logs_failed_tasks_log.0", "raw": "2021-12-14 17:00:23,014 | Failed at Play [deploy_vm_bios_nvme_vmxnet3] ***************\n2021-12-14 17:00:23,014 | TASK [Check VM 'test_vm_1639495953017' IP address] *********\ntask path: /home/worker/workspace/Ansible_exclude_Regression_SLED_15.x/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1639495953017' IP Address\n2021-12-14 17:01:56,014 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_exclude_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_nvme_vmxnet3", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios nvme vmxnet number timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios nvme vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1301, "name": "__Ansible_Regression_Ubuntu_21_10_Server_ISO-30_logs_failed_tasks_log.1", "raw": "2021-12-02 22:38:25,002 | Failed at Play [vmxnet3_network_device_ops] ****************\n2021-12-02 22:38:25,002 | TASK [Apply netplan configuration file for new added nic ens224] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/linux/network_device_ops/enable_new_ethernet.yml:62\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.185.13.124 port 22: Connection timed out", "category": "ssh_to_host_failure", "processed": "timestamp failed at play vmxnet number network device ops timestamp task apply net plan configuration file for new added nic fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1257, "name": "ansible_gosv_logs_FAILURE_Ansible_exclude_Regression_SLED_15_x-5_logs_failed_tasks_log.0", "raw": "2021-12-14 08:21:58,014 | Failed at Play [deploy_vm_bios_nvme_vmxnet3] ***************\n2021-12-14 08:21:58,014 | TASK [Check VM 'test_vm_1639464945501' IP address] *********\ntask path: /home/worker/workspace/Ansible_exclude_Regression_SLED_15.x/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1639464945501' IP Address\n2021-12-14 08:23:17,014 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_exclude_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_nvme_vmxnet3", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios nvme vmxnet number timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios nvme vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1258, "name": "ansible_gosv_logs_FAILURE_Ansible_exclude_Regression_SLED_15_x-8_logs_failed_tasks_log.0", "raw": "2021-12-14 11:47:06,014 | Failed at Play [deploy_vm_bios_nvme_vmxnet3] ***************\n2021-12-14 11:47:06,014 | TASK [Check VM 'test_vm_1639476962068' IP address] *********\ntask path: /home/worker/workspace/Ansible_exclude_Regression_SLED_15.x/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1639476962068' IP Address\n2021-12-14 11:49:16,014 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_exclude_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_nvme_vmxnet3", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios nvme vmxnet number timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios nvme vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1259, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Cycle_Ubuntu_21_10_ISO-83_failed_tasks_log.0", "raw": "2022-05-04 10:51:33,004 | Failed at Play [e1000e_network_device_ops] *****************\n2022-05-04 10:51:33,004 | TASK [Try to ping IP] **************************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "ping_issue", "processed": "timestamp failed at play network device ops timestamp task try to ping ip fatal localhost failed non zero return code when ping", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1261, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Desktop_ISO-29_logs_failed_tasks_log.0", "raw": "2021-12-23 06:38:10,023 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-23 06:38:10,023 | TASK [Try to ping IP] **************************************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Desktop_ISO/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "ping_issue", "processed": "timestamp failed at play go sc perl dhcp timestamp task try to ping ip fatal localhost failed non zero return code when ping", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1262, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Debian_10_x_64bit_MAIN_PARAVIRTUAL_VMXNET3_EFI-13_failed_tasks_log.1", "raw": "2022-03-07 14:50:36,007 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-03-07 14:50:36,007 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:108\nfatal: [localhost]: FAILED! => [\"VM DNS servers are ['192.168.1.1'], not expected DNS servers ['192.168.1.1', '192.168.1.2']\", \"VM DNS domain search domains are  ['lan'] not expected search domains ['test.com', 'gosc.test.com']\", \"VM static IPv4 address is '192.168.1.194', expected IPv4 address is 192.168.1.101\"]", "category": "dns_not_expect", "processed": "timestamp failed at play go sc cloud init static ip timestamp task gos customization failed fatal localhost failed vm dns servers are ip address not expected dns servers ip address ip address vm dns domain search domains are lan not expected search domains test com go sc test com vm static ip vnumber address is ip address expected ip vnumber address is ip address", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1263, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_RHEL_9_x_70U3p04_NVME_VMXNET3_EFI-2_failed_tasks_log.0", "raw": "2022-03-18 07:53:40,018 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-03-18 07:53:40,018 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3p04_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:108\nfatal: [localhost]: FAILED! => [\"VM DNS servers are ['10.195.12.31', '10.172.40.1', 'fd01:1:3:1001::10'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"]", "category": "dns_not_expect", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task gos customization failed fatal localhost failed vm dns servers are ip address ip address ip address not expected dns servers ip address ip address", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1264, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Debian_10_x_64bit_MAIN_PARAVIRTUAL_VMXNET3_EFI-18_failed_tasks_log.1", "raw": "2022-04-01 09:44:07,001 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-04-01 09:44:07,001 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:108\nfatal: [localhost]: FAILED! => [\"VM DNS servers are ['192.168.192.1'], not expected DNS servers ['192.168.1.1', '192.168.1.2']\", \"VM DNS domain search domains are  ['lan'] not expected search domains ['test.com', 'gosc.test.com']\", \"VM static IPv4 address is '192.168.192.133', expected IPv4 address is 192.168.192.101\", \"VM static IPv4 gateway is '192.168.1.1', expected IPv4 gateway is 192.168.192.1\"]", "category": "dns_not_expect", "processed": "timestamp failed at play go sc cloud init static ip timestamp task gos customization failed fatal localhost failed vm dns servers are ip address not expected dns servers ip address ip address vm dns domain search domains are lan not expected search domains test com go sc test com vm static ip vnumber address is ip address expected ip vnumber address is ip address vm static ip vnumber gateway is ip address expected ip vnumber gateway is ip address", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1302, "name": "__Ansible_Regression_Ubuntu_21_10_Desktop_ISO-19_logs_failed_tasks_log.0", "raw": "2021-12-08 04:51:37,008 | Failed at Play [env_setup] *********************************\n2021-12-08 04:51:37,008 | TASK [Enable GuestIPHack on ESXi host '10.78.118.4'] *******\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Desktop_ISO/ansible-vsphere-gos-validation/common/esxi_enable_guest_ip_hack.yml:5\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via ssh: ssh: connect to host 10.78.118.4 port 22: Connection timed out", "category": "ssh_to_host_failure", "processed": "timestamp failed at play environment setup timestamp task enable guest ip hack on esxi host ip address fatal localhost un reachable failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1279, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_ISO-4_logs_failed_tasks_log.0", "raw": "2022-01-12 16:40:08,012 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-01-12 16:40:08,012 | TASK [Wait for autoinstall completed message] **************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_ISO/ansible-vsphere-gos-validation/linux/deploy_vm/wait_autoinstall_completed.yml:4\nfatal: [localhost -> 10.186.6.59]: FAILED! => non-zero return code when grep\n2022-01-12 16:41:06,012 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_ISO/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_paravirtual_vmxnet3", "category": "greperror", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for auto install completed message fatal localhost ip address failed non zero return code when grep timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1265, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_67U3-1_logs_failed_tasks_log.0", "raw": "2021-12-10 15:06:50,010 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-10 15:06:50,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_67U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is sc2-10-187-120-243, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.195.12.31', '10.172.40.1'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is scnumber expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is nimbus eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1266, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_70GA-3_logs_failed_tasks_log.0", "raw": "2021-12-15 12:18:52,015 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-15 12:18:52,015 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70GA/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is wdc-10-191-178-154, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.132.7.1', '10.142.7.1', '10.128.242.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['eng.vmware.com', 'vmware.com', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is wdc number expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is nimbus eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are eng vmware com vmware com nimbus eng vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1283, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-14_logs_failed_tasks_log.2", "raw": "2022-01-19 03:01:05,019 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-01-19 03:01:05,019 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm", "category": "unable_gather_information", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task get specified property info for vm test vm fatal localhost failed unable to gather information for non existing vm test vm", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1267, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_70U1-1_logs_failed_tasks_log.0", "raw": "2021-12-10 13:44:56,010 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-10 13:44:56,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70U1/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is sc1-10-78-88-80, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is scnumber expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1268, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_70U1-2_logs_failed_tasks_log.0", "raw": "2021-12-10 16:42:59,010 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-10 16:42:59,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70U1/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is sc1-10-78-82-157, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is scnumber expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1299, "name": "__Ansible_Regression_Ubuntu_21_10_Server_ISO-23_logs_failed_tasks_log.0", "raw": "2021-12-02 14:18:30,002 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-02 14:18:30,002 | TASK [Assert cloud-init guest customization checks all PASS] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['eng.vmware.com', 'nimbus.eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "domain_not_expect", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task assert cloud init guest customization checks all pass failed localhost item dns suffix success is false vm dns domain search domains are eng vmware com nimbus eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1269, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_70U2-1_logs_failed_tasks_log.0", "raw": "2021-12-10 14:06:45,010 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-10 14:06:45,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70U2/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is sc2-10-184-84-255, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is scnumber expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1270, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_70U2-2_logs_failed_tasks_log.0", "raw": "2021-12-10 17:01:54,010 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-10 17:01:54,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70U2/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is sc2-10-184-94-83, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is scnumber expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1271, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_70U3-1_logs_failed_tasks_log.0", "raw": "2021-12-10 13:47:50,010 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-10 13:47:50,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is sc1-10-168-185-206, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.162.204.1', '10.166.1.1'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is scnumber expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is nimbus eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1272, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_70U3-2_logs_failed_tasks_log.0", "raw": "2021-12-10 16:45:06,010 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-10 16:45:06,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is sc1-10-168-186-201, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.162.204.1', '10.166.1.1'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is scnumber expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is nimbus eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1273, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_AlmaLinux_8_x-17_logs_failed_tasks_log.0", "raw": "2021-12-10 17:13:48,010 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-10 17:13:48,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_Regression_AlmaLinux_8.x/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is sc1-10-78-161-170, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is scnumber expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1274, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_AlmaLinux_8_x-18_logs_failed_tasks_log.0", "raw": "2021-12-10 21:24:48,010 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-10 21:24:48,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_Regression_AlmaLinux_8.x/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is sc1-10-78-168-71, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is scnumber expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1275, "name": "ansible_gosv_logs_FAILURE_Ansible_RockyLinux_8_x_70GA-1_logs_failed_tasks_log.0", "raw": "2021-12-09 12:36:40,009 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-09 12:36:40,009 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70GA/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is wdc-10-191-154-234, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.132.7.1', '10.142.7.1', '10.128.242.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['eng.vmware.com', 'vmware.com', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is wdc number expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is nimbus eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are eng vmware com vmware com nimbus eng vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1276, "name": "ansible_gosv_logs_FAILURE_Ansible_RockyLinux_8_x_70GA-2_logs_failed_tasks_log.0", "raw": "2021-12-09 14:12:21,009 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-09 14:12:21,009 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70GA/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is wdc-10-191-159-69, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.132.7.1', '10.142.7.1', '10.128.242.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['eng.vmware.com', 'vmware.com', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is wdc number expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is nimbus eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are eng vmware com vmware com nimbus eng vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1277, "name": "ansible_gosv_logs_FAILURE_Ansible_RockyLinux_8_x_70U1-3_logs_failed_tasks_log.0", "raw": "2021-12-10 04:37:42,010 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-10 04:37:42,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70U1/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is wdc-10-191-232-218, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.132.7.1', '10.142.7.1', '10.128.242.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['eng.vmware.com', 'vmware.com', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is wdc number expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is nimbus eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are eng vmware com vmware com nimbus eng vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1278, "name": "ansible_gosv_logs_FAILURE_Ansible_RockyLinux_8_x_70U2-2_logs_failed_tasks_log.0", "raw": "2021-12-09 14:12:20,009 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-09 14:12:20,009 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70U2/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is wdc-10-191-224-30, expected hostname is gosc-dhcp-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.132.7.1', '10.142.7.1', '10.128.242.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['eng.vmware.com', 'vmware.com', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is wdc number expected hostname is go sc dhcp vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is nimbus eng vmware com expected domain name is go sc test com and dns domain success is false failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are eng vmware com vmware com nimbus eng vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1280, "name": "ansible_gosv_logs_FAILURE_Ansible_Ubuntu_Desktop_ISO_main-2_logs_failed_tasks_log.0", "raw": "2021-12-13 23:44:13,013 | Failed at Play [deploy_vm_bios_nvme_e1000e] ****************\n2021-12-13 23:44:13,013 | TASK [Wait for autoinstall early commands] *****************\ntask path: /home/worker/workspace/Ansible_Ubuntu_Desktop_ISO_main/ansible-vsphere-gos-validation/linux/deploy_vm/ubuntu/ubuntu_install_os.yml:9\nfatal: [localhost -> 10.186.53.51]: FAILED! => non-zero return code when grep\n2021-12-13 23:45:26,013 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Ubuntu_Desktop_ISO_main/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_nvme_e1000e", "category": "greperror", "processed": "timestamp failed at play deploy vm bios nvme timestamp task wait for auto install early commands fatal localhost ip address failed non zero return code when grep timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios nvme enumbere", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1281, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI-14_failed_tasks_log.0", "raw": "2022-05-04 02:47:21,004 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-05-04 02:47:21,004 | TASK [Guest OS connection failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_check_winrm.yml:26\nfatal: [localhost]: FAILED! => Guest winrm is not connectable in 1800 seconds.\n2022-05-04 02:48:12,004 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": "guest_winrm", "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task guest os connection failure fatal localhost failed guest win rm is not connectable in number seconds timestamp task testing exit due to failure fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas enumbere", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1282, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-14_logs_failed_tasks_log.1", "raw": "2022-01-19 03:01:01,019 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-01-19 03:01:01,019 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm", "category": "unable_gather_information", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task get specified property info for vm test vm fatal localhost failed unable to gather information for non existing vm test vm", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1284, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_Ubuntu_20_04_3_ISO-26_logs_failed_tasks_log.0", "raw": "2022-01-06 06:38:41,006 | Failed at Play [e1000e_network_device_ops] *****************\n2022-01-06 06:38:41,006 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.3_ISO/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm", "category": "unable_gather_information", "processed": "timestamp failed at play network device ops timestamp task get specified property info for vm test vm fatal localhost failed unable to gather information for non existing vm test vm", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1285, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_Ubuntu_20_04_3_ISO-26_logs_failed_tasks_log.1", "raw": "2022-01-06 06:38:44,006 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-01-06 06:38:44,006 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.3_ISO/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm", "category": "unable_gather_information", "processed": "timestamp failed at play vmxnet number network device ops timestamp task get specified property info for vm test vm fatal localhost failed unable to gather information for non existing vm test vm", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1286, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_OVA-46_logs_failed_tasks_log.2", "raw": "2022-01-20 09:37:07,020 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-01-20 09:37:07,020 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_OVA/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm", "category": "unable_gather_information", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task get specified property info for vm test vm fatal localhost failed unable to gather information for non existing vm test vm", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1287, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Regression_Flatcar_OVA-40_failed_tasks_log.0", "raw": "2022-02-21 04:59:29,021 | Failed at Play [deploy_flatcar_ova] ************************\n2022-02-21 04:59:29,021 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_Flatcar_OVA/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table\n2022-02-21 06:30:30,021 | TASK [Remove local path /tmp/nfs_ap9owhl3] *****************\ntask path: /home/worker/workspace/Ansible_Regression_Flatcar_OVA/ansible-vsphere-gos-validation/common/delete_local_file.yml:5\nfatal: [localhost]: FAILED! => rmtree failed: [Errno 30] Read-only file system: 'IPMICFG.EXE'", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play deploy flatcar ova timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table timestamp task remove local path tmp nfs fatal localhost failed rm tree failed error number read only file system ipmi configuration exe", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1288, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Regression_Photon_4_0_OVA-44_failed_tasks_log.0", "raw": "2022-02-21 04:56:15,021 | Failed at Play [deploy_vmwarephoton_ova] *******************\n2022-02-21 04:56:15,021 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table\n2022-02-21 06:26:55,021 | TASK [Remove local path /tmp/nfs_w3dyc958] *****************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/delete_local_file.yml:5\nfatal: [localhost]: FAILED! => rmtree failed: [Errno 30] Read-only file system: 'IPMICFG.EXE'", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play deploy vmware photon ova timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table timestamp task remove local path tmp nfs fatal localhost failed rm tree failed error number read only file system ipmi configuration exe", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1289, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_OVA-52_logs_failed_tasks_log.0", "raw": "2022-01-24 07:59:38,024 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-01-24 07:59:38,024 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_OVA/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table\n2022-01-24 08:02:02,024 | TASK [Remove local path /tmp/nfs_zt8f328j] *****************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_OVA/ansible-vsphere-gos-validation/common/delete_local_file.yml:5\nfatal: [localhost]: FAILED! => rmtree failed: [Errno 30] Read-only file system: '._.DS_Store'", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play deploy ubuntu ova timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table timestamp task remove local path tmp nfs fatal localhost failed rm tree failed error number read only file system ds store", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1290, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Regression_Windows_10_64-48_failed_tasks_log.0", "raw": "2022-03-10 08:27:36,010 | Failed at Play [wintools_complete_install_verify] **********\n2022-03-10 08:27:36,010 | TASK [Wait for VM power status to 'poweredOn'] *************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_64/ansible-vsphere-gos-validation/common/vm_wait_power_state.yml:11\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "failed_hardware_config_only", "processed": "timestamp failed at play win tools complete install verify timestamp task wait for vm power status to powered on fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1291, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Windows_Server_LTSC_vNext-38_logs_failed_tasks_log.0", "raw": "2022-01-15 07:52:35,015 | Failed at Play [sata_vhba_device_ops] **********************\n2022-01-15 07:52:35,015 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nsocket.timeout: The read operation timed out\nurllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='10.168.117.200', port=5986): Read timed out. (read timeout=30)\nrequests.exceptions.ReadTimeout: HTTPSConnectionPool(host='10.168.117.200', port=5986): Read timed out. (read timeout=30)\nansible.errors.AnsibleConnectionFailure: winrm connection error: HTTPSConnectionPool(host='10.168.117.200', port=5986): Read timed out. (read timeout=30)\nOSError: [Errno 113] No route to host\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7effc2630b80>: Failed to establish a new connection: [Errno 113] No route to host\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='10.168.117.200', port=5986): Max retries exceeded with url: /wsman (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7effc2630b80>: Failed to establish a new connection: [Errno 113] No route to host'))\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='10.168.117.200', port=5986): Max retries exceeded with url: /wsman (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7effc2630b80>: Failed to establish a new connection: [Errno 113] No route to host'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.\n2022-01-15 07:52:55,015 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "winconnectionerror", "processed": "timestamp failed at play sata v hba device ops timestamp task execute powershell command in windows guest socket timeout the read operation timed out url library number exceptions read timeout error https connection pool host ip address port number read timed out read timeout number requests exceptions read timeout https connection pool host ip address port number read timed out read timeout number ansible errors ansible connection failure win rm connection error https connection pool host ip address port number read timed out read timeout number os error error number no route to host url library number exceptions new connection error url library number connection https connection object at hex id failed to establish a new connection error number no route to host url library number exceptions max retry error https connection pool host ip address port number max retries exceeded with url wsman caused by new connection error url library number connection https connection object at hex id failed to establish a new connection error number no route to host requests exceptions connection error https connection pool host ip address port number max retries exceeded with url wsman caused by new connection error url library number connection https connection object at hex id failed to establish a new connection error number no route to host fatal localhost failed unexpected failure during module execution timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1292, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Regression_Photon_4_0_ISO-40_failed_tasks_log.0", "raw": "2022-03-25 07:44:02,025 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-03-25 07:44:02,025 | TASK [Execute guest OS shutdown] ***************************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_ISO/ansible-vsphere-gos-validation/linux/utils/shutdown.yml:5\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via ssh: ssh: connect to host 10.180.193.122 port 22: No route to host", "category": "ssh_to_host_failure", "processed": "timestamp failed at play cpu multi cores per socket timestamp task execute guest os shutdown fatal localhost un reachable failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1293, "name": "__Ansible_Regression_RHEL_9_x-8_logs_failed_tasks_log.0", "raw": "2021-11-25 08:14:44,025 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2021-11-25 08:14:44,025 | TASK [include_tasks] ***************************************\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/linux/deploy_vm/deploy_vm_from_iso.yml:49\nfatal: [localhost]: FAILED! => Could not find or access '/home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/common/vm_get_video_card.yml' on the Ansible Controller.\n2021-11-25 08:15:39,025 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "on_the_ansible_controller", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task include tasks fatal localhost failed could not find or access home worker workspace ansible regression rhel number x ansible vsphere gos validation common vm get video card yml on the ansible controller timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1300, "name": "__Ansible_Regression_Ubuntu_21_10_Server_ISO-30_logs_failed_tasks_log.0", "raw": "2021-12-02 20:22:17,002 | Failed at Play [e1000e_network_device_ops] *****************\n2021-12-02 20:22:17,002 | TASK [Apply netplan configuration file for new added nic ens224] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/linux/network_device_ops/enable_new_ethernet.yml:62\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.185.13.124 port 22: Connection timed out", "category": "ssh_to_host_failure", "processed": "timestamp failed at play network device ops timestamp task apply net plan configuration file for new added nic fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1303, "name": "__Ansible_Regression_Ubuntu_21_10_Server_OVA-8_logs_failed_tasks_log.0", "raw": "2021-12-03 02:35:29,003 | Failed at Play [device_list] *******************************\n2021-12-03 02:35:29,003 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_OVA/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nexception in /vmware.py when connect_to_api in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Unable to log on to vCenter or ESXi API at 10.186.230.253:443  as Administrator@vsphere.local: Cannot complete login due to an incorrect user name or password.", "category": "incorrect_username", "processed": "timestamp failed at play device list timestamp task get specified property info for vm test vm exception in vmware python when connect to api in soap adapter python when invoke method fatal localhost failed unable to log on to vcenter or esxi api at ip address as administrator vsphere local can not complete login due to an incorrect user name or password", "solution": "retry", "target": "nimbus", "version": 202205240000}, {"id": 1294, "name": "__Ansible_RHEL_9_x_70U2-2_logs_failed_tasks_log.0", "raw": "2021-12-08 09:31:42,008 | Failed at Play [deploy_vm_efi_sata_vmxnet3] ****************\n2021-12-08 09:31:42,008 | TASK [Create unattend config file from template] ***********\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U2/ansible-vsphere-gos-validation/linux/deploy_vm/create_unattend_install_iso.yml:49\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\nfatal: [localhost]: FAILED! => Could not find or access '/home/worker/workspace/Ansible_RHEL_9.x_70U2/ansible-vsphere-gos-validation/autoinstall/RHEL/9/server_with_GUI/ks.cfg' on the Ansible Controller.\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\n2021-12-08 09:32:14,008 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U2/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_sata_vmxnet3", "category": "on_the_ansible_controller", "processed": "timestamp failed at play deploy vm efi sata vmxnet number timestamp task create un attend configuration file from template if you are using a module and expect the file to exist on the remote see the remote source option fatal localhost failed could not find or access home worker workspace ansible rhel number x ansible vsphere gos validation auto install rhel number server with gui ks configuration on the ansible controller if you are using a module and expect the file to exist on the remote see the remote source option timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi sata vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1295, "name": "__Ansible_RHEL_9_x_70GA-1_logs_failed_tasks_log.0", "raw": "2021-12-08 09:04:39,008 | Failed at Play [deploy_vm_bios_paravirtual_e1000e] *********\n2021-12-08 09:04:39,008 | TASK [Create unattend config file from template] ***********\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70GA/ansible-vsphere-gos-validation/linux/deploy_vm/create_unattend_install_iso.yml:49\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\nfatal: [localhost]: FAILED! => Could not find or access '/home/worker/workspace/Ansible_RHEL_9.x_70GA/ansible-vsphere-gos-validation/autoinstall/RHEL/9/server_with_GUI/ks.cfg' on the Ansible Controller.\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\n2021-12-08 09:05:11,008 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70GA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_paravirtual_e1000e", "category": "on_the_ansible_controller", "processed": "timestamp failed at play deploy vm bios para virtual timestamp task create un attend configuration file from template if you are using a module and expect the file to exist on the remote see the remote source option fatal localhost failed could not find or access home worker workspace ansible rhel number x number ga ansible vsphere gos validation auto install rhel number server with gui ks configuration on the ansible controller if you are using a module and expect the file to exist on the remote see the remote source option timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios para virtual e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1296, "name": "__Ansible_RHEL_9_x_70U1-2_logs_failed_tasks_log.0", "raw": "2021-12-08 09:31:39,008 | Failed at Play [deploy_vm_bios_nvme_vmxnet3] ***************\n2021-12-08 09:31:39,008 | TASK [Create unattend config file from template] ***********\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U1/ansible-vsphere-gos-validation/linux/deploy_vm/create_unattend_install_iso.yml:49\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\nfatal: [localhost]: FAILED! => Could not find or access '/home/worker/workspace/Ansible_RHEL_9.x_70U1/ansible-vsphere-gos-validation/autoinstall/RHEL/9/server_with_GUI/ks.cfg' on the Ansible Controller.\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\n2021-12-08 09:32:12,008 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U1/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_nvme_vmxnet3", "category": "on_the_ansible_controller", "processed": "timestamp failed at play deploy vm bios nvme vmxnet number timestamp task create un attend configuration file from template if you are using a module and expect the file to exist on the remote see the remote source option fatal localhost failed could not find or access home worker workspace ansible rhel number x ansible vsphere gos validation auto install rhel number server with gui ks configuration on the ansible controller if you are using a module and expect the file to exist on the remote see the remote source option timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios nvme vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1297, "name": "__Ansible_RHEL_9_x_70U3-1_logs_failed_tasks_log.0", "raw": "2021-12-08 09:24:54,008 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2021-12-08 09:24:54,008 | TASK [Create unattend config file from template] ***********\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3/ansible-vsphere-gos-validation/linux/deploy_vm/create_unattend_install_iso.yml:49\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\nfatal: [localhost]: FAILED! => Could not find or access '/home/worker/workspace/Ansible_RHEL_9.x_70U3/ansible-vsphere-gos-validation/autoinstall/RHEL/9/server_with_GUI/ks.cfg' on the Ansible Controller.\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\n2021-12-08 09:25:26,008 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_paravirtual_e1000e", "category": "on_the_ansible_controller", "processed": "timestamp failed at play deploy vm efi para virtual timestamp task create un attend configuration file from template if you are using a module and expect the file to exist on the remote see the remote source option fatal localhost failed could not find or access home worker workspace ansible rhel number x ansible vsphere gos validation auto install rhel number server with gui ks configuration on the ansible controller if you are using a module and expect the file to exist on the remote see the remote source option timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi para virtual e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1311, "name": "__Ansible_Regression_RHEL_9_x-9_logs_failed_tasks_log.2", "raw": "2021-11-25 11:30:53,025 | Failed at Play [gosc_cloudinit_staticip] *******************\n2021-11-25 11:30:53,025 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play go sc cloud init static ip timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1304, "name": "__Ansible_Windows_11_64bit_70U1_IDE_BIOS-1_logs_failed_tasks_log.0", "raw": "2021-12-07 09:17:01,007 | Failed at Play [deploy_vm_bios_ide_e1000e] *****************\n2021-12-07 09:17:01,007 | TASK [Create a new VM 'test_vm' on server '10.78.230.177'] *\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_70U1_IDE_BIOS/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => value of controller_type must be one of: buslogic, lsilogic, paravirtual, lsilogicsas, sata, nvme, got: ide found in disk\n2021-12-07 09:17:32,007 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_70U1_IDE_BIOS/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "controller_type", "processed": "timestamp failed at play deploy vm bios ide timestamp task create a new vm test vm on server ip address fatal localhost failed value of controller type must be one of bus logic lsi logic para virtual lsi logic sas sata nvme got ide found in disk timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1305, "name": "__Ansible_Windows_11_64bit_70GA_IDE_EFI-1_logs_failed_tasks_log.0", "raw": "2021-12-07 08:56:28,007 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2021-12-07 08:56:28,007 | TASK [Create a new VM 'test_vm' on server '10.168.190.131'] \ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_70GA_IDE_EFI/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => value of controller_type must be one of: buslogic, lsilogic, paravirtual, lsilogicsas, sata, nvme, got: ide found in disk\n2021-12-07 08:56:58,007 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_70GA_IDE_EFI/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "controller_type", "processed": "timestamp failed at play deploy vm efi ide timestamp task create a new vm test vm on server ip address fatal localhost failed value of controller type must be one of bus logic lsi logic para virtual lsi logic sas sata nvme got ide found in disk timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1306, "name": "__Ansible_Cycle_Windows_Server_LTSC_vNext-6_logs_failed_tasks_log.2", "raw": "2021-11-30 11:41:24,030 | Failed at Play [memory_hot_add_basic] **********************\n2021-11-30 11:41:24,030 | TASK [Check memory size got from guest OS] *****************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/memory_hot_add_basic/hotadd_memory_verify.yml:24\nfatal: [localhost]: FAILED! => Get memory size in guest: 4096MB, not expected: 5120MB", "category": "memory_issue", "processed": "timestamp failed at play memory hot add basic timestamp task check memory size got from guest os fatal localhost failed get memory size in guest number mb not expected number mb", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1307, "name": "__Ansible_Regression_RHEL_8_x-18_logs_failed_tasks_log.0", "raw": "2021-12-06 08:46:52,006 | Failed at Play [testbed_deploy_nimbus] *********************\n2021-12-06 08:46:52,006 | TASK [Check nimbus testbed deploy status every 15 seconds] *\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:15\nfatal: [localhost -> wdc-dbc2108.eng.vmware.com]: FAILED! => nimbus-testbeddeploy error", "category": "nimbus_testbed_issue", "processed": "timestamp failed at play testbed deploy nimbus timestamp task check nimbus testbed deploy status every number seconds fatal localhost wdc eng vmware com failed nimbus testbed deploy error", "solution": "retry", "target": "nimbus", "version": 202205240000}, {"id": 1308, "name": "__Ansible_Windows_11_64bit_70U3_NVMe_BIOS-1_logs_failed_tasks_log.1", "raw": "2021-12-07 09:48:36,007 | Failed at Play [nvme_vhba_device_ops] **********************\n2021-12-07 09:48:36,007 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_70U3_NVMe_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => winrm connection error: HTTPSConnectionPool(host='10.187.157.121', port=5986): Read timed out. (read timeout=30)", "category": "winconnectionerror", "processed": "timestamp failed at play nvme v hba device ops timestamp task execute powershell command in windows guest fatal localhost un reachable win rm connection error https connection pool host ip address port number read timed out read timeout number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1309, "name": "__Ansible_Windows_11_64bit_70U3_PVSCSI_EFI-1_logs_failed_tasks_log.0", "raw": "2021-12-07 09:46:03,007 | Failed at Play [wintools_complete_install_verify] **********\n2021-12-07 09:46:03,007 | TASK [Download VMware tools] *******************************\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_70U3_PVSCSI_EFI/ansible-vsphere-gos-validation/windows/wintools_complete_install_verify/download_vmtools_and_transfer.yml:9\nfatal: [localhost]: FAILED! => Request failed HTTP Error 404: Not Found when get link", "category": "link_not_found", "processed": "timestamp failed at play win tools complete install verify timestamp task download vmware tools fatal localhost failed request failed http error number not found when get link", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1310, "name": "__Ansible_Regression_Ubuntu_21_10_Desktop_ISO-15_logs_failed_tasks_log.0", "raw": "2021-12-02 17:40:27,002 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2021-12-02 17:40:27,002 | TASK [Wait for autoinstall early commands] *****************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Desktop_ISO/ansible-vsphere-gos-validation/linux/deploy_vm/ubuntu/ubuntu_install_os.yml:9\nfatal: [localhost -> 10.78.188.168]: FAILED! => non-zero return code when grep\n2021-12-02 17:41:27,002 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Desktop_ISO/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "greperror", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task wait for auto install early commands fatal localhost ip address failed non zero return code when grep timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1312, "name": "__Ansible_Regression_Photon_4_0_OVA-6_logs_failed_tasks_log.1", "raw": "2021-12-02 10:17:13,002 | Failed at Play [gosc_perl_staticip] ************************\n2021-12-02 10:17:13,002 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play go sc perl static ip timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1314, "name": "__Ansible_Cycle_Ubuntu_20_04_3_ISO-8_logs_failed_tasks_log.0", "raw": "2021-11-30 07:07:52,030 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2021-11-30 07:07:52,030 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.3_ISO/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table\n2021-11-30 07:09:02,030 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.3_ISO/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_paravirtual_vmxnet3", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1315, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.8", "raw": "2021-12-01 12:45:25,001 | Failed at Play [check_efi_firmware] ************************\n2021-12-01 12:45:25,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play check efi firmware timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1316, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.15", "raw": "2021-12-01 15:31:23,001 | Failed at Play [vmxnet3_network_device_ops] ****************\n2021-12-01 15:31:23,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play vmxnet number network device ops timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1317, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.12", "raw": "2021-12-01 14:20:08,001 | Failed at Play [device_list] *******************************\n2021-12-01 14:20:08,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play device list timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1318, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.24", "raw": "2021-12-01 19:05:42,001 | Failed at Play [sata_vhba_device_ops] **********************\n2021-12-01 19:05:42,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play sata v hba device ops timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1319, "name": "__Ansible_Regression_Ubuntu_21_10_Server_OVA-7_logs_failed_tasks_log.0", "raw": "2021-12-02 13:13:39,002 | Failed at Play [deploy_ubuntu_ova] *************************\n2021-12-02 13:13:39,002 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_OVA/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table\n2021-12-02 13:14:20,002 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_OVA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_ubuntu_ova", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play deploy ubuntu ova timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy ubuntu ova", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1320, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.23", "raw": "2021-12-01 18:42:03,001 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2021-12-01 18:42:03,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1321, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.22", "raw": "\n2021-12-01 18:18:32,001 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2021-12-01 18:18:32,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1322, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.25", "raw": "2021-12-01 19:29:30,001 | Failed at Play [nvme_vhba_device_ops] **********************\n2021-12-01 19:29:30,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play nvme v hba device ops timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1323, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.13", "raw": "2021-12-01 14:43:54,001 | Failed at Play [secureboot_enable_disable] *****************\n2021-12-01 14:43:54,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play secure boot enable disable timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1324, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.14", "raw": "2021-12-01 15:07:34,001 | Failed at Play [e1000e_network_device_ops] *****************\n2021-12-01 15:07:34,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play network device ops timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1325, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.9", "raw": "2021-12-01 13:09:06,001 | Failed at Play [cpu_hot_add_basic] *************************\n2021-12-01 13:09:06,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play cpu hot add basic timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1326, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.11", "raw": "2021-12-01 13:56:27,001 | Failed at Play [memory_hot_add_basic] **********************\n2021-12-01 13:56:27,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play memory hot add basic timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1327, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.16", "raw": "2021-12-01 15:55:10,001 | Failed at Play [cpu_multicores_per_socket] *****************\n2021-12-01 15:55:10,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play cpu multi cores per socket timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1328, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.26", "raw": "2021-12-01 19:53:09,001 | Failed at Play [ovt_verify_uninstall] **********************\n2021-12-01 19:53:09,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play ovt verify un install timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1329, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.19", "raw": "2021-12-01 17:06:26,001 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-01 17:06:26,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1330, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.21", "raw": "2021-12-01 17:54:17,001 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2021-12-01 17:54:17,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play para virtual v hba device ops timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1331, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.10", "raw": "2021-12-01 13:32:44,001 | Failed at Play [check_quiesce_snapshot_custom_script] ******\n2021-12-01 13:32:44,001 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "vm_wait_guest_ip", "processed": "timestamp failed at play check quiesce snapshot custom script timestamp task wait for vmware tools collecting guest info fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1332, "name": "__Ansible_Autoinstall_CentOS_8_x-14_logs_failed_tasks_log.0", "raw": "2021-12-06 06:46:18,006 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2021-12-06 06:46:18,006 | TASK [Check VM 'test_vm_1638768091724' IP address] *********\ntask path: /home/worker/workspace/Ansible_Autoinstall_CentOS_8.x/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1638768091724' IP Address\n2021-12-06 06:46:48,006 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Autoinstall_CentOS_8.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_paravirtual_vmxnet3", "category": "ip_address", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1333, "name": "__Ansible_Regression_OracleLinux_8_x-5_logs_failed_tasks_log.0", "raw": "2021-12-01 12:48:09,001 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2021-12-01 12:48:09,001 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Regression_OracleLinux_8.x/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2021-12-01 12:49:13,001 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_OracleLinux_8.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "ip_address", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task check vm test vm ip address fatal localhost failed failed to get vm test vm ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1334, "name": "__Ansible_Autoinstall_CentOS_8_x-13_logs_failed_tasks_log.0", "raw": "2021-12-06 04:07:42,006 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2021-12-06 04:07:42,006 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Autoinstall_CentOS_8.x/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2021-12-06 04:08:15,006 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Autoinstall_CentOS_8.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_paravirtual_vmxnet3", "category": "ip_address", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task check vm test vm ip address fatal localhost failed failed to get vm test vm ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1335, "name": "__Ansible_Windows_Server_LTSC_64bit_70U1_SATA_BIOS-2_logs_failed_tasks_log.0", "raw": "2021-12-08 09:28:55,008 | Failed at Play [deploy_vm_bios_sata_e1000e] ****************\n2021-12-08 09:28:55,008 | TASK [Check VM 'test_vm_1638950974124' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_70U1_SATA_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1638950974124' IP Address\n2021-12-08 09:29:51,008 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_70U1_SATA_BIOS/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios sata timestamp task check vm test vm number ip address fatal localhost failed failed to get vm test vm number ip address timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1336, "name": "__Ansible_Windows_Server_LTSC_64bit_67U3_LSILogicSAS_EFI-1_logs_failed_tasks_log.0", "raw": "2021-12-07 10:13:57,007 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2021-12-07 10:13:57,007 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_67U3_LSILogicSAS_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2021-12-07 10:15:16,007 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_67U3_LSILogicSAS_EFI/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "ip_address", "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task check vm test vm ip address fatal localhost failed failed to get vm test vm ip address timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1337, "name": "__Ansible_Windows_Server_LTSC_64bit_70GA_SATA_BIOS-1_logs_failed_tasks_log.0", "raw": "2021-12-07 10:11:32,007 | Failed at Play [deploy_vm_bios_sata_e1000e] ****************\n2021-12-07 10:11:32,007 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_70GA_SATA_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2021-12-07 10:12:38,007 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_70GA_SATA_BIOS/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "ip_address", "processed": "timestamp failed at play deploy vm bios sata timestamp task check vm test vm ip address fatal localhost failed failed to get vm test vm ip address timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1338, "name": "__Ansible_Regression_Windows_10_32-5_logs_failed_tasks_log.0", "raw": "2021-11-23 11:08:45,023 | Failed at Play [deploy_vm_efi_nvme_e1000e] *****************\n2021-11-23 11:08:45,023 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_32/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2021-11-23 11:09:59,023 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_32/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "ip_address", "processed": "timestamp failed at play deploy vm efi nvme timestamp task check vm test vm ip address fatal localhost failed failed to get vm test vm ip address timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1339, "name": "__Ansible_Regression_RockyLinux_8_x-13_logs_failed_tasks_log.0", "raw": "2021-12-08 06:02:19,008 | Failed at Play [testbed_deploy_nimbus] *********************\n2021-12-08 06:02:19,008 | TASK [Check nimbus testbed deploy status every 15 seconds] *\ntask path: /home/worker/workspace/Ansible_Regression_RockyLinux_8.x/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:15\nfatal: [localhost -> wdc-dbc2108.eng.vmware.com]: FAILED! => ansible job 272064635345.25364 started and not finished", "category": "ansible_start_not_finish", "processed": "timestamp failed at play testbed deploy nimbus timestamp task check nimbus testbed deploy status every number seconds fatal localhost wdc eng vmware com failed ansible job hex id number started and not finished", "solution": "retry", "target": "nimbus", "version": 202205240000}, {"id": 1340, "name": "__Ansible_Cycle_Windows_Server_LTSC_vNext-6_logs_failed_tasks_log.4", "raw": "2021-11-30 11:46:29,030 | Failed at Play [cpu_multicores_per_socket] *****************\n2021-11-30 11:46:29,030 | TASK [Shutdown guest OS inside OS] *************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_shutdown_restart.yml:8\nfatal: [localhost]: UNREACHABLE! => basic: HTTPSConnectionPool(host='10.191.158.93', port=5986): Read timed out. (read timeout=30)", "category": "https_connection_pool", "processed": "timestamp failed at play cpu multi cores per socket timestamp task shutdown guest os inside os fatal localhost un reachable basic https connection pool host ip address port number read timed out read timeout number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1341, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.6", "raw": "2021-12-01 12:19:56,001 | Failed at Play [check_inbox_driver] ************************\n2021-12-01 12:19:56,001 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm'] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Detected an invalid snapshot configuration.", "category": "snapshot_config_error", "processed": "timestamp failed at play check inbox driver timestamp task create snapshot base snapshot on test vm fatal localhost failed detected an invalid snapshot configuration", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1342, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.1", "raw": "2021-12-01 12:13:00,001 | Failed at Play [ovt_verify_status] *************************\n2021-12-01 12:13:00,001 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm'] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Detected an invalid snapshot configuration.", "category": "snapshot_config_error", "processed": "timestamp failed at play ovt verify status timestamp task create snapshot base snapshot on test vm fatal localhost failed detected an invalid snapshot configuration", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1343, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.7", "raw": "2021-12-01 12:20:59,001 | Failed at Play [check_os_fullname] *************************\n2021-12-01 12:20:59,001 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm'] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Detected an invalid snapshot configuration.", "category": "snapshot_config_error", "processed": "timestamp failed at play check os full name timestamp task create snapshot base snapshot on test vm fatal localhost failed detected an invalid snapshot configuration", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1344, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.2", "raw": "2021-12-01 12:15:07,001 | Failed at Play [vgauth_check_service] **********************\n2021-12-01 12:15:07,001 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm'] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Detected an invalid snapshot configuration.", "category": "snapshot_config_error", "processed": "timestamp failed at play vg auth check service timestamp task create snapshot base snapshot on test vm fatal localhost failed detected an invalid snapshot configuration", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1345, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.5", "raw": "2021-12-01 12:18:54,001 | Failed at Play [stat_hosttime] *****************************\n2021-12-01 12:18:54,001 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm'] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Detected an invalid snapshot configuration.", "category": "snapshot_config_error", "processed": "timestamp failed at play stat host time timestamp task create snapshot base snapshot on test vm fatal localhost failed detected an invalid snapshot configuration", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1346, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.4", "raw": "2021-12-01 12:18:20,001 | Failed at Play [stat_balloon] ******************************\n2021-12-01 12:18:20,001 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm'] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Detected an invalid snapshot configuration.", "category": "snapshot_config_error", "processed": "timestamp failed at play stat balloon timestamp task create snapshot base snapshot on test vm fatal localhost failed detected an invalid snapshot configuration", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1347, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.3", "raw": "2021-12-01 12:17:10,001 | Failed at Play [check_ip_address] **************************\n2021-12-01 12:17:10,001 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm'] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Detected an invalid snapshot configuration.", "category": "snapshot_config_error", "processed": "timestamp failed at play check ip address timestamp task create snapshot base snapshot on test vm fatal localhost failed detected an invalid snapshot configuration", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1348, "name": "__Ansible_Regression_Windows_11_64-12_logs_failed_tasks_log.1", "raw": "2021-11-24 08:48:38,024 | Failed at Play [gosc_sanity_dhcp] **************************\n2021-11-24 08:48:38,024 | TASK [Customize Windows guest OS] **************************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/windows/guest_customization/win_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.", "category": "warn_detail_timeout", "processed": "timestamp failed at play go sc sanity dhcp timestamp task customize windows guest os fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1349, "name": "__Ansible_Regression_Photon_4_0_OVA-10_logs_failed_tasks_log.0", "raw": "2021-12-06 10:24:45,006 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-06 10:24:45,006 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.", "category": "warn_detail_timeout", "processed": "timestamp failed at play go sc perl dhcp timestamp task customize linux guest os fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1350, "name": "__Ansible_Regression_Photon_4_0_OVA-10_logs_failed_tasks_log.1", "raw": "2021-12-06 10:41:44,006 | Failed at Play [gosc_perl_staticip] ************************\n2021-12-06 10:41:44,006 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.", "category": "warn_detail_timeout", "processed": "timestamp failed at play go sc perl static ip timestamp task customize linux guest os fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1351, "name": "__Ansible_Regression_Windows_11_64-12_logs_failed_tasks_log.0", "raw": "2021-11-24 08:06:07,024 | Failed at Play [gosc_sanity_staticip] **********************\n2021-11-24 08:06:07,024 | TASK [Customize Windows guest OS] **************************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/windows/guest_customization/win_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.", "category": "warn_detail_timeout", "processed": "timestamp failed at play go sc sanity static ip timestamp task customize windows guest os fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1371, "name": "__Ansible_Regression_SLES_15_x-3_logs_failed_tasks_log.0", "raw": "2021-12-01 14:02:34,001 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2021-12-01 14:02:34,001 | TASK [Wait for device list changed] ************************\ntask path: /home/worker/workspace/Ansible_Regression_SLES_15.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:97\nfatal: [localhost -> 10.191.189.69]: FAILED! =>  when lsblk", "category": "origin", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task wait for device list changed fatal localhost ip address failed when ls blk", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1352, "name": "__Ansible_Regression_Flatcar_OVA-8_logs_failed_tasks_log.0", "raw": "2021-12-02 17:54:35,002 | Failed at Play [deploy_flatcar_ova] ************************\n2021-12-02 17:54:35,002 | TASK [Try to ping IP] **************************************\ntask path: /home/worker/workspace/Ansible_Regression_Flatcar_OVA/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping\n2021-12-02 17:55:21,002 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Flatcar_OVA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_flatcar_ova", "category": "ping_issue", "processed": "timestamp failed at play deploy flatcar ova timestamp task try to ping ip fatal localhost failed non zero return code when ping timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy flatcar ova", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1353, "name": "__Ansible_Regression_Ubuntu_21_10_Server_ISO-32_logs_failed_tasks_log.0", "raw": "2021-12-06 10:21:24,006 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2021-12-06 10:21:24,006 | TASK [Try to ping IP] **************************************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping\n2021-12-06 10:22:38,006 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "ping_issue", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task try to ping ip fatal localhost failed non zero return code when ping timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1354, "name": "__Ansible_Regression_RHEL_9_x-15_logs_failed_tasks_log.0", "raw": "2021-12-02 09:30:45,002 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-02 09:30:45,002 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.132.7.1', '10.142.7.1', '10.128.242.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item dns servers success is false vm dns servers are ip address ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1355, "name": "__Ansible_Regression_RHEL_9_x-20_logs_failed_tasks_log.0", "raw": "2021-12-07 07:28:16,007 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-07 07:28:16,007 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.162.204.1', '10.166.1.1'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item dns servers success is false vm dns servers are ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1356, "name": "__Ansible_Regression_Ubuntu_21_10_Server_OVA-13_logs_failed_tasks_log.0", "raw": "2021-12-03 07:54:22,003 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-03 07:54:22,003 | TASK [Assert cloud-init guest customization checks all PASS] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_OVA/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=dns_servers_success is False) => [\"VM DNS servers are ['10.195.12.31', '10.172.40.1'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\"] and dns_servers_success is False\nfailed: [localhost] => (item=dns_suffix_success is False) => [\"VM DNS domain search domains are  ['eng.vmware.com', 'nimbus.eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"] and dns_suffix_success is False", "category": "dns_not_expect", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task assert cloud init guest customization checks all pass failed localhost item dns servers success is false vm dns servers are ip address ip address not expected dns servers ip address ip address and dns servers success is false failed localhost item dns suffix success is false vm dns domain search domains are eng vmware com nimbus eng vmware com vmware com not expected search domains test com go sc test com and dns suffix success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1370, "name": "__Ansible_Regression_RHEL_8_x-14_logs_failed_tasks_log.0", "raw": "2021-12-01 12:11:42,001 | Failed at Play [ovt_verify_install] ************************\n2021-12-01 12:11:42,001 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm'] *********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Unable to retrieve the current working directory: 5 (Input/output error). Check if the directory has been deleted or unmounted. ", "category": "retrieve_current_dir", "processed": "timestamp failed at play ovt verify install timestamp task create snapshot base snapshot on test vm fatal localhost failed unable to retrieve the current working directory number input output error check if the directory has been deleted or un mounted", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1380, "name": "__Ansible_Autoinstall_Windows_11_64-27_logs_failed_tasks_log.0", "raw": "2021-12-07 01:54:01,007 | Failed at Play [secureboot_enable_disable] *****************\n2021-12-07 01:54:01,007 | TASK [Wait for VM power status to 'poweredOff'] ************\ntask path: /home/worker/workspace/Ansible_Autoinstall_Windows_11_64/ansible-vsphere-gos-validation/common/vm_wait_power_state.yml:11\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "failed_hardware_config_only", "processed": "timestamp failed at play secure boot enable disable timestamp task wait for vm power status to powered off fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1357, "name": "__Ansible_Regression_RHEL_7_x-7_logs_failed_tasks_log.0", "raw": "2021-11-25 12:08:26,025 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2021-11-25 12:08:26,025 | TASK [Create a new VM 'test_vm' on server '10.186.62.44'] **\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_7.x/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A component of the virtual machine is not accessible on the host.\n2021-11-25 12:08:57,025 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_7.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "component_not_accessible", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task create a new vm test vm on server ip address fatal localhost failed failed to create a virtual machine a component of the virtual machine is not accessible on the host timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1358, "name": "__Ansible_Regression_SLES_15_x-6_logs_failed_tasks_log.0", "raw": "2021-12-02 19:26:38,002 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-02 19:26:38,002 | TASK [Check service 'cloud-config' is enabled] *************\ntask path: /home/worker/workspace/Ansible_Regression_SLES_15.x/ansible-vsphere-gos-validation/linux/utils/check_service_status.yml:55\nfatal: [localhost]: FAILED! => Serivce 'cloud-config' is not enabled", "category": "cloud_config", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task check service cloud configuration is enabled fatal localhost failed ser i vc e cloud configuration is not enabled", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1359, "name": "__Ansible_Cycle_Windows_10_64-5_logs_failed_tasks_log.1", "raw": "2021-11-30 11:32:27,030 | Failed at Play [memory_hot_add_basic] **********************\n2021-11-30 11:32:27,030 | TASK [Set VM power state to 'shutdown-guest'] **************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:34\nfatal: [localhost]: FAILED! => Timeout while waiting for VM power off.\n2021-11-30 11:32:43,030 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "timeout_poweroff", "processed": "timestamp failed at play memory hot add basic timestamp task set vm power state to shutdown guest fatal localhost failed timeout while waiting for vm power off timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1360, "name": "__Ansible_exclude_Regression_CentOS_8_4-350_logs_failed_tasks_log.0", "raw": "2021-09-08 06:27:05,008 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-09-08 06:27:05,008 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /root/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:54\nfailed: [localhost] => (item=gosc_log_success is False) => perl guest customization with dhcp IP failed on: {'key': 'gosc_log_success', 'value': False} and gosc_log_success is False", "category": "static_ip_failed_on", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item go sc log success is false perl guest customization with dhcp ip failed on key go sc log success value false and go sc log success is false", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1361, "name": "__Ansible_exclude_Regression_CentOS_8_4-350_logs_failed_tasks_log.1", "raw": "2021-09-08 06:35:15,008 | Failed at Play [gosc_perl_staticip] ************************\n2021-09-08 06:35:15,008 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /root/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:54\nfailed: [localhost] => (item=gosc_log_success is False) => perl guest customization with static IP failed on: {'key': 'gosc_log_success', 'value': False} and gosc_log_success is False", "category": "static_ip_failed_on", "processed": "timestamp failed at play go sc perl static ip timestamp task assert perl guest customization checks all pass failed localhost item go sc log success is false perl guest customization with static ip failed on key go sc log success value false and go sc log success is false", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1362, "name": "__Ansible_Regression_Ubuntu_21_10_Server_OVA-17_logs_failed_tasks_log.0", "raw": "2021-12-08 05:01:26,008 | Failed at Play [deploy_ova] ********************************\n2021-12-08 05:01:26,008 | TASK [Deploy VM from ovf template] *************************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_OVA/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nexception in /vmware_deploy_ovf.py when run in /request.py when do_open\nfatal: [localhost]: FAILED! => <urlopen error The write operation timed out> Problem validating OVF import spec: Line 128: No space left for device '9' on parent controller '3'.\n2021-12-08 05:02:02,008 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_OVA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_ova", "category": "deploy_ovf_request_error", "processed": "timestamp failed at play deploy ova timestamp task deploy vm from ovf template exception in vmware deploy ovf python when run in request python when do open fatal localhost failed url open error the write operation timed out problem validating ovf import spec line number no space left for device number on parent controller number timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy ova", "solution": "deepdive", "target": "nimbus", "version": 202205240000}, {"id": 1363, "name": "__Ansible_Windows_Server_LTSC_64bit_70U3_LSILogicSAS_BIOS-1_logs_failed_tasks_log.0", "raw": "2021-12-07 08:58:23,007 | Failed at Play [deploy_vm_bios_lsilogicsas_e1000e] *********\n2021-12-07 08:58:23,007 | TASK [Create a new VM 'test_vm' on server '10.187.105.78'] *\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_70U3_LSILogicSAS_BIOS/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\n2021-12-07 08:58:53,007 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_70U3_LSILogicSAS_BIOS/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "parameter_not_correct", "processed": "timestamp failed at play deploy vm bios lsi logic sas timestamp task create a new vm test vm on server ip address fatal localhost failed failed to create a virtual machine a specified parameter was not correct configuration spec guest id timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1364, "name": "__Ansible_Windows_Server_LTSC_64bit_70U3_PVSCSI_EFI-2_logs_failed_tasks_log.0", "raw": "2021-12-08 09:20:40,008 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2021-12-08 09:20:40,008 | TASK [Create a new VM 'test_vm' on server '10.182.12.168'] *\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_70U3_PVSCSI_EFI/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\n2021-12-08 09:21:10,008 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_70U3_PVSCSI_EFI/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "parameter_not_correct", "processed": "timestamp failed at play deploy vm efi para virtual timestamp task create a new vm test vm on server ip address fatal localhost failed failed to create a virtual machine a specified parameter was not correct configuration spec guest id timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1365, "name": "__Ansible_Windows_Server_LTSC_64bit_Main_NVMe_BIOS-2_logs_failed_tasks_log.0", "raw": "2021-12-08 09:23:41,008 | Failed at Play [deploy_vm_bios_nvme_e1000e] ****************\n2021-12-08 09:23:41,008 | TASK [Create a new VM 'test_vm' on server '10.184.107.211'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_Main_NVMe_BIOS/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\n2021-12-08 09:24:13,008 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_Main_NVMe_BIOS/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "parameter_not_correct", "processed": "timestamp failed at play deploy vm bios nvme timestamp task create a new vm test vm on server ip address fatal localhost failed failed to create a virtual machine a specified parameter was not correct configuration spec guest id timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1366, "name": "__Ansible_Photon_4_x_67U3-2_logs_failed_tasks_log.0", "raw": "2021-11-26 11:40:30,026 | Failed at Play [deploy_vmwarephoton_ova] *******************\n2021-11-26 11:40:30,026 | TASK [include_tasks] ***************************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_67U3/ansible-vsphere-gos-validation/linux/deploy_vm/photon/reconfigure_photon_vm.yml:10\nfatal: [localhost]: FAILED! => The conditional check 'guest_id == \"vmwarePhoton64Guest\"' failed. The error was: error while evaluating conditional (guest_id == \"vmwarePhoton64Guest\"): 'guest_id' is undefined\nThe error appears to be in '/home/worker/workspace/Ansible_Photon_4.x_67U3/ansible-vsphere-gos-validation/linux/deploy_vm/photon/reconfigure_photon_vm.yml': line 10, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n# Photon OS OVA for HWv11 is using other3xlinux64 as guest id\n- include_tasks: ../../../common/vm_set_guest_id.yml\n  ^ here\n2021-11-26 11:41:05,026 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Photon_4.x_67U3/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vmwarephoton_ova", "category": "script_here_issue", "processed": "timestamp failed at play deploy vmware photon ova timestamp task include tasks fatal localhost failed the conditional check guest id vmware photon number guest failed the error was error while evaluating conditional guest id vmware photon number guest guest id is un defined the error appears to be in home worker workspace ansible photon number x ansible vsphere gos validation linux deploy vm photon re configure photon vm yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vmware photon ova", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1367, "name": "__Ansible_Photon_4_x_67GA-2_logs_failed_tasks_log.0", "raw": "2021-11-26 11:40:02,026 | Failed at Play [deploy_vmwarephoton_ova] *******************\n2021-11-26 11:40:02,026 | TASK [include_tasks] ***************************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/linux/deploy_vm/photon/reconfigure_photon_vm.yml:10\nfatal: [localhost]: FAILED! => The conditional check 'guest_id == \"vmwarePhoton64Guest\"' failed. The error was: error while evaluating conditional (guest_id == \"vmwarePhoton64Guest\"): 'guest_id' is undefined\nThe error appears to be in '/home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/linux/deploy_vm/photon/reconfigure_photon_vm.yml': line 10, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n# Photon OS OVA for HWv11 is using other3xlinux64 as guest id\n- include_tasks: ../../../common/vm_set_guest_id.yml\n  ^ here\n2021-11-26 11:40:38,026 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vmwarephoton_ova", "category": "script_here_issue", "processed": "timestamp failed at play deploy vmware photon ova timestamp task include tasks fatal localhost failed the conditional check guest id vmware photon number guest failed the error was error while evaluating conditional guest id vmware photon number guest guest id is un defined the error appears to be in home worker workspace ansible photon number x number ga ansible vsphere gos validation linux deploy vm photon re configure photon vm yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vmware photon ova", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1368, "name": "__Ansible_Regression_Windows_10_32-11_logs_failed_tasks_log.0", "raw": "2021-12-03 08:23:32,003 | Failed at Play [deploy_vm_efi_nvme_e1000e] *****************\n2021-12-03 08:23:32,003 | TASK [Guest winrm should be connectable] *******************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_check_winrm.yml:29\nfatal: [localhost]: FAILED! => Guest winrm is not connectable in 1800 seconds.\n2021-12-03 08:24:42,003 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_32/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "guest_winrm", "processed": "timestamp failed at play deploy vm efi nvme timestamp task guest win rm should be connectable fatal localhost failed guest win rm is not connectable in number seconds timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1369, "name": "__Ansible_Autoinstall_Windows_10_32-8_logs_failed_tasks_log.0", "raw": "2021-11-29 06:50:16,029 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2021-11-29 06:50:16,029 | TASK [Guest winrm should be connectable] *******************\ntask path: /home/worker/workspace/Ansible_Autoinstall_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_check_winrm.yml:29\nfatal: [localhost]: FAILED! => Guest winrm is not connectable in 1800 seconds.\n2021-11-29 06:50:52,029 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Autoinstall_Windows_10_32/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "guest_winrm", "processed": "timestamp failed at play deploy vm efi para virtual timestamp task guest win rm should be connectable fatal localhost failed guest win rm is not connectable in number seconds timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1373, "name": "__Ansible_Regression_Windows_Server_LTSC_vNext-22_logs_failed_tasks_log.1", "raw": "2021-12-02 17:46:06,002 | Failed at Play [mouse_driver_vmtools] **********************\n2021-12-02 17:46:06,002 | TASK [Wait for getting VM 'test_vm' IP address on ESXi '10.186.134.230'] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/common/vm_get_ip_esxcli.yml:51\nfatal: [localhost]: UNREACHABLE! => Failed to create temporary directory.In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /vmfs/volumes/datastore2 `\"&& mkdir \"` echo /vmfs/volumes/datastore2/ansible-tmp-1638467165.8364687-9162-114504268753598 `\" && echo ansible-tmp-1638467165.8364687-9162-114504268753598=\"` echo /vmfs/volumes/datastore2/ansible-tmp-1638467165.8364687-9162-114504268753598 `\" ), exited with result 1", "category": "origin", "processed": "timestamp failed at play mouse driver vm tools timestamp task wait for getting vm test vm ip address on esxi ip address fatal localhost un reachable failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo vmfs volumes data store number mkdir echo vmfs volumes data store number ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo vmfs volumes data store number ansible tmp hex id number timestamp exited with result number", "solution": "retry", "target": "infra", "version": 202205240000}, {"id": 1374, "name": "__Ansible_Autoinstall_RHEL_8_x-15_logs_failed_tasks_log.0", "raw": "\n\n2021-12-07 02:18:44,007 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2021-12-07 02:18:44,007 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_Autoinstall_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.185.240.35:22\n2021-12-07 02:19:25,007 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Autoinstall_RHEL_8.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_paravirtual_vmxnet3\n", "category": "origin", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202205240000}, {"id": 1375, "name": "__Ansible_Cycle_Windows_Server_LTSC_vNext-7_logs_failed_tasks_log.1", "raw": "2021-11-30 11:45:19,030 | Failed at Play [check_ip_address] **************************\n2021-11-30 11:45:19,030 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nrequests.exceptions.HTTPError: 500 Server Error:  for url: https://10.191.158.93:5986/wsman\nwinrm.exceptions.WinRMTransportError: Bad HTTP response returned from server. Code 500\nwinrm.exceptions.WinRMError: The WS-Management service cannot process the request because the request contained invalid selectors for the resource.  (extended fault data: Bad HTTP response returned from server. Code 500 at s:Sender and w:InvalidSelectors)\nrequests.exceptions.HTTPError: 500 Server Error:  for url: https://10.191.158.93:5986/wsman\nwinrm.exceptions.WinRMTransportError: Bad HTTP response returned from server. Code 500\nwinrm.exceptions.WinRMError: The WS-Management service cannot process the request because the request contained invalid selectors for the resource.  (extended fault data: Bad HTTP response returned from server. Code 500 at s:Sender and w:InvalidSelectors)\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "origin", "processed": "timestamp failed at play check ip address timestamp task execute powershell command in windows guest requests exceptions http error number server error for url https ip address wsman win rm exceptions win rm transport error bad http response returned from server code number win rm exceptions win rm error the ws management service can not process the request because the request contained invalid selectors for the resource extended fault data bad http response returned from server code number ats sender and w invalid selectors requests exceptions http error number server error for url https ip address wsman win rm exceptions win rm transport error bad http response returned from server code number win rm exceptions win rm error the ws management service can not process the request because the request contained invalid selectors for the resource extended fault data bad http response returned from server code number ats sender and w invalid selectors fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "infra", "version": 202205240000}, {"id": 1376, "name": "__Ansible_Regression_RockyLinux_8_x-2_logs_failed_tasks_log.0", "raw": "2021-11-23 11:02:10,023 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-11-23 11:02:10,023 | TASK [Assert cloud-init guest customization checks all PASS] \ntask path: /home/worker/workspace/Ansible_Regression_RockyLinux_8.x/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=timezone_success is False) => ['VM timezone is America/Los_Angeles, expected timezone is Europe/Berlin'] and timezone_success is False", "category": "timezone_issue", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task assert cloud init guest customization checks all pass failed localhost item time zone success is false vm time zone is america los angeles expected time zone is europe berlin and time zone success is false", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1377, "name": "__Ansible_Regression_Windows_10_64-9_logs_failed_tasks_log.0", "raw": "2021-11-26 02:54:46,026 | Failed at Play [secureboot_enable_disable] *****************\n2021-11-26 02:54:46,026 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_64/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm\n2021-11-26 02:54:49,026 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_64/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "unable_gather_information", "processed": "timestamp failed at play secure boot enable disable timestamp task get specified property info for vm test vm fatal localhost failed unable to gather information for non existing vm test vm timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1379, "name": "__Ansible_Regression_Windows_11_64-15_logs_failed_tasks_log.1", "raw": "2021-12-02 10:22:24,002 | Failed at Play [memory_hot_add_basic] **********************\n2021-12-02 10:22:24,002 | TASK [Wait for VM power status to 'poweredOff'] ************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/common/vm_wait_power_state.yml:11\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "failed_hardware_config_only", "processed": "timestamp failed at play memory hot add basic timestamp task wait for vm power status to powered off fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1381, "name": "__Ansible_Regression_Photon_3_0_ISO-7_logs_failed_tasks_log.0", "raw": "2021-12-03 08:27:26,003 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2021-12-03 08:27:26,003 | TASK [Wait for VMware Tools collecting guest OS fullname] **\ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/common/vm_wait_guest_fullname.yml:17\nfatal: [localhost]: FAILED! => hardware configuration table\n2021-12-03 08:29:03,003 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "failed_hardware_config_only", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task wait for vmware tools collecting guest os full name fatal localhost failed hardware configuration table timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1382, "name": "__Ansible_Cycle_Windows_10_64-6_logs_failed_tasks_log.2", "raw": "2021-11-30 12:28:40,030 | Failed at Play [gosc_sanity_dhcp] **************************\n2021-11-30 12:28:40,030 | TASK [Wait for VM power status to 'poweredOff'] ************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/vm_wait_power_state.yml:11\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "failed_hardware_config_only", "processed": "timestamp failed at play go sc sanity dhcp timestamp task wait for vm power status to powered off fatal localhost failed hardware configuration table", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1383, "name": "__Ansible_Cycle_Windows_Server_LTSC_vNext-5_logs_failed_tasks_log.0", "raw": "2021-11-30 08:20:40,030 | Failed at Play [check_efi_firmware] ************************\n2021-11-30 08:20:40,030 | TASK [Get firmware type in Windows guest OS] ***************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_get_firmware.yml:5\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play check efi firmware timestamp task get firmware type in windows guest os connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1384, "name": "__Ansible_Cycle_Windows_Server_LTSC_vNext-5_logs_failed_tasks_log.1", "raw": "2021-11-30 08:22:24,030 | Failed at Play [check_ip_address] **************************\n2021-11-30 08:22:24,030 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => basic: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play check ip address timestamp task execute powershell command in windows guest fatal localhost un reachable basic connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1412, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_8_x_65U3-6_logs_failed_tasks_log.2", "raw": "2022-01-20 11:10:49,020 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-01-20 11:10:49,020 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:89\nfatal: [localhost]: FAILED! => ['Not found GOSC completed state keyword in vmware.log']", "category": "gosc_keyword_not_found", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task gos customization failed fatal localhost failed not found go sc completed state keyword in vmware log", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1385, "name": "__Ansible_Cycle_Windows_10_64-5_logs_failed_tasks_log.0", "raw": "2021-11-30 10:10:50,030 | Failed at Play [sata_vhba_device_ops] **********************\n2021-11-30 10:10:50,030 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play sata v hba device ops timestamp task execute powershell command in windows guest connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1386, "name": "__Ansible_Cycle_Windows_11_64-7_logs_failed_tasks_log.1", "raw": "2021-12-05 15:45:59,005 | Failed at Play [cpu_multicores_per_socket] *****************\n2021-12-05 15:45:59,005 | TASK [Shutdown guest OS inside OS] *************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_shutdown_restart.yml:8\nfatal: [localhost]: UNREACHABLE! => basic: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "connectionreset", "processed": "timestamp failed at play cpu multi cores per socket timestamp task shutdown guest os inside os fatal localhost un reachable basic connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1387, "name": "__Ansible_Cycle_Windows_11_64-7_logs_failed_tasks_log.0", "raw": "2021-12-05 14:21:41,005 | Failed at Play [nvme_vhba_device_ops] **********************\n2021-12-05 14:21:41,005 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play nvme v hba device ops timestamp task execute powershell command in windows guest connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1388, "name": "__Ansible_Regression_Windows_Server_LTSC_vNext-10_logs_failed_tasks_log.5", "raw": "2021-11-25 13:41:31,025 | Failed at Play [wintools_uninstall_verify] *****************\n2021-11-25 13:41:31,025 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play win tools un install verify timestamp task execute powershell command in windows guest connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1389, "name": "__Ansible_Cycle_Windows_Server_LTSC_vNext-7_logs_failed_tasks_log.3", "raw": "2021-11-30 13:21:42,030 | Failed at Play [check_quiesce_snapshot] ********************\n2021-11-30 13:21:42,030 | TASK [Copy file from local to Windows guest] ***************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_copy_file_from_local.yml:9\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play check quiesce snapshot timestamp task copy file from local to windows guest connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1390, "name": "__Ansible_Regression_Windows_Server_LTSC_vNext-10_logs_failed_tasks_log.2", "raw": "2021-11-25 12:18:12,025 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2021-11-25 12:18:12,025 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task execute powershell command in windows guest connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1413, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_8_x_65U3-6_logs_failed_tasks_log.3", "raw": "2022-01-20 11:36:17,020 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-01-20 11:36:17,020 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:89\nfatal: [localhost]: FAILED! => ['Not found GOSC completed state keyword in vmware.log']", "category": "gosc_keyword_not_found", "processed": "timestamp failed at play go sc cloud init static ip timestamp task gos customization failed fatal localhost failed not found go sc completed state keyword in vmware log", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1391, "name": "__Ansible_Cycle_Windows_Server_LTSC_vNext-7_logs_failed_tasks_log.2", "raw": "2021-11-30 11:51:21,030 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2021-11-30 11:51:21,030 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play para virtual v hba device ops timestamp task execute powershell command in windows guest connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1392, "name": "__Ansible_Regression_Windows_Server_LTSC_vNext-24_logs_failed_tasks_log.1", "raw": "2021-12-06 11:34:39,006 | Failed at Play [memory_hot_add_basic] **********************\n2021-12-06 11:34:39,006 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play memory hot add basic timestamp task execute powershell command in windows guest connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1427, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_67U3-4_logs_failed_tasks_log.0", "raw": "2021-12-20 14:54:08,020 | Failed at Play [ovt_verify_install] ************************\n2021-12-20 14:54:08,020 | TASK [Add zypper repository SLED-15.4-dvd-cdrom] ***********\ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_67U3/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.168.123.242]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd cdrom fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1393, "name": "__Ansible_Autoinstall_Windows_11_64-10_logs_failed_tasks_log.0", "raw": "2021-11-30 10:24:07,030 | Failed at Play [secureboot_enable_disable] *****************\n2021-11-30 10:24:07,030 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Autoinstall_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.\n", "category": "connectionreset", "processed": "timestamp failed at play secure boot enable disable timestamp task execute powershell command in windows guest connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1394, "name": "__Ansible_Cycle_Windows_10_64-6_logs_failed_tasks_log.0", "raw": "2021-11-30 11:43:17,030 | Failed at Play [sata_vhba_device_ops] **********************\n2021-11-30 11:43:17,030 | TASK [Get VMware tools version and build number in guest OS] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_get_vmtools_version_build.yml:13\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play sata v hba device ops timestamp task get vmware tools version and build number in guest os connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1395, "name": "__Ansible_Cycle_Windows_10_64-6_logs_failed_tasks_log.1", "raw": "2021-11-30 11:45:22,030 | Failed at Play [memory_hot_add_basic] **********************\n2021-11-30 11:45:22,030 | TASK [Get VMware tools version and build number in guest OS] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_get_vmtools_version_build.yml:13\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play memory hot add basic timestamp task get vmware tools version and build number in guest os connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1396, "name": "__Ansible_Cycle_Windows_Server_LTSC_vNext-6_logs_failed_tasks_log.0", "raw": "2021-11-30 10:42:50,030 | Failed at Play [check_ip_address] **************************\n2021-11-30 10:42:50,030 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play check ip address timestamp task execute powershell command in windows guest connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1397, "name": "__Ansible_Regression_Windows_Server_LTSC_vNext-26_logs_failed_tasks_log.0", "raw": "2021-12-08 09:40:54,008 | Failed at Play [check_quiesce_snapshot] ********************\n2021-12-08 09:40:54,008 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play check quiesce snapshot timestamp task execute powershell command in windows guest connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1398, "name": "__Ansible_Cycle_Windows_Server_LTSC_vNext-7_logs_failed_tasks_log.0", "raw": "2021-11-30 11:41:47,030 | Failed at Play [check_efi_firmware] ************************\n2021-11-30 11:41:47,030 | TASK [Get VMware tools version and build number in guest OS] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_get_vmtools_version_build.yml:13\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play check efi firmware timestamp task get vmware tools version and build number in guest os connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1399, "name": "__Ansible_Cycle_Windows_10_64-7_logs_failed_tasks_log.0", "raw": "2021-12-05 13:40:41,005 | Failed at Play [check_os_fullname] *************************\n2021-12-05 13:40:41,005 | TASK [Get OS name in Windows guest OS] *********************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_get_fullname.yml:5\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play check os full name timestamp task get os name in windows guest os connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1400, "name": "__Ansible_Cycle_Windows_11_64-10_logs_failed_tasks_log.1", "raw": "2021-12-06 08:57:00,006 | Failed at Play [check_quiesce_snapshot] ********************\n2021-12-06 08:57:00,006 | TASK [Get relative time value from 01/01/1970 in Windows guest OS] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_get_time.yml:5\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "connectionreset", "processed": "timestamp failed at play check quiesce snapshot timestamp task get relative time value from number in windows guest os connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1403, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_ISO-85_logs_failed_tasks_log.0", "raw": "2022-01-14 16:21:16,014 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2022-01-14 16:21:16,014 | TASK [Upload local file to ESXi datastore] *****************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/common/esxi_upload_datastore_file.yml:10\nexception in /vsphere_copy.py when main in /request.py when http_error_default\nfatal: [localhost]: FAILED! => HTTP Error 404: Not Found\n2022-01-14 16:21:23,014 | TASK [Set VM power state to 'powered-off'] *****************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:34\nfatal: [localhost]: FAILED! => Unable to communicate with the remote host, since it is disconnected.", "category": "unable_to_communicate", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task upload local file to esxi data store exception in vsphere copy python when main in request python when http error default fatal localhost failed http error number not found timestamp task set vm power state to powered off fatal localhost failed unable to communicate with the remote host since it is disconnected", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1404, "name": "ansible_gosv_logs_FAILURE_Ansible_Photon_4_x_67GA-16_logs_failed_tasks_log.0", "raw": "2022-01-21 04:43:18,021 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-21 04:43:18,021 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:108\nfatal: [localhost]: FAILED! => [\"VM DHCP IPv4 gateway is '', expected IPv4 gateway should not be empty\"]", "category": "gateway_empty", "processed": "timestamp failed at play go sc perl dhcp timestamp task gos customization failed fatal localhost failed vm dhcp ip v number gateway is expected ip v number gateway should not be empty", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1405, "name": "ansible_gosv_logs_FAILURE_Ansible_Photon_4_x_67GA-16_logs_failed_tasks_log.2", "raw": "2022-01-21 04:54:29,021 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-01-21 04:54:29,021 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:108\nfatal: [localhost]: FAILED! => [\"VM DHCP IPv4 gateway is '', expected IPv4 gateway should not be empty\"]", "category": "gateway_empty", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task gos customization failed fatal localhost failed vm dhcp ip v number gateway is expected ip v number gateway should not be empty", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1406, "name": "ansible_gosv_logs_FAILURE_Ansible_CentOS_8_x_65U3-5_logs_failed_tasks_log.0", "raw": "2021-12-10 14:54:27,010 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-10 14:54:27,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=gosc_state_keyword_found is False) => ['Not found GOSC completed state keyword in vmware.log'] and gosc_state_keyword_found is False\n", "category": "gosc_keyword_not_found", "processed": "timestamp failed at play go sc perl dhcp timestamp task assert perl guest customization checks all pass failed localhost item go sc state keyword found is false not found go sc completed state keyword in vmware log and go sc state keyword found is false", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1407, "name": "ansible_gosv_logs_FAILURE_Ansible_CentOS_8_x_65U3-5_logs_failed_tasks_log.1", "raw": "2021-12-10 15:19:08,010 | Failed at Play [gosc_perl_staticip] ************************\n2021-12-10 15:19:08,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=gosc_state_keyword_found is False) => ['Not found GOSC completed state keyword in vmware.log'] and gosc_state_keyword_found is False", "category": "gosc_keyword_not_found", "processed": "timestamp failed at play go sc perl static ip timestamp task assert perl guest customization checks all pass failed localhost item go sc state keyword found is false not found go sc completed state keyword in vmware log and go sc state keyword found is false", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1408, "name": "ansible_gosv_logs_FAILURE_Ansible_CentOS_8_x_65U3-5_logs_failed_tasks_log.2", "raw": "2021-12-10 15:45:07,010 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-10 15:45:07,010 | TASK [Assert cloud-init guest customization checks all PASS] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=gosc_state_keyword_found is False) => ['Not found GOSC completed state keyword in vmware.log'] and gosc_state_keyword_found is False", "category": "gosc_keyword_not_found", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task assert cloud init guest customization checks all pass failed localhost item go sc state keyword found is false not found go sc completed state keyword in vmware log and go sc state keyword found is false", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1409, "name": "ansible_gosv_logs_FAILURE_Ansible_CentOS_8_x_65U3-5_logs_failed_tasks_log.3", "raw": "2021-12-10 16:10:37,010 | Failed at Play [gosc_cloudinit_staticip] *******************\n2021-12-10 16:10:37,010 | TASK [Assert cloud-init guest customization checks all PASS] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=gosc_state_keyword_found is False) => ['Not found GOSC completed state keyword in vmware.log'] and gosc_state_keyword_found is False", "category": "gosc_keyword_not_found", "processed": "timestamp failed at play go sc cloud init static ip timestamp task assert cloud init guest customization checks all pass failed localhost item go sc state keyword found is false not found go sc completed state keyword in vmware log and go sc state keyword found is false", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1410, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_8_x_65U3-6_logs_failed_tasks_log.0", "raw": "2022-01-20 10:21:26,020 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-20 10:21:26,020 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:89\nfatal: [localhost]: FAILED! => ['Not found GOSC completed state keyword in vmware.log']", "category": "gosc_keyword_not_found", "processed": "timestamp failed at play go sc perl dhcp timestamp task gos customization failed fatal localhost failed not found go sc completed state keyword in vmware log", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1411, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_8_x_65U3-6_logs_failed_tasks_log.1", "raw": "2022-01-20 10:45:42,020 | Failed at Play [gosc_perl_staticip] ************************\n2022-01-20 10:45:42,020 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:89\nfatal: [localhost]: FAILED! => ['Not found GOSC completed state keyword in vmware.log']", "category": "gosc_keyword_not_found", "processed": "timestamp failed at play go sc perl static ip timestamp task gos customization failed fatal localhost failed not found go sc completed state keyword in vmware log", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1414, "name": "ansible_gosv_logs_ABORTED_Ansible_Ubuntu_Desktop_ISO_70GA-8_logs_failed_tasks_log.0", "raw": "2021-12-17 11:48:05,017 | Failed at Play [ovt_verify_install] ************************\n2021-12-17 11:48:05,017 | TASK [Set VM power state to 'powered-on'] ******************\ntask path: /home/worker/workspace/Ansible_Ubuntu_Desktop_ISO_70GA/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:34\nfatal: [localhost]: FAILED! => The serial port output file \"serial.log\" already exists. Do you want to replace it with new content or append new content to the end of the file?", "category": "file_already_exist", "processed": "timestamp failed at play ovt verify install timestamp task set vm power state to powered on fatal localhost failed the serial port output file serial log already exists do you want to replace it with new content or append new content to the end of the file", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1415, "name": "ansible_gosv_logs_FAILURE_Ansible_CentOS_8_x_65U3-11_logs_failed_tasks_log.0", "raw": "2022-01-24 08:20:46,024 | Failed at Play [deploy_vm_bios_nvme_e1000e] ****************\n2022-01-24 08:20:46,024 | TASK [Set VM power state to 'powered-on'] ******************\ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:34\nfatal: [localhost]: FAILED! => The serial port output file \"serial-20220124075631.log\" already exists. Do you want to replace it with new content or append new content to the end of the file?\n2022-01-24 08:26:32,024 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_nvme_e1000e", "category": "file_already_exist", "processed": "timestamp failed at play deploy vm bios nvme timestamp task set vm power state to powered on fatal localhost failed the serial port output file serial timestamp log already exists do you want to replace it with new content or append new content to the end of the file timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios nvme e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1424, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-3_logs_failed_tasks_log.0", "raw": "2021-12-20 11:27:13,020 | Failed at Play [ovt_verify_install] ************************\n2021-12-20 11:27:13,020 | TASK [Add zypper repository SLED-15.4-dvd-Module-Live-Patching] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.184.94.66]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd module live patching fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1416, "name": "ansible_gosv_logs_FAILURE_Ansible_OracleLinux_8_x_67U1-11_logs_failed_tasks_log.0", "raw": "2022-01-24 08:21:26,024 | Failed at Play [deploy_vm_efi_lsilogic_e1000e] *************\n2022-01-24 08:21:26,024 | TASK [Set VM power state to 'powered-on'] ******************\ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_67U1/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:34\nfatal: [localhost]: FAILED! => The serial port output file \"serial-20220124074909.log\" already exists. Do you want to replace it with new content or append new content to the end of the file?\n2022-01-24 08:27:38,024 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_67U1/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_lsilogic_e1000e", "category": "file_already_exist", "processed": "timestamp failed at play deploy vm efi lsi logic timestamp task set vm power state to powered on fatal localhost failed the serial port output file serial timestamp log already exists do you want to replace it with new content or append new content to the end of the file timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi lsi logic e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1417, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_8_x_65U3-11_logs_failed_tasks_log.0", "raw": "2022-01-24 09:38:34,024 | Failed at Play [deploy_vm_efi_sata_e1000e] *****************\n2022-01-24 09:38:34,024 | TASK [Set VM power state to 'powered-on'] ******************\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_65U3/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:34\nfatal: [localhost]: FAILED! => The serial port output file \"serial-20220124091447.log\" already exists. Do you want to replace it with new content or append new content to the end of the file?\n2022-01-24 09:43:53,024 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_RHEL_8.x_65U3/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_sata_e1000e", "category": "file_already_exist", "processed": "timestamp failed at play deploy vm efi sata timestamp task set vm power state to powered on fatal localhost failed the serial port output file serial timestamp log already exists do you want to replace it with new content or append new content to the end of the file timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi sata e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1418, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Windows_Server_LTSC_vNext-30_logs_failed_tasks_log.2", "raw": "2022-01-04 14:36:57,004 | Failed at Play [check_quiesce_snapshot] ********************\n2022-01-04 14:36:57,004 | TASK [Check specified file status until it exists in Windows guest] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/utils/win_wait_file_exist.yml:8\nfatal: [localhost -> 10.185.248.27]: FAILED! => {\n    \"attempts\": 40,\n    \"changed\": false,\n    \"invocation\": {\n        \"module_args\": {\n            \"checksum_algorithm\": \"sha1\",\n            \"follow\": false,\n            \"get_checksum\": true,\n            \"path\": \"C:\\\\test_pre_freeze.txt\"\n        }\n    },\n    \"stat\": {\n        \"exists\": false\n    }\n}\n2022-01-04 14:37:26,004 | TASK [Get vmware.log from ESXi host] ***********************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/check_quiesce_snapshot/collect_vss_logs.yml:5\nfatal: [localhost]: FAILED! => failed to transfer file to /vmfs/volumes/datastore2/test_vm//vmware.log /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/logs/test_vm/2022-01-04-12-12-28/check_quiesce_snapshot/vmware.log:\ndd: can't open '/vmfs/volumes/datastore2/test_vm//vmware.log': Operation not permitted", "category": "operation_not_permitted", "processed": "timestamp failed at play check quiesce snapshot timestamp task check specified file status until it exists in windows guest fatal localhost ip address failed timestamp task get vmware log from esxi host fatal localhost failed failed to transfer file to vmfs volumes data store number test vm vmware log home worker workspace ansible regression windows server lts c v next ansible vsphere gos validation logs test vm timestamp number check quiesce snapshot vmware log dd can t open vmfs volumes data store number test vm vmware log operation not permitted", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1419, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-12_logs_failed_tasks_log.0", "raw": "2021-12-21 17:02:39,021 | Failed at Play [ovt_verify_install] ************************\n2021-12-21 17:02:39,021 | TASK [Add zypper repository SLED-15.4-dvd-Module-Web-Scripting] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:97\nfatal: [localhost -> 10.186.140.235]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd module web scripting fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1420, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-13_logs_failed_tasks_log.0", "raw": "2021-12-22 03:43:25,022 | Failed at Play [ovt_verify_install] ************************\n2021-12-22 03:43:25,022 | TASK [Add zypper repository SLED-15.4-dvd-Module-SAP-Applications] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.184.79.142]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd module sap applications fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1421, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-14_logs_failed_tasks_log.0", "raw": "2021-12-22 06:08:35,022 | Failed at Play [ovt_verify_install] ************************\n2021-12-22 06:08:35,022 | TASK [Add zypper repository SLED-15.4-dvd-Module-Server-Applications] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.184.79.142]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd module server applications fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1422, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-16_logs_failed_tasks_log.0", "raw": "2021-12-22 11:08:38,022 | Failed at Play [ovt_verify_install] ************************\n2021-12-22 11:08:38,022 | TASK [Add zypper repository SLED-15.4-dvd-Module-HPC] ******\ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.184.72.187]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd module h pc fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1423, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-18_logs_failed_tasks_log.0", "raw": "2021-12-22 16:26:49,022 | Failed at Play [ovt_verify_install] ************************\n2021-12-22 16:26:49,022 | TASK [Uninstall package on SLED 15.4] **********************\ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nfatal: [localhost -> 10.186.105.31]: FAILED! => Zypper run command failed with return code 7. when /usr/bin/zypper", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task un install package on sled number fatal localhost ip address failed zypper run command failed with return code number when user bin zypper", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1425, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-5_logs_failed_tasks_log.0", "raw": "2021-12-21 11:46:50,021 | Failed at Play [ovt_verify_install] ************************\n2021-12-21 11:46:50,021 | TASK [Add zypper repository SLED-15.4-dvd-Module-Legacy] ***\ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.185.232.155]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd module legacy fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1426, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_67GA-10_logs_failed_tasks_log.0", "raw": "2021-12-22 11:33:21,022 | Failed at Play [ovt_verify_install] ************************\n2021-12-22 11:33:21,022 | TASK [Add zypper repository SLED-15.4-dvd-Module-Basesystem] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_67GA/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.182.130.171]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd module base system fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1428, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_70GA-11_logs_failed_tasks_log.0", "raw": "2021-12-22 11:51:32,022 | Failed at Play [ovt_verify_install] ************************\n2021-12-22 11:51:32,022 | TASK [Add zypper repository SLED-15.4-dvd-Module-Public-Cloud] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70GA/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.78.228.205]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd module public cloud fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1429, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_70GA-12_logs_failed_tasks_log.0", "raw": "2021-12-22 12:21:14,022 | Failed at Play [ovt_verify_install] ************************\n2021-12-22 12:21:14,022 | TASK [Add zypper repository SLED-15.4-dvd-Module-Transactional-Server] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70GA/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.78.228.205]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd module transactional server fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1430, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_70GA-4_logs_failed_tasks_log.0", "raw": "2021-12-20 14:54:10,020 | Failed at Play [ovt_verify_install] ************************\n2021-12-20 14:54:10,020 | TASK [Add zypper repository SLED-15.4-dvd-Product-SLES] ****\ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70GA/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.78.85.182]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd product sles fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1431, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_70U1-3_logs_failed_tasks_log.0", "raw": "2021-12-20 11:24:57,020 | Failed at Play [ovt_verify_install] ************************\n2021-12-20 11:24:57,020 | TASK [Add zypper repository SLED-15.4-dvd-Module-Desktop-Applications] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70U1/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.168.173.121]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd module desktop applications fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1432, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_70U3-12_logs_failed_tasks_log.0", "raw": "2021-12-21 13:19:19,021 | Failed at Play [ovt_verify_install] ************************\n2021-12-21 13:19:19,021 | TASK [Add zypper repository SLED-15.4-dvd-Product-SLED] ****\ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70U3/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.182.8.249]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd product sled fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1433, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_70U3-17_logs_failed_tasks_log.0", "raw": "2021-12-22 12:02:35,022 | Failed at Play [ovt_verify_install] ************************\n2021-12-22 12:02:35,022 | TASK [Add zypper repository SLED-15.4-dvd-Product-SLES_SAP] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70U3/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.187.117.192]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd product sles sap fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1434, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_70U3-8_logs_failed_tasks_log.0", "raw": "2021-12-20 12:13:07,020 | Failed at Play [ovt_verify_install] ************************\n2021-12-20 12:13:07,020 | TASK [Add zypper repository SLED-15.4-dvd-Product-WE] ******\ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70U3/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nfatal: [localhost -> 10.184.84.180]: FAILED! => Zypper failed with rc 7", "category": "zypper_failed", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd product we fatal localhost ip address failed zypper failed with rc number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1457, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_RHEL_7_x-20_logs_failed_tasks_log.0", "raw": "2021-12-21 11:34:52,021 | Failed at Play [env_setup] *********************************\n2021-12-21 11:34:52,021 | TASK [Assert VM network adapters are in different port groups] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_7.x/ansible-vsphere-gos-validation/env_setup/check_vm_settings.yml:23\nfatal: [localhost]: FAILED! => Two or more VM network adapters are in same port group, which is not supported.\n", "category": "same_port_group", "processed": "timestamp failed at play environment setup timestamp task assert vm network adapters are in different port groups fatal localhost failed two or more vm network adapters are in same port group which is not supported", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1435, "name": "ansible_gosv_logs_FAILURE_Ansible_Photon_4_x_67GA-25_logs_failed_tasks_log.0", "raw": "2022-01-24 08:23:55,024 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-24 08:23:55,024 | TASK [Get guest FQDN] **************************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/linux/guest_customization/check_hostname_and_domain.yml:31\nfatal: [localhost]: FAILED! => An unhandled exception occurred while running the lookup plugin 'file'. Error was a <class 'ansible.errors.AnsibleError'>, original message: could not locate file in lookup: /home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/logs/test_vm/2022-01-24-08-18-37/gosc_perl_dhcphostname_f.txt. could not locate file in lookup: /home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/logs/test_vm/2022-01-24-08-18-37/gosc_perl_dhcphostname_f.txt\n2022-01-24 08:24:46,024 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case gosc_perl_dhcp", "category": "unhandled_exception_occurred", "processed": "timestamp failed at play go sc perl dhcp timestamp task get guest fqdn fatal localhost failed an un handled exception occurred while running the lookup plugin file error was a class ansible errors ansible error original message could not locate file in lookup home worker workspace ansible photon number x number ga ansible vsphere gos validation logs test vm timestamp number go sc perl dhcp hostname f text could not locate file in lookup home worker workspace ansible photon number x number ga ansible vsphere gos validation logs test vm timestamp number go sc perl dhcp hostname f text timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case go sc perl dhcp", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1449, "name": "ansible_gosv_logs_FAILURE_Ansible_Flatcar_67GA-10_logs_failed_tasks_log.1", "raw": "2021-12-20 06:00:07,020 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2021-12-20 06:00:07,020 | TASK [Wait for device list changed] ************************\ntask path: /home/worker/workspace/Ansible_Flatcar_67GA/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:97\nfatal: [localhost -> 10.187.140.58]: FAILED! =>  when lsblk", "category": "lsblk", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task wait for device list changed fatal localhost ip address failed when ls blk", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1436, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_RHEL_9_x-26_logs_failed_tasks_log.2", "raw": "2021-12-21 18:46:10,021 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-21 18:46:10,021 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.184.103.233]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 0, 1, 'n', 'on', 't', 'true', 'f', '0', 'false', 'y', 'off', '1', 'no', 'yes'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include number n on t true f number false y off number no yes", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1437, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_RHEL_9_x-26_logs_failed_tasks_log.3", "raw": "2021-12-21 18:48:29,021 | Failed at Play [gosc_cloudinit_staticip] *******************\n2021-12-21 18:48:29,021 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.184.103.233]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 'f', 1, 'y', 0, 'on', 'off', 'true', 'false', 't', 'n', '1', 'no', '0', 'yes'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init static ip timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include f number y number on off true false t n number no number yes", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1438, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_OVA-24_logs_failed_tasks_log.0", "raw": "2021-12-21 16:57:30,021 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-21 16:57:30,021 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_OVA/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.187.124.234]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 0, 1, 'yes', 'on', '0', 'no', 't', 'true', '1', 'n', 'off', 'false', 'y', 'f'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include number yes on number no t true number n off false y f", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1454, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_8_x_70U2-2_logs_failed_tasks_log.0", "raw": "2021-12-09 09:00:21,009 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-09 09:00:21,009 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_70U2/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization start event timed out.", "category": "waiting_custom_timeout", "processed": "timestamp failed at play go sc perl dhcp timestamp task customize linux guest os fatal localhost failed customization failed for detailed information see warnings waiting for customization start event timed out", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1439, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_65U3-18_logs_failed_tasks_log.3", "raw": "2021-12-22 05:38:26,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-22 05:38:26,022 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_65U3/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.78.125.176]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 0, 1, 'on', 't', 'true', 'false', 'yes', 'no', 'f', '0', 'y', 'n', '1', 'off'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include number on t true false yes no f number y n number off", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1440, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_65U3-18_logs_failed_tasks_log.4", "raw": "2021-12-22 05:40:56,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2021-12-22 05:40:56,022 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_65U3/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.78.125.176]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 'on', 1, 0, 'f', 'y', 'n', '1', 'yes', 'no', 'off', 't', 'true', 'false', '0'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init static ip timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include on number f y n number yes no off t true false number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1441, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_67GA-19_logs_failed_tasks_log.2", "raw": "2021-12-22 04:19:32,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-22 04:19:32,022 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_67GA/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.168.123.112]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 0, 1, 'false', 'on', 'yes', 't', 'true', 'f', 'no', 'n', 'off', '0', 'y', '1'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include number false on yes t true f no n off number y number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1442, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_67GA-19_logs_failed_tasks_log.3", "raw": "2021-12-22 04:21:01,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2021-12-22 04:21:01,022 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_67GA/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.168.123.112]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 0, 1, 'on', 'yes', 'y', 'true', 'false', 'off', 'no', 'f', 'n', '1', '0', 't'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init static ip timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include number on yes y true false off no f n number t", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1443, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_67U3-7_logs_failed_tasks_log.0", "raw": "2021-12-22 04:12:30,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-22 04:12:30,022 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_67U3/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.168.200.136]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 'n', 1, 0, 'true', 'yes', 't', 'false', 'f', 'off', '1', 'no', 'on', '0', 'y'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include n number true yes t false f off number no on number y", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1444, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_67U3-7_logs_failed_tasks_log.1", "raw": "2021-12-22 04:14:05,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2021-12-22 04:14:05,022 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_67U3/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.168.200.136]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 0, 1, 'f', 'n', 'off', 'false', 'true', 'no', 'on', '0', 't', 'y', 'yes', '1'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init static ip timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include number f n off false true no on number t y yes number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1445, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_70GA-7_logs_failed_tasks_log.0", "raw": "2021-12-22 04:26:47,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-22 04:26:47,022 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_70GA/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.78.186.101]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 'n', 't', 1, 0, 'true', 'yes', 'no', '0', 'false', 'on', 'y', '1', 'off', 'f'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include n t number true yes no number false on y number off f", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1446, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_70GA-7_logs_failed_tasks_log.1", "raw": "2021-12-22 04:28:17,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2021-12-22 04:28:17,022 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_70GA/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.78.186.101]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 0, '1', 1, '0', 'f', 'yes', 'no', 'n', 'true', 'on', 'false', 't', 'y', 'off'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init static ip timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include number f yes no n true on false t y off", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1447, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_70U3-10_logs_failed_tasks_log.0", "raw": "2021-12-22 04:19:24,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-22 04:19:24,022 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_70U3/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.168.191.122]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 0, 1, 'f', 'on', '0', 'true', '1', 'yes', 'y', 'false', 'off', 'n', 'no', 't'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include number f on number true number yes y false off n no t", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1448, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_70U3-10_logs_failed_tasks_log.1", "raw": "2021-12-22 04:20:28,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2021-12-22 04:20:28,022 | TASK [Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_70U3/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.168.191.122]: FAILED! => argument 'enabled' is of type <class 'str'> and we were unable to convert to bool: The value 'enabled' is not a valid boolean.  Valid booleans include: 0, 1, 'n', '1', 'f', 'false', 'on', 't', 'yes', '0', 'y', 'no', 'off', 'true'", "category": "value_is_not_valid", "processed": "timestamp failed at play go sc cloud init static ip timestamp task update service cloud init local enabled true state started fatal localhost ip address failed argument enabled is of type class string and we were unable to convert to bool the value enabled is not a valid boolean valid booleans include number n number f false on t yes number y no off true", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1450, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_11_64bit_70U3_LSILogicSAS_EFI-12_logs_failed_tasks_log.0", "raw": "2021-12-24 02:19:43,024 | Failed at Play [check_os_fullname] *************************\n2021-12-24 02:19:43,024 | TASK [Verify guest fullname in guest info is expected] *****\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_70U3_LSILogicSAS_EFI/ansible-vsphere-gos-validation/windows/check_os_fullname/check_os_fullname.yml:66\nfatal: [localhost]: FAILED! => Guest fullname in guest info: Windows 11 Enterprise, 64-bit (Build 22000.194), is not the same as expected one: Microsoft Windows 10 (64-bit).", "category": "not_the_same_one", "processed": "timestamp failed at play check os full name timestamp task verify guest full name in guest info is expected fatal localhost failed guest full name in guest info windows number enterprise number b it build number is not the same as expected one microsoft windows number b it", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1451, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_Server_LTSC_64bit_Main_LSILogicSAS_EFI-13_logs_failed_tasks_log.0", "raw": "2022-01-20 08:18:13,020 | Failed at Play [check_os_fullname] *************************\n2022-01-20 08:18:13,020 | TASK [Verify guest fullname in guest info is expected] *****\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_Main_LSILogicSAS_EFI/ansible-vsphere-gos-validation/windows/check_os_fullname/check_os_fullname.yml:66\nfatal: [localhost]: FAILED! => Guest fullname in guest info: Microsoft Windows Server 2022 (64-bit), is not the same as expected one: .", "category": "not_the_same_one", "processed": "timestamp failed at play check os full name timestamp task verify guest full name in guest info is expected fatal localhost failed guest full name in guest info microsoft windows server number b it is not the same as expected one", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1452, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_70GA-8_logs_failed_tasks_log.0", "raw": "2021-12-22 02:10:18,022 | Failed at Play [ovt_verify_install] ************************\n2021-12-22 02:10:18,022 | TASK [Update service packagekit.service, enabled: , state: stopped] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70GA/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:27\nfatal: [localhost -> 10.168.169.146]: FAILED! => Unable to stop service packagekit.service: Job for packagekit.service canceled.", "category": "unable_to_stop_service", "processed": "timestamp failed at play ovt verify install timestamp task update service package kit service enabled state stopped fatal localhost ip address failed unable to stop service package kit service job for package kit service canceled", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1453, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_ISO-79_logs_failed_tasks_log.0", "raw": "2022-01-14 09:45:04,014 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2022-01-14 09:45:04,014 | TASK [Datastore file operation] ****************************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => Failed to query for file '1642152524-seed.iso'\n2022-01-14 09:46:02,014 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "fail_to_query_file", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task data store file operation fatal localhost failed failed to query for file hex id seed iso timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1455, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_9_x_70U3-11_logs_failed_tasks_log.3", "raw": "2021-12-13 04:44:27,013 | Failed at Play [gosc_perl_staticip] ************************\n2021-12-13 04:44:27,013 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization start event timed out.\n\n\npriority.target\ndeepdive.bug(product)", "category": "waiting_custom_timeout", "processed": "timestamp failed at play go sc perl static ip timestamp task customize linux guest os fatal localhost failed customization failed for detailed information see warnings waiting for customization start event timed out", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1456, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-10_logs_failed_tasks_log.0", "raw": "2021-12-21 16:31:08,021 | Failed at Play [ovt_verify_install] ************************\n2021-12-21 16:31:08,021 | TASK [Check the value of 'expected_service_state' and 'expected_service_status'] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/utils/check_service_status.yml:23\nfatal: [localhost]: FAILED! => Invalid parameters: expected_service_state=inactive, expected_service_status=", "category": "invalid_parameters", "processed": "timestamp failed at play ovt verify install timestamp task check the value of expected service state and expected service status fatal localhost failed invalid parameters expected service state inactive expected service status", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1503, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.14", "raw": "2022-01-13 06:23:45,013 | Failed at Play [e1000e_network_device_ops] *****************\n2022-01-13 06:23:45,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play network device ops timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1458, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_Windows_Server_LTSC_vNext-13_logs_failed_tasks_log.2", "raw": "2021-12-10 08:17:19,010 | Failed at Play [check_quiesce_snapshot] ********************\n2021-12-10 08:17:19,010 | TASK [Check if get expected config in vmx] *****************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_LTSC_vNext/ansible-vsphere-gos-validation/windows/check_quiesce_snapshot/check_vmx_disk_enable_uuid.yml:12\nfatal: [localhost]: FAILED! => 'disk.EnableUUID = TRUE' is not in VM vmx file.\n", "category": "vmx_file_error", "processed": "timestamp failed at play check quiesce snapshot timestamp task check if get expected configuration in vmx fatal localhost failed disk enable uuid true is not in vm vmx file", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1494, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_11_64bit_67U3_LSILogicSAS_EFI-14_logs_failed_tasks_log.1", "raw": "2021-12-14 10:57:02,014 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2021-12-14 10:57:02,014 | TASK [Verify disk controller number increases in guest OS] *\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_67U3_LSILogicSAS_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hotadd_vm_disk_new_ctrl.yml:25\nfatal: [localhost]: FAILED! => Disk controller number not increase 1, before hotadd: 1, after hotadd: 1", "category": "disk_controller_number", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task verify disk controller number increases in guest os fatal localhost failed disk controller number not increase number before hot add number after hot add number", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1459, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_ISO-97_logs_failed_tasks_log.0", "raw": "2022-01-19 04:18:57,019 | Failed at Play [deploy_vm_efi_buslogic_vmxnet3] ************\n2022-01-19 04:18:57,019 | TASK [Set VM power state to 'powered-on'] ******************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:34\nfatal: [localhost]: FAILED! => The BusLogic SCSI adapter is not supported for 64-bit guests. See the documentation for the appropriate type of SCSI adapter to use with 64-bit guests. \n2022-01-19 04:19:00,019 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_buslogic_vmxnet3", "category": "adapter_not_support", "processed": "timestamp failed at play deploy vm efi bus logic vmxnet number timestamp task set vm power state to powered on fatal localhost failed the bus logic scsi adapter is not supported for number b it guests see the documentation for the appropriate type of scsi adapter to use with number b it guests timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi bus logic vmxnet number", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1460, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.1", "raw": "2022-01-17 04:30:49,017 | Failed at Play [cpu_hot_add_basic] *************************\n2022-01-17 04:30:49,017 | TASK [Set VM CPU number and/or cores per socket number] ****\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_set_cpu_number.yml:9\nfatal: [localhost]: FAILED! => The available Memory resources in the parent resource pool are insufficient for the operation.", "category": "resource_insufficient", "processed": "timestamp failed at play cpu hot add basic timestamp task set vm cpu number and or cores per socket number fatal localhost failed the available memory resources in the parent resource pool are insufficient for the operation", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1461, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.10", "raw": "2022-01-17 05:02:03,017 | Failed at Play [sata_vhba_device_ops] **********************\n2022-01-17 05:02:03,017 | TASK [Add new disk to VM via vmware_guest module] **********\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_hot_add_ctrl_disk.yml:28\nfatal: [localhost]: FAILED! => The available Memory resources in the parent resource pool are insufficient for the operation.", "category": "resource_insufficient", "processed": "timestamp failed at play sata v hba device ops timestamp task add new disk to vm via vmware guest module fatal localhost failed the available memory resources in the parent resource pool are insufficient for the operation", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1462, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.11", "raw": "2022-01-17 05:03:33,017 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-01-17 05:03:33,017 | TASK [Add new disk to VM via vmware_guest module] **********\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_hot_add_ctrl_disk.yml:28\nfatal: [localhost]: FAILED! => The available Memory resources in the parent resource pool are insufficient for the operation.", "category": "resource_insufficient", "processed": "timestamp failed at play nvme v hba device ops timestamp task add new disk to vm via vmware guest module fatal localhost failed the available memory resources in the parent resource pool are insufficient for the operation", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1463, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.2", "raw": "2022-01-17 04:36:15,017 | Failed at Play [memory_hot_add_basic] **********************\n2022-01-17 04:36:15,017 | TASK [Set memory size to 3072 MB] **************************\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_set_memory_size.yml:4\nfatal: [localhost]: FAILED! => The available Memory resources in the parent resource pool are insufficient for the operation.", "category": "resource_insufficient", "processed": "timestamp failed at play memory hot add basic timestamp task set memory size to number mb fatal localhost failed the available memory resources in the parent resource pool are insufficient for the operation", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1464, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.9", "raw": "2022-01-17 04:59:22,017 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-01-17 04:59:22,017 | TASK [Add new disk to VM via vmware_guest module] **********\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_hot_add_ctrl_disk.yml:28\nfatal: [localhost]: FAILED! => The available Memory resources in the parent resource pool are insufficient for the operation.", "category": "resource_insufficient", "processed": "timestamp failed at play para virtual v hba device ops timestamp task add new disk to vm via vmware guest module fatal localhost failed the available memory resources in the parent resource pool are insufficient for the operation", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1465, "name": "ansible_gosv_logs_FAILURE_Ansible_Flatcar_67GA-10_logs_failed_tasks_log.0", "raw": "2021-12-20 05:54:46,020 | Failed at Play [check_ip_address] **************************\n2021-12-20 05:54:46,020 | TASK [Check IP address lists difference] *******************\ntask path: /home/worker/workspace/Ansible_Flatcar_67GA/ansible-vsphere-gos-validation/linux/check_ip_address/validate_ip_address.yml:17\nfatal: [localhost]: FAILED! => \"VM IP addresses got in guest info: ['10.187.140.58', 'fe80::250:56ff:feb7:42ad'], expected IP addresses got in guest OS: ['10.187.140.58', 'metric', '1024', 'fe80::250:56ff:feb7:42ad']\"\n", "category": "expect_ip_address3", "processed": "timestamp failed at play check ip address timestamp task check ip address lists difference fatal localhost failed vm ip addresses got in guest info ip address ip address expected ip addresses got in guest os ip address metric number ip address", "solution": "deepdive", "target": "targetvm", "version": 202205240000}, {"id": 1466, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_RHEL_7_x-30_logs_failed_tasks_log.0", "raw": "2022-01-04 12:50:41,004 | Failed at Play [ovt_verify_status] *************************\n2022-01-04 12:50:41,004 | TASK [Check process 'vmtoolsd -n vmusr' is running] ********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_7.x/ansible-vsphere-gos-validation/linux/utils/check_process_status.yml:34\nfatal: [localhost]: FAILED! => User 'vmware' doesn't have running process 'vmtoolsd -n vmusr'\n\n\npriority.target\ndeepdive.testcase boot hang no auto login", "category": "no_running_process", "processed": "timestamp failed at play ovt verify status timestamp task check process vmtoolsd n vm user is running fatal localhost failed user vmware doesn t have running process vmtoolsd n vm user deep dive bug test case boot hang no auto login", "solution": "deepdive", "target": "targetvm", "version": 202205240000}, {"id": 1467, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_RHEL_7_x-34_logs_failed_tasks_log.0", "raw": "2022-01-12 16:23:17,012 | Failed at Play [ovt_verify_status] *************************\n2022-01-12 16:23:17,012 | TASK [Check process 'vmtoolsd -n vmusr' is running] ********\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_7.x/ansible-vsphere-gos-validation/linux/utils/check_process_status.yml:34\nfatal: [localhost]: FAILED! => User 'vmware' doesn't have running process 'vmtoolsd -n vmusr'", "category": "no_running_process", "processed": "timestamp failed at play ovt verify status timestamp task check process vmtoolsd n vm user is running fatal localhost failed user vmware doesn t have running process vmtoolsd n vm user", "solution": "deepdive", "target": "targetvm", "version": 202205240000}, {"id": 1468, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.0", "raw": "2022-01-17 04:26:27,017 | Failed at Play [check_os_fullname] *************************\n2022-01-17 04:26:27,017 | TASK [Assert Guest OS fullname is either Other 4.x or later Linux (64-bit) or Other 4.x Linux (64-bit)] \ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/linux/check_os_fullname/validate_os_fullname.yml:17\nfatal: [localhost]: FAILED! => \"VM 'test_vm' guest OS fullname is: 'AlmaLinux (64-bit)', not expected 'Other 4.x or later Linux (64-bit)' or 'Other 4.x Linux (64-bit)'\"", "category": "guestos_fullname", "processed": "timestamp failed at play check os full name timestamp task assert guest os full name is either other number x or later linux number b it or other number x linux number b it fatal localhost failed vm test vm guest os full name is alma linux number b it not expected other number x or later linux number b it or other number x linux number b it", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1469, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_RockyLinux_8_x-15_logs_failed_tasks_log.0", "raw": "2022-01-21 08:04:44,021 | Failed at Play [check_os_fullname] *************************\n2022-01-21 08:04:44,021 | TASK [Assert Guest OS fullname is either Other 4.x or later Linux (64-bit) or Other 4.x Linux (64-bit)] \ntask path: /home/worker/workspace/Ansible_Cycle_RockyLinux_8.x/ansible-vsphere-gos-validation/linux/check_os_fullname/validate_os_fullname.yml:17\nfatal: [localhost]: FAILED! => \"VM 'test_vm' guest OS fullname is: 'Rocky Linux (64-bit)', not expected 'Other 4.x or later Linux (64-bit)' or 'Other 4.x Linux (64-bit)'\"", "category": "guestos_fullname", "processed": "timestamp failed at play check os full name timestamp task assert guest os full name is either other number x or later linux number b it or other number x linux number b it fatal localhost failed vm test vm guest os full name is rocky linux number b it not expected other number x or later linux number b it or other number x linux number b it", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1470, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_RockyLinux_8_x-9_logs_failed_tasks_log.0", "raw": "2022-01-19 03:54:05,019 | Failed at Play [check_os_fullname] *************************\n2022-01-19 03:54:05,019 | TASK [Assert Guest OS fullname is either Other 4.x or later Linux (64-bit) or Other 4.x Linux (64-bit)] \ntask path: /home/worker/workspace/Ansible_Cycle_RockyLinux_8.x/ansible-vsphere-gos-validation/linux/check_os_fullname/validate_os_fullname.yml:17\nfatal: [localhost]: FAILED! => \"VM 'test_vm_1642560924961' guest OS fullname is: 'Rocky Linux (64-bit)', not expected 'Other 4.x or later Linux (64-bit)' or 'Other 4.x Linux (64-bit)'\"", "category": "guestos_fullname", "processed": "timestamp failed at play check os full name timestamp task assert guest os full name is either other number x or later linux number b it or other number x linux number b it fatal localhost failed vm test vm number guest os full name is rocky linux number b it not expected other number x or later linux number b it or other number x linux number b it", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1471, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_9_x_70GA-3_logs_failed_tasks_log.0", "raw": "2021-12-09 04:05:40,009 | Failed at Play [check_os_fullname] *************************\n2021-12-09 04:05:40,009 | TASK [Assert Guest OS fullname is Red Hat Enterprise Linux 9 (64-bit)] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70GA/ansible-vsphere-gos-validation/linux/check_os_fullname/validate_os_fullname.yml:29\nfatal: [localhost]: FAILED! => VM 'test_vm_1639020563600' guest OS fullname is: 'Red Hat Enterprise Linux 8 (64-bit)', not expected 'Red Hat Enterprise Linux 9 (64-bit)'", "category": "guestos_fullname", "processed": "timestamp failed at play check os full name timestamp task assert guest os full name is red hat enterprise linux number b it fatal localhost failed vm test vm number guest os full name is red hat enterprise linux number b it not expected red hat enterprise linux number b it", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1472, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.11", "raw": "2022-01-13 06:10:50,013 | Failed at Play [memory_hot_add_basic] **********************\n2022-01-13 06:10:50,013 | TASK [Get ESXi host specified property] ********************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/esxi_get_property.yml:8\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.185.254.254:443 : [Errno 110] Connection timed out", "category": "unknown_error", "processed": "timestamp failed at play memory hot add basic timestamp task get esxi host specified property exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1473, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.16", "raw": "2022-01-13 06:32:21,013 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-01-13 06:32:21,013 | TASK [Get ESXi host specified property] ********************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/esxi_get_property.yml:8\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.185.254.254:443 : [Errno 110] Connection timed out", "category": "unknown_error", "processed": "timestamp failed at play cpu multi cores per socket timestamp task get esxi host specified property exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1474, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.9", "raw": "2022-01-13 06:02:14,013 | Failed at Play [cpu_hot_add_basic] *************************\n2022-01-13 06:02:14,013 | TASK [Get ESXi host specified property] ********************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/esxi_get_property.yml:8\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.185.254.254:443 : [Errno 110] Connection timed out", "category": "unknown_error", "processed": "timestamp failed at play cpu hot add basic timestamp task get esxi host specified property exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1475, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-8_logs_failed_tasks_log.0", "raw": "2021-12-21 15:47:58,021 | Failed at Play [ovt_verify_install] ************************\n2021-12-21 15:47:58,021 | TASK [Check service 'packagekit.service' is disabled] ******\ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/utils/check_service_status.yml:55\nfatal: [localhost]: FAILED! => Serivce 'packagekit.service' is not disabled", "category": "is_not_disabled", "processed": "timestamp failed at play ovt verify install timestamp task check service package kit service is disabled fatal localhost failed ser i vc e package kit service is not disabled", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1476, "name": "ansible_gosv_logs_FAILURE_Ansible_RockyLinux_8_x_70U1-1_logs_failed_tasks_log.0", "raw": "2021-12-09 11:04:39,009 | Failed at Play [deploy_vm_bios_nvme_e1000e] ****************\n2021-12-09 11:04:39,009 | TASK [Set VM boot options] *********************************\ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70U1/ansible-vsphere-gos-validation/common/vm_set_boot_options.yml:4\nfatal: [localhost]: FAILED! => EFI secure boot cannot be enabled when boot_firmware = bios.  VM's boot_firmware currently set to bios\n2021-12-09 11:05:24,009 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70U1/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_nvme_e1000e", "category": "secure_boot_cannot_enable", "processed": "timestamp failed at play deploy vm bios nvme timestamp task set vm boot options fatal localhost failed efi secure boot can not be enabled when boot firmware bios vm s boot firmware currently set to bios timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios nvme e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1477, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Flatcar_OVA-23_logs_failed_tasks_log.0", "raw": "2022-01-13 15:23:52,013 | Failed at Play [check_ip_address] **************************\n2022-01-13 15:23:52,013 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Regression_Flatcar_OVA/ansible-vsphere-gos-validation/linux/check_ip_address/vm_wait_guest_all_ip.yml:58\nfatal: [localhost]: FAILED! => Failed to collect all guest OS IP addresses in guest info", "category": "fail_guest_os_ip", "processed": "timestamp failed at play check ip address timestamp task fail fatal localhost failed failed to collect all guest os ip addresses in guest info", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1478, "name": "ansible_gosv_logs_FAILURE_Ansible_CentOS_8_x_65U3-12_logs_failed_tasks_log.0", "raw": "2022-01-24 09:59:49,024 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-24 09:59:49,024 | TASK [Get guest DNS servers and search domains from /etc/resolv.conf] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/check_dns.yml:98\nfatal: [localhost]: FAILED! => Unexpected templating type error occurred on ({{ guest_resolv_conf | select('match', '^search\\\\s*') | map('regex_replace', '^search\\\\s*', '') | split() | map('regex_replace', '\\\\.$', '') }}): descriptor 'split' for 'str' objects doesn't apply to a 'list' object", "category": "unexpecting_templaing_type", "processed": "timestamp failed at play go sc perl dhcp timestamp task get guest dns servers and search domains from etc resolve configuration fatal localhost failed unexpected templating type error occurred on guest resolve configuration select match search s map regex replace search s split map regex replace descriptor split for string objects doesn t apply to a list object", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1479, "name": "ansible_gosv_logs_FAILURE_Ansible_CentOS_8_x_65U3-12_logs_failed_tasks_log.1", "raw": "2022-01-24 10:13:34,024 | Failed at Play [gosc_perl_staticip] ************************\n2022-01-24 10:13:34,024 | TASK [Get guest DNS servers and search domains from /etc/resolv.conf] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/check_dns.yml:98\nfatal: [localhost]: FAILED! => Unexpected templating type error occurred on ({{ guest_resolv_conf | select('match', '^search\\\\s*') | map('regex_replace', '^search\\\\s*', '') | split() | map('regex_replace', '\\\\.$', '') }}): descriptor 'split' for 'str' objects doesn't apply to a 'list' object", "category": "unexpecting_templaing_type", "processed": "timestamp failed at play go sc perl static ip timestamp task get guest dns servers and search domains from etc resolve configuration fatal localhost failed unexpected templating type error occurred on guest resolve configuration select match search s map regex replace search s split map regex replace descriptor split for string objects doesn t apply to a list object", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1480, "name": "ansible_gosv_logs_FAILURE_Ansible_CentOS_8_x_65U3-12_logs_failed_tasks_log.2", "raw": "2022-01-24 10:27:53,024 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-01-24 10:27:53,024 | TASK [Get guest DNS servers and search domains from /etc/resolv.conf] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/check_dns.yml:98\nfatal: [localhost]: FAILED! => Unexpected templating type error occurred on ({{ guest_resolv_conf | select('match', '^search\\\\s*') | map('regex_replace', '^search\\\\s*', '') | split() | map('regex_replace', '\\\\.$', '') }}): descriptor 'split' for 'str' objects doesn't apply to a 'list' object", "category": "unexpecting_templaing_type", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task get guest dns servers and search domains from etc resolve configuration fatal localhost failed unexpected templating type error occurred on guest resolve configuration select match search s map regex replace search s split map regex replace descriptor split for string objects doesn t apply to a list object", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1481, "name": "ansible_gosv_logs_FAILURE_Ansible_CentOS_8_x_65U3-12_logs_failed_tasks_log.3", "raw": "2022-01-24 10:42:09,024 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-01-24 10:42:09,024 | TASK [Get guest DNS servers and search domains from /etc/resolv.conf] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/check_dns.yml:98\nfatal: [localhost]: FAILED! => Unexpected templating type error occurred on ({{ guest_resolv_conf | select('match', '^search\\\\s*') | map('regex_replace', '^search\\\\s*', '') | split() | map('regex_replace', '\\\\.$', '') }}): descriptor 'split' for 'str' objects doesn't apply to a 'list' object", "category": "unexpecting_templaing_type", "processed": "timestamp failed at play go sc cloud init static ip timestamp task get guest dns servers and search domains from etc resolve configuration fatal localhost failed unexpected templating type error occurred on guest resolve configuration select match search s map regex replace search s split map regex replace descriptor split for string objects doesn t apply to a list object", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1504, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.15", "raw": "2022-01-13 06:28:03,013 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-01-13 06:28:03,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play vmxnet number network device ops timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1495, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_11_64bit_67U3_LSILogicSAS_EFI-15_logs_failed_tasks_log.2", "raw": "2021-12-14 13:35:27,014 | Failed at Play [nvme_vhba_device_ops] **********************\n2021-12-14 13:35:27,014 | TASK [Verify disk controller number increases in guest OS] *\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_67U3_LSILogicSAS_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hotadd_vm_disk_new_ctrl.yml:25\nfatal: [localhost]: FAILED! => Disk controller number not increase 1, before hotadd: 0, after hotadd: 0", "category": "disk_controller_number", "processed": "timestamp failed at play nvme v hba device ops timestamp task verify disk controller number increases in guest os fatal localhost failed disk controller number not increase number before hot add number after hot add number", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1482, "name": "ansible_gosv_logs_FAILURE_Ansible_CentOS_8_x_65U3-13_logs_failed_tasks_log.0", "raw": "2022-01-24 11:30:24,024 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-24 11:30:24,024 | TASK [Get guest DNS servers and search domains from /etc/resolv.conf] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/check_dns.yml:98\nfatal: [localhost]: FAILED! => Unexpected templating type error occurred on ({{ guest_resolv_conf | select('match', '^search\\\\s*') | map('regex_replace', '^search\\\\s*', '') | split() | map('regex_replace', '\\\\.$', '') }}): descriptor 'split' for 'str' objects doesn't apply to a 'list' object\n2022-01-24 11:35:02,024 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_65U3/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case gosc_perl_dhcp", "category": "unexpecting_templaing_type", "processed": "timestamp failed at play go sc perl dhcp timestamp task get guest dns servers and search domains from etc resolve configuration fatal localhost failed unexpected templating type error occurred on guest resolve configuration select match search s map regex replace search s split map regex replace descriptor split for string objects doesn t apply to a list object timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case go sc perl dhcp", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1483, "name": "ansible_gosv_logs_FAILURE_Ansible_OracleLinux_8_x_67U1-12_logs_failed_tasks_log.0", "raw": "2022-01-24 09:20:12,024 | Failed at Play [env_setup] *********************************\n2022-01-24 09:20:12,024 | TASK [Check VM 'test_vm_2' exists] *************************\ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_67U1/ansible-vsphere-gos-validation/env_setup/env_setup.yml:32\nfatal: [localhost]: FAILED! => VM 'test_vm_2' doesn't exist. Please set new_vm to True to deploy the VM or provide an existing VM name.", "category": "new_vm", "processed": "timestamp failed at play environment setup timestamp task check vm test vm number exists fatal localhost failed vm test vm number doesn t exist please set new vm to true to deploy the vm or provide an existing vm name", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1484, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_8_x_65U3-14_logs_failed_tasks_log.0", "raw": "2022-01-25 02:23:07,025 | Failed at Play [env_setup] *********************************\n2022-01-25 02:23:07,025 | TASK [Check VM 'test_vm-2_1643077366890' exists] ***********\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_65U3/ansible-vsphere-gos-validation/env_setup/env_setup.yml:32\nfatal: [localhost]: FAILED! => VM 'test_vm-2_1643077366890' doesn't exist. Please set new_vm to True to deploy the VM or provide an existing VM name.", "category": "new_vm", "processed": "timestamp failed at play environment setup timestamp task check vm test vm number exists fatal localhost failed vm test vm number doesn t exist please set new vm to true to deploy the vm or provide an existing vm name", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1485, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Flatcar_OVA-27_logs_failed_tasks_log.0", "raw": "2022-01-17 07:33:39,017 | Failed at Play [env_setup] *********************************\n2022-01-17 07:33:39,017 | TASK [Check VM 'test_vm' exists] ***************************\ntask path: /home/worker/workspace/Ansible_Regression_Flatcar_OVA/ansible-vsphere-gos-validation/env_setup/env_setup.yml:32\nfatal: [localhost]: FAILED! => VM 'test_vm' doesn't exist. Please set new_vm to True to deploy the VM or provide an existing VM name.", "category": "new_vm", "processed": "timestamp failed at play environment setup timestamp task check vm test vm exists fatal localhost failed vm test vm doesn t exist please set new vm to true to deploy the vm or provide an existing vm name", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1486, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_70GA-3_logs_failed_tasks_log.1", "raw": "2021-12-15 12:24:23,015 | Failed at Play [gosc_perl_staticip] ************************\n2021-12-15 12:24:23,015 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70GA/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is wdc-10-191-178-154, expected hostname is gosc-static-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is nimbus.eng.vmware.com.gosc.test.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=hwclock_success is False) => ['VM hwclockUTC is True, expected hwclockUTC is False'] and hwclock_success is False", "category": "expect_domain_name", "processed": "timestamp failed at play go sc perl static ip timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is wdc number expected hostname is go sc static vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is nimbus eng vmware com g osc test com expected domain name is go sc test com and dns domain success is false failed localhost item hw clock success is false vm hw clock utc is true expected hw clock utc is false and hw clock success is false", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1487, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_70U1-1_logs_failed_tasks_log.1", "raw": "2021-12-10 13:49:48,010 | Failed at Play [gosc_perl_staticip] ************************\n2021-12-10 13:49:48,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70U1/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is sc1-10-78-88-80, expected hostname is gosc-static-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is eng.vmware.com.gosc.test.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=hwclock_success is False) => ['VM hwclockUTC is True, expected hwclockUTC is False'] and hwclock_success is False", "category": "expect_domain_name", "processed": "timestamp failed at play go sc perl static ip timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is sc number expected hostname is go sc static vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is eng vmware com g osc test com expected domain name is go sc test com and dns domain success is false failed localhost item hw clock success is false vm hw clock utc is true expected hw clock utc is false and hw clock success is false", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1493, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_11_64bit_67U3_LSILogicSAS_EFI-14_logs_failed_tasks_log.0", "raw": "2021-12-14 10:55:08,014 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2021-12-14 10:55:08,014 | TASK [Verify disk controller number increases in guest OS] *\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_67U3_LSILogicSAS_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hotadd_vm_disk_new_ctrl.yml:25\nfatal: [localhost]: FAILED! => Disk controller number not increase 1, before hotadd: 0, after hotadd: 0", "category": "disk_controller_number", "processed": "timestamp failed at play para virtual v hba device ops timestamp task verify disk controller number increases in guest os fatal localhost failed disk controller number not increase number before hot add number after hot add number", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1488, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_70U3-1_logs_failed_tasks_log.1", "raw": "2021-12-10 13:52:31,010 | Failed at Play [gosc_perl_staticip] ************************\n2021-12-10 13:52:31,010 | TASK [Assert perl guest customization checks all PASS] *****\ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_70U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:46\nfailed: [localhost] => (item=hostname_success is False) => ['VM hostname is sc1-10-168-185-206, expected hostname is gosc-static-vm-01'] and hostname_success is False\nfailed: [localhost] => (item=dns_domain_success is False) => ['VM DNS domain name is nimbus.eng.vmware.com.gosc.test.com, expected domain name is gosc.test.com'] and dns_domain_success is False\nfailed: [localhost] => (item=hwclock_success is False) => ['VM hwclockUTC is True, expected hwclockUTC is False'] and hwclock_success is False", "category": "expect_domain_name", "processed": "timestamp failed at play go sc perl static ip timestamp task assert perl guest customization checks all pass failed localhost item hostname success is false vm hostname is sc number expected hostname is go sc static vm number and hostname success is false failed localhost item dns domain success is false vm dns domain name is nimbus eng vmware com g osc test com expected domain name is go sc test com and dns domain success is false failed localhost item hw clock success is false vm hw clock utc is true expected hw clock utc is false and hw clock success is false", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1489, "name": "ansible_gosv_logs_FAILURE_Ansible_Ubuntu_Server_ISO_main-5_logs_failed_tasks_log.0", "raw": "2021-12-17 11:07:16,017 | Failed at Play [gosc_perl_staticip] ************************\n2021-12-17 11:07:16,017 | TASK [Fetch file /tmp/hostname_f.txt from VM guest] ********\ntask path: /home/worker/workspace/Ansible_Ubuntu_Server_ISO_main/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nfatal: [localhost]: FAILED! => Failed to fetch file : HTTP Error 500: Internal Server Error", "category": "failed_to_fetch_file", "processed": "timestamp failed at play go sc perl static ip timestamp task fetch file tmp hostname f text from vm guest fatal localhost failed failed to fetch file http error number internal server error", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1490, "name": "ansible_gosv_logs_FAILURE_Ansible_Ubuntu_Desktop_ISO_70GA-11_logs_failed_tasks_log.0", "raw": "2021-12-20 09:20:33,020 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-20 09:20:33,020 | TASK [Install package on Ubuntu 21.10] *********************\ntask path: /home/worker/workspace/Ansible_Ubuntu_Desktop_ISO_70GA/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:103\nfatal: [localhost -> 10.191.196.52]: FAILED! => '/usr/bin/apt-get -y -o \"Dpkg::Options::=--force-confdef\" -o \"Dpkg::Options::=--force-confold\"       install 'cloud-init'' failed: E: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/libe/libeatmydata/libeatmydata1_105-9build2_amd64.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/libe/libeatmydata/eatmydata_105-9build2_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/python-attrs/python3-attr_20.3.0-1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/c/configobj/python3-configobj_5.0.6-4_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/python3-stdlib-extensions/python3-distutils_3.9.7-1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/j/jinja2/python3-jinja2_2.11.3-1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/python-json-pointer/python3-json-pointer_2.0-0ubuntu1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/python-json-patch/python3-jsonpatch_1.25-3_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/s/setuptools/python3-setuptools_52.0.0-4_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/pyrsistent/python3-pyrsistent_0.15.5-1build3_amd64.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/python-jsonschema/python3-jsonschema_3.2.0-0ubuntu2_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/pyserial/python3-serial_3.5%7eb0-1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/c/cloud-utils/cloud-guest-utils_0.32-22-g45fe84a5-0ubuntu1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/c/cloud-init/cloud-init_21.4-0ubuntu1%7e21.10.1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n\n\n\npriority.target\ndeepdive.testcase", "category": "unable_to_fetch_some_archive", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task install package on ubuntu number fatal localhost ip address failed user bin apt get y o dpkg options force configuration def o dpkg options force configuration old install cloud init failed e failed to fetch http us archive ubuntu com ubuntu pool main li be library eat my data library eat my data number build number amd number deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main li be library eat my data eat my data number build number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python attributes python number attribute version id all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main c configuration object python number configuration object version id all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python number standard library extensions python number dist utilities version id all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main j jinja number python number jinja number version id all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python json pointer python number j son pointer number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python json patch python number j son patch number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main s setup tools python number setup tools version id all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p p yrs i stent python number p yrs i stent version id build number amd number deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python json schema python number j son schema version id ubuntu number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python serial python number serial number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main c cloud utilities cloud guest utilities number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main c cloud init cloud init number id all deb temporary failure resolving us archive ubuntu com e unable to fetch some archives maybe run apt get update or try with fix missing", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1491, "name": "ansible_gosv_logs_FAILURE_Ansible_Ubuntu_Desktop_ISO_70GA-11_logs_failed_tasks_log.1", "raw": "2021-12-20 09:22:07,020 | Failed at Play [gosc_cloudinit_staticip] *******************\n2021-12-20 09:22:07,020 | TASK [Install package on Ubuntu 21.10] *********************\ntask path: /home/worker/workspace/Ansible_Ubuntu_Desktop_ISO_70GA/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:103\nfatal: [localhost -> 10.191.196.52]: FAILED! => '/usr/bin/apt-get -y -o \"Dpkg::Options::=--force-confdef\" -o \"Dpkg::Options::=--force-confold\"       install 'cloud-init'' failed: E: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/libe/libeatmydata/libeatmydata1_105-9build2_amd64.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/libe/libeatmydata/eatmydata_105-9build2_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/python-attrs/python3-attr_20.3.0-1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/c/configobj/python3-configobj_5.0.6-4_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/python3-stdlib-extensions/python3-distutils_3.9.7-1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/j/jinja2/python3-jinja2_2.11.3-1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/python-json-pointer/python3-json-pointer_2.0-0ubuntu1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/python-json-patch/python3-jsonpatch_1.25-3_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/s/setuptools/python3-setuptools_52.0.0-4_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/pyrsistent/python3-pyrsistent_0.15.5-1build3_amd64.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/python-jsonschema/python3-jsonschema_3.2.0-0ubuntu2_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/p/pyserial/python3-serial_3.5%7eb0-1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/c/cloud-utils/cloud-guest-utils_0.32-22-g45fe84a5-0ubuntu1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/c/cloud-init/cloud-init_21.4-0ubuntu1%7e21.10.1_all.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?", "category": "unable_to_fetch_some_archive", "processed": "timestamp failed at play go sc cloud init static ip timestamp task install package on ubuntu number fatal localhost ip address failed user bin apt get y o dpkg options force configuration def o dpkg options force configuration old install cloud init failed e failed to fetch http us archive ubuntu com ubuntu pool main li be library eat my data library eat my data number build number amd number deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main li be library eat my data eat my data number build number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python attributes python number attribute version id all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main c configuration object python number configuration object version id all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python number standard library extensions python number dist utilities version id all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main j jinja number python number jinja number version id all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python json pointer python number j son pointer number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python json patch python number j son patch number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main s setup tools python number setup tools version id all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p p yrs i stent python number p yrs i stent version id build number amd number deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python json schema python number j son schema version id ubuntu number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main p python serial python number serial number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main c cloud utilities cloud guest utilities number all deb temporary failure resolving us archive ubuntu com e failed to fetch http us archive ubuntu com ubuntu pool main c cloud init cloud init number id all deb temporary failure resolving us archive ubuntu com e unable to fetch some archives maybe run apt get update or try with fix missing", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1492, "name": "ansible_gosv_logs_FAILURE_Ansible_Ubuntu_Desktop_ISO_70GA-11_logs_failed_tasks_log.2", "raw": "2021-12-20 09:23:56,020 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2021-12-20 09:23:56,020 | TASK [Install package on Ubuntu 21.10] *********************\ntask path: /home/worker/workspace/Ansible_Ubuntu_Desktop_ISO_70GA/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:103\nfatal: [localhost -> 10.191.196.52]: FAILED! => '/usr/bin/apt-get -y -o \"Dpkg::Options::=--force-confdef\" -o \"Dpkg::Options::=--force-confold\"       install 'sg3-utils'' failed: E: Failed to fetch http://us.archive.ubuntu.com/ubuntu/pool/main/s/sg3-utils/sg3-utils_1.45-1ubuntu2_amd64.deb  Temporary failure resolving 'us.archive.ubuntu.com'\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?", "category": "unable_to_fetch_some_archive", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task install package on ubuntu number fatal localhost ip address failed user bin apt get y o dpkg options force configuration def o dpkg options force configuration old install sg number utilities failed e failed to fetch http us archive ubuntu com ubuntu pool main s sg number utilities sg number utilities number amd number deb temporary failure resolving us archive ubuntu com e unable to fetch some archives maybe run apt get update or try with fix missing", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1505, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.17", "raw": "2022-01-13 06:36:39,013 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-13 06:36:39,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play go sc perl dhcp timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1496, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-1_logs_failed_tasks_log.0", "raw": "2021-12-22 06:42:04,022 | Failed at Play [ovt_verify_install] ************************\n2021-12-22 06:42:04,022 | TASK [Add zypper repository SLED-15.3-dvd-Module-Live-Patching] \ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/add_repo_from_baseurl.yml:84\nImportError: No module named xml.dom.minidom\nfatal: [localhost -> 10.184.98.172]: FAILED! => Failed to import the required Python library (python-xml) on sc2-10-184-98-172.eng.vmware.com's Python /usr/bin/python. Please read the module documentation and install it in the appropriate location. If the required library is installed, but Ansible is using the wrong Python interpreter, please consult the documentation on ansible_python_interpreter\n", "category": "import_error_no_module", "processed": "timestamp failed at play ovt verify install timestamp task add zypper repository sled number dvd module live patching import error no module named xml dom mini dom fatal localhost ip address failed failed to import the required python library python xml on sc number eng vmware com s python user bin python please read the module documentation and install it in the appropriate location if the required library is installed but ansible is using the wrong python interpreter please consult the documentation on ansible python interpreter", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1497, "name": "ansible_gosv_logs_FAILURE_Ansible_Photon_4_x_67GA-16_logs_failed_tasks_log.1", "raw": "2022-01-21 04:48:24,021 | Failed at Play [gosc_perl_staticip] ************************\n2022-01-21 04:48:24,021 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:108\nfatal: [localhost]: FAILED! => [\"VM static IPv4 gateway is '', expected IPv4 gateway is 192.168.1.1\"]", "category": "expect_ipv4_address2", "processed": "timestamp failed at play go sc perl static ip timestamp task gos customization failed fatal localhost failed vm static ip v number gateway is expected ip v number gateway is ip address", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1498, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_8_x_65U3-8_logs_failed_tasks_log.3", "raw": "2022-01-21 06:17:51,021 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-01-21 06:17:51,021 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_65U3/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:108\nfatal: [localhost]: FAILED! => [\"VM static IPv4 gateway is '', expected IPv4 gateway is 192.168.1.1\"]", "category": "expect_ipv4_address2", "processed": "timestamp failed at play go sc cloud init static ip timestamp task gos customization failed fatal localhost failed vm static ip v number gateway is expected ip v number gateway is ip address", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1499, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.1", "raw": "2022-01-13 05:27:50,013 | Failed at Play [ovt_verify_status] *************************\n2022-01-13 05:27:50,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play ovt verify status timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1500, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.10", "raw": "2022-01-13 06:06:33,013 | Failed at Play [check_quiesce_snapshot_custom_script] ******\n2022-01-13 06:06:33,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play check quiesce snapshot custom script timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1501, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.12", "raw": "2022-01-13 06:15:08,013 | Failed at Play [device_list] *******************************\n2022-01-13 06:15:08,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play device list timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1502, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.13", "raw": "2022-01-13 06:19:26,013 | Failed at Play [secureboot_enable_disable] *****************\n2022-01-13 06:19:26,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play secure boot enable disable timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1506, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.18", "raw": "2022-01-13 06:40:57,013 | Failed at Play [gosc_perl_staticip] ************************\n2022-01-13 06:40:57,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play go sc perl static ip timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1507, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.19", "raw": "2022-01-13 06:45:15,013 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-01-13 06:45:15,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play go sc cloud init static ip timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1508, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.2", "raw": "2022-01-13 05:32:08,013 | Failed at Play [vgauth_check_service] **********************\n2022-01-13 05:32:08,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play vg auth check service timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1509, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.3", "raw": "2022-01-13 05:36:26,013 | Failed at Play [check_ip_address] **************************\n2022-01-13 05:36:26,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play check ip address timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1510, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.4", "raw": "2022-01-13 05:40:45,013 | Failed at Play [stat_balloon] ******************************\n2022-01-13 05:40:45,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play stat balloon timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1511, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.5", "raw": "2022-01-13 05:45:03,013 | Failed at Play [stat_hosttime] *****************************\n2022-01-13 05:45:03,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play stat host time timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1512, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.6", "raw": "2022-01-13 05:49:21,013 | Failed at Play [check_inbox_driver] ************************\n2022-01-13 05:49:21,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play check inbox driver timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1513, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.7", "raw": "2022-01-13 05:53:39,013 | Failed at Play [check_os_fullname] *************************\n2022-01-13 05:53:39,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play check os full name timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1514, "name": "ansible_gosv_logs_ABORTED_Ansible_Cycle_Ubuntu_21_10_ISO-7_logs_failed_tasks_log.8", "raw": "2022-01-13 05:57:57,013 | Failed at Play [check_efi_firmware] ************************\n2022-01-13 05:57:57,013 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_21.10_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play check efi firmware timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1515, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_Windows_11_64-23_logs_failed_tasks_log.0", "raw": "2022-01-06 11:45:18,006 | Failed at Play [memory_hot_add_basic] **********************\n2022-01-06 11:45:18,006 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed\n2022-01-06 11:45:22,006 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play memory hot add basic timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1516, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_9_x_70U3-7_logs_failed_tasks_log.5", "raw": "2021-12-10 14:02:45,010 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2021-12-10 14:02:45,010 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1517, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_9_x_70U3-7_logs_failed_tasks_log.6", "raw": "2021-12-10 14:02:49,010 | Failed at Play [sata_vhba_device_ops] **********************\n2021-12-10 14:02:49,010 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play sata v hba device ops timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1518, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_9_x_70U3-7_logs_failed_tasks_log.7", "raw": "2021-12-10 14:02:53,010 | Failed at Play [nvme_vhba_device_ops] **********************\n2021-12-10 14:02:53,010 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play nvme v hba device ops timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1519, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_9_x_70U3-7_logs_failed_tasks_log.8", "raw": "2021-12-10 14:02:56,010 | Failed at Play [ovt_verify_uninstall] **********************\n2021-12-10 14:02:56,010 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play ovt verify un install timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1520, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_3_0_ISO-31_logs_failed_tasks_log.1", "raw": "2022-01-18 16:36:59,018 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-01-18 16:36:59,018 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "snapshot_fail", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task revert snapshot failed fatal localhost failed revert to snapshot base snapshot failed", "solution": "retry", "target": "testcase", "version": 202205240000}, {"id": 1553, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_65U3-18_logs_failed_tasks_log.5", "raw": "2021-12-22 05:56:35,022 | Failed at Play [nvme_vhba_device_ops] **********************\n2021-12-22 05:56:35,022 | TASK [Get block devices] ***********************************\ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_65U3/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/get_vm_device_list.yml:8\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via ssh: Connection timed out during banner exchange\nConnection to 10.78.125.176 port 22 timed out", "category": "connection_timeout_during", "processed": "timestamp failed at play nvme v hba device ops timestamp task get block devices fatal localhost un reachable failed to connect to the host via ssh connection timed out during banner exchange connection to ip address port number timed out", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1521, "name": "ansible_gosv_logs_FAILURE_Ansible_OracleLinux_8_x_67U1-13_logs_failed_tasks_log.0", "raw": "2022-01-24 11:24:08,024 | Failed at Play [deploy_vm_efi_lsilogic_e1000e] *************\n2022-01-24 11:24:08,024 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_67U1/ansible-vsphere-gos-validation/linux/deploy_vm/deploy_vm_from_iso.yml:176\nfatal: [localhost]: FAILED! => Failed to remove serial port from VM\n2022-01-24 11:24:40,024 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_67U1/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_lsilogic_e1000e", "category": "fail_to_remove_from_vm", "processed": "timestamp failed at play deploy vm efi lsi logic timestamp task fail fatal localhost failed failed to remove serial port from vm timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi lsi logic e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1522, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_8_x_65U3-12_logs_failed_tasks_log.0", "raw": "2022-01-24 11:14:18,024 | Failed at Play [deploy_vm_efi_sata_e1000e] *****************\n2022-01-24 11:14:18,024 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_65U3/ansible-vsphere-gos-validation/linux/deploy_vm/deploy_vm_from_iso.yml:176\nfatal: [localhost]: FAILED! => Failed to remove serial port from VM\n2022-01-24 11:14:51,024 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_RHEL_8.x_65U3/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_sata_e1000e", "category": "fail_to_remove_from_vm", "processed": "timestamp failed at play deploy vm efi sata timestamp task fail fatal localhost failed failed to remove serial port from vm timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi sata e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1523, "name": "ansible_gosv_logs_FAILURE_Ansible_Autoinstall_OracleLinux_8_x-29_logs_failed_tasks_log.0", "raw": "2021-12-16 10:01:27,016 | Failed at Play [testbed_deploy_nimbus] *********************\n2021-12-16 10:01:27,016 | TASK [Parameter error] *************************************\ntask path: /home/worker/workspace/Ansible_Autoinstall_OracleLinux_8.x/newgos_testing_internal/testbed_deploy/testbed_deploy_nimbus.yml:60\nfatal: [localhost]: FAILED! => Please specify vCenter build number in command line or in vars/internal_test.yml using 'vcenter_build_num' parameter", "category": "vcenter_number", "processed": "timestamp failed at play testbed deploy nimbus timestamp task parameter error fatal localhost failed please specify vcenter build number in command line or in vars internal test yml using vcenter build number parameter", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1524, "name": "ansible_gosv_logs_FAILURE_Ansible_Autoinstall_AlmaLinux_8_x-18_logs_failed_tasks_log.0", "raw": "2021-12-09 14:52:15,009 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2021-12-09 14:52:15,009 | TASK [Create a new VM 'test_vm' on server '10.187.103.183'] \ntask path: /home/worker/workspace/Ansible_Autoinstall_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : The operation is not supported on the object.\n2021-12-09 14:52:15,009 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Autoinstall_AlmaLinux_8.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_paravirtual_vmxnet3", "category": "operation_not_support", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task create a new vm test vm on server ip address fatal localhost failed failed to create a virtual machine the operation is not supported on the object timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1525, "name": "ansible_gosv_logs_FAILURE_Ansible_Autoinstall_AlmaLinux_8_x-20_logs_failed_tasks_log.0", "raw": "2021-12-10 03:20:30,010 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2021-12-10 03:20:30,010 | TASK [Create a new VM 'test_vm_1639106335896' on server '10.185.107.93'] \ntask path: /home/worker/workspace/Ansible_Autoinstall_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : The operation is not supported on the object.\n2021-12-10 03:20:31,010 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Autoinstall_AlmaLinux_8.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_paravirtual_vmxnet3", "category": "operation_not_support", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task create a new vm test vm number on server ip address fatal localhost failed failed to create a virtual machine the operation is not supported on the object timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1526, "name": "ansible_gosv_logs_FAILURE_Ansible_Autoinstall_Windows_10_32-18_logs_failed_tasks_log.0", "raw": "2021-12-09 14:56:10,009 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2021-12-09 14:56:10,009 | TASK [Create a new VM 'test_vm' on server '10.185.10.254'] *\ntask path: /home/worker/workspace/Ansible_Autoinstall_Windows_10_32/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : The operation is not supported on the object.\n2021-12-09 14:56:11,009 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Autoinstall_Windows_10_32/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "operation_not_support", "processed": "timestamp failed at play deploy vm efi para virtual timestamp task create a new vm test vm on server ip address fatal localhost failed failed to create a virtual machine the operation is not supported on the object timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1554, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_9_x_70U3-10_logs_failed_tasks_log.1", "raw": "2021-12-10 16:26:16,010 | Failed at Play [memory_hot_add_basic] **********************\n2021-12-10 16:26:16,010 | TASK [Set memory size to 3072 MB] **************************\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3/ansible-vsphere-gos-validation/common/vm_set_memory_size.yml:4\nfatal: [localhost]: FAILED! => Invalid virtual machine configuration.", "category": "invalid_vm_config", "processed": "timestamp failed at play memory hot add basic timestamp task set memory size to number mb fatal localhost failed invalid virtual machine configuration", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1527, "name": "ansible_gosv_logs_FAILURE_Ansible_Autoinstall_Windows_10_32-21_logs_failed_tasks_log.0", "raw": "2021-12-10 03:20:35,010 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2021-12-10 03:20:35,010 | TASK [Create a new VM 'test_vm_1639106326532' on server '10.78.194.73'] \ntask path: /home/worker/workspace/Ansible_Autoinstall_Windows_10_32/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : The operation is not supported on the object.\n2021-12-10 03:20:36,010 | TASK [Check if exit testing parameter is set] **************\ntask path: /home/worker/workspace/Ansible_Autoinstall_Windows_10_32/ansible-vsphere-gos-validation/windows/setup/rescue_cleanup.yml:46\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True", "category": "operation_not_support", "processed": "timestamp failed at play deploy vm efi para virtual timestamp task create a new vm test vm number on server ip address fatal localhost failed failed to create a virtual machine the operation is not supported on the object timestamp task check if exit testing parameter is set fatal localhost failed exit testing when exit testing when fail is set to true", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1528, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_67GA-12_logs_failed_tasks_log.0", "raw": "2021-12-22 16:34:37,022 | Failed at Play [vgauth_check_service] **********************\n2021-12-22 16:34:37,022 | TASK [Check service 'vgauthd' is enabled] ******************\ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_67GA/ansible-vsphere-gos-validation/linux/utils/check_service_status.yml:55\nfatal: [localhost]: FAILED! => Serivce 'vgauthd' is not enabled\n", "category": "not_enable", "processed": "timestamp failed at play vg auth check service timestamp task check service vg authd is enabled fatal localhost failed ser i vc e vg authd is not enabled", "solution": "deepdive", "target": "targetvm", "version": 202205240000}, {"id": 1529, "name": "ansible_gosv_logs_ABORTED_Ansible_Regression_Ubuntu_21_10_Desktop_ISO-26_logs_failed_tasks_log.0", "raw": "2021-12-22 09:39:23,022 | Failed at Play [e1000e_network_device_ops] *****************\n2021-12-22 09:39:23,022 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Desktop_ISO/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.185.252.26:22", "category": "timeout_openssh__2", "processed": "timestamp failed at play network device ops timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1530, "name": "ansible_gosv_logs_ABORTED_Ansible_Regression_Ubuntu_21_10_Desktop_ISO-26_logs_failed_tasks_log.1", "raw": "2021-12-22 09:55:16,022 | Failed at Play [vmxnet3_network_device_ops] ****************\n2021-12-22 09:55:16,022 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Desktop_ISO/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.185.252.26:22", "category": "timeout_openssh__2", "processed": "timestamp failed at play vmxnet number network device ops timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1531, "name": "ansible_gosv_logs_ABORTED_Ansible_Regression_Ubuntu_21_10_Desktop_ISO-26_logs_failed_tasks_log.2", "raw": "2021-12-22 10:11:08,022 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-22 10:11:08,022 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Desktop_ISO/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.185.252.26:22", "category": "timeout_openssh__2", "processed": "timestamp failed at play go sc perl dhcp timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1532, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_70U3-1_logs_failed_tasks_log.0", "raw": "2021-12-15 05:07:59,015 | Failed at Play [deploy_vm_efi_nvme_e1000e] *****************\n2021-12-15 05:07:59,015 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70U3/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.206.209.59:22\n2021-12-15 05:09:16,015 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70U3/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_e1000e", "category": "timeout_openssh__2", "processed": "timestamp failed at play deploy vm efi nvme timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1533, "name": "ansible_gosv_logs_FAILURE_Ansible_exclude_Regression_SLED_15_x-12_logs_failed_tasks_log.0", "raw": "2021-12-15 03:07:09,015 | Failed at Play [deploy_vm_bios_nvme_vmxnet3] ***************\n2021-12-15 03:07:09,015 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_exclude_Regression_SLED_15.x/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.184.91.128:22\n2021-12-15 03:09:14,015 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_exclude_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_nvme_vmxnet3", "category": "timeout_openssh__2", "processed": "timestamp failed at play deploy vm bios nvme vmxnet number timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios nvme vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1534, "name": "ansible_gosv_logs_FAILURE_Ansible_exclude_Regression_SLED_15_x-13_logs_failed_tasks_log.0", "raw": "2021-12-15 04:03:36,015 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2021-12-15 04:03:36,015 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_exclude_Regression_SLED_15.x/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.206.109.85:22\n2021-12-15 04:05:03,015 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_exclude_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "timeout_openssh__2", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1536, "name": "ansible_gosv_logs_FAILURE_Ansible_RHEL_9_x_70U3-9_logs_failed_tasks_log.0", "raw": "2021-12-10 14:10:40,010 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2021-12-10 14:10:40,010 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.168.207.211:22\n2021-12-10 14:11:33,010 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_paravirtual_e1000e", "category": "timeout_openssh__2", "processed": "timestamp failed at play deploy vm efi para virtual timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi para virtual e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1537, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-1_logs_failed_tasks_log.0", "raw": "2021-12-15 05:00:34,015 | Failed at Play [deploy_vm_bios_paravirtual_vmxnet3] ********\n2021-12-15 05:00:34,015 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.191.166.147:22\n2021-12-15 05:02:32,015 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_paravirtual_vmxnet3", "category": "timeout_openssh__2", "processed": "timestamp failed at play deploy vm bios para virtual vmxnet number timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios para virtual vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1598, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_OVA-23_logs_failed_tasks_log.1", "raw": "2022-01-04 13:14:05,004 | Failed at Play [gosc_perl_staticip] ************************\n2022-01-04 13:14:05,004 | TASK [Install package on VMware Photon OS 4.0] *************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:53\nfatal: [localhost -> 10.187.130.143]: FAILED! => non-zero return code when tdnf", "category": "non_zero_return", "processed": "timestamp failed at play go sc perl static ip timestamp task install package on vmware photon os number fatal localhost ip address failed non zero return code when tdnf", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1538, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_67GA-1_logs_failed_tasks_log.0", "raw": "2021-12-15 04:52:03,015 | Failed at Play [deploy_vm_efi_lsilogic_e1000e] *************\n2021-12-15 04:52:03,015 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_67GA/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.180.199.62:22\n2021-12-15 04:53:13,015 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_67GA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_lsilogic_e1000e", "category": "timeout_openssh__2", "processed": "timestamp failed at play deploy vm efi lsi logic timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi lsi logic e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1539, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_67U3-1_logs_failed_tasks_log.0", "raw": "2021-12-15 04:48:19,015 | Failed at Play [deploy_vm_bios_lsilogicsas_vmxnet3] ********\n2021-12-15 04:48:19,015 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_67U3/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.180.106.45:22\n2021-12-15 04:49:37,015 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_67U3/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_lsilogicsas_vmxnet3", "category": "timeout_openssh__2", "processed": "timestamp failed at play deploy vm bios lsi logic sas vmxnet number timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios lsi logic sas vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1540, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_70GA-1_logs_failed_tasks_log.0", "raw": "2021-12-15 04:56:48,015 | Failed at Play [deploy_vm_bios_sata_e1000e] ****************\n2021-12-15 04:56:48,015 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70GA/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.206.94.224:22\n2021-12-15 04:58:41,015 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_70GA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_bios_sata_e1000e", "category": "timeout_openssh__2", "processed": "timestamp failed at play deploy vm bios sata timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm bios sata e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1599, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_OVA-23_logs_failed_tasks_log.2", "raw": "2022-01-04 13:15:45,004 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-01-04 13:15:45,004 | TASK [Install package on VMware Photon OS 4.0] *************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:53\nfatal: [localhost -> 10.187.130.143]: FAILED! => non-zero return code when tdnf", "category": "non_zero_return", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task install package on vmware photon os number fatal localhost ip address failed non zero return code when tdnf", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1600, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_OVA-23_logs_failed_tasks_log.3", "raw": "2022-01-04 13:17:14,004 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-01-04 13:17:14,004 | TASK [Install package on VMware Photon OS 4.0] *************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:53\nfatal: [localhost -> 10.187.130.143]: FAILED! => non-zero return code when tdnf", "category": "non_zero_return", "processed": "timestamp failed at play go sc cloud init static ip timestamp task install package on vmware photon os number fatal localhost ip address failed non zero return code when tdnf", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1603, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-3_logs_failed_tasks_log.1", "raw": "2021-12-20 11:56:42,020 | Failed at Play [e1000e_network_device_ops] *****************\n2021-12-20 11:56:42,020 | TASK [Bring link 'eth1' up] ********************************\ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/network_device_ops/enable_new_ethernet.yml:88\nfatal: [localhost -> 10.184.94.66]: FAILED! => non-zero return code when ifdown", "category": "non_zero_return", "processed": "timestamp failed at play network device ops timestamp task bring link eth number up fatal localhost ip address failed non zero return code when if down", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1550, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_Windows_10_64-20_logs_failed_tasks_log.0", "raw": "2022-01-05 05:49:28,005 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-01-05 05:49:28,005 | TASK [Check diskpart script file copied to guest OS] *******\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/create_partition_raw_disk.yml:69\nfatal: [localhost]: FAILED! => diskpart file not found in guest OS: C:\\test_diskpart\\diskpart.txt", "category": "file_not_found_in_guest", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task check diskpart script file copied to guest os fatal localhost failed diskpart file not found in guest os c test diskpart diskpart text", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1551, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_AlmaLinux_8_x-22_logs_failed_tasks_log.0", "raw": "2022-01-04 06:28:23,004 | Failed at Play [env_setup] *********************************\n2022-01-04 06:28:23,004 | TASK [Enable GuestIPHack on ESXi host '10.186.44.158'] *****\ntask path: /home/worker/workspace/Ansible_Regression_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/esxi_enable_guest_ip_hack.yml:5\nfatal: [localhost]: UNREACHABLE! => Failed to create temporary directory.In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /vmfs/volumes/datastore2 `\"&& mkdir \"` echo /vmfs/volumes/datastore2/ansible-tmp-1641277703.4004283-1648-2345984331423 `\" && echo ansible-tmp-1641277703.4004283-1648-2345984331423=\"` echo /vmfs/volumes/datastore2/ansible-tmp-1641277703.4004283-1648-2345984331423 `\" ), exited with result 1\n\n", "category": "failed_create_temporary_directory", "processed": "timestamp failed at play environment setup timestamp task enable guest ip hack on esxi host ip address fatal localhost un reachable failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo vmfs volumes data store number mkdir echo vmfs volumes data store number ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo vmfs volumes data store number ansible tmp hex id number timestamp exited with result number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1552, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_3_0_ISO-32_logs_failed_tasks_log.0", "raw": "2022-01-18 17:22:23,018 | Failed at Play [env_setup] *********************************\n2022-01-18 17:22:23,018 | TASK [Enable GuestIPHack on ESXi host '10.187.150.27'] *****\ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/common/esxi_enable_guest_ip_hack.yml:5\nfatal: [localhost]: UNREACHABLE! => Failed to create temporary directory.In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /vmfs/volumes/datastore2 `\"&& mkdir \"` echo /vmfs/volumes/datastore2/ansible-tmp-1642526542.8186202-501-229866794392168 `\" && echo ansible-tmp-1642526542.8186202-501-229866794392168=\"` echo /vmfs/volumes/datastore2/ansible-tmp-1642526542.8186202-501-229866794392168 `\" ), exited with result 1", "category": "failed_create_temporary_directory", "processed": "timestamp failed at play environment setup timestamp task enable guest ip hack on esxi host ip address fatal localhost un reachable failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo vmfs volumes data store number mkdir echo vmfs volumes data store number ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo vmfs volumes data store number ansible tmp hex id number timestamp exited with result number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1555, "name": "ansible_gosv_logs_FAILURE_Ansible_Photon_4_x_67GA-26_logs_failed_tasks_log.0", "raw": "2022-01-24 08:38:42,024 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-24 08:38:42,024 | TASK [Set fact of guest OS hostname and domain] ************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/linux/guest_customization/check_hostname_and_domain.yml:35\njinja2.exceptions.TemplateRuntimeError: No filter named 'joint' found.\nfatal: [localhost]: FAILED! => {\n    \"changed\": false\n}\n2022-01-24 08:39:16,024 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Photon_4.x_67GA/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case gosc_perl_dhcp", "category": "jinja_exception", "processed": "timestamp failed at play go sc perl dhcp timestamp task set fact of guest os hostname and domain jinja number exceptions template runtime error no filter named joint found fatal localhost failed timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case go sc perl dhcp", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1556, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_ISO-107_logs_failed_tasks_log.0", "raw": "2022-01-24 09:36:13,024 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-24 09:36:13,024 | TASK [Set fact of guest OS hostname and domain] ************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/linux/guest_customization/check_hostname_and_domain.yml:35\njinja2.exceptions.TemplateRuntimeError: No filter named 'joint' found.\nfatal: [localhost]: FAILED! => {\n    \"changed\": false\n}", "category": "jinja_exception", "processed": "timestamp failed at play go sc perl dhcp timestamp task set fact of guest os hostname and domain jinja number exceptions template runtime error no filter named joint found fatal localhost failed", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1557, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_ISO-107_logs_failed_tasks_log.1", "raw": "2022-01-24 09:40:35,024 | Failed at Play [gosc_perl_staticip] ************************\n2022-01-24 09:40:35,024 | TASK [Set fact of guest OS hostname and domain] ************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/linux/guest_customization/check_hostname_and_domain.yml:35\njinja2.exceptions.TemplateRuntimeError: No filter named 'joint' found.\nfatal: [localhost]: FAILED! => {\n    \"changed\": false\n}", "category": "jinja_exception", "processed": "timestamp failed at play go sc perl static ip timestamp task set fact of guest os hostname and domain jinja number exceptions template runtime error no filter named joint found fatal localhost failed", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1558, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_ISO-107_logs_failed_tasks_log.2", "raw": "2022-01-24 09:45:06,024 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-01-24 09:45:06,024 | TASK [Set fact of guest OS hostname and domain] ************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/linux/guest_customization/check_hostname_and_domain.yml:35\njinja2.exceptions.TemplateRuntimeError: No filter named 'joint' found.\nfatal: [localhost]: FAILED! => {\n    \"changed\": false\n}", "category": "jinja_exception", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task set fact of guest os hostname and domain jinja number exceptions template runtime error no filter named joint found fatal localhost failed", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1559, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_ISO-107_logs_failed_tasks_log.3", "raw": "2022-01-24 09:49:32,024 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-01-24 09:49:32,024 | TASK [Set fact of guest OS hostname and domain] ************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/linux/guest_customization/check_hostname_and_domain.yml:35\njinja2.exceptions.TemplateRuntimeError: No filter named 'joint' found.\nfatal: [localhost]: FAILED! => {\n    \"changed\": false\n}", "category": "jinja_exception", "processed": "timestamp failed at play go sc cloud init static ip timestamp task set fact of guest os hostname and domain jinja number exceptions template runtime error no filter named joint found fatal localhost failed", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1560, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Windows_11_64-45_logs_failed_tasks_log.1", "raw": "2022-01-21 04:54:11,021 | Failed at Play [gosc_sanity_dhcp] **************************\n2022-01-21 04:54:11,021 | TASK [Check guest customization state] *********************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/common/vm_wait_gosc_completed.yml:35\nfatal: [localhost]: FAILED! => Not found 'ToolsDeployPkgPublishState: state=5, code=0' in vmware.log", "category": "tools_deploy_pkg_publish_state", "processed": "timestamp failed at play go sc sanity dhcp timestamp task check guest customization state fatal localhost failed not found tools deploy package publish state state number code number in vmware log", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1561, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_Windows_10_64-26_logs_failed_tasks_log.0", "raw": "2022-01-06 11:47:40,006 | Failed at Play [check_efi_firmware] ************************\n2022-01-06 11:47:40,006 | TASK [Get VMware tools version and build number in guest OS] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_get_vmtools_version_build.yml:13\nrequests.exceptions.HTTPError: 500 Server Error:  for url: https://10.185.96.250:5986/wsman\nwinrm.exceptions.WinRMTransportError: Bad HTTP response returned from server. Code 500\nwinrm.exceptions.WinRMError: The WS-Management service cannot process the request because the request contained invalid selectors for the resource.  (extended fault data: Bad HTTP response returned from server. Code 500 at s:Sender and w:InvalidSelectors)\nrequests.exceptions.HTTPError: 500 Server Error:  for url: https://10.185.96.250:5986/wsman\nwinrm.exceptions.WinRMTransportError: Bad HTTP response returned from server. Code 500\nwinrm.exceptions.WinRMError: The WS-Management service cannot process the request because the request contained invalid selectors for the resource.  (extended fault data: Bad HTTP response returned from server. Code 500 at s:Sender and w:InvalidSelectors)\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.\n", "category": "bad_http_from_server", "processed": "timestamp failed at play check efi firmware timestamp task get vmware tools version and build number in guest os requests exceptions http error number server error for url https ip address wsman win rm exceptions win rm transport error bad http response returned from server code number win rm exceptions win rm error the ws management service can not process the request because the request contained invalid selectors for the resource extended fault data bad http response returned from server code number ats sender and w invalid selectors requests exceptions http error number server error for url https ip address wsman win rm exceptions win rm transport error bad http response returned from server code number win rm exceptions win rm error the ws management service can not process the request because the request contained invalid selectors for the resource extended fault data bad http response returned from server code number ats sender and w invalid selectors fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1562, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_11_64bit_67U3_LSILogicSAS_EFI-14_logs_failed_tasks_log.3", "raw": "2021-12-14 11:06:22,014 | Failed at Play [e1000e_network_device_ops] *****************\n2021-12-14 11:06:22,014 | TASK [Check powershell command result] *********************\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_67U3_LSILogicSAS_EFI/ansible-vsphere-gos-validation/windows/utils/win_get_ethernet_name.yml:14\nfatal: [localhost]: FAILED! => Not get Ethernet name with specified MAC address: 00:50:56:af:3d:68\n", "category": "no_ethernet_name", "processed": "timestamp failed at play network device ops timestamp task check powershell command result fatal localhost failed not get ethernet name with specified mac address mac address", "solution": "deepdive", "target": "targetvm", "version": 202205240000}, {"id": 1563, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_11_64bit_67U3_LSILogicSAS_EFI-14_logs_failed_tasks_log.4", "raw": "2021-12-14 11:09:02,014 | Failed at Play [vmxnet3_network_device_ops] ****************\n2021-12-14 11:09:02,014 | TASK [Check powershell command result] *********************\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_67U3_LSILogicSAS_EFI/ansible-vsphere-gos-validation/windows/utils/win_get_ethernet_name.yml:14\nfatal: [localhost]: FAILED! => Not get Ethernet name with specified MAC address: 00:50:56:af:78:71", "category": "no_ethernet_name", "processed": "timestamp failed at play vmxnet number network device ops timestamp task check powershell command result fatal localhost failed not get ethernet name with specified mac address mac address", "solution": "deepdive", "target": "targetvm", "version": 202205240000}, {"id": 1564, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.3", "raw": "2022-01-17 04:40:42,017 | Failed at Play [e1000e_network_device_ops] *****************\n2022-01-17 04:40:42,017 | TASK [Add a new 'e1000e' adapter in 'vSwitch2022-01-17-03-48-31_PG' to VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_add_network_adapter.yml:4\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play network device ops timestamp task add a new adapter in vswitch timestamp number page to vm test vm fatal localhost failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1565, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.4", "raw": "2022-01-17 04:42:11,017 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-01-17 04:42:11,017 | TASK [Add a new 'vmxnet3' adapter in 'vSwitch2022-01-17-03-48-31_PG' to VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_add_network_adapter.yml:4\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play vmxnet number network device ops timestamp task add a new vmxnet number adapter in vswitch timestamp number page to vm test vm fatal localhost failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1566, "name": "ansible_gosv_logs_FAILURE_Ansible_OracleLinux_8_x_67U1-14_logs_failed_tasks_log.0", "raw": "2022-01-24 11:46:34,024 | Failed at Play [deploy_vm_efi_lsilogic_e1000e] *************\n2022-01-24 11:46:34,024 | TASK [Add a serial port using output file] *****************\ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_67U1/ansible-vsphere-gos-validation/common/vm_add_serial_port.yml:15\nKeyError: 'type'\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error\n2022-01-24 11:47:06,024 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_67U1/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_lsilogic_e1000e", "category": "module_failure_3_cases___", "processed": "timestamp failed at play deploy vm efi lsi logic timestamp task add a serial port using output file fatal localhost failed module failure see stdout stderr for the exact error timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi lsi logic e number e", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1567, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.0", "raw": "2022-01-05 09:30:58,005 | Failed at Play [ovt_verify_install] ************************\n2022-01-05 09:30:58,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play ovt verify install timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1568, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.1", "raw": "2022-01-05 09:32:00,005 | Failed at Play [ovt_verify_status] *************************\n2022-01-05 09:32:00,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play ovt verify status timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1569, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.10", "raw": "2022-01-05 09:41:02,005 | Failed at Play [check_quiesce_snapshot_custom_script] ******\n2022-01-05 09:41:02,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play check quiesce snapshot custom script timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1570, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.11", "raw": "2022-01-05 09:42:09,005 | Failed at Play [memory_hot_add_basic] **********************\n2022-01-05 09:42:09,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play memory hot add basic timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1571, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.12", "raw": "2022-01-05 09:43:11,005 | Failed at Play [device_list] *******************************\n2022-01-05 09:43:11,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play device list timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1572, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.13", "raw": "2022-01-05 09:44:21,005 | Failed at Play [secureboot_enable_disable] *****************\n2022-01-05 09:44:21,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play secure boot enable disable timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1573, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.14", "raw": "2022-01-05 09:45:22,005 | Failed at Play [e1000e_network_device_ops] *****************\n2022-01-05 09:45:22,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play network device ops timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1574, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.15", "raw": "2022-01-05 09:46:20,005 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-01-05 09:46:20,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play vmxnet number network device ops timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1575, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.16", "raw": "2022-01-05 09:47:22,005 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-01-05 09:47:22,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play cpu multi cores per socket timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1576, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.17", "raw": "2022-01-05 09:48:21,005 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-05 09:48:21,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play go sc perl dhcp timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1577, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.18", "raw": "2022-01-05 09:49:20,005 | Failed at Play [gosc_perl_staticip] ************************\n2022-01-05 09:49:20,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play go sc perl static ip timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1578, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.19", "raw": "2022-01-05 09:50:17,005 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-01-05 09:50:17,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1579, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.2", "raw": "2022-01-05 09:32:58,005 | Failed at Play [vgauth_check_service] **********************\n2022-01-05 09:32:58,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play vg auth check service timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1580, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.20", "raw": "2022-01-05 09:51:14,005 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-01-05 09:51:14,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play go sc cloud init static ip timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1581, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.21", "raw": "2022-01-05 09:52:19,005 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-01-05 09:52:19,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play para virtual v hba device ops timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1582, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.22", "raw": "2022-01-05 09:53:18,005 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-01-05 09:53:18,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1583, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.23", "raw": "2022-01-05 09:54:19,005 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-01-05 09:54:19,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1584, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.24", "raw": "2022-01-05 09:55:18,005 | Failed at Play [sata_vhba_device_ops] **********************\n2022-01-05 09:55:18,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play sata v hba device ops timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1585, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.25", "raw": "2022-01-05 09:56:18,005 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-01-05 09:56:18,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play nvme v hba device ops timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1586, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.26", "raw": "2022-01-05 09:57:14,005 | Failed at Play [ovt_verify_uninstall] **********************\n2022-01-05 09:57:14,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play ovt verify un install timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1587, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.3", "raw": "2022-01-05 09:33:54,005 | Failed at Play [check_ip_address] **************************\n2022-01-05 09:33:54,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play check ip address timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1588, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.4", "raw": "2022-01-05 09:34:53,005 | Failed at Play [stat_balloon] ******************************\n2022-01-05 09:34:53,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play stat balloon timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1589, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.5", "raw": "2022-01-05 09:35:52,005 | Failed at Play [stat_hosttime] *****************************\n2022-01-05 09:35:52,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play stat host time timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1590, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.6", "raw": "2022-01-05 09:36:57,005 | Failed at Play [check_inbox_driver] ************************\n2022-01-05 09:36:57,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play check inbox driver timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1591, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.7", "raw": "2022-01-05 09:37:55,005 | Failed at Play [check_os_fullname] *************************\n2022-01-05 09:37:55,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play check os full name timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1592, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.8", "raw": "2022-01-05 09:38:56,005 | Failed at Play [check_efi_firmware] ************************\n2022-01-05 09:38:56,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play check efi firmware timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1593, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_SLED_15_x-5_logs_failed_tasks_log.9", "raw": "2022-01-05 09:39:56,005 | Failed at Play [cpu_hot_add_basic] *************************\n2022-01-05 09:39:56,005 | TASK [Uninstall package on SLED 15.3] **********************\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:96\nImportError: No module named xml\nfatal: [localhost -> 10.168.201.82]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "module_failure_3_cases___", "processed": "timestamp failed at play cpu hot add basic timestamp task un install package on sled number import error no module named xml fatal localhost ip address failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1594, "name": "ansible_gosv_logs_FAILURE_Ansible_OracleLinux_8_x_67U1-3_logs_failed_tasks_log.0", "raw": "2021-12-10 14:02:41,010 | Failed at Play [gosc_cloudinit_staticip] *******************\n2021-12-10 14:02:41,010 | TASK [Get default gateway] *********************************\ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_67U1/ansible-vsphere-gos-validation/linux/guest_customization/check_network_config.yml:53\nfatal: [localhost]: FAILED! => non-zero return code when cat", "category": "non_zero_return", "processed": "timestamp failed at play go sc cloud init static ip timestamp task get default gateway fatal localhost failed non zero return code when cat", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1595, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_3_0_ISO-27_logs_failed_tasks_log.0", "raw": "2022-01-12 11:37:30,012 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2022-01-12 11:37:30,012 | TASK [Rebuild ISO image with unattend install config file] *\ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/linux/deploy_vm/rebuild_unattend_install_iso.yml:99\nfatal: [localhost]: FAILED! => non-zero return code when xorriso\n2022-01-12 11:38:02,012 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3", "category": "non_zero_return", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task rebuild iso image with un attend install configuration file fatal localhost failed non zero return code when x orr iso timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1596, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_ISO-1_logs_failed_tasks_log.0", "raw": "2022-01-12 11:41:50,012 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-01-12 11:41:50,012 | TASK [Rebuild ISO image with unattend install config file] *\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_ISO/ansible-vsphere-gos-validation/linux/deploy_vm/rebuild_unattend_install_iso.yml:99\nfatal: [localhost]: FAILED! => non-zero return code when xorriso\n2022-01-12 11:42:24,012 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_ISO/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_paravirtual_vmxnet3", "category": "non_zero_return", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task rebuild iso image with un attend install configuration file fatal localhost failed non zero return code when x orr iso timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1597, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_OVA-23_logs_failed_tasks_log.0", "raw": "2022-01-04 13:12:00,004 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-04 13:12:00,004 | TASK [Install package on VMware Photon OS 4.0] *************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:53\nfatal: [localhost -> 10.187.130.143]: FAILED! => non-zero return code when tdnf", "category": "non_zero_return", "processed": "timestamp failed at play go sc perl dhcp timestamp task install package on vmware photon os number fatal localhost ip address failed non zero return code when tdnf", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1601, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_OVA-26_logs_failed_tasks_log.0", "raw": "2022-01-05 10:00:30,005 | Failed at Play [ovt_verify_install] ************************\n2022-01-05 10:00:30,005 | TASK [Install packages ['open-vm-tools']] ******************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/linux/open_vm_tools/install_ovt.yml:31\nfatal: [localhost -> 10.185.13.207]: FAILED! => non-zero return code when tdnf", "category": "non_zero_return", "processed": "timestamp failed at play ovt verify install timestamp task install packages open vm tools fatal localhost ip address failed non zero return code when tdnf", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1602, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_RHEL_7_x-25_logs_failed_tasks_log.0", "raw": "2021-12-21 13:26:11,021 | Failed at Play [e1000e_network_device_ops] *****************\n2021-12-21 13:26:11,021 | TASK [Bring link 'ens224' down for RedHat] *****************\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_7.x/ansible-vsphere-gos-validation/linux/network_device_ops/enable_new_ethernet.yml:77\nfatal: [localhost -> 10.182.48.70]: FAILED! => non-zero return code when nmcli", "category": "non_zero_return", "processed": "timestamp failed at play network device ops timestamp task bring link down for redhat fatal localhost ip address failed non zero return code when nm cli", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1604, "name": "ansible_gosv_logs_FAILURE_Ansible_SLED_15_SP4_65U3-3_logs_failed_tasks_log.2", "raw": "2021-12-20 12:00:11,020 | Failed at Play [vmxnet3_network_device_ops] ****************\n2021-12-20 12:00:11,020 | TASK [Bring link 'eth1' up] ********************************\ntask path: /home/worker/workspace/Ansible_SLED_15_SP4_65U3/ansible-vsphere-gos-validation/linux/network_device_ops/enable_new_ethernet.yml:88\nfatal: [localhost -> 10.184.94.66]: FAILED! => non-zero return code when ifdown", "category": "non_zero_return", "processed": "timestamp failed at play vmxnet number network device ops timestamp task bring link eth number up fatal localhost ip address failed non zero return code when if down", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1605, "name": "ansible_gosv_logs_FAILURE_Ansible_SLES_15_SP4_67GA-10_logs_failed_tasks_log.0", "raw": "2021-12-14 10:24:51,014 | Failed at Play [ovt_verify_install] ************************\n2021-12-14 10:24:51,014 | TASK [Install packages ['open-vm-tools', 'libvmtools0']] ***\ntask path: /home/worker/workspace/Ansible_SLES_15_SP4_67GA/ansible-vsphere-gos-validation/linux/open_vm_tools/install_ovt.yml:31\nfatal: [localhost -> 10.191.155.173]: FAILED! => non-zero return code when zypper", "category": "non_zero_return", "processed": "timestamp failed at play ovt verify install timestamp task install packages open vm tools library vm tools number fatal localhost ip address failed non zero return code when zypper", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1606, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.5", "raw": "2022-01-17 04:53:31,017 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-17 04:53:31,017 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization of the guest operating system is not supported due to the given reason: ", "category": "customization_is_not_supported", "processed": "timestamp failed at play go sc perl dhcp timestamp task customize linux guest os fatal localhost failed customization of the guest operating system is not supported due to the given reason", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1607, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.6", "raw": "2022-01-17 04:54:50,017 | Failed at Play [gosc_perl_staticip] ************************\n2022-01-17 04:54:50,017 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization of the guest operating system is not supported due to the given reason: ", "category": "customization_is_not_supported", "processed": "timestamp failed at play go sc perl static ip timestamp task customize linux guest os fatal localhost failed customization of the guest operating system is not supported due to the given reason", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1608, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.7", "raw": "2022-01-17 04:56:42,017 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-01-17 04:56:42,017 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization of the guest operating system is not supported due to the given reason: ", "category": "customization_is_not_supported", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task customize linux guest os fatal localhost failed customization of the guest operating system is not supported due to the given reason", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1609, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_AlmaLinux_8_x-10_logs_failed_tasks_log.8", "raw": "2022-01-17 04:58:32,017 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-01-17 04:58:32,017 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_8.x/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization of the guest operating system is not supported due to the given reason: ", "category": "customization_is_not_supported", "processed": "timestamp failed at play go sc cloud init static ip timestamp task customize linux guest os fatal localhost failed customization of the guest operating system is not supported due to the given reason", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1610, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_11_64bit_Main_NVMe_EFI-12_logs_failed_tasks_log.6", "raw": "2022-01-20 05:13:13,020 | Failed at Play [gosc_sanity_staticip] **********************\n2022-01-20 05:13:13,020 | TASK [Customize Windows guest OS] **************************\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_Main_NVMe_EFI/ansible-vsphere-gos-validation/windows/guest_customization/win_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization of the guest operating system is not supported due to the given reason: ", "category": "customization_is_not_supported", "processed": "timestamp failed at play go sc sanity static ip timestamp task customize windows guest os fatal localhost failed customization of the guest operating system is not supported due to the given reason", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1611, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_11_64bit_Main_NVMe_EFI-12_logs_failed_tasks_log.7", "raw": "2022-01-20 05:16:21,020 | Failed at Play [gosc_sanity_dhcp] **************************\n2022-01-20 05:16:21,020 | TASK [Customize Windows guest OS] **************************\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_Main_NVMe_EFI/ansible-vsphere-gos-validation/windows/guest_customization/win_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization of the guest operating system is not supported due to the given reason: ", "category": "customization_is_not_supported", "processed": "timestamp failed at play go sc sanity dhcp timestamp task customize windows guest os fatal localhost failed customization of the guest operating system is not supported due to the given reason", "solution": "deepdive", "target": "product", "version": 202205240000}, {"id": 1612, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_RHEL_9_x-38_logs_failed_tasks_log.0", "raw": "2022-01-12 09:58:25,012 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2022-01-12 09:58:25,012 | TASK [Datastore file operation] ****************************\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Linux/RedHatEnterpriseLinux/9/9.0/Beta/RHEL-9.0.0-20211026.10-x86_64-dvd1.iso' is absent, cannot continue\n2022-01-12 09:59:01,012 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_nvme_vmxnet3\n", "category": "file_absent", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task data store file operation fatal localhost failed file os linux redhat enterprise linux number beta rhel version id hex id number x number dvd number i so is absent can not continue timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi nvme vmxnet number", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1161, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_ISO_70GA_IDE_VMXNET3_EFI-1_failed_tasks_log.0", "raw": "2022-03-28 03:39:54,028 | Failed at Play [deploy_vm_efi_ide_vmxnet3] *****************\n2022-03-28 03:39:54,028 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:30\nfatal: [localhost]: FAILED! => IP 10.185.250.54 is not pingable\n2022-03-28 03:40:58,028 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_ide_vmxnet3", "category": "failed_not_pingable_0509", "processed": "timestamp failed at play deploy vm efi ide vmxnet number timestamp task fail fatal localhost failed ip ip address is not pingable timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi ide vmxnet number", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1060, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Regression_SLED_15_x-19_failed_tasks_log.0", "raw": "2022-02-21 03:28:34,021 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-02-21 03:28:34,021 | TASK [Create snapshot 'BaseSnapshot' on 'test_sled15'] *****\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play nvme v hba device ops timestamp task create snapshot base snapshot on test sled number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1095, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS-17_failed_tasks_log.11", "raw": "2022-02-16 10:53:05,016 | Failed at Play [memory_hot_add_basic] **********************\n2022-02-16 10:53:05,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006378126'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play memory hot add basic timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1140, "name": "ansible_gosv_logs_202203_FAILURE_zyh_ansible_ubuntu_existing_testbed_3_1-28_failed_tasks_log.0", "raw": "2022-03-04 14:31:54,004 | Failed at Play [ovt_verify_install] ************************\n2022-03-04 14:31:54,004 | TASK [Collect filtered guest information for '10.117.16.201'] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/common/get_system_info.yml:22\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via ssh: Warning: Permanently added '10.117.16.201' (ED25519) to the list of known hosts.\nroot@10.117.16.201: Permission denied (publickey).", "category": "unreachable_ssh_0509_2", "processed": "timestamp failed at play ovt verify install timestamp task collect filtered guest information for ip address fatal localhost un reachable failed to connect to the host via ssh warning permanently added ip address to the list of known hosts root ip address permission denied public key", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1159, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Cycle_SLES_15_x-49_failed_tasks_log.0", "raw": "2022-03-07 22:05:59,007 | Failed at Play [sata_vhba_device_ops] **********************\n2022-03-07 22:05:59,007 | TASK [Run iozone test on new added disk] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_SLES_15.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/run_iozone_test.yml:14\nfatal: [localhost -> 10.168.170.251]: FAILED! => non-zero return code", "category": "non_zero_return_only_0509", "processed": "timestamp failed at play sata v hba device ops timestamp task run io zone test on new added disk fatal localhost ip address failed non zero return code", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1061, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.1", "raw": "2022-02-16 10:44:36,016 | Failed at Play [ovt_verify_status] *************************\n2022-02-16 10:44:36,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify status timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1062, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.11", "raw": "2022-02-16 10:49:41,016 | Failed at Play [memory_hot_add_basic] **********************\n2022-02-16 10:49:41,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play memory hot add basic timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1063, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.13", "raw": "2022-02-16 10:50:43,016 | Failed at Play [secureboot_enable_disable] *****************\n2022-02-16 10:50:43,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play secure boot enable disable timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1064, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.15", "raw": "2022-02-16 10:51:48,016 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-02-16 10:51:48,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006362701'] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play vmxnet number network device ops timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1121, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.5", "raw": "2022-02-16 10:42:58,016 | Failed at Play [stat_hosttime] *****************************\n2022-02-16 10:42:58,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play stat host time timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1122, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.6", "raw": "2022-02-16 10:43:30,016 | Failed at Play [check_inbox_driver] ************************\n2022-02-16 10:43:30,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check inbox driver timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1123, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.7", "raw": "2022-02-16 10:44:05,016 | Failed at Play [check_os_fullname] *************************\n2022-02-16 10:44:05,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check os full name timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1124, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot-17_failed_tasks_log.8", "raw": "2022-02-16 10:44:37,016 | Failed at Play [check_efi_firmware] ************************\n2022-02-16 10:44:37,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006396465'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_65U3_IDE_VMXNET3_EFI_SecureBoot/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check efi firmware timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1125, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.0", "raw": "2022-02-16 10:38:20,016 | Failed at Play [ovt_verify_install] ************************\n2022-02-16 10:38:20,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify install timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1126, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.1", "raw": "2022-02-16 10:38:51,016 | Failed at Play [ovt_verify_status] *************************\n2022-02-16 10:38:51,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify status timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1127, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.11", "raw": "2022-02-16 10:44:24,016 | Failed at Play [memory_hot_add_basic] **********************\n2022-02-16 10:44:24,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play memory hot add basic timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1128, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.13", "raw": "2022-02-16 10:45:26,016 | Failed at Play [secureboot_enable_disable] *****************\n2022-02-16 10:45:26,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play secure boot enable disable timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1129, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.17", "raw": "2022-02-16 10:47:33,016 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-02-16 10:47:33,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc perl dhcp timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1130, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.18", "raw": "2022-02-16 10:48:04,016 | Failed at Play [gosc_perl_staticip] ************************\n2022-02-16 10:48:04,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc perl static ip timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1131, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.19", "raw": "2022-02-16 10:48:37,016 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-02-16 10:48:37,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1132, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.2", "raw": "2022-02-16 10:39:23,016 | Failed at Play [vgauth_check_service] **********************\n2022-02-16 10:39:23,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play vg auth check service timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1133, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.20", "raw": "2022-02-16 10:49:10,016 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-02-16 10:49:10,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play go sc cloud init static ip timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1134, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.26", "raw": "2022-02-16 10:52:32,016 | Failed at Play [ovt_verify_uninstall] **********************\n2022-02-16 10:52:32,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play ovt verify un install timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1135, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.3", "raw": "2022-02-16 10:39:55,016 | Failed at Play [check_ip_address] **************************\n2022-02-16 10:39:55,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check ip address timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1136, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.5", "raw": "2022-02-16 10:40:59,016 | Failed at Play [stat_hosttime] *****************************\n2022-02-16 10:40:59,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play stat host time timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1137, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.6", "raw": "2022-02-16 10:41:30,016 | Failed at Play [check_inbox_driver] ************************\n2022-02-16 10:41:30,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check inbox driver timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1139, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS-15_failed_tasks_log.8", "raw": "2022-02-16 10:42:33,016 | Failed at Play [check_efi_firmware] ************************\n2022-02-16 10:42:33,016 | TASK [Create snapshot 'BaseSnapshot' on 'test_vm_1645006395172'] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_67U3_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.", "category": "unsufficient_disk_space_0509", "processed": "timestamp failed at play check efi firmware timestamp task create snapshot base snapshot on test vm number fatal localhost failed insufficient disk space on data store", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1167, "name": "ansible_gosv_logs_202202_FAILURE_Ansible_Photon_4_x_OVA_MAIN-1_failed_tasks_log.0", "raw": "2022-02-21 07:45:05,021 | Failed at Play [sata_vhba_device_ops] **********************\n2022-02-21 07:45:05,021 | TASK [absent disk to VM] ***********************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_OVA_MAIN/ansible-vsphere-gos-validation/common/vm_hot_add_remove_disk.yml:44\nexception in /vmware_guest_disk.py when main in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => Failed to manage disks for virtual machine 'test_vm' with exception : ('Unable to communicate with the remote host, since it is disconnected.', None)", "category": "unable_to_commu_as_disconnect_0509", "processed": "timestamp failed at play sata v hba device ops timestamp task absent disk to vm exception in vmware guest disk python when main in vmware python when wait for task fatal localhost failed failed to manage disks for virtual machine test vm with exception unable to communicate with the remote host since it is disconnected none", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1168, "name": "ansible_gosv_logs_202203_FAILURE_zyh_ansible_ubuntu_existing_testbed_3_1-30_failed_tasks_log.4", "raw": "2022-03-05 10:57:35,005 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-03-05 10:57:35,005 | TASK [Set VM CPU number and/or cores per socket number] ****\ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/common/vm_set_cpu_number.yml:9\nfatal: [localhost]: FAILED! => Unable to communicate with the remote host, since it is disconnected.", "category": "unable_to_commu_as_disconnect_0509", "processed": "timestamp failed at play cpu multi cores per socket timestamp task set vm cpu number and or cores per socket number fatal localhost failed unable to communicate with the remote host since it is disconnected", "solution": "retry", "target": "testbed", "version": 202205240000}, {"id": 1146, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Debian_11_x_64bit_MAIN_PARAVIRTUAL_VMXNET3_EFI-14_failed_tasks_log.0", "raw": "2022-03-05 18:14:29,005 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-03-05 18:14:29,005 | TASK [Fetch file /etc/network/interfaces.d/50-cloud-init.cfg from VM guest] \ntask path: /home/worker/workspace/Ansible_Debian_11.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Guest file /etc/network/interfaces.d/50-cloud-init.cfg does not exist : File /etc/network/interfaces.d/50-cloud-init.cfg was not found", "category": "guest_file_not_exist_0509", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task fetch file etc network interfaces d number cloud init configuration from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed guest file etc network interfaces d number cloud init configuration does not exist file etc network interfaces d number cloud init configuration was not found", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1147, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Debian_11_x_64bit_MAIN_PARAVIRTUAL_VMXNET3_EFI-14_failed_tasks_log.1", "raw": "2022-03-05 18:23:07,005 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-03-05 18:23:07,005 | TASK [Fetch file /etc/network/interfaces.d/50-cloud-init.cfg from VM guest] \ntask path: /home/worker/workspace/Ansible_Debian_11.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Guest file /etc/network/interfaces.d/50-cloud-init.cfg does not exist : File /etc/network/interfaces.d/50-cloud-init.cfg was not found", "category": "guest_file_not_exist_0509", "processed": "timestamp failed at play go sc cloud init static ip timestamp task fetch file etc network interfaces d number cloud init configuration from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed guest file etc network interfaces d number cloud init configuration does not exist file etc network interfaces d number cloud init configuration was not found", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1372, "name": "__Ansible_Regression_Ubuntu_21_10_Server_ISO-30_logs_failed_tasks_log.2", "raw": "2021-12-02 23:03:27,002 | Failed at Play [gosc_perl_staticip] ************************\n2021-12-02 23:03:27,002 | TASK [Fetch file /tmp/hostname_f.txt from VM guest] ********\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Failed to Fetch file from Vm VMware exception : (vim.fault.GuestOperationsUnavailable) {\n   dynamicType = <unset>,\n   dynamicProperty = (vmodl.DynamicProperty) [],\n   msg = 'The guest operations agent could not be contacted.',\n   faultCause = <unset>,\n   faultMessage = (vmodl.LocalizableMessage) []\n}\n", "category": "origin", "processed": "timestamp failed at play go sc perl static ip timestamp task fetch file tmp hostname f text from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed failed to fetch file from vm vmware exception vim fault guest operations unavailable dynamic property vmodl dynamic property message the guest operations agent could not be contacted fault message vmodl localizable message", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1541, "name": "ansible_gosv_logs_FAILURE_Ansible_Cycle_Ubuntu_20_04_3_ISO-24_logs_failed_tasks_log.2", "raw": "2022-01-05 15:46:48,005 | Failed at Play [gosc_perl_staticip] ************************\n2022-01-05 15:46:48,005 | TASK [Fetch file /etc/hosts from VM guest] *****************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.3_ISO/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Failed to Fetch file from Vm VMware exception : (vim.fault.GuestOperationsUnavailable) {\n   dynamicType = <unset>,\n   dynamicProperty = (vmodl.DynamicProperty) [],\n   msg = 'The guest operations agent could not be contacted.',\n   faultCause = <unset>,\n   faultMessage = (vmodl.LocalizableMessage) []\n}", "category": "soap_adapter", "processed": "timestamp failed at play go sc perl static ip timestamp task fetch file etc hosts from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed failed to fetch file from vm vmware exception vim fault guest operations unavailable dynamic property vmodl dynamic property message the guest operations agent could not be contacted fault message vmodl localizable message", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1542, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_OVA-27_logs_failed_tasks_log.0", "raw": "2022-01-12 11:50:39,012 | Failed at Play [check_quiesce_snapshot_custom_script] ******\n2022-01-12 11:50:39,012 | TASK [Fetch file /vss.log from VM guest] *******************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "soap_adapter", "processed": "timestamp failed at play check quiesce snapshot custom script timestamp task fetch file vss log from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1543, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_OVA-27_logs_failed_tasks_log.1", "raw": "2022-01-12 12:17:14,012 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-01-12 12:17:14,012 | TASK [Fetch file /var/log/vmware-imc/toolsDeployPkg.log from VM guest] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "soap_adapter", "processed": "timestamp failed at play go sc perl dhcp timestamp task fetch file var log vmware imc tools deploy package log from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1544, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_OVA-27_logs_failed_tasks_log.2", "raw": "2022-01-12 12:22:53,012 | Failed at Play [gosc_perl_staticip] ************************\n2022-01-12 12:22:53,012 | TASK [Fetch file /var/log/vmware-imc/toolsDeployPkg.log from VM guest] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "soap_adapter", "processed": "timestamp failed at play go sc perl static ip timestamp task fetch file var log vmware imc tools deploy package log from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1545, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_OVA-27_logs_failed_tasks_log.3", "raw": "2022-01-12 12:29:05,012 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-01-12 12:29:05,012 | TASK [Fetch file /var/log/vmware-imc/toolsDeployPkg.log from VM guest] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "soap_adapter", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task fetch file var log vmware imc tools deploy package log from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1546, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Photon_4_0_OVA-27_logs_failed_tasks_log.4", "raw": "2022-01-12 12:35:16,012 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-01-12 12:35:16,012 | TASK [Fetch file /var/log/vmware-imc/toolsDeployPkg.log from VM guest] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "soap_adapter", "processed": "timestamp failed at play go sc cloud init static ip timestamp task fetch file var log vmware imc tools deploy package log from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1547, "name": "ansible_gosv_logs_FAILURE_Ansible_Regression_Ubuntu_21_10_Server_ISO-38_logs_failed_tasks_log.0", "raw": "2021-12-09 12:08:39,009 | Failed at Play [gosc_perl_staticip] ************************\n2021-12-09 12:08:39,009 | TASK [Fetch file /tmp/ip_addr_show.txt from VM guest] ******\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Failed to Fetch file from Vm VMware exception : (vim.fault.GuestOperationsUnavailable) {\n   dynamicType = <unset>,\n   dynamicProperty = (vmodl.DynamicProperty) [],\n   msg = 'The guest operations agent could not be contacted.',\n   faultCause = <unset>,\n   faultMessage = (vmodl.LocalizableMessage) []\n}", "category": "soap_adapter", "processed": "timestamp failed at play go sc perl static ip timestamp task fetch file tmp ip address show text from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed failed to fetch file from vm vmware exception vim fault guest operations unavailable dynamic property vmodl dynamic property message the guest operations agent could not be contacted fault message vmodl localizable message", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1548, "name": "ansible_gosv_logs_FAILURE_Ansible_RockyLinux_8_x_70U1-3_logs_failed_tasks_log.2", "raw": "2021-12-10 04:50:55,010 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-10 04:50:55,010 | TASK [Fetch file /tmp/readlink_etc_localtime.txt from VM guest] \ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70U1/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Failed to Fetch file from Vm VMware exception : (vim.fault.GuestOperationsUnavailable) {\n   dynamicType = <unset>,\n   dynamicProperty = (vmodl.DynamicProperty) [],\n   msg = 'The guest operations agent could not be contacted.',\n   faultCause = <unset>,\n   faultMessage = (vmodl.LocalizableMessage) []\n}", "category": "soap_adapter", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task fetch file tmp read link etc local time text from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed failed to fetch file from vm vmware exception vim fault guest operations unavailable dynamic property vmodl dynamic property message the guest operations agent could not be contacted fault message vmodl localizable message", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1549, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_11_64bit_Main_NVMe_EFI-22_logs_failed_tasks_log.0", "raw": "2022-01-21 05:15:40,021 | Failed at Play [gosc_sanity_staticip] **********************\n2022-01-21 05:15:40,021 | TASK [Execute powershell command in Windows guest] *********\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_Main_NVMe_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.187.96.110]: FAILED! => non-zero return code when Remove-AppxProvisionedPackage\n2022-01-21 05:16:07,021 | TASK [Fetch file C:\\Windows\\Temp\\vmware-imc\\guestcust.log from VM guest] \ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_Main_NVMe_EFI/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Guest file C:\\Windows\\Temp\\vmware-imc\\guestcust.log does not exist : File C:\\Windows\\Temp\\vmware-imc\\guestcust.log was not found", "category": "soap_adapter", "processed": "timestamp failed at play go sc sanity static ip timestamp task execute powershell command in windows guest fatal localhost ip address failed non zero return code when remove appx provisioned package timestamp task fetch file c windows temp vmware imc guest cust log from vm guest exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed guest file c windows temp vmware imc guest cust log does not exist file c windows temp vmware imc guest cust log was not found", "solution": "deepdive", "target": "testcase", "version": 202205240000}, {"id": 1196, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_Server_LTSC_64bit_70U1_IDE_BIOS-4_logs_failed_tasks_log.0", "raw": "2021-12-09 04:15:32,009 | Failed at Play [testbed_deploy_nimbus] *********************\n2021-12-09 04:15:32,009 | TASK [Transfer nimbus testbed deploy spec to dbc server] ***\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_64bit_70U1_IDE_BIOS/newgos_testing_internal/testbed_deploy/deploy_from_dbc_prepare.yml:52\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added 'wdc-dbc2108.eng.vmware.com' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nsvc.gosv-automation@wdc-dbc2108.eng.vmware.com: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).", "category": "login_lockout", "processed": "timestamp failed at play testbed deploy nimbus timestamp task transfer nimbus testbed deploy spec to database c server fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added wdc eng vmware com to the list of known hosts permission denied please try again permission denied please try again svc go sv automation wdc eng vmware com permission denied public key gss api k eye x gss api with mic password", "solution": "deepdive", "target": "infra", "version": 202205240000}, {"id": 1197, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Photon_4_x_ISO_MAIN_NVME_E1000E_EFI-13_failed_tasks_log.0", "raw": "2022-03-04 11:45:45,004 | Failed at Play [ovt_verify_uninstall] **********************\n2022-03-04 11:45:45,004 | TASK [Check testbed deploy results folder exists] **********\ntask path: /home/worker/workspace/Ansible_Photon_4.x_ISO_MAIN_NVME_E1000E_EFI/newgos_testing_internal/testbed_deploy/testbed_cleanup.yml:65\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added 'wdc-dbc2108.eng.vmware.com' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nsvc.gosv-automation@wdc-dbc2108.eng.vmware.com: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).", "category": "login_lockout", "processed": "timestamp failed at play ovt verify un install timestamp task check testbed deploy results folder exists fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added wdc eng vmware com to the list of known hosts permission denied please try again permission denied please try again svc go sv automation wdc eng vmware com permission denied public key gss api k eye x gss api with mic password", "solution": "deepdive", "target": "infra", "version": 202205240000}, {"id": 1198, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Cycle_Photon_4_0_ISO-41_failed_tasks_log.2", "raw": "2022-03-02 03:37:59,002 | Failed at Play [gosc_perl_staticip] ************************\n2022-03-02 03:37:59,002 | TASK [Check testbed deploy results folder exists] **********\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.0_ISO/newgos_testing_internal/testbed_deploy/testbed_cleanup.yml:65\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added 'wdc-dbc2108.eng.vmware.com' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nsvc.gosv-automation@wdc-dbc2108.eng.vmware.com: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).", "category": "login_lockout", "processed": "timestamp failed at play go sc perl static ip timestamp task check testbed deploy results folder exists fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added wdc eng vmware com to the list of known hosts permission denied please try again permission denied please try again svc go sv automation wdc eng vmware com permission denied public key gss api k eye x gss api with mic password", "solution": "deepdive", "target": "infra", "version": 202205240000}, {"id": 1199, "name": "ansible_gosv_logs_FAILURE_Ansible_Windows_11_64bit_Main_PVSCSI_BIOS-3_logs_failed_tasks_log.5", "raw": "2021-12-08 18:59:10,008 | Failed at Play [cpu_hot_add_basic] *************************\n2021-12-08 18:59:10,008 | TASK [Check testbed deploy results folder exists] **********\ntask path: /home/worker/workspace/Ansible_Windows_11_64bit_Main_PVSCSI_BIOS/newgos_testing_internal/testbed_deploy/testbed_cleanup.yml:65\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added 'wdc-dbc2108.eng.vmware.com' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nsvc.gosv-automation@wdc-dbc2108.eng.vmware.com: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).", "category": "login_lockout", "processed": "timestamp failed at play cpu hot add basic timestamp task check testbed deploy results folder exists fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added wdc eng vmware com to the list of known hosts permission denied please try again permission denied please try again svc go sv automation wdc eng vmware com permission denied public key gss api k eye x gss api with mic password", "solution": "deepdive", "target": "infra", "version": 202205240000}, {"id": 1200, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Debian_10_x_32bit_MAIN_NVME_VMXNET3_EFI-6_failed_tasks_log.2", "raw": "2022-03-05 16:37:17,005 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-03-05 16:37:17,005 | TASK [Check testbed deploy results folder exists] **********\ntask path: /home/worker/workspace/Ansible_Debian_10.x_32bit_MAIN_NVME_VMXNET3_EFI/newgos_testing_internal/testbed_deploy/testbed_cleanup.yml:65\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added 'wdc-dbc2108.eng.vmware.com' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nsvc.gosv-automation@wdc-dbc2108.eng.vmware.com: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).", "category": "login_lockout", "processed": "timestamp failed at play go sc cloud init static ip timestamp task check testbed deploy results folder exists fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added wdc eng vmware com to the list of known hosts permission denied please try again permission denied please try again svc go sv automation wdc eng vmware com permission denied public key gss api k eye x gss api with mic password", "solution": "deepdive", "target": "infra", "version": 202205240000}, {"id": 1201, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Photon_4_x_ISO_70GA_SATA_E1000E_BIOS-17_failed_tasks_log.0", "raw": "2022-03-31 17:34:39,031 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-03-31 17:34:39,031 | TASK [Rescan all scsi devices] *****************************\ntask path: /home/worker/workspace/Ansible_Photon_4.x_ISO_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:30\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.206.85.113' (ECDSA) to the list of known hosts.\nroot@10.206.85.113: Permission denied (publickey,keyboard-interactive).", "category": "login_lockout", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task re scan all scsi devices fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1202, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI-31_failed_tasks_log.0", "raw": "2022-03-31 15:03:29,031 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-03-31 15:03:29,031 | TASK [Run iozone test on new added disk] *******************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/run_iozone_test.yml:14\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.206.98.221' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.206.98.221: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).", "category": "login_lockout", "processed": "timestamp failed at play para virtual v hba device ops timestamp task run io zone test on new added disk fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key gss api k eye x gss api with mic password", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1203, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI-31_failed_tasks_log.1", "raw": "2022-03-31 15:06:14,031 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-03-31 15:06:14,031 | TASK [Run iozone test on new added disk] *******************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/run_iozone_test.yml:14\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.206.98.221' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.206.98.221: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).", "category": "login_lockout", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task run io zone test on new added disk fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key gss api k eye x gss api with mic password", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1204, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI-31_failed_tasks_log.2", "raw": "2022-03-31 15:08:52,031 | Failed at Play [sata_vhba_device_ops] **********************\n2022-03-31 15:08:52,031 | TASK [Run iozone test on new added disk] *******************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/run_iozone_test.yml:14\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.206.98.221' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.206.98.221: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).", "category": "login_lockout", "processed": "timestamp failed at play sata v hba device ops timestamp task run io zone test on new added disk fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key gss api k eye x gss api with mic password", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1205, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_OVA_MAIN-7_failed_tasks_log.0", "raw": "2022-03-07 11:52:50,007 | Failed at Play [e1000e_network_device_ops] *****************\n2022-03-07 11:52:50,007 | TASK [Apply netplan configuration file for new added nic ens224 in Ubuntu] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_MAIN/ansible-vsphere-gos-validation/linux/network_device_ops/enable_new_ethernet.yml:86\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.185.2.170' (ECDSA) to the list of known hosts.\nroot@10.185.2.170: Permission denied (publickey,keyboard-interactive).", "category": "login_lockout", "processed": "timestamp failed at play network device ops timestamp task apply net plan configuration file for new added nic in ubuntu fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1206, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_OVA_67GA-10_failed_tasks_log.1", "raw": "2022-03-07 13:55:07,007 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-03-07 13:55:07,007 | TASK [Apply netplan configuration file for new added nic ens224 in Ubuntu] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_67GA/ansible-vsphere-gos-validation/linux/network_device_ops/enable_new_ethernet.yml:86\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.186.66.64' (ECDSA) to the list of known hosts.\nroot@10.186.66.64: Permission denied (publickey,keyboard-interactive).", "category": "login_lockout", "processed": "timestamp failed at play vmxnet number network device ops timestamp task apply net plan configuration file for new added nic in ubuntu fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1207, "name": "ansible_gosv_logs_202203_FAILURE_Ansible_Ubuntu_LTS_Server_OVA_67GA-9_failed_tasks_log.0", "raw": "2022-03-07 06:20:10,007 | Failed at Play [e1000e_network_device_ops] *****************\n2022-03-07 06:20:10,007 | TASK [Apply netplan configuration file for new added nic ens224 in Ubuntu] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_67GA/ansible-vsphere-gos-validation/linux/network_device_ops/enable_new_ethernet.yml:86\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.186.66.64' (RSA) to the list of known hosts.\nroot@10.186.66.64: Permission denied (publickey,keyboard-interactive).", "category": "login_lockout", "processed": "timestamp failed at play network device ops timestamp task apply net plan configuration file for new added nic in ubuntu fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address rsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1208, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS-36_failed_tasks_log.0", "raw": "2022-04-01 06:10:07,001 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-04-01 06:10:07,001 | TASK [Get block devices] ***********************************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/get_vm_device_list.yml:8\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.170.66.69' (ECDSA) to the list of known hosts.\nroot@10.170.66.69: Permission denied (publickey,keyboard-interactive).", "category": "login_lockout", "processed": "timestamp failed at play para virtual v hba device ops timestamp task get block devices fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1209, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Ubuntu_LTS_Server_ISO_MAIN_LSILOGIC_E1000E_EFI-17_failed_tasks_log.0", "raw": "2022-04-28 04:32:52,028 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-04-28 04:32:52,028 | TASK [Get link 'ens192' status] ****************************\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_MAIN_LSILOGIC_E1000E_EFI/ansible-vsphere-gos-validation/linux/network_device_ops/enable_new_ethernet.yml:138\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.78.112.161' (ECDSA) to the list of known hosts.\nroot@10.78.112.161: Permission denied (publickey,keyboard-interactive).", "category": "login_lockout", "processed": "timestamp failed at play vmxnet number network device ops timestamp task get link status fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1210, "name": "ansible_gosv_logs_202205_FAILURE_Ansible_RHEL_8_x_67GA_PARAVIRTUAL_E1000E_EFI-13_failed_tasks_log.0", "raw": "2022-05-04 05:07:27,004 | Failed at Play [ovt_verify_install] ************************\n2022-05-04 05:07:27,004 | TASK [Uninstall packages ['open-vm-tools', 'open-vm-tools-desktop']] \ntask path: /home/worker/workspace/Ansible_RHEL_8.x_67GA_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/linux/open_vm_tools/uninstall_ovt.yml:12\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: mux_client_request_session: read from master failed: Broken pipe\nWarning: Permanently added '10.186.28.173' (RSA) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.186.28.173: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).", "category": "login_lockout", "processed": "timestamp failed at play ovt verify install timestamp task un install packages open vm tools open vm tools desktop fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout mux client request session read from master failed broken pipe warning permanently added ip address rsa to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key gss api k eye x gss api with mic password", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1211, "name": "ansible_gosv_logs_FAILURE_Ansible_Ubuntu_Server_ISO_70U1-1_logs_failed_tasks_log.1", "raw": "2021-12-17 11:52:20,017 | Failed at Play [vmxnet3_network_device_ops] ****************\n2021-12-17 11:52:20,017 | TASK [Apply netplan configuration file for new added nic ens224] \ntask path: /home/worker/workspace/Ansible_Ubuntu_Server_ISO_70U1/ansible-vsphere-gos-validation/linux/network_device_ops/enable_new_ethernet.yml:62\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.206.98.67' (RSA) to the list of known hosts.\nroot@10.206.98.67: Permission denied (publickey,keyboard-interactive).", "category": "login_lockout", "processed": "timestamp failed at play vmxnet number network device ops timestamp task apply net plan configuration file for new added nic fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address rsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1401, "name": "__Ansible_Regression_Ubuntu_21_10_Server_ISO-22_logs_failed_tasks_log.0", "raw": "\n\n2021-12-02 09:17:10,002 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2021-12-02 09:17:10,002 | TASK [Collect filtered guest information for '10.180.101.53'] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_21.10_Server_ISO/ansible-vsphere-gos-validation/common/get_system_info.yml:22\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.180.101.53' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.180.101.53: Permission denied (publickey,password).", "category": "login_lockout", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task collect filtered guest information for ip address fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key password", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1402, "name": "__Ansible_Ubuntu_Server_ISO_70GA-8_logs_failed_tasks_log.0", "raw": "2021-12-01 07:42:05,001 | Failed at Play [gosc_perl_dhcp] ****************************\n2021-12-01 07:42:05,001 | TASK [Collect filtered guest information for '10.78.127.216'] \ntask path: /home/worker/workspace/Ansible_Ubuntu_Server_ISO_70GA/ansible-vsphere-gos-validation/common/get_system_info.yml:22\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.78.127.216' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.78.127.216: Permission denied (publickey,password).", "category": "login_lockout", "processed": "timestamp failed at play go sc perl dhcp timestamp task collect filtered guest information for ip address fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key password", "solution": "retry", "target": "targetvm", "version": 202205240000}, {"id": 1158, "name": "ansible_gosv_logs_202204_FAILURE_Ansible_Debian_11_x_64bit_MAIN_PARAVIRTUAL_VMXNET3_EFI-39_failed_tasks_log.0", "raw": "2022-04-21 01:24:12,021 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-04-21 01:24:12,021 | TASK [Copy testbed spec file to dbc server failed] *********\ntask path: /home/worker/workspace/Ansible_Debian_11.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_EFI/newgos_testing_internal/testbed_deploy/deploy_from_dbc_prepare.yml:70\nfatal: [localhost]: FAILED! => Copy nimbus testbed spec to dbc server sc-dbc2154.eng.vmware.com failed, can not execute nimbus command on it", "category": "copy_nimbus_spec_fail_0509", "processed": "timestamp failed at play testbed deploy nimbus timestamp task copy testbed spec file to database c server failed fatal localhost failed copy nimbus testbed spec to database c servers c eng vmware com failed can not execute nimbus command on it", "solution": "deepdive", "target": "infra", "version": 202205240000}, {"id": 1535, "name": "ansible_gosv_logs_FAILURE_Ansible_AlmaLinux_8_x_67U3-2_logs_failed_tasks_log.0", "raw": "2021-12-10 17:54:49,010 | Failed at Play [deploy_vm_efi_sata_vmxnet3] ****************\n2021-12-10 17:54:49,010 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_67U3/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.187.118.239:22\n2021-12-10 17:56:07,010 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_8.x_67U3/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_sata_vmxnet3", "category": "timeout_openssh__2", "processed": "timestamp failed at play deploy vm efi sata vmxnet number timestamp task wait for port number to become open or contain specific keyword fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task exit testing when exit testing when fail is set true fatal localhost failed failed to run test case deploy vm efi sata vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202205240000}, {"id": 1378, "name": "__Ansible_Regression_RockyLinux_8_x-5_logs_failed_tasks_log.0", "raw": "2021-11-26 07:24:36,026 | Failed at Play [env_setup] *********************************\n2021-11-26 07:24:36,026 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Regression_RockyLinux_8.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "vm_same_name", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202205240000}, {"id": 1613, "name": "22.txt", "raw": "2022-05-17 08:20:47,017 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-05-17 08:20:47,017 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220517070407.log] \ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-05-17 08:21:26,017 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\n", "category": "output_hidden_no_log", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1614, "name": "26.txt", "raw": "2022-05-16 15:14:19,016 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-05-16 15:14:19,016 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220516135726.log] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-05-16 15:14:51,016 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_CentOS_8.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "output_hidden_no_log", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible centos number x main para virtual vmxnet number efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible centos number x main para virtual vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1615, "name": "27.txt", "raw": "2022-05-17 05:42:48,017 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2022-05-17 05:42:48,017 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220517042722.log] \ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-05-17 05:43:21,017 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_nvme_vmxnet3", "category": "output_hidden_no_log", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible rocky linux number x nvme vmxnet number efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible rocky linux number x nvme vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi nvme vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1616, "name": "39.txt", "raw": "2022-05-17 06:01:20,017 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-05-17 06:01:20,017 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220517044602.log] \ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-05-17 06:01:52,017 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "output_hidden_no_log", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible rocky linux number x main para virtual vmxnet number efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible rocky linux number x main para virtual vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1628, "name": "19.txt", "raw": "2022-06-06 08:39:53,006 | Failed at Play [vgauth_check_service] **********************\n2022-06-06 08:39:53,006 | TASK [Skip testcase: vgauth_check_service, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'vgauth_check_service' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play vg auth check service timestamp task skip test case vg auth check service reason blocked task path home worker workspace ansible windows number main para virtual vmxnet number efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case vg auth check service is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1617, "name": "47.txt", "raw": "2022-05-17 19:06:08,017 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-05-17 19:06:08,017 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220517175148.log] \ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_9.x/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-05-17 19:06:41,017 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_9.x/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "output_hidden_no_log", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle rhel number x ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible cycle rhel number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1618, "name": "48.txt", "raw": "2022-05-17 19:21:54,017 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-05-17 19:21:54,017 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220517180421.log] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_22.04_ISO/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-05-17 19:22:28,017 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_22.04_ISO/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "output_hidden_no_log", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1619, "name": "80.txt", "raw": "2022-05-30 07:33:38,030 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-05-30 07:33:38,030 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220530054052.log] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-05-30 07:34:10,030 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_RHEL_9.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "output_hidden_no_log", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible rhel number x main para virtual vmxnet number efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible rhel number x main para virtual vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1625, "name": "1.txt", "raw": "2022-06-06 07:38:54,006 | Failed at Play [mouse_driver_vmtools] **********************\n2022-06-06 07:38:54,006 | TASK [Skip testcase: mouse_driver_vmtools, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'mouse_driver_vmtools' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play mouse driver vm tools timestamp task skip test case mouse driver vm tools reason blocked task path home worker workspace ansible windows server lts c main para virtual vmxnet number bios ansible vsphere gos validation common skip test case yml number fatal localhost failed test case mouse driver vm tools is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1620, "name": "89.txt", "raw": "2022-06-03 00:50:07,003 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-06-03 00:50:07,003 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220602233434.log] \ntask path: /home/worker/workspace/Ansible_Cycle_CentOS_8.x/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-06-03 00:50:42,003 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Cycle_CentOS_8.x/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "output_hidden_no_log", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle centos number x ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible cycle centos number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1621, "name": "58.txt", "raw": "2022-05-23 16:42:16,023 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-05-23 16:42:16,023 | TASK [Wait for new device to be present] *******************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:80\nfatal: [localhost]: UNREACHABLE! => Failed to create temporary directory. In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1653324136.7846885-16090-258133457335772 `\" && echo ansible-tmp-1653324136.7846885-16090-258133457335772=\"` echo /tmp/ansible-tmp-1653324136.7846885-16090-258133457335772 `\" ), exited with result 1\n2022-05-23 16:42:16,023 | TASK [Guest OS unreachable] ********************************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:91\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': 'Failed to create temporary directory. In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1653324136.7846885-16090-258133457335772 `\" && echo ansible-tmp-1653324136.7846885-16090-258133457335772=\"` echo /tmp/ansible-tmp-1653324136.7846885-16090-258133457335772 `\" ), exited with result 1', 'skip_reason': 'Host localhost is unreachable', 'unreachable': True}", "category": "fail_to_create_tem_dir", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task wait for new device to be present task path home worker workspace ansible regression ubuntu number server iso ansible vsphere gos validation linux v hba hot add remove wait device list changed yml number fatal localhost un reachable failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo tmp ansible tmp hex id number timestamp exited with result number timestamp task guest os un reachable task path home worker workspace ansible regression ubuntu number server iso ansible vsphere gos validation linux v hba hot add remove wait device list changed yml number fatal localhost failed changed false message failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo tmp ansible tmp hex id number timestamp exited with result number skip reason host localhost is un reachable un reachable true", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1624, "name": "0.txt", "raw": "2022-06-06 06:03:18,006 | Failed at Play [mouse_driver_vmtools] **********************\n2022-06-06 06:03:18,006 | TASK [Skip testcase: mouse_driver_vmtools, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'mouse_driver_vmtools' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play mouse driver vm tools timestamp task skip test case mouse driver vm tools reason blocked task path home worker workspace ansible windows server lts c main nvme bios ansible vsphere gos validation common skip test case yml number fatal localhost failed test case mouse driver vm tools is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1626, "name": "17.txt", "raw": "2022-06-06 08:39:01,006 | Failed at Play [vgauth_check_service] **********************\n2022-06-06 08:39:01,006 | TASK [Skip testcase: vgauth_check_service, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'vgauth_check_service' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play vg auth check service timestamp task skip test case vg auth check service reason blocked task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case vg auth check service is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1627, "name": "18.txt", "raw": "2022-06-06 07:44:14,006 | Failed at Play [vgauth_check_service] **********************\n2022-06-06 07:44:14,006 | TASK [Skip testcase: vgauth_check_service, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'vgauth_check_service' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play vg auth check service timestamp task skip test case vg auth check service reason blocked task path home worker workspace ansible windows server lts c main nvme bios ansible vsphere gos validation common skip test case yml number fatal localhost failed test case vg auth check service is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1629, "name": "2.txt", "raw": "2022-06-06 06:22:35,006 | Failed at Play [mouse_driver_vmtools] **********************\n2022-06-06 06:22:35,006 | TASK [Skip testcase: mouse_driver_vmtools, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'mouse_driver_vmtools' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play mouse driver vm tools timestamp task skip test case mouse driver vm tools reason blocked task path home worker workspace ansible windows number main para virtual vmxnet number efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case mouse driver vm tools is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1630, "name": "3.txt", "raw": "2022-06-06 07:38:49,006 | Failed at Play [mouse_driver_vmtools] **********************\n2022-06-06 07:38:49,006 | TASK [Skip testcase: mouse_driver_vmtools, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'mouse_driver_vmtools' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play mouse driver vm tools timestamp task skip test case mouse driver vm tools reason blocked task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case mouse driver vm tools is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1631, "name": "4.txt", "raw": "2022-06-06 08:37:33,006 | Failed at Play [mouse_driver_vmtools] **********************\n2022-06-06 08:37:33,006 | TASK [Skip testcase: mouse_driver_vmtools, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'mouse_driver_vmtools' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play mouse driver vm tools timestamp task skip test case mouse driver vm tools reason blocked task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case mouse driver vm tools is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1632, "name": "5.txt", "raw": "2022-06-06 08:41:44,006 | Failed at Play [stat_balloon] ******************************\n2022-06-06 08:41:44,006 | TASK [Skip testcase: stat_balloon, reason: Blocked] ********\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'stat_balloon' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play stat balloon timestamp task skip test case stat balloon reason blocked task path home worker workspace ansible windows number main para virtual vmxnet number efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case stat balloon is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1633, "name": "6.txt", "raw": "2022-06-06 07:42:00,006 | Failed at Play [stat_balloon] ******************************\n2022-06-06 07:42:00,006 | TASK [Skip testcase: stat_balloon, reason: Blocked] ********\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'stat_balloon' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play stat balloon timestamp task skip test case stat balloon reason blocked task path home worker workspace ansible windows server lts c main para virtual vmxnet number bios ansible vsphere gos validation common skip test case yml number fatal localhost failed test case stat balloon is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1683, "name": "10.txt", "raw": "2022-05-25 16:49:06,025 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-25 16:49:06,025 | TASK [Testbed deploy result is not PASS] *******************\ntask path: /home/worker/workspace/Ansible_SLED_15SP4_MAIN_NVME_E1000E_BIOS/newgos_testing_internal/testbed_deploy/parse_testbed_info.yml:10\nfatal: [localhost]: FAILED! => Get deployment result in testbedInfo.json is 'FAIL', not PASS", "category": "testbedinfo_fail", "processed": "timestamp failed at play testbed deploy nimbus timestamp task testbed deploy result is not pass task path home worker workspace ansible sled main nvme bios new gos testing internal testbed deploy parse testbed info yml number fatal localhost failed get deployment result in testbed info json is fail not pass", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1634, "name": "7.txt", "raw": "2022-06-06 06:06:32,006 | Failed at Play [stat_balloon] ******************************\n2022-06-06 06:06:32,006 | TASK [Skip testcase: stat_balloon, reason: Blocked] ********\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'stat_balloon' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play stat balloon timestamp task skip test case stat balloon reason blocked task path home worker workspace ansible windows server lts c main nvme bios ansible vsphere gos validation common skip test case yml number fatal localhost failed test case stat balloon is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1635, "name": "70.txt", "raw": "2022-05-25 14:40:06,025 | Failed at Play [stat_balloon] ******************************\n2022-05-25 14:40:06,025 | TASK [Skip testcase: stat_balloon, reason: Blocked] ********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'stat_balloon' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play stat balloon timestamp task skip test case stat balloon reason blocked task path home worker workspace ansible cycle windows number ansible vsphere gos validation common skip test case yml number fatal localhost failed test case stat balloon is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1636, "name": "8.txt", "raw": "2022-06-06 07:40:32,006 | Failed at Play [stat_balloon] ******************************\n2022-06-06 07:40:32,006 | TASK [Skip testcase: stat_balloon, reason: Blocked] ********\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'stat_balloon' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play stat balloon timestamp task skip test case stat balloon reason blocked task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case stat balloon is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1637, "name": "9.txt", "raw": "2022-06-06 08:40:53,006 | Failed at Play [stat_balloon] ******************************\n2022-06-06 08:40:53,006 | TASK [Skip testcase: stat_balloon, reason: Blocked] ********\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'stat_balloon' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play stat balloon timestamp task skip test case stat balloon reason blocked task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case stat balloon is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1638, "name": "92.txt", "raw": "2022-06-06 07:40:59,006 | Failed at Play [vgauth_check_service] **********************\n2022-06-06 07:40:59,006 | TASK [Skip testcase: vgauth_check_service, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'vgauth_check_service' is blocked because VMware tools installed: False, running: False", "category": "vmtools_not_installed", "processed": "timestamp failed at play vg auth check service timestamp task skip test case vg auth check service reason blocked task path home worker workspace ansible windows server lts c main para virtual vmxnet number bios ansible vsphere gos validation common skip test case yml number fatal localhost failed test case vg auth check service is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1639, "name": "11.txt", "raw": "2022-05-17 16:43:54,017 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-17 16:43:54,017 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_9.x/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Cycle_AlmaLinux_9.x-1/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible cycle alma linux number x new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible cycle alma linux number x number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1640, "name": "12.txt", "raw": "2022-05-17 16:45:07,017 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-17 16:45:07,017 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Cycle_RockyLinux_8.x/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Cycle_RockyLinux_8.x-88/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible cycle rocky linux number x new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible cycle rocky linux number x number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1641, "name": "13.txt", "raw": "2022-05-17 17:38:03,017 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-17 17:38:03,017 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Cycle_Windows_11_64-119/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible cycle windows number new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible cycle windows number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1642, "name": "14.txt", "raw": "2022-05-17 19:23:08,017 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-17 19:23:08,017 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Cycle_SLED_15.x/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Cycle_SLED_15.x-82/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible cycle sled number x new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible cycle sled number x number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1643, "name": "54.txt", "raw": "2022-05-16 13:43:47,016 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-16 13:43:47,016 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Flatcar_main/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Flatcar_main-39/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible flatcar main new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible flatcar main number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1686, "name": "50.txt", "raw": "2022-05-16 08:04:47,016 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-16 08:04:47,016 | TASK [Testbed deploy result is not PASS] *******************\ntask path: /home/worker/workspace/Ansible_Flatcar_70U3/newgos_testing_internal/testbed_deploy/parse_testbed_info.yml:10\nfatal: [localhost]: FAILED! => Get deployment result in testbedInfo.json is 'INVALID', not PASS", "category": "testbedinfo_fail", "processed": "timestamp failed at play testbed deploy nimbus timestamp task testbed deploy result is not pass task path home worker workspace ansible flatcar new gos testing internal testbed deploy parse testbed info yml number fatal localhost failed get deployment result in testbed info json is invalid not pass", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1644, "name": "64.txt", "raw": "2022-05-24 08:22:04,024 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-24 08:22:04,024 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_7.x/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Cycle_RHEL_7.x-85/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible cycle rhel number x new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible cycle rhel number x number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1645, "name": "65.txt", "raw": "2022-05-17 16:43:37,017 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-17 16:43:37,017 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Cycle_OracleLinux_8.x-77/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible cycle oracle linux number x new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible cycle oracle linux number x number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1646, "name": "67.txt", "raw": "2022-05-25 05:40:55,025 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-25 05:40:55,025 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U1_IDE_E1000E_BIOS/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Windows_Server_LTSC_70U1_IDE_E1000E_BIOS-32/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible windows server lts c ide bios new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible windows server lts c ide bios number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1647, "name": "74.txt", "raw": "2022-05-26 05:43:00,026 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-26 05:43:00,026 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U1_SATA_E1000E_EFI/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Windows_Server_LTSC_70U1_SATA_E1000E_EFI-31/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible windows server lts c sata efi new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible windows server lts c sata efi number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1648, "name": "77.txt", "raw": "2022-05-26 17:55:18,026 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-26 17:55:18,026 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_70U3_NVME_VMXNET3_EFI/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_AlmaLinux_9.x_70U3_NVME_VMXNET3_EFI-18/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible alma linux number x nvme vmxnet number efi new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible alma linux number x nvme vmxnet number efi number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1649, "name": "78.txt", "raw": "2022-05-26 17:55:33,026 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-26 17:55:33,026 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_70GA_IDE_VMXNET3_BIOS/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_AlmaLinux_9.x_70GA_IDE_VMXNET3_BIOS-12/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible alma linux number x number ga ide vmxnet number bios new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible alma linux number x number ga ide vmxnet number bios number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1650, "name": "87.txt", "raw": "2022-06-01 07:32:44,001 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-06-01 07:32:44,001 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_64/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Regression_Windows_10_64-65/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible regression windows number new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible regression windows number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1651, "name": "93.txt", "raw": "2022-06-06 15:46:07,006 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-06-06 15:46:07,006 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Regression_Photon_4.0_OVA-73/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible regression photon number ova new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible regression photon number ova number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1652, "name": "95.txt", "raw": "2022-06-07 10:51:56,007 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-06-07 10:51:56,007 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/newgos_testing_internal/testbed_deploy/deploy_from_dbc_run.yml:75\nfatal: [localhost]: FAILED! => Failed to fetch nimbus testbed deploy result http://sc-dbc2154.eng.vmware.com/qiz/nimbus/Ansible_Windows_11_MAIN_NVME_E1000E_EFI-43/testbed_result/testbedInfo.json from dbc server sc-dbc2154.eng.vmware.com. Nimbus testbed deployment failed.", "category": "fail_fetch_nimbus_result", "processed": "timestamp failed at play testbed deploy nimbus timestamp task fail task path home worker workspace ansible windows number main nvme efi new gos testing internal testbed deploy deploy from database c run yml number fatal localhost failed failed to fetch nimbus testbed deploy result http sc eng vmware com qiz nimbus ansible windows number main nvme efi number testbed result testbed info json from database c servers c eng vmware com nimbus testbed deployment failed", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1653, "name": "23.txt", "raw": "2022-02-06 14:48:45,006 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-02-06 14:48:45,006 | TASK [Execute powershell command in Windows guest] *********task path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, ''Connection reset by peer'))\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.1232123", "category": "connect_reset_unexpect_failure", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task execute powershell command in windows guest task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number connection reset error error number connection reset by peer url library number exceptions protocol error connection aborted connection reset error number connection reset by peer requests exceptions connection error connection aborted connection reset error number connection reset by peer fatal localhost failed unexpected failure during module execution number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1654, "name": "69.txt", "raw": "2022-05-25 06:50:25,025 | Failed at Play [wintools_complete_install_verify] **********\n2022-05-25 06:50:25,025 | TASK [Get VM 'test_vm' snapshot facts] *********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_snapshot_facts.yml:4\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.170.69.228:443 : [Errno 110] Connection timed out", "category": "unknown_connect_vc", "processed": "timestamp failed at play win tools complete install verify timestamp task get vm test vm snapshot facts task path home worker workspace ansible windows server lts c main nvme bios ansible vsphere gos validation common vm get snapshot facts yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1655, "name": "76.txt", "raw": "2022-05-26 10:18:37,026 | Failed at Play [vmlibrary_main] ****************************\n2022-05-26 10:18:37,026 | TASK [Execute powershell command 'C:\\vmlibrary\\winVMlibrary.ps1 -Config -CleanupFiles -ExeSdelete'] \ntask path: /home/worker/workspace/test_vmlibrary/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.191.137.205]: FAILED! => non-zero return code when C:\\vmlibrary\\winVMlibrary.ps1\n2022-05-26 10:19:50,026 | TASK [Fail the task and quit] ******************************\ntask path: /home/worker/workspace/test_vmlibrary/newgos_testing_internal/vmlibrary/vmlibrary_main.yml:41\nfatal: [localhost]: FAILED! => Failed to export windows-11-25126-64bit as VMLibrary template from 10.191.141.87", "category": "fail_export_lib_template", "processed": "timestamp failed at play vm library main timestamp task execute powershell command c vm library win vm library ps number configuration cleanup files exes delete task path home worker workspace test vm library ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when c vm library win vm library ps number timestamp task fail the task and quit task path home worker workspace test vm library new gos testing internal vm library vm library main yml number fatal localhost failed failed to export windows number b it as vm library template from ip address", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1656, "name": "91.txt", "raw": "2022-06-06 02:31:46,006 | Failed at Play [deploy_vm_efi_lsilogic_vmxnet3] ************\n2022-06-06 02:31:46,006 | TASK [Datastore file operation] ****************************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_ISO/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Linux/Ubuntu/22.04/GA/ubuntu-22.04-live-server-amd64.iso' is absent, cannot continue\n2022-06-06 02:31:47,006 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_ISO/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogic_vmxnet3", "category": "absent_cannot_continue", "processed": "timestamp failed at play deploy vm efi lsi logic vmxnet number timestamp task data store file operation task path home worker workspace ansible regression ubuntu number server iso ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed file os linux ubuntu number ga ubuntu number live server amd number i so is absent can not continue timestamp task testing exit due to failure task path home worker workspace ansible regression ubuntu number server iso ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic vmxnet number", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1657, "name": "20.txt", "raw": "2022-05-23 04:57:25,023 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-05-23 04:57:25,023 | TASK [Wait for new device to be absent] ********************\ntask path: /home/worker/workspace/Ansible_Regression_Flatcar_OVA/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:80\nfatal: [localhost -> 10.191.154.59]: FAILED! =>  when lsblk", "category": "failed_when_lsblk", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task wait for new device to be absent task path home worker workspace ansible regression flatcar ova ansible vsphere gos validation linux v hba hot add remove wait device list changed yml number fatal localhost ip address failed when ls blk", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1658, "name": "79.txt", "raw": "2022-05-28 04:24:47,028 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-05-28 04:24:47,028 | TASK [Wait for new device to be absent] ********************\ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_LTS/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:80\nfatal: [localhost -> 10.180.106.131]: FAILED! =>  when lsblk", "category": "failed_when_lsblk", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task wait for new device to be absent task path home worker workspace ansible cycle flatcar lts ansible vsphere gos validation linux v hba hot add remove wait device list changed yml number fatal localhost ip address failed when ls blk", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1687, "name": "85.txt", "raw": "2022-06-02 07:26:17,002 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-06-02 07:26:17,002 | TASK [Testbed deploy result is not PASS] *******************\ntask path: /home/worker/workspace/test_vmlibrary/newgos_testing_internal/testbed_deploy/parse_testbed_info.yml:10\nfatal: [localhost]: FAILED! => Get deployment result in testbedInfo.json is 'FAIL', not PASS", "category": "testbedinfo_fail", "processed": "timestamp failed at play testbed deploy nimbus timestamp task testbed deploy result is not pass task path home worker workspace test vm library new gos testing internal testbed deploy parse testbed info yml number fatal localhost failed get deployment result in testbed info json is fail not pass", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1682, "name": "83.txt", "raw": "2022-05-30 16:49:53,030 | Failed at Play [check_quiesce_snapshot] ********************\n2022-05-30 16:49:53,030 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_64/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "fail_to_revert_base_snap", "processed": "timestamp failed at play check quiesce snapshot timestamp task revert snapshot failed task path home worker workspace ansible regression windows number ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1684, "name": "15.txt", "raw": "2022-05-25 17:16:26,025 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-25 17:16:26,025 | TASK [Testbed deploy result is not PASS] *******************\ntask path: /home/worker/workspace/Ansible_SLES_15SP4_MAIN_PARAVIRTUAL_VMXNET3_EFI/newgos_testing_internal/testbed_deploy/parse_testbed_info.yml:10\nfatal: [localhost]: FAILED! => Get deployment result in testbedInfo.json is 'FAIL', not PASS", "category": "testbedinfo_fail", "processed": "timestamp failed at play testbed deploy nimbus timestamp task testbed deploy result is not pass task path home worker workspace ansible sles main para virtual vmxnet number efi new gos testing internal testbed deploy parse testbed info yml number fatal localhost failed get deployment result in testbed info json is fail not pass", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1685, "name": "16.txt", "raw": "2022-05-25 16:54:11,025 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-25 16:54:11,025 | TASK [Testbed deploy result is not PASS] *******************\ntask path: /home/worker/workspace/Ansible_SLES_15SP4_MAIN_NVME_E1000E_BIOS/newgos_testing_internal/testbed_deploy/parse_testbed_info.yml:10\nfatal: [localhost]: FAILED! => Get deployment result in testbedInfo.json is 'FAIL', not PASS", "category": "testbedinfo_fail", "processed": "timestamp failed at play testbed deploy nimbus timestamp task testbed deploy result is not pass task path home worker workspace ansible sles main nvme bios new gos testing internal testbed deploy parse testbed info yml number fatal localhost failed get deployment result in testbed info json is fail not pass", "solution": "retry", "target": "nimbus", "version": 202206171000}, {"id": 1674, "name": "68.txt", "raw": "2022-05-25 06:58:17,025 | Failed at Play [memory_hot_add_basic] **********************\n2022-05-25 06:58:17,025 | TASK [Execute powershell command '(get-wmiobject win32_computersystem | select numberoflogicalprocessors, numberofprocessors | ft -hide | Out-String).trim()'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nsocket.timeout: The read operation timed out\nurllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='10.191.225.51', port=5986): Read timed out. (read timeout=600)\nrequests.exceptions.ReadTimeout: HTTPSConnectionPool(host='10.191.225.51', port=5986): Read timed out. (read timeout=600)\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "socket_timeout", "processed": "timestamp failed at play memory hot add basic timestamp task execute powershell command get wmi object win number computer system select number of logical processors number of processors fth ide out string trim task path home worker workspace ansible windows server lts c lsi logic sas bios ansible vsphere gos validation windows utilities win execute command yml number socket timeout the read operation timed out url library number exceptions read timeout error https connection pool host ip address port number read timed out read timeout number requests exceptions read timeout https connection pool host ip address port number read timed out read timeout number fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1675, "name": "72.txt", "raw": "2022-05-26 03:14:26,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-05-26 03:14:26,026 | TASK [Execute powershell command '(Get-Disk | measure).Count'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nsocket.timeout: The read operation timed out\nurllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='10.191.154.92', port=5986): Read timed out. (read timeout=600)\nrequests.exceptions.ReadTimeout: HTTPSConnectionPool(host='10.191.154.92', port=5986): Read timed out. (read timeout=600)\nfatal: [localhost]: FAILED! => Unexpected failure during module execution.", "category": "socket_timeout", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task execute powershell command get disk measure count task path home worker workspace ansible windows server lts c number ga ide efi ansible vsphere gos validation windows utilities win execute command yml number socket timeout the read operation timed out url library number exceptions read timeout error https connection pool host ip address port number read timed out read timeout number requests exceptions read timeout https connection pool host ip address port number read timed out read timeout number fatal localhost failed unexpected failure during module execution", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1676, "name": "51.txt", "raw": "2022-05-16 08:17:27,016 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-05-16 08:17:27,016 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Flatcar_main/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "fail_to_revert_base_snap", "processed": "timestamp failed at play go sc perl dhcp timestamp task revert snapshot failed task path home worker workspace ansible flatcar main ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1677, "name": "52.txt", "raw": "2022-05-16 08:18:01,016 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-05-16 08:18:01,016 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Flatcar_main/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "fail_to_revert_base_snap", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task revert snapshot failed task path home worker workspace ansible flatcar main ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1678, "name": "53.txt", "raw": "2022-05-16 08:18:38,016 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-05-16 08:18:38,016 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Flatcar_main/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "fail_to_revert_base_snap", "processed": "timestamp failed at play para virtual v hba device ops timestamp task revert snapshot failed task path home worker workspace ansible flatcar main ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1679, "name": "56.txt", "raw": "2022-05-16 08:18:57,016 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-05-16 08:18:57,016 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Flatcar_main/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "fail_to_revert_base_snap", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task revert snapshot failed task path home worker workspace ansible flatcar main ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1680, "name": "71.txt", "raw": "2022-05-26 03:18:39,026 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-05-26 03:18:39,026 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "fail_to_revert_base_snap", "processed": "timestamp failed at play vmxnet number network device ops timestamp task revert snapshot failed task path home worker workspace ansible windows server lts c number ga ide efi ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1681, "name": "73.txt", "raw": "2022-05-26 03:22:52,026 | Failed at Play [memory_hot_add_basic] **********************\n2022-05-26 03:22:52,026 | TASK [Revert snapshot failed] ******************************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:46\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed", "category": "fail_to_revert_base_snap", "processed": "timestamp failed at play memory hot add basic timestamp task revert snapshot failed task path home worker workspace ansible windows server lts c number ga ide efi ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1690, "name": "38.txt", "raw": "2022-05-16 14:44:39,016 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-05-16 14:44:39,016 | TASK [Try to ping IP] **************************************\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping\n2022-05-16 14:45:11,016 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "non_zero_when_ping_2", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task try to ping ip task path home worker workspace ansible rhel number x main para virtual vmxnet number efi ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping timestamp task testing exit due to failure task path home worker workspace ansible rhel number x main para virtual vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1691, "name": "57.txt", "raw": "2022-05-17 19:27:19,017 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-05-17 19:27:19,017 | TASK [Try to ping IP] **************************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.x_Update/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping\n2022-05-17 19:27:49,017 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.x_Update/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "non_zero_when_ping_2", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task try to ping ip task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping timestamp task testing exit due to failure task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1694, "name": "37.txt", "raw": "2022-05-16 14:34:30,016 | Failed at Play [check_efi_firmware] ************************\n2022-05-16 14:34:30,016 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm", "category": "no_info_from_not_exist_vm", "processed": "timestamp failed at play check efi firmware timestamp task get specified property info for vm test vm task path home worker workspace ansible windows number main para virtual vmxnet number efi ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test vm", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1695, "name": "81.txt", "raw": "2022-05-30 07:52:59,030 | Failed at Play [check_os_fullname] *************************\n2022-05-30 07:52:59,030 | TASK [Get specified property info for VM 'test_vm'] ********\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm", "category": "no_info_from_not_exist_vm", "processed": "timestamp failed at play check os full name timestamp task get specified property info for vm test vm task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test vm", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1697, "name": "84.txt", "raw": "2022-05-30 16:56:19,030 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-05-30 16:56:19,030 | TASK [Set VM power state to 'shutdown-guest'] **************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_64/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:34\nfatal: [localhost]: FAILED! => Timeout while waiting for VM power off.", "category": "timeout_while_waiting", "processed": "timestamp failed at play cpu multi cores per socket timestamp task set vm power state to shutdown guest task path home worker workspace ansible regression windows number ansible vsphere gos validation common vm set power state yml number fatal localhost failed timeout while waiting for vm power off", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1698, "name": "30.txt", "raw": "2022-05-23 04:28:18,023 | Failed at Play [paravirtual_vhba_device_ops] ***************\n2022-05-23 04:28:18,023 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": "origin", "processed": "timestamp failed at play para virtual v hba device ops timestamp task wait for vmware tools collecting guest info task path home worker workspace ansible regression sled number x ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed hardware configuration table", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1699, "name": "40.txt", "raw": "2022-05-23 06:16:02,023 | Failed at Play [cpu_hot_add_basic] *************************\n2022-05-23 06:16:02,023 | TASK [VM log info check failure] ***************************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_Server_LTSC/ansible-vsphere-gos-validation/windows/utils/win_check_winbsod.yml:19\nfatal: [localhost]: FAILED! => Get Windows guest BSOD keyword 'WinBSOD:' in vmware.log.", "category": "origin", "processed": "timestamp failed at play cpu hot add basic timestamp task vm log info check failure task path home worker workspace ansible regression windows server lts c ansible vsphere gos validation windows utilities win check win bsod yml number fatal localhost failed get windows guest bsod keyword win bsod in vmware log", "solution": "deepdive", "target": "product", "version": 202206171000}, {"id": 1700, "name": "86.txt", "raw": "2022-06-03 02:03:55,003 | Failed at Play [ovt_verify_install] ************************\n2022-06-03 02:03:55,003 | TASK [Install packages ['open-vm-tools']] ******************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update/ansible-vsphere-gos-validation/linux/open_vm_tools/install_ovt.yml:35\nfatal: [localhost -> 10.180.111.126]: FAILED! => non-zero return code when tdnf\n2022-06-03 02:04:37,003 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install", "category": "origin", "processed": "timestamp failed at play ovt verify install timestamp task install packages open vm tools task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation linux open vm tools install ovt yml number fatal localhost ip address failed nonzero return code when tdnf timestamp task testing exit due to failure task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case ovt verify install", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1692, "name": "59.txt", "raw": "2022-05-23 08:04:33,023 | Failed at Play [secureboot_enable_disable] *****************\n2022-05-23 08:04:33,023 | TASK [Try to ping IP] **************************************\ntask path: /home/worker/workspace/Ansible_Regression_CentOS_8.x/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "non_zero_when_ping_2", "processed": "timestamp failed at play secure boot enable disable timestamp task try to ping ip task path home worker workspace ansible regression centos number x ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "deepdive", "target": "targetvm", "version": 202206171000}, {"id": 1701, "name": "90.txt", "raw": "2022-06-03 02:29:30,003 | Failed at Play [deploy_vm_efi_lsilogic_vmxnet3] ************\n2022-06-03 02:29:30,003 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_ISO/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table\n2022-06-03 02:29:58,003 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_ISO/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogic_vmxnet3", "category": "origin", "processed": "timestamp failed at play deploy vm efi lsi logic vmxnet number timestamp task wait for vmware tools collecting guest info task path home worker workspace ansible regression photon number iso ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed hardware configuration table timestamp task testing exit due to failure task path home worker workspace ansible regression photon number iso ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1702, "name": "29.txt", "raw": "2022-05-23 04:00:26,023 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-05-23 04:00:26,023 | TASK [Wait for new device to be present] *******************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_OVA/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:80\nfatal: [localhost]: UNREACHABLE! => Failed to create temporary directory.In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1653278426.1675-6193-215757058899332 `\" && echo ansible-tmp-1653278426.1675-6193-215757058899332=\"` echo /tmp/ansible-tmp-1653278426.1675-6193-215757058899332 `\" ), exited with result 1\n2022-05-23 04:00:26,023 | TASK [Guest OS unreachable] ********************************\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_OVA/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:91\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': 'Failed to create temporary directory.In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1653278426.1675-6193-215757058899332 `\" && echo ansible-tmp-1653278426.1675-6193-215757058899332=\"` echo /tmp/ansible-tmp-1653278426.1675-6193-215757058899332 `\" ), exited with result 1', 'skip_reason': 'Host localhost is unreachable', 'unreachable': True}", "category": "fail_create_temporary_dir_auth", "processed": "timestamp failed at play lsi logic v hba device ops timestamp task wait for new device to be present task path home worker workspace ansible regression ubuntu number server ova ansible vsphere gos validation linux v hba hot add remove wait device list changed yml number fatal localhost un reachable failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id timestamp echo ansible tmp hex id timestamp echo tmp ansible tmp hex id timestamp exited with result number timestamp task guest os un reachable task path home worker workspace ansible regression ubuntu number server ova ansible vsphere gos validation linux v hba hot add remove wait device list changed yml number fatal localhost failed changed false message failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id timestamp echo ansible tmp hex id timestamp echo tmp ansible tmp hex id timestamp exited with result number skip reason host localhost is un reachable un reachable true", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1703, "name": "32.txt", "raw": "2022-05-16 04:08:01,016 | Failed at Play [gosc_sanity_dhcp] **************************\n2022-05-16 04:08:01,016 | TASK [Get the absolute path in Windows guest] **************\ntask path: /home/worker/workspace/Ansible_Windows_11_67U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_get_path.yml:10\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "unreachable_host", "processed": "timestamp failed at play go sc sanity dhcp timestamp task get the absolute path in windows guest task path home worker workspace ansible windows number lsi logic sas efi ansible vsphere gos validation windows utilities win get path yml number fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1704, "name": "96.txt", "raw": "2022-06-10 06:54:11,010 | Failed at Play [gosc_sanity_dhcp] **************************\n2022-06-10 06:54:11,010 | TASK [Shutdown guest OS inside OS] *************************\ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_shutdown_restart.yml:8\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))", "category": "unreachable_host", "processed": "timestamp failed at play go sc sanity dhcp timestamp task shutdown guest os inside os task path home worker workspace ansible windows number ga ide efi ansible vsphere gos validation windows utilities win shutdown restart yml number fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1705, "name": "49.txt", "raw": "2022-05-23 03:57:01,023 | Failed at Play [deploy_vm_efi_nvme_e1000e] *****************\n2022-05-23 03:57:01,023 | TASK [Check VM 'test_windows11_22538' IP address] **********\ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_windows11_22538' IP Address\n2022-05-23 03:58:21,023 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_nvme_e1000e", "category": "fail_to_get_vm", "processed": "timestamp failed at play deploy vm efi nvme timestamp task check vm test windows number ip address task path home worker workspace ansible regression windows number ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test windows number ip address timestamp task testing exit due to failure task path home worker workspace ansible regression windows number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi nvme e number e", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1706, "name": "75.txt", "raw": "2022-05-26 05:02:19,026 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2022-05-26 05:02:19,026 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-05-26 05:02:56,026 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_e1000e", "category": "fail_to_get_vm", "processed": "timestamp failed at play deploy vm efi para virtual timestamp task check vm test vm ip address task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure task path home worker workspace ansible cycle windows number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual e number e", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1707, "name": "94.txt", "raw": "2022-06-07 02:17:42,007 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-06-07 02:17:42,007 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/test_vmlibrary/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-06-07 02:18:16,007 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/test_vmlibrary/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "fail_to_get_vm", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task check vm test vm ip address task path home worker workspace test vm library ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure task path home worker workspace test vm library ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1708, "name": "25.txt", "raw": "2022-05-16 11:19:43,016 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-05-16 11:19:43,016 | TASK [Parameter error] *************************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update/newgos_testing_internal/testbed_deploy/testbed_deploy_nimbus.yml:60\nfatal: [localhost]: FAILED! => Please specify vCenter build number in command line or in vars/internal_test.yml using 'vcenter_build_num' parameter", "category": "no_vcenter_number", "processed": "timestamp failed at play testbed deploy nimbus timestamp task parameter error task path home worker workspace ansible cycle photon number x update new gos testing internal testbed deploy testbed deploy nimbus yml number fatal localhost failed please specify vcenter build number in command line or in vars internal test yml using vcenter build number parameter", "solution": "deepdive", "target": "usererror", "version": 202206171000}, {"id": 1709, "name": "60.txt", "raw": "2022-05-17 18:47:02,017 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-05-17 18:47:02,017 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_Cycle_CentOS_8.x/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.191.184.117:22\n2022-05-17 18:47:39,017 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Cycle_CentOS_8.x/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "timeout_waiting_search_openssh", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for port number to become open or contain specific keyword task path home worker workspace ansible cycle centos number x ansible vsphere gos validation common vm wait ssh yml number fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task testing exit due to failure task path home worker workspace ansible cycle centos number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1710, "name": "100.txt", "raw": "2022-06-10 05:51:17,010 | Failed at Play [deploy_vm_efi_sata_vmxnet3] ****************\n2022-06-10 05:51:17,010 | TASK [Datastore file operation] ****************************\ntask path: /home/worker/workspace/dw_RHEL9_ARM/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => Failed to query for file 'ISO/rhel-baseos-9.0-aarch64-boot.iso'\n2022-06-10 05:51:18,010 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/dw_RHEL9_ARM/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_vmxnet3", "category": "manual", "processed": "timestamp failed at play deploy vm efi sata vmxnet number timestamp task data store file operation task path home worker workspace dw rhel number arm ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed failed to query for file iso rhel base os number a arch number boot iso timestamp task testing exit due to failure task path home worker workspace dw rhel number arm ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi sata vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1711, "name": "101.txt", "raw": "2022-06-08 07:21:26,008 | Failed at Play [deploy_vm_efi_sata_vmxnet3] ****************\n2022-06-08 07:21:26,008 | TASK [Set VM power state to 'powered-on'] ******************\ntask path: /home/worker/workspace/dw_RHEL9_ARM/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:34\nfatal: [localhost]: FAILED! => The serial port output file \"serial-20220608072106.log\" already exists. Do you want to replace it with new content or append new content to the end of the file?\n2022-06-08 07:21:51,008 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/dw_RHEL9_ARM/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_vmxnet3", "category": "manual", "processed": "timestamp failed at play deploy vm efi sata vmxnet number timestamp task set vm power state to powered on task path home worker workspace dw rhel number arm ansible vsphere gos validation common vm set power state yml number fatal localhost failed the serial port output file serial timestamp log already exists do you want to replace it with new content or append new content to the end of the file timestamp task testing exit due to failure task path home worker workspace dw rhel number arm ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi sata vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1622, "name": "66.txt", "raw": "2022-05-24 09:04:09,024 | Failed at Play [gosc_perl_staticip] ************************\n2022-05-24 09:04:09,024 | TASK [Fetch file /tmp/resolv.conf from VM guest] ***********\ntask path: /home/worker/workspace/Ansible_Cycle_Debian_11.x_32bit/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Failed to Fetch file from Vm VMware exception : (vim.fault.GuestOperationsUnavailable) {\n   dynamicType = <unset>,\n   dynamicProperty = (vmodl.DynamicProperty) [],\n   msg = 'The guest operations agent could not be contacted.',\n   faultCause = <unset>,\n   faultMessage = (vmodl.LocalizableMessage) []\n}", "category": "guestos_not_contacted", "processed": "timestamp failed at play go sc perl static ip timestamp task fetch file tmp resolve configuration from vm guest task path home worker workspace ansible cycle debian number x number b it ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed failed to fetch file from vm vmware exception vim fault guest operations unavailable dynamic type un set dynamic property vmodl dynamic property message the guest operations agent could not be contacted fault cause un set fault message vmodl localizable message ", "solution": "retry", "target": "targetvm", "version": 202206171000}, {"id": 1623, "name": "97.txt", "raw": "2022-06-10 07:38:49,010 | Failed at Play [gosc_sanity_staticip] **********************\n2022-06-10 07:38:49,010 | TASK [Customize Windows guest OS] **************************\ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/windows/guest_customization/win_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.\n2022-06-10 07:39:35,010 | TASK [Fetch file C:\\Windows\\Temp\\vmware-imc\\guestcust.log from VM guest] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Failed to Fetch file from Vm VMware exception : (vim.fault.GuestOperationsUnavailable) {\n   dynamicType = <unset>,\n   dynamicProperty = (vmodl.DynamicProperty) [],\n   msg = 'The guest operations agent could not be contacted.',\n   faultCause = <unset>,\n   faultMessage = (vmodl.LocalizableMessage) []\n}", "category": "guestos_not_contacted", "processed": "timestamp failed at play go sc sanity static ip timestamp task customize windows guest os task path home worker workspace ansible windows number sata efi ansible vsphere gos validation windows guest customization win go sc execution yml number fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out timestamp task fetch file c windows temp vmware imc guest cust log from vm guest task path home worker workspace ansible windows number sata efi ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed failed to fetch file from vm vmware exception vim fault guest operations unavailable dynamic type un set dynamic property vmodl dynamic property message the guest operations agent could not be contacted fault cause un set fault message vmodl localizable message ", "solution": "deepdive", "target": "testcase", "version": 202206171000}, {"id": 1696, "name": "82.txt", "raw": "2022-05-30 16:46:51,030 | Failed at Play [memory_hot_add_basic] **********************\n2022-05-30 16:46:51,030 | TASK [Set VM power state to 'shutdown-guest'] **************\ntask path: /home/worker/workspace/Ansible_Regression_Windows_10_64/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:34\nfatal: [localhost]: FAILED! => Timeout while waiting for VM power off.", "category": "timeout_while_waiting", "processed": "timestamp failed at play memory hot add basic timestamp task set vm power state to shutdown guest task path home worker workspace ansible regression windows number ansible vsphere gos validation common vm set power state yml number fatal localhost failed timeout while waiting for vm power off", "solution": "deepdive", "target": "targetvm", "version": 202206171000}, {"id": 1688, "name": "31.txt", "raw": "2022-05-23 08:49:41,023 | Failed at Play [secureboot_enable_disable] *****************\n2022-05-23 08:49:41,023 | TASK [Try to ping IP] **************************************\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_7.x/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "non_zero_when_ping_2", "processed": "timestamp failed at play secure boot enable disable timestamp task try to ping ip task path home worker workspace ansible regression rhel number x ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "deepdive", "target": "targetvm", "version": 202206171000}, {"id": 1689, "name": "36.txt", "raw": "2022-05-16 11:50:01,016 | Failed at Play [secureboot_enable_disable] *****************\n2022-05-16 11:50:01,016 | TASK [Try to ping IP] **************************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "non_zero_when_ping_2", "processed": "timestamp failed at play secure boot enable disable timestamp task try to ping ip task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "deepdive", "target": "targetvm", "version": 202206171000}, {"id": 1693, "name": "88.txt", "raw": "2022-06-03 05:14:47,003 | Failed at Play [secureboot_enable_disable] *****************\n2022-06-03 05:14:47,003 | TASK [Try to ping IP] **************************************\ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_7.x/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "non_zero_when_ping_2", "processed": "timestamp failed at play secure boot enable disable timestamp task try to ping ip task path home worker workspace ansible cycle rhel number x ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "deepdive", "target": "targetvm", "version": 202206171000}, {"id": 5419, "name": "log-3171", "raw": "2022-07-18 08:14:34,018 | Failed at Play [vbs_enable_disable] ************************\n2022-07-18 08:14:34,018 | TASK [Check VM 'dw0409_win10_v21H2' IP address] ************\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'dw0409_win10_v21H2' IP Address", "category": "", "processed": "timestamp failed at play vbs enable disable timestamp task check vm v number h number ip address task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm v number h number ip address", "solution": "deepdive", "target": "testcase", "version": 202207021500}, {"id": 5395, "name": "log-2838", "raw": "2022-07-12 08:05:33,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:05:33,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_9.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle alma linux number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5396, "name": "log-2839", "raw": "2022-07-12 08:06:08,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:06:08,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_CentOS_8.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle centos number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5397, "name": "log-2841", "raw": "2022-07-12 08:06:47,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:06:47,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Debian_11.x_64bit/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle debian number x number b it ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5398, "name": "log-2842", "raw": "2022-07-12 08:07:12,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:12,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_LTS/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle flatcar lts ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5399, "name": "log-2843", "raw": "2022-07-12 08:07:14,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:14,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_Stable/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle flatcar stable ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5400, "name": "log-2844", "raw": "2022-07-12 08:07:23,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:23,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update_OVA/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle photon number x update ova ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5401, "name": "log-2847", "raw": "2022-07-12 08:07:34,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:34,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.x_Update_ISO/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle photon number x update iso ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5402, "name": "log-2848", "raw": "2022-07-12 08:07:37,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:37,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5403, "name": "log-2849", "raw": "2022-07-12 08:07:48,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:48,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.0_ISO/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle photon number iso ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5404, "name": "log-2932", "raw": "2022-07-12 14:01:09,012 | Failed at Play [env_setup] *********************************\n2022-07-12 14:01:09,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_9.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle rhel number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5405, "name": "log-2852", "raw": "2022-07-12 08:09:46,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:09:46,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_SLED_15.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle sled number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5406, "name": "log-2853", "raw": "2022-07-12 08:10:51,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:10:51,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_RockyLinux_8.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle rocky linux number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5407, "name": "log-2854", "raw": "2022-07-12 08:11:06,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:11:06,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_SLES_15.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle sles number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5408, "name": "log-2931", "raw": "2022-07-12 14:00:27,012 | Failed at Play [env_setup] *********************************\n2022-07-12 14:00:27,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.4_ISO/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle ubuntu version id iso ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5409, "name": "log-2858", "raw": "2022-07-12 08:13:55,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:13:55,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle windows number ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5410, "name": "log-2930", "raw": "2022-07-12 14:00:23,012 | Failed at Play [env_setup] *********************************\n2022-07-12 14:00:23,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_22.04_ISO/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207021500}, {"id": 5411, "name": "log-2944", "raw": "2022-07-13 02:02:56,013 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-07-13 02:02:56,013 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220713004650.log] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_22.04_ISO/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-07-13 02:03:06,013 | TASK [Fetch file /tmp/cloud-init_2022-07-13-02-02-59.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_22.04_ISO/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "retry", "target": "testbed", "version": 202207021500}, {"id": 5412, "name": "log-2962", "raw": "2022-07-13 04:18:23,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 04:18:23,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.0_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.185.229.209' (ECDSA) to the list of known hosts.\nroot@10.185.229.209: Permission denied (publickey,keyboard-interactive).", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle photon number iso ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202207021500}, {"id": 5413, "name": "log-3007", "raw": "2022-07-13 05:05:25,013 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-07-13 05:05:25,013 | TASK [Collect filtered guest information for '10.185.110.153'] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_22.04_ISO/ansible-vsphere-gos-validation/common/get_system_info.yml:22\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.185.110.153' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.185.110.153: Permission denied (publickey,password).", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task collect filtered guest information for ip address task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common get system info yml number fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key password", "solution": "retry", "target": "targetvm", "version": 202207021500}, {"id": 5414, "name": "log-3046", "raw": "2022-07-13 08:57:53,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 08:57:53,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.x_Update_OVA/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.78.116.253' (ECDSA) to the list of known hosts.\nroot@10.78.116.253: Permission denied (publickey,keyboard-interactive).", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle photon number x update ova ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202207021500}, {"id": 5415, "name": "log-3148", "raw": "2022-07-14 00:31:12,014 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-07-14 00:31:12,014 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220713231750.log] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.4_ISO/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-07-14 00:31:23,014 | TASK [Fetch file /tmp/cloud-init_2022-07-14-00-31-15.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.4_ISO/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle ubuntu version id iso ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible cycle ubuntu version id iso ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "retry", "target": "testbed", "version": 202207021500}, {"id": 5416, "name": "log-3150", "raw": "2022-07-14 13:39:59,014 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-07-14 13:39:59,014 | TASK [Collect filtered guest information for '10.182.132.140'] \ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_Stable/ansible-vsphere-gos-validation/common/get_system_info.yml:22\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.182.132.140' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nReceived disconnect from 10.182.132.140 port 22:2: Too many authentication failures\nDisconnected from 10.182.132.140 port 22", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task collect filtered guest information for ip address task path home worker workspace ansible cycle flatcar stable ansible vsphere gos validation common get system info yml number fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again received disconnect from ip address port number too many authentication failures disconnected from ip address port number", "solution": "retry", "target": "targetvm", "version": 202207021500}, {"id": 5417, "name": "log-3151", "raw": "2022-07-15 03:02:59,015 | Failed at Play [env_setup] *********************************\n2022-07-15 03:02:59,015 | TASK [Enable debug log by default] *************************\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/env_setup/set_default_variables.yml:13\nfatal: [localhost]: FAILED! => The variable name 'enable_ansible.builtin.debug' is not valid. Variables must start with a letter or underscore character, and contain only letters, numbers and underscores.", "category": "", "processed": "timestamp failed at play environment setup timestamp task enable debug log by default task path home worker workspace ansible regression rhel number x ansible vsphere gos validation environment setup set default variables yml number fatal localhost failed the variable name enable ansible builtin debug is not valid variables must start with a letter or underscore character and contain only letters numbers and underscores", "solution": "deepdive", "target": "testcase", "version": 202207021500}, {"id": 5418, "name": "log-3166", "raw": "2022-07-15 16:44:29,015 | Failed at Play [deploy_vm_efi_sata_e1000e] *****************\n2022-07-15 16:44:29,015 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_RockyLinux_9.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table\n2022-07-15 16:45:08,015 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_RockyLinux_9.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi sata timestamp task wait for vmware tools collecting guest info task path home worker workspace ansible rocky linux number x number ga sata efi ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed hardware configuration table timestamp task testing exit due to failure task path home worker workspace ansible rocky linux number x number ga sata efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi sata e number e", "solution": "retry", "target": "targetvm", "version": 202207021500}, {"id": 5420, "name": "log-3180", "raw": "2022-07-19 10:37:30,019 | Failed at Play [nvme_vhba_device_ops_spec13] ***************\n2022-07-19 10:37:30,019 | TASK [Hot add or remove VM disk controller] ****************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/common/vm_hot_add_remove_disk_ctrl.yml:12\nexception in /vmware_guest_controller.py when configure_disk_controllers in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => ('The guest operating system did not respond to a hot-remove request for device nvme0 in a timely manner.', None)", "category": "", "processed": "timestamp failed at play nvme v hba device ops spec number timestamp task hot add or remove vm disk controller task path home worker workspace ansible cycle windows server number ansible vsphere gos validation common vm hot add remove disk ctrl yml number exception in vmware guest controller python when configure disk controllers in vmware python when wait for task fatal localhost failed the guest operating system did not respond to a hot remove request for device nvme number in a timely manner none", "solution": "deepdive", "target": "product", "version": 202207021500}, {"id": 5421, "name": "log-3181", "raw": "2022-07-19 10:22:17,019 | Failed at Play [nvme_vhba_device_ops_spec13] ***************\n2022-07-19 10:22:17,019 | TASK [Hot add or remove VM disk controller] ****************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/vm_hot_add_remove_disk_ctrl.yml:12\nexception in /vmware_guest_controller.py when configure_disk_controllers in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => ('The guest operating system did not respond to a hot-remove request for device nvme0 in a timely manner.', None)", "category": "", "processed": "timestamp failed at play nvme v hba device ops spec number timestamp task hot add or remove vm disk controller task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm hot add remove disk ctrl yml number exception in vmware guest controller python when configure disk controllers in vmware python when wait for task fatal localhost failed the guest operating system did not respond to a hot remove request for device nvme number in a timely manner none", "solution": "deepdive", "target": "product", "version": 202207021500}, {"id": 5422, "name": "log-3184", "raw": "2022-07-20 02:50:38,020 | Failed at Play [vmlibrary_main] ****************************\n2022-07-20 02:50:38,020 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Create_New_VMLibrary_Template/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:69\nfatal: [localhost]: FAILED! => (vmodl.fault.SystemError) {\n   dynamicType = <unset>,\n   dynamicProperty = (vmodl.DynamicProperty) [],\n   msg = 'A general system error occurred: Invalid fault',\n   faultCause = <unset>,\n   faultMessage = (vmodl.LocalizableMessage) [],\n   reason = 'Invalid fault'\n} ", "category": "", "processed": "timestamp failed at play vm library main timestamp task fail task path home worker workspace create new vm library template ansible vsphere gos validation common vm set power state yml number fatal localhost failed vmodl fault system error dynamic type un set dynamic property vmodl dynamic property message a general system error occurred invalid fault fault cause un set fault message vmodl localizable message reason invalid fault ", "solution": "deepdive", "target": "testcase", "version": 202207021500}, {"id": 3513, "name": "log-2332", "raw": "2022-06-28 14:27:33,028 | Failed at Play [check_ip_address] **************************\n2022-06-28 14:27:33,028 | TASK [Skip testcase: check_ip_address, reason: Blocked] ****\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'check_ip_address' is blocked because VMware tools installed: False, running: False", "category": "", "processed": "timestamp failed at play check ip address timestamp task skip test case check ip address reason blocked task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case check ip address is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202207041600}, {"id": 3514, "name": "log-2380", "raw": "2022-06-30 06:40:43,030 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-06-30 06:40:43,030 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3515, "name": "log-2371", "raw": "2022-06-30 04:33:26,030 | Failed at Play [deploy_vmwarephoton_ova] *******************\n2022-06-30 04:33:26,030 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_Photon_3.x_OVA_65U3/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.185.228.195:22\n2022-06-30 04:34:08,030 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Photon_3.x_OVA_65U3/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova", "category": "", "processed": "timestamp failed at play deploy vmware photon ova timestamp task wait for port number to become open or contain specific keyword task path home worker workspace ansible photon number x ova ansible vsphere gos validation common vm wait ssh yml number fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task testing exit due to failure task path home worker workspace ansible photon number x ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vmware photon ova", "solution": "deepdive", "target": "testcase", "version": 202207041600}, {"id": 3516, "name": "log-2338", "raw": "2022-06-29 06:56:46,029 | Failed at Play [deploy_vmwarephoton_ova] *******************\n2022-06-29 06:56:46,029 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_Photon_3.x_OVA_67GA/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.186.85.86:22\n2022-06-29 06:57:19,029 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Photon_3.x_OVA_67GA/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova", "category": "", "processed": "timestamp failed at play deploy vmware photon ova timestamp task wait for port number to become open or contain specific keyword task path home worker workspace ansible photon number x ova number ga ansible vsphere gos validation common vm wait ssh yml number fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task testing exit due to failure task path home worker workspace ansible photon number x ova number ga ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vmware photon ova", "solution": "deepdive", "target": "testcase", "version": 202207041600}, {"id": 3517, "name": "log-2450", "raw": "2022-07-02 11:29:05,002 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-07-02 11:29:05,002 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible windows number main para virtual vmxnet number efi ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3518, "name": "log-2372", "raw": "2022-06-30 06:01:47,030 | Failed at Play [vmlibrary_main] ****************************\n2022-06-30 06:01:47,030 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Create_New_VMLibrary_Template/newgos_testing_internal/vmlibrary/download_and_execute_script.yml:164\nfatal: [localhost]: FAILED! => ['At least one guest configuration is incorrect in 111563-Photon-3.0-Rev3U1-64-EFI-Open-VM-Tools', '2022-06-30 06:01:35 UTC |  ERROR| Check 2(SERVFAIL) is set in /etc/hostname ... NO!', '2022-06-30 06:01:35 UTC |  ERROR| Check STAF is running ... NO!', '2022-06-30 06:01:35 UTC |  ERROR| Check staf 10.168.218.1 ping ping get PONG ... NO!', '2022-06-30 06:01:45 UTC |  ERROR| Fail to download http://gosvpub.eng.vmware.com/GOSV-OVF/Scripts/apps/linux/vetSendIP.pl']", "category": "", "processed": "timestamp failed at play vm library main timestamp task fail task path home worker workspace create new vm library template new gos testing internal vm library download and execute script yml number fatal localhost failed at least one guest configuration is incorrect in number photon number efi open vm tools timestamp utc error check number serv fail is set in etc hostname no timestamp utc error check sta f is running no timestamp utc error check sta f ip address ping ping get pong no timestamp utc error fail to download http go sv pub eng vmware com go sv ovf scripts apps linux vet send ip pl", "solution": "deepdive", "target": "testcase", "version": 202207041600}, {"id": 3519, "name": "log-2381", "raw": "2022-06-30 06:40:50,030 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-06-30 06:40:50,030 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible windows server lts c main para virtual vmxnet number bios ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3520, "name": "log-2382", "raw": "2022-06-30 06:40:35,030 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-06-30 06:40:35,030 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible windows server lts c nvme efi ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3521, "name": "log-2384", "raw": "2022-06-30 06:41:13,030 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-06-30 06:41:13,030 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible windows server lts c para virtual efi ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3522, "name": "log-2383", "raw": "2022-06-30 06:41:01,030 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-06-30 06:41:01,030 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible windows server lts c lsi logic sas bios ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3523, "name": "log-2385", "raw": "2022-06-30 06:41:12,030 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-06-30 06:41:12,030 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible windows server lts c main nvme bios ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3524, "name": "log-2386", "raw": "2022-06-30 07:20:31,030 | Failed at Play [vmlibrary_main] ****************************\n2022-06-30 07:20:31,030 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Create_New_VMLibrary_Template/newgos_testing_internal/vmlibrary/download_and_execute_script.yml:164\nfatal: [localhost]: FAILED! => ['At least one guest configuration is incorrect in 111563-Photon-3.0-Rev3U1-64-EFI-Open-VM-Tools', '2022-06-30 07:20:19 UTC |  ERROR| Check reached is set in /etc/hostname ... NO!', '2022-06-30 07:20:19 UTC |  ERROR| Check STAF is running ... NO!', '2022-06-30 07:20:19 UTC |  ERROR| Check staf 10.168.218.1 ping ping get PONG ... NO!', '2022-06-30 07:20:30 UTC |  ERROR| Fail to download http://gosvpub.eng.vmware.com/GOSV-OVF/Scripts/apps/linux/vetSendIP.pl']", "category": "", "processed": "timestamp failed at play vm library main timestamp task fail task path home worker workspace create new vm library template new gos testing internal vm library download and execute script yml number fatal localhost failed at least one guest configuration is incorrect in number photon number efi open vm tools timestamp utc error check reached is set in etc hostname no timestamp utc error check sta f is running no timestamp utc error check sta f ip address ping ping get pong no timestamp utc error fail to download http go sv pub eng vmware com go sv ovf scripts apps linux vet send ip pl", "solution": "deepdive", "target": "testcase", "version": 202207041600}, {"id": 3525, "name": "log-2429", "raw": "2022-07-02 02:10:56,002 | Failed at Play [deploy_vm_ovf] *****************************\n2022-07-02 02:10:56,002 | TASK [Deploy VM from ovf template] *************************\ntask path: /home/worker/workspace/Ansible_Windows_MS_Template_70U2/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nexception in /vmware_deploy_ovf.py when run in /request.py when do_open\nfatal: [localhost]: FAILED! => <urlopen error The write operation timed out> Problem validating OVF import spec: Line 98: Invalid value 'hostonly' for element 'Connection'.\n2022-07-02 02:10:57,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_MS_Template_70U2/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_ovf", "category": "", "processed": "timestamp failed at play deploy vm ovf timestamp task deploy vm from ovf template task path home worker workspace ansible windows ms template ansible vsphere gos validation common ovf deploy yml number exception in vmware deploy ovf python when run in request python when do open fatal localhost failed url open error the write operation timed out problem validating ovf import spec line number invalid value host only for element connection timestamp task testing exit due to failure task path home worker workspace ansible windows ms template ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm ovf", "solution": "retry", "target": "targetvm", "version": 202207041600}, {"id": 3526, "name": "log-2514", "raw": "2022-07-04 00:11:31,004 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-07-04 00:11:31,004 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3527, "name": "log-2519", "raw": "2022-07-04 00:21:16,004 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-07-04 00:21:16,004 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3528, "name": "log-2517", "raw": "2022-07-04 00:13:53,004 | Failed at Play [nvme_vhba_device_ops_spec13] ***************\n2022-07-04 00:13:53,004 | TASK [Verify disk number increases in guest OS] ************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hotadd_vm_disk_existing_ctrl.yml:35\nfatal: [localhost]: FAILED! => Disk number not increase 2, before hotadd: 1, after hotadd: 2", "category": "", "processed": "timestamp failed at play nvme v hba device ops spec number timestamp task verify disk number increases in guest os task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows v hba hot add remove hot add vm disk existing ctrl yml number fatal localhost failed disk number not increase number before hot add number after hot add number", "solution": "deepdive", "target": "usererror", "version": 202207041600}, {"id": 3529, "name": "log-2444", "raw": "2022-07-02 04:51:48,002 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-07-02 04:51:48,002 | TASK [Wait for port 22 to become stopped] ******************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.x_Update/ansible-vsphere-gos-validation/linux/utils/shutdown.yml:13\nfatal: [localhost]: FAILED! => Timeout when waiting for 10.182.58.98:22 to stop.", "category": "", "processed": "timestamp failed at play cpu multi cores per socket timestamp task wait for port number to become stopped task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation linux utilities shutdown yml number fatal localhost failed timeout when waiting for ip address to stop", "solution": "retry", "target": "targetvm", "version": 202207041600}, {"id": 3530, "name": "log-2449", "raw": "2022-07-02 11:28:51,002 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-07-02 11:28:51,002 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3531, "name": "log-2451", "raw": "2022-07-02 11:29:19,002 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-07-02 11:29:19,002 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible windows number lsi logic sas vmxnet number efi ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3532, "name": "log-2452", "raw": "2022-07-02 11:29:20,002 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-07-02 11:29:20,002 | TASK [Check disk size after hot extend] ********************\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hot_extend_disk_test.yml:40\nfatal: [localhost]: FAILED! => Got disk size '1' GB in guest OS after hot extend to 2GB.", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task check disk size after hot extend task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation windows v hba hot add remove hot extend disk test yml number fatal localhost failed got disk size number gb in guest os after hot extend to number gb", "solution": "deepdive", "target": "product", "version": 202207041600}, {"id": 3533, "name": "log-2498", "raw": "2022-07-03 06:01:04,003 | Failed at Play [check_quiesce_snapshot] ********************\n2022-07-03 06:01:04,003 | TASK [Skip testcase: check_quiesce_snapshot, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'check_quiesce_snapshot' is blocked because VMware tools installed: False, running: False", "category": "", "processed": "timestamp failed at play check quiesce snapshot timestamp task skip test case check quiesce snapshot reason blocked task path home worker workspace ansible cycle windows number ansible vsphere gos validation common skip test case yml number fatal localhost failed test case check quiesce snapshot is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202207041600}, {"id": 3534, "name": "log-2488", "raw": "2022-07-03 04:23:18,003 | Failed at Play [check_ip_address] **************************\n2022-07-03 04:23:18,003 | TASK [Skip testcase: check_ip_address, reason: Blocked] ****\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'check_ip_address' is blocked because VMware tools installed: False, running: False", "category": "", "processed": "timestamp failed at play check ip address timestamp task skip test case check ip address reason blocked task path home worker workspace ansible cycle windows number ansible vsphere gos validation common skip test case yml number fatal localhost failed test case check ip address is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202207041600}, {"id": 3535, "name": "log-2527", "raw": "2022-07-04 07:20:55,004 | Failed at Play [deploy_vmwarephoton_ova] *******************\n2022-07-04 07:20:55,004 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.185.229.182:22\n2022-07-04 07:21:14,004 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova", "category": "", "processed": "timestamp failed at play deploy vmware photon ova timestamp task wait for port number to become open or contain specific keyword task path home worker workspace ansible regression photon number ova ansible vsphere gos validation common vm wait ssh yml number fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task testing exit due to failure task path home worker workspace ansible regression photon number ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vmware photon ova", "solution": "deepdive", "target": "testcase", "version": 202207041600}, {"id": 3536, "name": "log-2534", "raw": "2022-07-04 17:09:20,004 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-07-04 17:09:20,004 | TASK [Create a new VM 'zyh_ansible_ubuntu_2204_desktop_autoinstall' on server '10.185.45.140'] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A component of the virtual machine is not accessible on the host.\n2022-07-04 17:09:22,004 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task create a new vm zyh ansible ubuntu number desktop auto install on server ip address task path home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation common vm create yml number fatal localhost failed failed to create a virtual machine a component of the virtual machine is not accessible on the host timestamp task testing exit due to failure task path home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "usererror", "version": 202207041600}, {"id": 3537, "name": "log-2535", "raw": "2022-07-05 05:13:14,005 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-07-05 05:13:14,005 | TASK [Fetch file /var/log/cloud-init.log from VM guest] ****\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_ISO/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nfatal: [localhost]: FAILED! => Failed to fetch file : Request failed: <urlopen error timed out>", "category": "", "processed": "timestamp failed at play go sc cloud init static ip timestamp task fetch file var log cloud init log from vm guest task path home worker workspace ansible regression ubuntu number server iso ansible vsphere gos validation common vm guest file operation yml number fatal localhost failed failed to fetch file request failed url open error timed out", "solution": "retry", "target": "targetvm", "version": 202207041600}, {"id": 4464, "name": "log-2908", "raw": "2022-07-12 13:23:36,012 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-12 13:23:36,012 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.4_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.187.131.94 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle ubuntu version id iso ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4465, "name": "log-2984", "raw": "2022-07-13 04:37:22,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 04:37:22,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_9.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.187.157.231 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle rhel number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4432, "name": "log-3013", "raw": "2022-07-13 08:17:23,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 08:17:23,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_Stable/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.168.207.54 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle flatcar stable ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4433, "name": "log-3062", "raw": "2022-07-13 09:15:10,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 09:15:10,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Debian_11.x_64bit/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.168.212.147 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle debian number x number b it ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4434, "name": "log-3137", "raw": "2022-07-13 13:00:08,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 13:00:08,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_CentOS_8.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.78.120.117 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle centos number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4435, "name": "log-3075", "raw": "2022-07-13 09:22:23,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 09:22:23,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_9.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.184.104.196 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle alma linux number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4436, "name": "log-3095", "raw": "2022-07-13 09:52:38,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 09:52:38,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.78.237.191 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle photon number x update iso ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4437, "name": "log-3071", "raw": "2022-07-13 09:20:15,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 09:20:15,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_8.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.78.119.71 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle rhel number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4438, "name": "log-2744", "raw": "2022-07-11 14:25:34,011 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-11 14:25:34,011 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.0_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.184.80.131 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle photon number iso ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4439, "name": "log-3078", "raw": "2022-07-13 09:25:19,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 09:25:19,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Debian_11.x_32bit/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.186.45.163 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle debian number x number b it ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4440, "name": "log-2611", "raw": "2022-07-10 15:47:27,010 | Failed at Play [wintools_complete_install_verify] **********\n2022-07-10 15:47:27,010 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task check vm test vm ip address task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "testcase", "version": 202207151000}, {"id": 4466, "name": "log-2946", "raw": "2022-07-13 01:21:10,013 | Failed at Play [check_ip_address] **************************\n2022-07-13 01:21:10,013 | TASK [Execute powershell command '(Get-NetAdapter | where-object {$_.Name -notmatch 'vEthernet'}).Name'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n2022-07-13 01:21:30,013 | TASK [Guest OS unreachable] ********************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:39\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': \"Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\", 'skip_reason': 'Host localhost is unreachable', 'unreachable': True}", "category": "", "processed": "timestamp failed at play check ip address timestamp task execute powershell command get net adapter where object name not match v ethernet name task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer timestamp task guest os un reachable task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost failed changed false message failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer skip reason host localhost is un reachable un reachable true", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4467, "name": "log-2947", "raw": "2022-07-13 01:43:50,013 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-07-13 01:43:50,013 | TASK [Execute powershell command '(Get-WmiObject Win32_SCSIController | where-object {$_.Name -like '*NVM Express*'} | measure).Count'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n2022-07-13 01:44:08,013 | TASK [Guest OS unreachable] ********************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:39\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': \"Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\", 'skip_reason': 'Host localhost is unreachable', 'unreachable': True}", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task execute powershell command get wmi object win number scsi controller where object name like nvm express measure count task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer timestamp task guest os un reachable task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost failed changed false message failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer skip reason host localhost is un reachable un reachable true", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4429, "name": "log-2569", "raw": "2022-07-06 01:07:51,006 | Failed at Play [deploy_vmwarephoton_ova] *******************\n2022-07-06 01:07:51,006 | TASK [Wait for VMware Tools collecting guest OS fullname] **\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.x_Update_OVA/ansible-vsphere-gos-validation/common/vm_wait_guest_fullname.yml:17\nfatal: [localhost]: FAILED! => hardware configuration table\n2022-07-06 01:08:10,006 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.x_Update_OVA/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova", "category": "", "processed": "timestamp failed at play deploy vmware photon ova timestamp task wait for vmware tools collecting guest os full name task path home worker workspace ansible cycle photon number x update ova ansible vsphere gos validation common vm wait guest full name yml number fatal localhost failed hardware configuration table timestamp task testing exit due to failure task path home worker workspace ansible cycle photon number x update ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vmware photon ova", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4430, "name": "log-3027", "raw": "2022-07-13 08:29:26,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 08:29:26,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_LTS/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.168.186.92 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle flatcar lts ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4431, "name": "log-3016", "raw": "2022-07-13 06:53:02,013 | Failed at Play [nvme_vhba_device_ops_spec13] ***************\n2022-07-13 06:53:02,013 | TASK [Execute powershell command '(Get-WmiObject Win32_SCSIController | where-object {$_.Name -like '*NVM Express*'} | measure).Count'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n2022-07-13 06:53:27,013 | TASK [Guest OS unreachable] ********************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:39\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': \"Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\", 'skip_reason': 'Host localhost is unreachable', 'unreachable': True}", "category": "", "processed": "timestamp failed at play nvme v hba device ops spec number timestamp task execute powershell command get wmi object win number scsi controller where object name like nvm express measure count task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer timestamp task guest os un reachable task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost failed changed false message failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer skip reason host localhost is un reachable un reachable true", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4441, "name": "log-2762", "raw": "2022-07-11 14:36:58,011 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-11 14:36:58,011 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_9.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.168.187.97 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle alma linux number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4442, "name": "log-2910", "raw": "2022-07-12 13:24:36,012 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-12 13:24:36,012 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_22.04_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.186.109.116 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4443, "name": "log-3079", "raw": "2022-07-13 09:25:34,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 09:25:34,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_SLED_15.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.186.55.32 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle sled number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4444, "name": "log-3089", "raw": "2022-07-13 09:43:59,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 09:43:59,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_9.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.182.134.215 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4445, "name": "log-3081", "raw": "2022-07-13 09:37:33,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 09:37:33,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_RockyLinux_8.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.78.114.221 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle rocky linux number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4446, "name": "log-3006", "raw": "2022-07-13 04:55:02,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 04:55:02,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_SLES_15.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.162.90.4 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle sles number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4449, "name": "log-2661", "raw": "2022-07-11 00:14:20,011 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-07-11 00:14:20,011 | TASK [Execute powershell command '(Get-Disk | measure).Count'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n2022-07-11 00:14:39,011 | TASK [Guest OS unreachable] ********************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:39\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': \"Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\", 'skip_reason': 'Host localhost is unreachable', 'unreachable': True}", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task execute powershell command get disk measure count task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer timestamp task guest os un reachable task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost failed changed false message failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer skip reason host localhost is un reachable un reachable true", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4451, "name": "log-3103", "raw": "2022-07-13 10:22:22,013 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-07-13 10:22:22,013 | TASK [Execute powershell command '(Get-WmiObject Win32_SCSIController | where-object {$_.Name -like '*NVM Express*'} | measure).Count'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n2022-07-13 10:22:44,013 | TASK [Guest OS unreachable] ********************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:39\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': \"Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\", 'skip_reason': 'Host localhost is unreachable', 'unreachable': True}", "category": "", "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task execute powershell command get wmi object win number scsi controller where object name like nvm express measure count task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer timestamp task guest os un reachable task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost failed changed false message failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer skip reason host localhost is un reachable un reachable true", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4452, "name": "log-3037", "raw": "2022-07-13 08:37:48,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 08:37:48,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_Stable/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.78.126.169 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle flatcar stable ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4453, "name": "log-2722", "raw": "2022-07-11 13:42:50,011 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-11 13:42:50,011 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_LTS/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.186.53.55 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle flatcar lts ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4454, "name": "log-2749", "raw": "2022-07-11 14:28:33,011 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-11 14:28:33,011 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.4_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.184.86.75 port 22: Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle ubuntu version id iso ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection timed out", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4455, "name": "log-2751", "raw": "2022-07-11 14:29:07,011 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-11 14:29:07,011 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_SLED_15.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.168.191.15 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle sled number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4456, "name": "log-2765", "raw": "2022-07-11 14:40:34,011 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-11 14:40:34,011 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_RockyLinux_8.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.187.119.176 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle rocky linux number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4457, "name": "log-3041", "raw": "2022-07-13 08:44:54,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 08:44:54,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update_OVA/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.187.114.134 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle photon number x update ova ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4458, "name": "log-3069", "raw": "2022-07-13 09:19:19,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 09:19:19,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.187.151.29 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4460, "name": "log-2873", "raw": "2022-07-12 11:40:10,012 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-07-12 11:40:10,012 | TASK [Execute powershell command '(get-wmiobject win32_computersystem | select numberoflogicalprocessors, numberofprocessors | ft -hide | Out-String).trim()'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n2022-07-12 11:40:33,012 | TASK [Guest OS unreachable] ********************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:39\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': \"Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\", 'skip_reason': 'Host localhost is unreachable', 'unreachable': True}", "category": "", "processed": "timestamp failed at play cpu multi cores per socket timestamp task execute powershell command get wmi object win number computer system select number of logical processors number of processors fth ide out string trim task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer timestamp task guest os un reachable task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost failed changed false message failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer skip reason host localhost is un reachable un reachable true", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4461, "name": "log-2882", "raw": "2022-07-12 10:10:16,012 | Failed at Play [check_os_fullname] *************************\n2022-07-12 10:10:16,012 | TASK [Execute powershell command '(Get-WmiObject win32_OperatingSystem).name'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n2022-07-12 10:10:38,012 | TASK [Guest OS unreachable] ********************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:39\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': \"Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\", 'skip_reason': 'Host localhost is unreachable', 'unreachable': True}", "category": "", "processed": "timestamp failed at play check os full name timestamp task execute powershell command get wmi object win number operating system name task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer timestamp task guest os un reachable task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost failed changed false message failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer skip reason host localhost is un reachable un reachable true", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4462, "name": "log-3061", "raw": "2022-07-13 09:14:52,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 09:14:52,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.0_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.168.168.111 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle photon number iso ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4463, "name": "log-2903", "raw": "2022-07-12 13:16:30,012 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-12 13:16:30,012 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_CentOS_8.x/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.187.148.32 port 22: No route to host", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle centos number x ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4448, "name": "log-2813", "raw": "2022-07-12 00:06:32,012 | Failed at Play [env_setup] *********************************\n2022-07-12 00:06:32,012 | TASK [Get shell executable on ESXi server] *****************\ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:54\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.78.127.199' (ECDSA) to the list of known hosts.\nShared connection to 10.78.127.199 closed.", "category": "", "processed": "timestamp failed at play environment setup timestamp task get shell executable on esxi server task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts shared connection to ip address closed", "solution": "retry", "target": "nimbus", "version": 202207151000}, {"id": 4450, "name": "log-2830", "raw": "2022-07-12 01:03:37,012 | Failed at Play [gosc_sanity_staticip] **********************\n2022-07-12 01:03:37,012 | TASK [Customize Windows guest OS] **************************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/guest_customization/win_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.\n2022-07-12 01:04:36,012 | TASK [Fetch file C:\\Windows\\Temp\\vmware-imc\\guestcust.log from VM guest] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user Administrator", "category": "", "processed": "timestamp failed at play go sc sanity static ip timestamp task customize windows guest os task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows guest customization win go sc execution yml number fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out timestamp task fetch file c windows temp vmware imc guest cust log from vm guest task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user administrator", "solution": "deepdive", "target": "testcase", "version": 202207151000}, {"id": 4459, "name": "log-2929", "raw": "2022-07-12 13:56:36,012 | Failed at Play [check_inbox_driver] ************************\n2022-07-12 13:56:36,012 | TASK [Collect filtered guest information for '10.182.11.84'] \ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_Stable/ansible-vsphere-gos-validation/common/get_system_info.yml:22\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.182.11.84' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nReceived disconnect from 10.182.11.84 port 22:2: Too many authentication failures\nDisconnected from 10.182.11.84 port 22", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task collect filtered guest information for ip address task path home worker workspace ansible cycle flatcar stable ansible vsphere gos validation common get system info yml number fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again received disconnect from ip address port number too many authentication failures disconnected from ip address port number", "solution": "retry", "target": "targetvm", "version": 202207151000}, {"id": 4447, "name": "log-2837", "raw": "2022-07-12 07:56:11,012 | Failed at Play [env_setup] *********************************\n2022-07-12 07:56:11,012 | TASK [Get shell executable on ESXi server] *****************\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_9.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:54\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.168.191.210' (ECDSA) to the list of known hosts.\nShared connection to 10.168.191.210 closed.", "category": "", "processed": "timestamp failed at play environment setup timestamp task get shell executable on esxi server task path home worker workspace ansible cycle alma linux number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts shared connection to ip address closed", "solution": "retry", "target": "nimbus", "version": 202207151000}, {"id": 5391, "name": "log-3171", "raw": "2022-07-18 08:14:34,018 | Failed at Play [vbs_enable_disable] ************************\n2022-07-18 08:14:34,018 | TASK [Check VM 'dw0409_win10_v21H2' IP address] ************\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'dw0409_win10_v21H2' IP Address", "category": "", "processed": "timestamp failed at play vbs enable disable timestamp task check vm v number h number ip address task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm v number h number ip address", "solution": "deepdive", "target": "testcase", "version": 202207221500}, {"id": 5367, "name": "log-2838", "raw": "2022-07-12 08:05:33,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:05:33,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_9.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle alma linux number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5368, "name": "log-2839", "raw": "2022-07-12 08:06:08,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:06:08,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_CentOS_8.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle centos number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5392, "name": "log-3180", "raw": "2022-07-19 10:37:30,019 | Failed at Play [nvme_vhba_device_ops_spec13] ***************\n2022-07-19 10:37:30,019 | TASK [Hot add or remove VM disk controller] ****************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/common/vm_hot_add_remove_disk_ctrl.yml:12\nexception in /vmware_guest_controller.py when configure_disk_controllers in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => ('The guest operating system did not respond to a hot-remove request for device nvme0 in a timely manner.', None)", "category": "", "processed": "timestamp failed at play nvme v hba device ops spec number timestamp task hot add or remove vm disk controller task path home worker workspace ansible cycle windows server number ansible vsphere gos validation common vm hot add remove disk ctrl yml number exception in vmware guest controller python when configure disk controllers in vmware python when wait for task fatal localhost failed the guest operating system did not respond to a hot remove request for device nvme number in a timely manner none", "solution": "deepdive", "target": "product", "version": 202207221500}, {"id": 5393, "name": "log-3181", "raw": "2022-07-19 10:22:17,019 | Failed at Play [nvme_vhba_device_ops_spec13] ***************\n2022-07-19 10:22:17,019 | TASK [Hot add or remove VM disk controller] ****************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/vm_hot_add_remove_disk_ctrl.yml:12\nexception in /vmware_guest_controller.py when configure_disk_controllers in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => ('The guest operating system did not respond to a hot-remove request for device nvme0 in a timely manner.', None)", "category": "", "processed": "timestamp failed at play nvme v hba device ops spec number timestamp task hot add or remove vm disk controller task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm hot add remove disk ctrl yml number exception in vmware guest controller python when configure disk controllers in vmware python when wait for task fatal localhost failed the guest operating system did not respond to a hot remove request for device nvme number in a timely manner none", "solution": "deepdive", "target": "product", "version": 202207221500}, {"id": 5394, "name": "log-3184", "raw": "2022-07-20 02:50:38,020 | Failed at Play [vmlibrary_main] ****************************\n2022-07-20 02:50:38,020 | TASK [fail] ************************************************\ntask path: /home/worker/workspace/Create_New_VMLibrary_Template/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:69\nfatal: [localhost]: FAILED! => (vmodl.fault.SystemError) {\n   dynamicType = <unset>,\n   dynamicProperty = (vmodl.DynamicProperty) [],\n   msg = 'A general system error occurred: Invalid fault',\n   faultCause = <unset>,\n   faultMessage = (vmodl.LocalizableMessage) [],\n   reason = 'Invalid fault'\n} ", "category": "", "processed": "timestamp failed at play vm library main timestamp task fail task path home worker workspace create new vm library template ansible vsphere gos validation common vm set power state yml number fatal localhost failed vmodl fault system error dynamic type un set dynamic property vmodl dynamic property message a general system error occurred invalid fault fault cause un set fault message vmodl localizable message reason invalid fault ", "solution": "deepdive", "target": "testcase", "version": 202207221500}, {"id": 5369, "name": "log-2841", "raw": "2022-07-12 08:06:47,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:06:47,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Debian_11.x_64bit/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle debian number x number b it ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5370, "name": "log-2842", "raw": "2022-07-12 08:07:12,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:12,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_LTS/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle flatcar lts ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5371, "name": "log-2843", "raw": "2022-07-12 08:07:14,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:14,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_Stable/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle flatcar stable ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5372, "name": "log-2844", "raw": "2022-07-12 08:07:23,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:23,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update_OVA/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle photon number x update ova ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5373, "name": "log-2847", "raw": "2022-07-12 08:07:34,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:34,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.x_Update_ISO/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle photon number x update iso ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5374, "name": "log-2848", "raw": "2022-07-12 08:07:37,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:37,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5375, "name": "log-2849", "raw": "2022-07-12 08:07:48,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:07:48,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.0_ISO/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle photon number iso ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5378, "name": "log-2853", "raw": "2022-07-12 08:10:51,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:10:51,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_RockyLinux_8.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle rocky linux number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5376, "name": "log-2932", "raw": "2022-07-12 14:01:09,012 | Failed at Play [env_setup] *********************************\n2022-07-12 14:01:09,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_9.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle rhel number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5377, "name": "log-2852", "raw": "2022-07-12 08:09:46,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:09:46,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_SLED_15.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle sled number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5379, "name": "log-2854", "raw": "2022-07-12 08:11:06,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:11:06,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_SLES_15.x/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle sles number x ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5380, "name": "log-2931", "raw": "2022-07-12 14:00:27,012 | Failed at Play [env_setup] *********************************\n2022-07-12 14:00:27,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.4_ISO/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle ubuntu version id iso ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5381, "name": "log-2858", "raw": "2022-07-12 08:13:55,012 | Failed at Play [env_setup] *********************************\n2022-07-12 08:13:55,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle windows number ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5382, "name": "log-2930", "raw": "2022-07-12 14:00:23,012 | Failed at Play [env_setup] *********************************\n2022-07-12 14:00:23,012 | TASK [Check VM 'test_vm' does not exist] *******************\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_22.04_ISO/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'test_vm' already exists. Please provide a new vm_name.", "category": "", "processed": "timestamp failed at play environment setup timestamp task check vm test vm does not exist task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name test vm already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202207221500}, {"id": 5383, "name": "log-2944", "raw": "2022-07-13 02:02:56,013 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-07-13 02:02:56,013 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220713004650.log] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_22.04_ISO/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-07-13 02:03:06,013 | TASK [Fetch file /tmp/cloud-init_2022-07-13-02-02-59.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_22.04_ISO/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "retry", "target": "testbed", "version": 202207221500}, {"id": 5384, "name": "log-2962", "raw": "2022-07-13 04:18:23,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 04:18:23,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.0_ISO/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.185.229.209' (ECDSA) to the list of known hosts.\nroot@10.185.229.209: Permission denied (publickey,keyboard-interactive).", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle photon number iso ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202207221500}, {"id": 5385, "name": "log-3007", "raw": "2022-07-13 05:05:25,013 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-07-13 05:05:25,013 | TASK [Collect filtered guest information for '10.185.110.153'] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_22.04_ISO/ansible-vsphere-gos-validation/common/get_system_info.yml:22\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.185.110.153' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.185.110.153: Permission denied (publickey,password).", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task collect filtered guest information for ip address task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common get system info yml number fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key password", "solution": "retry", "target": "targetvm", "version": 202207221500}, {"id": 5386, "name": "log-3046", "raw": "2022-07-13 08:57:53,013 | Failed at Play [sata_vhba_device_ops] **********************\n2022-07-13 08:57:53,013 | TASK [Create ext4 file system] *****************************\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.x_Update_OVA/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/prepare_new_disk.yml:50\nfatal: [localhost]: FAILED! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.78.116.253' (ECDSA) to the list of known hosts.\nroot@10.78.116.253: Permission denied (publickey,keyboard-interactive).", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task create ext number file system task path home worker workspace ansible cycle photon number x update ova ansible vsphere gos validation linux v hba hot add remove prepare new disk yml number fatal localhost failed invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202207221500}, {"id": 5387, "name": "log-3148", "raw": "2022-07-14 00:31:12,014 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-07-14 00:31:12,014 | TASK [Wait for message 'Autoinstall is completed.' appear in VM log serial-20220713231750.log] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.4_ISO/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-07-14 00:31:23,014 | TASK [Fetch file /tmp/cloud-init_2022-07-14-00-31-15.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04.4_ISO/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:93\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle ubuntu version id iso ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible cycle ubuntu version id iso ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "retry", "target": "testbed", "version": 202207221500}, {"id": 5388, "name": "log-3150", "raw": "2022-07-14 13:39:59,014 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-07-14 13:39:59,014 | TASK [Collect filtered guest information for '10.182.132.140'] \ntask path: /home/worker/workspace/Ansible_Cycle_Flatcar_Stable/ansible-vsphere-gos-validation/common/get_system_info.yml:22\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.182.132.140' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nReceived disconnect from 10.182.132.140 port 22:2: Too many authentication failures\nDisconnected from 10.182.132.140 port 22", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task collect filtered guest information for ip address task path home worker workspace ansible cycle flatcar stable ansible vsphere gos validation common get system info yml number fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again received disconnect from ip address port number too many authentication failures disconnected from ip address port number", "solution": "retry", "target": "targetvm", "version": 202207221500}, {"id": 5389, "name": "log-3151", "raw": "2022-07-15 03:02:59,015 | Failed at Play [env_setup] *********************************\n2022-07-15 03:02:59,015 | TASK [Enable debug log by default] *************************\ntask path: /home/worker/workspace/Ansible_Regression_RHEL_9.x/ansible-vsphere-gos-validation/env_setup/set_default_variables.yml:13\nfatal: [localhost]: FAILED! => The variable name 'enable_ansible.builtin.debug' is not valid. Variables must start with a letter or underscore character, and contain only letters, numbers and underscores.", "category": "", "processed": "timestamp failed at play environment setup timestamp task enable debug log by default task path home worker workspace ansible regression rhel number x ansible vsphere gos validation environment setup set default variables yml number fatal localhost failed the variable name enable ansible builtin debug is not valid variables must start with a letter or underscore character and contain only letters numbers and underscores", "solution": "deepdive", "target": "testcase", "version": 202207221500}, {"id": 5390, "name": "log-3166", "raw": "2022-07-15 16:44:29,015 | Failed at Play [deploy_vm_efi_sata_e1000e] *****************\n2022-07-15 16:44:29,015 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_RockyLinux_9.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table\n2022-07-15 16:45:08,015 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_RockyLinux_9.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi sata timestamp task wait for vmware tools collecting guest info task path home worker workspace ansible rocky linux number x number ga sata efi ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed hardware configuration table timestamp task testing exit due to failure task path home worker workspace ansible rocky linux number x number ga sata efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi sata e number e", "solution": "retry", "target": "targetvm", "version": 202207221500}, {"id": 5423, "name": "3189", "raw": "2022-07-20 13:42:04,020 | Failed at Play [vbs_enable_disable] ************************\n2022-07-20 13:42:04,020 | TASK [Check VBS and running security service status] *******\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/vbs_enable_disable/vbs_enable_test.yml:41\nfatal: [localhost]: FAILED! => VBS is not running '2', or HVCI is not running '['1', ' 2']'.", "category": null, "processed": "timestamp failed at play vbs enable disable timestamp task check vbs and running security service status task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation windows vbs enable disable vbs enable test yml number fatal localhost failed vbs is not running number or h vci is not running number number", "solution": "deepdive", "target": "testcase", "version": 202207260728}, {"id": 5424, "name": "3190", "raw": "2022-07-20 14:38:04,020 | Failed at Play [vbs_enable_disable] ************************\n2022-07-20 14:38:04,020 | TASK [Check VBS and running security service status] *******\ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/vbs_enable_disable/vbs_enable_test.yml:41\nfatal: [localhost]: FAILED! => VBS is not running '2', or HVCI is not running '['1', ' 2']'.", "category": null, "processed": "timestamp failed at play vbs enable disable timestamp task check vbs and running security service status task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation windows vbs enable disable vbs enable test yml number fatal localhost failed vbs is not running number or h vci is not running number number", "solution": "deepdive", "target": "testcase", "version": 202207260728}, {"id": 5425, "name": "3187", "raw": "2022-07-20 04:27:02,020 | Failed at Play [env_setup] *********************************\n2022-07-20 04:27:02,020 | TASK [Check variables for new VM settings] *****************\ntask path: /home/worker/workspace/Create_New_VMLibrary_Template/ansible-vsphere-gos-validation/env_setup/check_testing_vars.yml:57\nfatal: [localhost]: FAILED! => Invalid variables for new VM settings", "category": null, "processed": "timestamp failed at play environment setup timestamp task check variables for new vm settings task path home worker workspace create new vm library template ansible vsphere gos validation environment setup check testing vars yml number fatal localhost failed invalid variables for new vm settings", "solution": "deepdive", "target": "testcase", "version": 202207260728}, {"id": 5427, "name": "3270", "raw": "2022-07-27 02:32:30,027 | Failed at Play [env_setup] *********************************\n2022-07-27 02:32:30,027 | TASK [Get all registerd VMs and templates on 10.78.127.86] *\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nhttp.client.HTTPException: 503 Service Unavailable\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": null, "processed": "timestamp failed at play environment setup timestamp task get all register dv ms and templates on ip address task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation common vm check exist yml number http client http exception number service unavailable fatal localhost failed module failure see stdout stderr for the exact error", "solution": "retry", "target": "testbed", "version": 202208031141}, {"id": 5426, "name": "3276", "raw": "2022-07-27 07:21:22,027 | Failed at Play [deploy_ova] ********************************\n2022-07-27 07:21:22,027 | TASK [Deploy VM from ovf template] *************************\ntask path: /home/worker/workspace/Hyrule_OVF/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nfatal: [localhost]: FAILED! => Datastore 'datasore2' could not be located on specified ESXi host or datacenter\n2022-07-27 07:21:24,027 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Hyrule_OVF/ansible-vsphere-gos-validation/common/test_rescue.yml:55\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ova\n2022-07-27 07:21:26,027 | TASK [Get specified property info for VM 'test_hyrule_ovf'] \ntask path: /home/worker/workspace/Hyrule_OVF/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_hyrule_ovf", "category": null, "processed": "timestamp failed at play deploy ova timestamp task deploy vm from ovf template task path home worker workspace hy rule ovf ansible vsphere gos validation common ovf deploy yml number fatal localhost failed data store data sore number could not be located on specified esxi host or data center timestamp task testing exit due to failure task path home worker workspace hy rule ovf ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ova timestamp task get specified property info for vm test hy rule ovf task path home worker workspace hy rule ovf ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test hy rule ovf", "solution": "deepdive", "target": "usererror", "version": 202208031141}, {"id": 5428, "name": "3336", "raw": "2022-08-02 08:30:39,002 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-08-02 08:30:39,002 | TASK [Check VM 'test_vm_1659424377814' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1659424377814' IP Address\n2022-08-02 08:31:15,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task check vm test vm number ip address task path home worker workspace ansible windows number b it lsi logic sas efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm number ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it lsi logic sas efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5430, "name": "3337", "raw": "2022-08-02 08:30:27,002 | Failed at Play [deploy_vm_efi_sata_e1000e] *****************\n2022-08-02 08:30:27,002 | TASK [Check VM 'test_vm_1659424395015' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67U2_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1659424395015' IP Address\n2022-08-02 08:31:17,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67U2_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi sata timestamp task check vm test vm number ip address task path home worker workspace ansible windows number b it sata efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm number ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it sata efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi sata e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5432, "name": "3310", "raw": "2022-08-02 06:43:58,002 | Failed at Play [deploy_vm_bios_ide_e1000e] *****************\n2022-08-02 06:43:58,002 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67U2_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-08-02 06:44:51,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67U2_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios ide timestamp task check vm test vm ip address task path home worker workspace ansible windows number b it ide bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it ide bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios ide e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5429, "name": "3313", "raw": "2022-08-02 06:46:05,002 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2022-08-02 06:46:05,002 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-08-02 06:46:42,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi ide timestamp task check vm test vm ip address task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5431, "name": "3311", "raw": "2022-08-02 06:44:40,002 | Failed at Play [deploy_vm_efi_sata_e1000e] *****************\n2022-08-02 06:44:40,002 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67U2_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-08-02 06:45:37,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67U2_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi sata timestamp task check vm test vm ip address task path home worker workspace ansible windows number b it sata efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it sata efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi sata e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5433, "name": "3324", "raw": "2022-08-02 08:28:35,002 | Failed at Play [deploy_vm_efi_sata_e1000e] *****************\n2022-08-02 08:28:35,002 | TASK [Check VM 'test_vm_1659424345087' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1659424345087' IP Address\n2022-08-02 08:29:17,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi sata timestamp task check vm test vm number ip address task path home worker workspace ansible windows number b it sata efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm number ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it sata efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi sata e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5435, "name": "3331", "raw": "2022-08-02 08:30:14,002 | Failed at Play [deploy_vm_bios_paravirtual_e1000e] *********\n2022-08-02 08:30:14,002 | TASK [Check VM 'test_vm_1659424394338' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1659424394338' IP Address\n2022-08-02 08:30:42,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_paravirtual_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios para virtual timestamp task check vm test vm number ip address task path home worker workspace ansible windows number b it para virtual bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm number ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it para virtual bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios para virtual e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5434, "name": "3309", "raw": "2022-08-02 06:43:19,002 | Failed at Play [deploy_vm_efi_sata_e1000e] *****************\n2022-08-02 06:43:19,002 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-08-02 06:43:56,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi sata timestamp task check vm test vm ip address task path home worker workspace ansible windows number b it sata efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it sata efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi sata e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5436, "name": "3307", "raw": "2022-08-02 06:43:46,002 | Failed at Play [deploy_vm_bios_paravirtual_e1000e] *********\n2022-08-02 06:43:46,002 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-08-02 06:44:12,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_paravirtual_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios para virtual timestamp task check vm test vm ip address task path home worker workspace ansible windows number b it para virtual bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it para virtual bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios para virtual e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5439, "name": "3340", "raw": "2022-08-02 08:25:39,002 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-08-02 08:25:39,002 | TASK [Check VM 'test_vm_1659424562274' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1659424562274' IP Address\n2022-08-02 08:26:01,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task check vm test vm number ip address task path home worker workspace ansible windows number b it lsi logic sas efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm number ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it lsi logic sas efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5441, "name": "3302", "raw": "2022-08-02 06:41:24,002 | Failed at Play [deploy_vm_bios_nvme_e1000e] ****************\n2022-08-02 06:41:24,002 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-08-02 06:42:10,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_nvme_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios nvme timestamp task check vm test vm ip address task path home worker workspace ansible windows number b it number ga nvme bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it number ga nvme bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios nvme e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5443, "name": "3325", "raw": "2022-08-02 08:29:32,002 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2022-08-02 08:29:32,002 | TASK [Check VM 'test_vm_1659424375453' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_67GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1659424375453' IP Address\n2022-08-02 08:29:42,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_67GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi ide timestamp task check vm test vm number ip address task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm number ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5437, "name": "3333", "raw": "2022-08-02 08:29:56,002 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-08-02 08:29:56,002 | TASK [Check VM 'test_vm_1659424423700' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1659424423700' IP Address\n2022-08-02 08:30:48,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task check vm test vm number ip address task path home worker workspace ansible windows number b it number ga lsi logic sas efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm number ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it number ga lsi logic sas efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5438, "name": "3304", "raw": "2022-08-02 06:41:54,002 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-08-02 06:41:54,002 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-08-02 06:42:48,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task check vm test vm ip address task path home worker workspace ansible windows number b it number ga lsi logic sas efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it number ga lsi logic sas efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5440, "name": "3326", "raw": "2022-08-02 08:29:38,002 | Failed at Play [deploy_vm_bios_nvme_e1000e] ****************\n2022-08-02 08:29:38,002 | TASK [Check VM 'test_vm_1659424419696' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1659424419696' IP Address\n2022-08-02 08:30:22,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_nvme_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios nvme timestamp task check vm test vm number ip address task path home worker workspace ansible windows number b it number ga nvme bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm number ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it number ga nvme bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios nvme e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5442, "name": "3301", "raw": "2022-08-02 06:40:31,002 | Failed at Play [deploy_vm_bios_ide_e1000e] *****************\n2022-08-02 06:40:31,002 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_67U3_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-08-02 06:41:24,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_67U3_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios ide timestamp task check vm test vm ip address task path home worker workspace ansible windows number b it ide bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it ide bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios ide e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5444, "name": "3300", "raw": "2022-08-02 06:39:30,002 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2022-08-02 06:39:30,002 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_67GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address\n2022-08-02 06:40:48,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_67GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi ide timestamp task check vm test vm ip address task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5445, "name": "3321", "raw": "2022-08-02 08:27:54,002 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-08-02 08:27:54,002 | TASK [Check VM 'test_vm_1659424362336' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_67U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1659424362336' IP Address\n2022-08-02 08:28:44,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_32bit_67U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task check vm test vm number ip address task path home worker workspace ansible windows number b it lsi logic sas efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm number ip address timestamp task testing exit due to failure task path home worker workspace ansible windows number b it lsi logic sas efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e", "solution": "retry", "target": "targetvm", "version": 202208031141}, {"id": 5469, "name": "log-3509", "raw": "2022-08-15 08:13:15,015 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-08-15 08:13:15,015 | TASK [vmxnet3_network_device_ops][Try to ping IP] **********\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops try to ping ip task path home worker workspace ansible ubuntu lts server iso nvme vmxnet number efi ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "retry", "target": "targetvm", "version": 202208172100}, {"id": 5470, "name": "log-3519", "raw": "2022-08-15 14:03:22,015 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2022-08-15 14:03:22,015 | TASK [deploy_vm_efi_nvme_vmxnet3][Configure VM CDROM to 'client'] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nexception in /vmware.py when connect_to_api in /ssl.py when do_handshake\nfatal: [localhost]: FAILED! => Unable to connect to vCenter or ESXi API at 10.191.149.181 on TCP/443: EOF occurred in violation of protocol (_ssl.c:1123)\n2022-08-15 14:04:02,015 | TASK [deploy_vm_efi_nvme_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_nvme_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task deploy vm efi nvme vmxnet number configure vm cdrom to client task path home worker workspace ansible ubuntu lts server iso nvme vmxnet number efi ansible vsphere gos validation common vm configure cdrom yml number exception in vmware python when connect to api in ssl python when do handshake fatal localhost failed unable to connect to vcenter or esxi api at ip address on tcp number eof occurred in violation of protocol ssl c number timestamp task deploy vm efi nvme vmxnet number testing exit due to failure task path home worker workspace ansible ubuntu lts server iso nvme vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi nvme vmxnet number", "solution": "retry", "target": "nimbus", "version": 202208172100}, {"id": 5471, "name": "log-3520", "raw": "2022-08-15 14:38:00,015 | Failed at Play [deploy_vm_bios_lsilogicsas_e1000e] *********\n2022-08-15 14:38:00,015 | TASK [deploy_vm_bios_lsilogicsas_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20220815132317.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-08-15 14:38:13,015 | TASK [deploy_vm_bios_lsilogicsas_e1000e][Fetch file /tmp/cloud-init_2022-08-15-14-38-04.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "", "processed": "timestamp failed at play deploy vm bios lsi logic sas timestamp task deploy vm bios lsi logic sas wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible ubuntu lts server iso lsi logic sas bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm bios lsi logic sas fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible ubuntu lts server iso lsi logic sas bios ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5472, "name": "log-3521", "raw": "2022-08-15 14:43:58,015 | Failed at Play [deploy_vm_bios_sata_e1000e] ****************\n2022-08-15 14:43:58,015 | TASK [deploy_vm_bios_sata_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20220815132730.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-08-15 14:44:12,015 | TASK [deploy_vm_bios_sata_e1000e][Fetch file /tmp/cloud-init_2022-08-15-14-44-03.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "", "processed": "timestamp failed at play deploy vm bios sata timestamp task deploy vm bios sata wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible ubuntu lts server iso number ga sata bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm bios sata fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible ubuntu lts server iso number ga sata bios ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5450, "name": "log-3364", "raw": "2022-08-03 13:09:49,003 | Failed at Play [secureboot_enable_disable] *****************\n2022-08-03 13:09:49,003 | TASK [secureboot_enable_disable][Check secure boot enable and disable test results] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_OVA/ansible-vsphere-gos-validation/linux/secureboot_enable_disable/secureboot_enable_disable.yml:96\nfatal: [localhost]: FAILED! => Enable secureboot result: False, disable secureboot result: True", "category": "", "processed": "timestamp failed at play secure boot enable disable timestamp task secure boot enable disable check secure boot enable and disable test results task path home worker workspace ansible regression photon number ova ansible vsphere gos validation linux secure boot enable disable secure boot enable disable yml number fatal localhost failed enable secure boot result false disable secure boot result true", "solution": "deepdive", "target": "product", "version": 202208172100}, {"id": 5451, "name": "log-3369", "raw": "2022-08-04 23:07:54,004 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-08-04 23:07:54,004 | TASK [testbed_deploy_nimbus][Get ESXi datastores info] *****\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/esxi_get_datastores.yml:5\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.206.190.129:443 : [Errno 113] No route to host", "category": "", "processed": "timestamp failed at play testbed deploy nimbus timestamp task testbed deploy nimbus get esxi data stores info task path home worker workspace ansible cycle windows number ansible vsphere gos validation common esxi get data stores yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number no route to host", "solution": "retry", "target": "nimbus", "version": 202208172100}, {"id": 5447, "name": "log-3294", "raw": "2022-08-02 02:11:43,002 | Failed at Play [wintools_complete_install_verify] **********\n2022-08-02 02:11:43,002 | TASK [Execute powershell command 'get-service -Name VMTools | foreach {$_.Status}'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.185.13.217]: FAILED! => non-zero return code when get-service", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task execute powershell command get service name vm tools for each status task path home worker workspace ansible windows server lts c main para virtual vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when get service", "solution": "retry", "target": "targetvm", "version": 202208172100}, {"id": 5467, "name": "log-3495", "raw": "2022-08-15 07:56:43,015 | Failed at Play [e1000e_network_device_ops] *****************\n2022-08-15 07:56:43,015 | TASK [e1000e_network_device_ops][Try to ping IP] ***********\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "", "processed": "timestamp failed at play network device ops timestamp task network device ops try to ping ip task path home worker workspace ansible ubuntu lts server iso main nvme bios ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "retry", "target": "targetvm", "version": 202208172100}, {"id": 5446, "name": "log-3288", "raw": "2022-08-01 08:28:53,001 | Failed at Play [env_setup] *********************************\n2022-08-01 08:28:53,001 | TASK [Get all registerd VMs and templates on 10.117.17.83] *\ntask path: /home/worker/workspace/Ansible_RHEL_8.x_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nAttributeError: 'NoneType' object has no attribute 'datastoreUrl'\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "", "processed": "timestamp failed at play environment setup timestamp task get all register dv ms and templates on ip address task path home worker workspace ansible rhel number x para virtual bios ansible vsphere gos validation common vm check exist yml number attribute error none type object has no attribute data store url fatal localhost failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testbed", "version": 202208172100}, {"id": 5448, "name": "log-3319", "raw": "2022-08-02 07:16:33,002 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2022-08-02 07:16:33,002 | TASK [Add IDE boot disk] ***********************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_create_with_ide_disk.yml:49\nfatal: [localhost -> 10.78.89.80]: FAILED! => non-zero return code when vim-cmd\n2022-08-02 07:16:34,002 | TASK [Testing exit due to failure] *************************\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi ide timestamp task add ide boot disk task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common vm create with ide disk yml number fatal localhost ip address failed nonzero return code when vim command timestamp task testing exit due to failure task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5449, "name": "log-3363", "raw": "2022-08-03 12:22:06,003 | Failed at Play [ovt_verify_install] ************************\n2022-08-03 12:22:06,003 | TASK [ovt_verify_install][Wait for port 22 to become stopped] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/linux/utils/shutdown.yml:13\nfatal: [localhost]: FAILED! => Timeout when waiting for 10.170.67.119:22 to stop.\n2022-08-03 12:22:20,003 | TASK [ovt_verify_install][Testing exit due to failure] *****\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install", "category": "", "processed": "timestamp failed at play ovt verify install timestamp task ovt verify install wait for port number to become stopped task path home worker workspace ansible regression photon number ova ansible vsphere gos validation linux utilities shutdown yml number fatal localhost failed timeout when waiting for ip address to stop timestamp task ovt verify install testing exit due to failure task path home worker workspace ansible regression photon number ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case ovt verify install", "solution": "retry", "target": "targetvm", "version": 202208172100}, {"id": 5452, "name": "log-3381", "raw": "2022-08-05 03:57:53,005 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2022-08-05 03:57:53,005 | TASK [deploy_vm][Datastore file operation] *****************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Other/Windows/Windows10/v22H2/19045.1826/Windows10_InsiderPreview_EnterpriseVL_x32_en-us_19045.1826.iso' is absent, cannot continue\n2022-08-05 03:57:54,005 | TASK [deploy_vm][Testing exit due to failure] **************\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual timestamp task deploy vm data store file operation task path home worker workspace ansible cycle windows number ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed file os other windows windows number v number h number windows number insider preview enterprise vl en us number i so is absent can not continue timestamp task deploy vm testing exit due to failure task path home worker workspace ansible cycle windows number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual e number e", "solution": "deepdive", "target": "testbed", "version": 202208172100}, {"id": 5453, "name": "log-3413", "raw": "2022-08-09 05:38:57,009 | Failed at Play [check_inbox_driver] ************************\n2022-08-09 05:38:57,009 | TASK [check_inbox_driver][Check inbox driver's filename is valid] \ntask path: /home/worker/workspace/Ansible_Regression_SLES_15.x/ansible-vsphere-gos-validation/linux/check_inbox_driver/get_inbox_drivers.yml:123\nfailed: [localhost] => (item=vmxnet3 is /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst) => Invalid inbox driver vmxnet3 filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst and vmxnet3 is /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst\nfailed: [localhost] => (item=vmw_vmci is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst) => Invalid inbox driver vmw_vmci filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst and vmw_vmci is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst\nfailed: [localhost] => (item=vsock is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst) => Invalid inbox driver vsock filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst and vsock is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst\nfailed: [localhost] => (item=vmw_vsock_vmci_transport is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst) => Invalid inbox driver vmw_vsock_vmci_transport filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst and vmw_vsock_vmci_transport is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst\nfailed: [localhost] => (item=vmw_vsock_virtio_transport_common is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst) => Invalid inbox driver vmw_vsock_virtio_transport_common filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst and vmw_vsock_virtio_transport_common is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst\nfailed: [localhost] => (item=vmw_pvscsi is /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst) => Invalid inbox driver vmw_pvscsi filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst and vmw_pvscsi is /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst\nfailed: [localhost] => (item=vmw_balloon is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst) => Invalid inbox driver vmw_balloon filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst and vmw_balloon is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst\nfailed: [localhost] => (item=vmwgfx is /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst) => Invalid inbox driver vmwgfx filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst and vmwgfx is /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst\nfailed: [localhost] => (item=vmw_pvrdma is /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst) => Invalid inbox driver vmw_pvrdma filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst and vmw_pvrdma is /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst\nfailed: [localhost] => (item=ptp_vmw is /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst) => Invalid inbox driver ptp_vmw filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst and ptp_vmw is /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver check inbox driver s filename is valid task path home worker workspace ansible regression sles number x ansible vsphere gos validation linux check inbox driver get inbox drivers yml number failed localhost item vmxnet number is library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st invalid inbox driver vmxnet number filename library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st and vmxnet number is library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st failed localhost item vmware vmci is library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st invalid inbox driver vmware vmci filename library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st and vmware vmci is library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st failed localhost item vsock is library modules version id number default kernel net vmware vsock vsock ko z st invalid inbox driver vsock filename library modules version id number default kernel net vmware vsock vsock ko z st and vsock is library modules version id number default kernel net vmware vsock vsock ko z st failed localhost item vmware vsock vmci transport is library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st invalid inbox driver vmware vsock vmci transport filename library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st and vmware vsock vmci transport is library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st failed localhost item vmware vsock virtio transport common is library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st invalid inbox driver vmware vsock virtio transport common filename library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st and vmware vsock virtio transport common is library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st failed localhost item vmware pvscsi is library modules version id number default kernel drivers scsi vmware pvscsi ko z st invalid inbox driver vmware pvscsi filename library modules version id number default kernel drivers scsi vmware pvscsi ko z st and vmware pvscsi is library modules version id number default kernel drivers scsi vmware pvscsi ko z st failed localhost item vmware balloon is library modules version id number default kernel drivers misc vmware balloon ko z st invalid inbox driver vmware balloon filename library modules version id number default kernel drivers misc vmware balloon ko z st and vmware balloon is library modules version id number default kernel drivers misc vmware balloon ko z st failed localhost item vmware gfx is library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st invalid inbox driver vmware gfx filename library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st and vmware gfx is library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st failed localhost item vmware pvrdma is library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st invalid inbox driver vmware pvrdma filename library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st and vmware pvrdma is library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st failed localhost item p tp vmware is library modules version id number default kernel drivers p tp p tp vmware ko z st invalid inbox driver p tp vmware filename library modules version id number default kernel drivers p tp p tp vmware ko z st and p tp vmware is library modules version id number default kernel drivers p tp p tp vmware ko z st", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5454, "name": "log-3414", "raw": "2022-08-09 05:42:22,009 | Failed at Play [check_inbox_driver] ************************\n2022-08-09 05:42:22,009 | TASK [check_inbox_driver][Check inbox driver's filename is valid] \ntask path: /home/worker/workspace/Ansible_Regression_SLED_15.x/ansible-vsphere-gos-validation/linux/check_inbox_driver/get_inbox_drivers.yml:123\nfailed: [localhost] => (item=vmxnet3 is /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst) => Invalid inbox driver vmxnet3 filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst and vmxnet3 is /lib/modules/5.14.21-150400.22-default/kernel/drivers/net/vmxnet3/vmxnet3.ko.zst\nfailed: [localhost] => (item=vmw_vmci is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst) => Invalid inbox driver vmw_vmci filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst and vmw_vmci is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_vmci/vmw_vmci.ko.zst\nfailed: [localhost] => (item=vsock is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst) => Invalid inbox driver vsock filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst and vsock is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vsock.ko.zst\nfailed: [localhost] => (item=vmw_vsock_vmci_transport is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst) => Invalid inbox driver vmw_vsock_vmci_transport filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst and vmw_vsock_vmci_transport is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_vmci_transport.ko.zst\nfailed: [localhost] => (item=vmw_vsock_virtio_transport_common is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst) => Invalid inbox driver vmw_vsock_virtio_transport_common filename /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst and vmw_vsock_virtio_transport_common is /lib/modules/5.14.21-150400.22-default/kernel/net/vmw_vsock/vmw_vsock_virtio_transport_common.ko.zst\nfailed: [localhost] => (item=vmw_pvscsi is /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst) => Invalid inbox driver vmw_pvscsi filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst and vmw_pvscsi is /lib/modules/5.14.21-150400.22-default/kernel/drivers/scsi/vmw_pvscsi.ko.zst\nfailed: [localhost] => (item=vmw_balloon is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst) => Invalid inbox driver vmw_balloon filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst and vmw_balloon is /lib/modules/5.14.21-150400.22-default/kernel/drivers/misc/vmw_balloon.ko.zst\nfailed: [localhost] => (item=vmwgfx is /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst) => Invalid inbox driver vmwgfx filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst and vmwgfx is /lib/modules/5.14.21-150400.22-default/kernel/drivers/gpu/drm/vmwgfx/vmwgfx.ko.zst\nfailed: [localhost] => (item=vmw_pvrdma is /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst) => Invalid inbox driver vmw_pvrdma filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst and vmw_pvrdma is /lib/modules/5.14.21-150400.22-default/kernel/drivers/infiniband/hw/vmw_pvrdma/vmw_pvrdma.ko.zst\nfailed: [localhost] => (item=ptp_vmw is /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst) => Invalid inbox driver ptp_vmw filename /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst and ptp_vmw is /lib/modules/5.14.21-150400.22-default/kernel/drivers/ptp/ptp_vmw.ko.zst", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver check inbox driver s filename is valid task path home worker workspace ansible regression sled number x ansible vsphere gos validation linux check inbox driver get inbox drivers yml number failed localhost item vmxnet number is library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st invalid inbox driver vmxnet number filename library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st and vmxnet number is library modules version id number default kernel drivers net vmxnet number vmxnet number ko z st failed localhost item vmware vmci is library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st invalid inbox driver vmware vmci filename library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st and vmware vmci is library modules version id number default kernel drivers misc vmware vmci vmware vmci ko z st failed localhost item vsock is library modules version id number default kernel net vmware vsock vsock ko z st invalid inbox driver vsock filename library modules version id number default kernel net vmware vsock vsock ko z st and vsock is library modules version id number default kernel net vmware vsock vsock ko z st failed localhost item vmware vsock vmci transport is library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st invalid inbox driver vmware vsock vmci transport filename library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st and vmware vsock vmci transport is library modules version id number default kernel net vmware vsock vmware vsock vmci transport ko z st failed localhost item vmware vsock virtio transport common is library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st invalid inbox driver vmware vsock virtio transport common filename library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st and vmware vsock virtio transport common is library modules version id number default kernel net vmware vsock vmware vsock virtio transport common ko z st failed localhost item vmware pvscsi is library modules version id number default kernel drivers scsi vmware pvscsi ko z st invalid inbox driver vmware pvscsi filename library modules version id number default kernel drivers scsi vmware pvscsi ko z st and vmware pvscsi is library modules version id number default kernel drivers scsi vmware pvscsi ko z st failed localhost item vmware balloon is library modules version id number default kernel drivers misc vmware balloon ko z st invalid inbox driver vmware balloon filename library modules version id number default kernel drivers misc vmware balloon ko z st and vmware balloon is library modules version id number default kernel drivers misc vmware balloon ko z st failed localhost item vmware gfx is library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st invalid inbox driver vmware gfx filename library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st and vmware gfx is library modules version id number default kernel drivers gpu drm vmware gfx vmware gfx ko z st failed localhost item vmware pvrdma is library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st invalid inbox driver vmware pvrdma filename library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st and vmware pvrdma is library modules version id number default kernel drivers infiniband hw vmware pvrdma vmware pvrdma ko z st failed localhost item p tp vmware is library modules version id number default kernel drivers p tp p tp vmware ko z st invalid inbox driver p tp vmware filename library modules version id number default kernel drivers p tp p tp vmware ko z st and p tp vmware is library modules version id number default kernel drivers p tp p tp vmware ko z st", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5455, "name": "log-3435", "raw": "2022-08-10 06:16:36,010 | Failed at Play [secureboot_enable_disable] *****************\n2022-08-10 06:16:36,010 | TASK [secureboot_enable_disable][Check VM 'test_vm' IP address] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play secure boot enable disable timestamp task secure boot enable disable check vm test vm ip address task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5456, "name": "log-3441", "raw": "2022-08-11 02:57:18,011 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-08-11 02:57:18,011 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Wait for getting VM 'dw_test_winsrv_ansible' IP address on ESXi 'pek2-hs1-a0410.eng.vmware.com'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip_esxcli.yml:51\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect password: Warning: Permanently added 'pek2-hs1-a0410.eng.vmware.com' (ECDSA) to the list of known hosts.", "category": "", "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task deploy vm efi lsi logic sas wait for getting vm dw test win srv ansible ip address on esxi pe k number hs number eng vmware com task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation common vm get ip esx cli yml number fatal localhost un reachable invalid incorrect password warning permanently added pe k number hs number eng vmware com ecdsa to the list of known hosts", "solution": "retry", "target": "nimbus", "version": 202208172100}, {"id": 5457, "name": "log-3442", "raw": "2022-08-11 04:22:33,011 | Failed at Play [vbs_enable_disable] ************************\n2022-08-11 04:22:33,011 | TASK [vbs_enable_disable][Check VM 'dw_test_winsrv_ansible' IP address] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'dw_test_winsrv_ansible' IP Address", "category": "", "processed": "timestamp failed at play vbs enable disable timestamp task vbs enable disable check vm dw test win srv ansible ip address task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm dw test win srv ansible ip address", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5458, "name": "log-3444", "raw": "2022-08-11 04:33:38,011 | Failed at Play [check_inbox_driver] ************************\n2022-08-11 04:33:38,011 | TASK [check_inbox_driver][Check all mandatory inbox drivers exists in Ubuntu] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_OVA/ansible-vsphere-gos-validation/linux/check_inbox_driver/get_inbox_drivers.yml:181\nfailed: [localhost] => (item=None) => Unexpected templating type error occurred on ({{ mandatory_drivers - ['vmw_balloon']}}): unsupported operand type(s) for -: 'list' and 'list'", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver check all mandatory inbox drivers exists in ubuntu task path home worker workspace ansible regression ubuntu number server ova ansible vsphere gos validation linux check inbox driver get inbox drivers yml number failed localhost item none unexpected templating type error occurred on mandatory drivers vmware balloon un supported operand type s for list and list", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5459, "name": "log-3448", "raw": "2022-08-11 04:39:41,011 | Failed at Play [check_inbox_driver] ************************\n2022-08-11 04:39:41,011 | TASK [check_inbox_driver][Check all mandatory inbox drivers exists in Ubuntu] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_ISO/ansible-vsphere-gos-validation/linux/check_inbox_driver/get_inbox_drivers.yml:181\nfailed: [localhost] => (item=None) => Unexpected templating type error occurred on ({{ mandatory_drivers - ['vmw_balloon']}}): unsupported operand type(s) for -: 'list' and 'list'", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver check all mandatory inbox drivers exists in ubuntu task path home worker workspace ansible regression ubuntu number server iso ansible vsphere gos validation linux check inbox driver get inbox drivers yml number failed localhost item none unexpected templating type error occurred on mandatory drivers vmware balloon un supported operand type s for list and list", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5460, "name": "log-3461", "raw": "2022-08-11 04:58:51,011 | Failed at Play [check_inbox_driver] ************************\n2022-08-11 04:58:51,011 | TASK [check_inbox_driver][Check all mandatory inbox drivers exists in OracleLinux] \ntask path: /home/worker/workspace/Ansible_Regression_OracleLinux_8.x/ansible-vsphere-gos-validation/linux/check_inbox_driver/get_inbox_drivers.yml:181\nfailed: [localhost] => (item=None) => Unexpected templating type error occurred on ({{ mandatory_drivers - ['vmw_balloon']}}): unsupported operand type(s) for -: 'list' and 'list'", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver check all mandatory inbox drivers exists in oracle linux task path home worker workspace ansible regression oracle linux number x ansible vsphere gos validation linux check inbox driver get inbox drivers yml number failed localhost item none unexpected templating type error occurred on mandatory drivers vmware balloon un supported operand type s for list and list", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5468, "name": "log-3496", "raw": "2022-08-15 08:36:18,015 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-08-15 08:36:18,015 | TASK [vmxnet3_network_device_ops][Try to ping IP] **********\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops try to ping ip task path home worker workspace ansible ubuntu lts server iso main nvme bios ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "retry", "target": "targetvm", "version": 202208172100}, {"id": 5461, "name": "log-3464", "raw": "2022-08-11 06:25:20,011 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-08-11 06:25:20,011 | TASK [deploy_vm][Datastore file operation] *****************\ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Linux/Photon/3.0/Rev3U1/photon-3.0-913b49438.iso' is absent, cannot continue\n2022-08-11 06:25:21,011 | TASK [deploy_vm][Get specified property info for VM 'test_photon3_iso_1660198989405'] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_photon3_iso_1660198989405", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm data store file operation task path home worker workspace ansible regression photon number iso ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed file os linux photon number photon number b number i so is absent can not continue timestamp task deploy vm get specified property info for vm test photon number iso number task path home worker workspace ansible regression photon number iso ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test photon number iso number", "solution": "deepdive", "target": "testbed", "version": 202208172100}, {"id": 5462, "name": "log-3471", "raw": "2022-08-11 10:38:54,011 | Failed at Play [check_quiesce_snapshot] ********************\n2022-08-11 10:38:54,011 | TASK [check_quiesce_snapshot][Check pre-freeze script executed after snapshot take] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/windows/check_quiesce_snapshot/check_custom_scripts_executed.yml:46\nfatal: [localhost]: FAILED! => Assertion failed", "category": "", "processed": "timestamp failed at play check quiesce snapshot timestamp task check quiesce snapshot check pre freeze script executed after snapshot take task path home worker workspace ansible windows server lts c main nvme bios ansible vsphere gos validation windows check quiesce snapshot check custom scripts executed yml number fatal localhost failed assertion failed", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5463, "name": "log-3480", "raw": "2022-08-15 03:40:05,015 | Failed at Play [secureboot_enable_disable] *****************\n2022-08-15 03:40:05,015 | TASK [secureboot_enable_disable][Check VM 'test_vm' IP address] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play secure boot enable disable timestamp task secure boot enable disable check vm test vm ip address task path home worker workspace ansible windows number b it main nvme efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5464, "name": "log-3493", "raw": "2022-08-15 07:41:17,015 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-08-15 07:41:17,015 | TASK [deploy_ubuntu_ova][Configure VM CDROM to 'client'] ***\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_67GA/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nfatal: [localhost]: FAILED! => Connection control operation failed for disk 'sata0:0'.\n2022-08-15 07:41:41,015 | TASK [deploy_ubuntu_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_67GA/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova", "category": "", "processed": "timestamp failed at play deploy ubuntu ova timestamp task deploy ubuntu ova configure vm cdrom to client task path home worker workspace ansible ubuntu lts server ova number ga ansible vsphere gos validation common vm configure cdrom yml number fatal localhost failed connection control operation failed for disk sata number timestamp task deploy ubuntu ova testing exit due to failure task path home worker workspace ansible ubuntu lts server ova number ga ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ubuntu ova", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5465, "name": "log-3507", "raw": "2022-08-15 08:48:56,015 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-08-15 08:48:56,015 | TASK [deploy_ubuntu_ova][Configure VM CDROM to 'client'] ***\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_MAIN/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nfatal: [localhost]: FAILED! => Connection control operation failed for disk 'sata0:0'.\n2022-08-15 08:49:18,015 | TASK [deploy_ubuntu_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_MAIN/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova", "category": "", "processed": "timestamp failed at play deploy ubuntu ova timestamp task deploy ubuntu ova configure vm cdrom to client task path home worker workspace ansible ubuntu lts server ova main ansible vsphere gos validation common vm configure cdrom yml number fatal localhost failed connection control operation failed for disk sata number timestamp task deploy ubuntu ova testing exit due to failure task path home worker workspace ansible ubuntu lts server ova main ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ubuntu ova", "solution": "deepdive", "target": "testcase", "version": 202208172100}, {"id": 5466, "name": "log-3488", "raw": "2022-08-15 07:30:33,015 | Failed at Play [deploy_vm_efi_lsilogic_e1000e] *************\n2022-08-15 07:30:33,015 | TASK [deploy_vm_efi_lsilogic_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20220815061646.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_MAIN_LSILOGIC_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-08-15 07:30:46,015 | TASK [deploy_vm_efi_lsilogic_e1000e][Fetch file /tmp/cloud-init_2022-08-15-07-30-37.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_ISO_MAIN_LSILOGIC_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": "", "processed": "timestamp failed at play deploy vm efi lsi logic timestamp task deploy vm efi lsi logic wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible ubuntu lts server iso main lsi logic efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm efi lsi logic fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible ubuntu lts server iso main lsi logic efi ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "deepdive", "target": "targetvm", "version": 202208172100}, {"id": 5473, "name": "log-3524", "raw": "2022-08-16 07:38:39,016 | Failed at Play [env_setup] *********************************\n2022-08-16 07:38:39,016 | TASK [env_setup][Get all registerd VMs and templates on gosv-vc-70update.eng.vmware.com] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nAttributeError: 'NoneType' object has no attribute 'datastoreUrl'\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on go sv vc number update eng vmware com task path home worker workspace ansible windows server lts c lsi logic sas bios ansible vsphere gos validation common vm check exist yml number attribute error none type object has no attribute data store url fatal localhost failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "testbed", "version": 202208172100}, {"id": 5474, "name": "log-3526", "raw": "2022-08-16 08:35:19,016 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-08-16 08:35:19,016 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Create a new VM 'test_vm' on server 'gosv-vc-70update.eng.vmware.com'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : Invalid configuration for device '4'.\n2022-08-16 08:35:20,016 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task deploy vm efi lsi logic sas create a new vm test vm on server go sv vc number update eng vmware com task path home worker workspace ansible windows server lts c lsi logic sas bios ansible vsphere gos validation common vm create yml number fatal localhost failed failed to create a virtual machine invalid configuration for device number timestamp task deploy vm efi lsi logic sas testing exit due to failure task path home worker workspace ansible windows server lts c lsi logic sas bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e", "solution": "deepdive", "target": "usererror", "version": 202208172100}, {"id": 5477, "name": "3522", "raw": "2022-08-15 14:06:09,015 | Failed at Play [wintools_complete_install_verify] **********\n2022-08-15 14:06:09,015 | TASK [wintools_complete_install_verify][Create snapshot 'BaseSnapshot' on 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => An error occurred while communicating with the remote host.", "category": null, "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify create snapshot base snapshot on test vm task path home worker workspace ansible windows number main para virtual vmxnet number efi ansible vsphere gos validation common vm take snapshot yml number fatal localhost failed an error occurred while communicating with the remote host", "solution": "deepdive", "target": "targetvm", "version": 202208190706}, {"id": 5478, "name": "3510", "raw": "2022-08-15 10:02:25,015 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-08-15 10:02:25,015 | TASK [deploy_ubuntu_ova][Wait for VMware Tools collecting guest OS fullname] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_70U1/ansible-vsphere-gos-validation/common/vm_wait_guest_fullname.yml:17\nfatal: [localhost]: FAILED! => hardware configuration table\n2022-08-15 10:02:49,015 | TASK [deploy_ubuntu_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_70U1/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova", "category": null, "processed": "timestamp failed at play deploy ubuntu ova timestamp task deploy ubuntu ova wait for vmware tools collecting guest os full name task path home worker workspace ansible ubuntu lts server ova ansible vsphere gos validation common vm wait guest full name yml number fatal localhost failed hardware configuration table timestamp task deploy ubuntu ova testing exit due to failure task path home worker workspace ansible ubuntu lts server ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ubuntu ova", "solution": "retry", "target": "targetvm", "version": 202208190706}, {"id": 5479, "name": "3515", "raw": "2022-08-15 12:12:35,015 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-08-15 12:12:35,015 | TASK [deploy_ubuntu_ova][Wait for VMware Tools collecting guest OS fullname] \ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_70U1/ansible-vsphere-gos-validation/common/vm_wait_guest_fullname.yml:17\nfatal: [localhost]: FAILED! => hardware configuration table\n2022-08-15 12:12:58,015 | TASK [deploy_ubuntu_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Ubuntu_LTS_Server_OVA_70U1/ansible-vsphere-gos-validation/common/test_rescue.yml:54\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova", "category": null, "processed": "timestamp failed at play deploy ubuntu ova timestamp task deploy ubuntu ova wait for vmware tools collecting guest os full name task path home worker workspace ansible ubuntu lts server ova ansible vsphere gos validation common vm wait guest full name yml number fatal localhost failed hardware configuration table timestamp task deploy ubuntu ova testing exit due to failure task path home worker workspace ansible ubuntu lts server ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ubuntu ova", "solution": "retry", "target": "targetvm", "version": 202208190706}, {"id": 5480, "name": "3535", "raw": "2022-08-17 05:36:31,017 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-08-17 05:36:31,017 | TASK [gosc_cloudinit_staticip][Install package on Ubuntu 22.04] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:103\nfatal: [localhost -> 10.117.16.148]: FAILED! => Failed to update apt cache: unknown reason", "category": null, "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip install package on ubuntu number task path home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation linux utilities install un install package yml number fatal localhost ip address failed failed to update apt cache unknown reason", "solution": "deepdive", "target": "testcase", "version": 202208240500}, {"id": 5481, "name": "3534", "raw": "2022-08-17 05:32:53,017 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-08-17 05:32:53,017 | TASK [gosc_cloudinit_dhcp][Install package on Ubuntu 22.04] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:103\nfatal: [localhost -> 10.117.16.148]: FAILED! => Failed to update apt cache: unknown reason", "category": null, "processed": "timestamp failed at play go sc cloud init dhcp timestamp task go sc cloud init dhcp install package on ubuntu number task path home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation linux utilities install un install package yml number fatal localhost ip address failed failed to update apt cache unknown reason", "solution": "deepdive", "target": "testcase", "version": 202208240500}, {"id": 5504, "name": "3650", "raw": "2022-08-30 05:47:05,030 | Failed at Play [ovt_verify_status] *************************\n2022-08-30 05:47:05,030 | TASK [Check process 'vmtoolsd -n vmusr' is running] ********\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/check_process_status.yml:34\nfatal: [localhost]: FAILED! => User 'vmware' doesn't have running process 'vmtoolsd -n vmusr'", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check process vmtoolsd nvm user is running task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux utilities check process status yml number fatal localhost failed user vmware doesn t have running process vmtoolsd nvm user", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5482, "name": "3805", "raw": "2022-09-02 14:32:24,002 | Failed at Play [deploy_vm_bios_paravirtual_vmxnet3] ********\n2022-09-02 14:32:24,002 | TASK [deploy_vm_bios_paravirtual_vmxnet3][Upload local file to ESXi datastore] \ntask path: /home/worker/workspace/Ansible_Ubuntu_20.04_Server_ISO_67U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/esxi_upload_datastore_file.yml:11\nBrokenPipeError: [Errno 32] Broken pipe\nurllib.error.URLError: <urlopen error [Errno 32] Broken pipe>\nTypeError: 'URLError' object is not subscriptable\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error\n2022-09-02 14:32:25,002 | TASK [deploy_vm_bios_paravirtual_vmxnet3][Get specified property info for VM 'test_vm_1662129030524'] \ntask path: /home/worker/workspace/Ansible_Ubuntu_20.04_Server_ISO_67U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm_1662129030524", "category": null, "processed": "timestamp failed at play deploy vm bios para virtual vmxnet number timestamp task deploy vm bios para virtual vmxnet number upload local file to esxi data store task path home worker workspace ansible ubuntu number server iso para virtual vmxnet number bios ansible vsphere gos validation common esxi upload data store file yml number broken pipe error error number broken pipe url library error url error url open error error number broken pipe type error url error object is not sub scriptable fatal localhost failed module failure see stdout stderr for the exact error timestamp task deploy vm bios para virtual vmxnet number get specified property info for vm test vm number task path home worker workspace ansible ubuntu number server iso para virtual vmxnet number bios ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test vm number", "solution": "deepdive", "target": "testbed", "version": 202209081034}, {"id": 5483, "name": "3918", "raw": "2022-09-06 01:50:15,006 | Failed at Play [wintools_complete_install_verify] **********\n2022-09-06 01:50:15,006 | TASK [wintools_complete_install_verify][Upload local file to ESXi datastore] \ntask path: /home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/common/esxi_upload_datastore_file.yml:11\nConnectionResetError: [Errno 104] Connection reset by peer\nurllib.error.URLError: <urlopen error [Errno 104] Connection reset by peer>\nTypeError: 'URLError' object is not subscriptable\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": null, "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify upload local file to esxi data store task path home worker workspace dw existing ansible vsphere gos validation common esxi upload data store file yml number connection reset error error number connection reset by peer url library error url error url open error error number connection reset by peer type error url error object is not sub scriptable fatal localhost failed module failure see stdout stderr for the exact error", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5484, "name": "3713", "raw": "2022-08-31 05:01:01,031 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-08-31 05:01:01,031 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Guest OS connection failure] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_Physical_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_check_winrm.yml:26\nfatal: [localhost]: FAILED! => Guest winrm is not connectable in 1800 seconds.\n2022-08-31 05:02:29,031 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_Physical_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:57\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task deploy vm efi lsi logic sas guest os connection failure task path home worker workspace ansible windows number b it physical lsi logic sas efi ansible vsphere gos validation windows utilities win check win rm yml number fatal localhost failed guest win rm is not connectable in number seconds timestamp task deploy vm efi lsi logic sas testing exit due to failure task path home worker workspace ansible windows number b it physical lsi logic sas efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e", "solution": "retry", "target": "testbed", "version": 202209081034}, {"id": 5485, "name": "3938", "raw": "2022-09-06 06:57:55,006 | Failed at Play [wintools_complete_install_verify] **********\n2022-09-06 06:57:55,006 | TASK [wintools_complete_install_verify][Check VM 'test_vm' IP address] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": null, "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify check vm test vm ip address task path home worker workspace ansible windows number b it number ga sata bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5486, "name": "3947", "raw": "2022-09-06 09:17:51,006 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-09-06 09:17:51,006 | TASK [cpu_multicores_per_socket][Check VM 'test_vm' IP address] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": null, "processed": "timestamp failed at play cpu multi cores per socket timestamp task cpu multi cores per socket check vm test vm ip address task path home worker workspace ansible windows number b it number ga sata bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5487, "name": "3951", "raw": "2022-09-06 06:50:40,006 | Failed at Play [wintools_complete_install_verify] **********\n2022-09-06 06:50:40,006 | TASK [wintools_complete_install_verify][Check VM 'test_vm' IP address] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": null, "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify check vm test vm ip address task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "retry", "target": "targetvm", "version": 202209081034}, {"id": 5488, "name": "3956", "raw": "2022-09-06 07:44:36,006 | Failed at Play [stat_balloon] ******************************\n2022-09-06 07:44:36,006 | TASK [stat_balloon][Check VM 'test_vm' IP address] *********\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": null, "processed": "timestamp failed at play stat balloon timestamp task stat balloon check vm test vm ip address task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5489, "name": "3869", "raw": "2022-09-05 05:16:15,005 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2022-09-05 05:16:15,005 | TASK [deploy_vm_efi_ide_e1000e][Add IDE boot disk] *********\ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_create_with_ide_disk.yml:49\nfatal: [localhost -> 10.168.197.179]: FAILED! => non-zero return code when vim-cmd\n2022-09-05 05:16:16,005 | TASK [deploy_vm_efi_ide_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:57\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi ide timestamp task deploy vm efi ide add ide boot disk task path home worker workspace ansible windows number ga ide efi ansible vsphere gos validation common vm create with ide disk yml number fatal localhost ip address failed nonzero return code when vim command timestamp task deploy vm efi ide testing exit due to failure task path home worker workspace ansible windows number ga ide efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e", "solution": "deepdive", "target": "testbed", "version": 202209081034}, {"id": 5490, "name": "3712", "raw": "2022-08-31 04:58:38,031 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-08-31 04:58:38,031 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Guest OS connection failure] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_Physical_main_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_check_winrm.yml:26\nfatal: [localhost]: FAILED! => Guest winrm is not connectable in 1800 seconds.\n2022-08-31 05:00:08,031 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_Physical_main_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:57\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task deploy vm efi lsi logic sas guest os connection failure task path home worker workspace ansible windows number b it physical main lsi logic sas efi ansible vsphere gos validation windows utilities win check win rm yml number fatal localhost failed guest win rm is not connectable in number seconds timestamp task deploy vm efi lsi logic sas testing exit due to failure task path home worker workspace ansible windows number b it physical main lsi logic sas efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e", "solution": "retry", "target": "testbed", "version": 202209081034}, {"id": 5491, "name": "3653", "raw": "2022-08-30 07:02:45,030 | Failed at Play [deploy_vm_bios_ide_vmxnet3] ****************\n2022-08-30 07:02:45,030 | TASK [deploy_vm_bios_ide_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20220830053132.log] \ntask path: /home/worker/workspace/Ansible_RHEL_8.x_70GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-08-30 07:03:32,030 | TASK [deploy_vm_bios_ide_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_RHEL_8.x_70GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:57\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm bios ide vmxnet number timestamp task deploy vm bios ide vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible rhel number x number ga ide vmxnet number bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm bios ide vmxnet number testing exit due to failure task path home worker workspace ansible rhel number x number ga ide vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios ide vmxnet number", "solution": "retry", "target": "testbed", "version": 202209081034}, {"id": 5492, "name": "3705", "raw": "2022-08-31 03:01:23,031 | Failed at Play [ovt_verify_status] *************************\n2022-08-31 03:01:23,031 | TASK [Check process 'vmtoolsd -n vmusr' is running] ********\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/utils/check_process_status.yml:34\nfatal: [localhost]: FAILED! => User 'vmware' doesn't have running process 'vmtoolsd -n vmusr'", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check process vmtoolsd nvm user is running task path home worker workspace ansible debian number x number b it main para virtual vmxnet number efi ansible vsphere gos validation linux utilities check process status yml number fatal localhost failed user vmware doesn t have running process vmtoolsd nvm user", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5500, "name": "3610", "raw": "2022-08-29 14:56:49,029 | Failed at Play [cpu_hot_add_basic] *************************\n2022-08-29 14:56:49,029 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": null, "processed": "timestamp failed at play cpu hot add basic timestamp task check vm test vm ip address task path home worker workspace ansible debian number x number b it main para virtual vmxnet number bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5493, "name": "3731", "raw": "2022-08-31 08:24:33,031 | Failed at Play [ovt_verify_status] *************************\n2022-08-31 08:24:33,031 | TASK [Check GNOME configure file status] *******************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/enable_auto_login.yml:38\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'gdm_conf_path' is undefined\nThe error appears to be in '/home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/enable_auto_login.yml': line 38, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n- name: \"Check GNOME configure file status\"\n  ^ here", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check gnome configure file status task path home worker workspace ansible debian number x number b it main para virtual vmxnet number bios ansible vsphere gos validation linux utilities enable auto login yml number fatal localhost failed the task includes an option with an un defined variable the error was gdm configuration path is un defined the error appears to be in home worker workspace ansible debian number x number b it main para virtual vmxnet number bios ansible vsphere gos validation linux utilities enable auto login yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be name check gnome configure file status here", "solution": "deepdive", "target": "testcase", "version": 202209081034}, {"id": 5494, "name": "3736", "raw": "2022-08-31 09:35:48,031 | Failed at Play [gosc_perl_staticip] ************************\n2022-08-31 09:35:48,031 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.", "category": null, "processed": "timestamp failed at play go sc perl static ip timestamp task customize linux guest os task path home worker workspace ansible debian number x number b it main para virtual vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc execution yml number fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5495, "name": "3735", "raw": "2022-08-31 09:18:07,031 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-08-31 09:18:07,031 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.", "category": null, "processed": "timestamp failed at play go sc perl dhcp timestamp task customize linux guest os task path home worker workspace ansible debian number x number b it main para virtual vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc execution yml number fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5496, "name": "3620", "raw": "2022-08-29 16:59:00,029 | Failed at Play [ovt_verify_uninstall] **********************\n2022-08-29 16:59:00,029 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": null, "processed": "timestamp failed at play ovt verify un install timestamp task check vm test vm ip address task path home worker workspace ansible debian number x number b it main para virtual vmxnet number bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5497, "name": "3607", "raw": "2022-08-29 13:43:44,029 | Failed at Play [ovt_verify_status] *************************\n2022-08-29 13:43:44,029 | TASK [Check GNOME configure file status] *******************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/enable_auto_login.yml:38\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'gdm_conf_path' is undefined\nThe error appears to be in '/home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/enable_auto_login.yml': line 38, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n- name: \"Check GNOME configure file status\"\n  ^ here", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check gnome configure file status task path home worker workspace ansible debian number x number b it main para virtual vmxnet number bios ansible vsphere gos validation linux utilities enable auto login yml number fatal localhost failed the task includes an option with an un defined variable the error was gdm configuration path is un defined the error appears to be in home worker workspace ansible debian number x number b it main para virtual vmxnet number bios ansible vsphere gos validation linux utilities enable auto login yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be name check gnome configure file status here", "solution": "deepdive", "target": "testbed", "version": 202209081034}, {"id": 5498, "name": "3609", "raw": "2022-08-29 14:27:55,029 | Failed at Play [memory_hot_add_basic] **********************\n2022-08-29 14:27:55,029 | TASK [Check VM 'test_vm' IP address] ***********************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": null, "processed": "timestamp failed at play memory hot add basic timestamp task check vm test vm ip address task path home worker workspace ansible debian number x number b it main para virtual vmxnet number bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5499, "name": "3608", "raw": "2022-08-29 13:56:37,029 | Failed at Play [device_list] *******************************\n2022-08-29 13:56:37,029 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": null, "processed": "timestamp failed at play device list timestamp task wait for vmware tools collecting guest info task path home worker workspace ansible debian number x number b it main para virtual vmxnet number bios ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed hardware configuration table", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5501, "name": "3742", "raw": "2022-08-31 14:07:08,031 | Failed at Play [ovt_verify_status] *************************\n2022-08-31 14:07:08,031 | TASK [Check process 'vmtoolsd -n vmusr' is running] ********\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/check_process_status.yml:34\nfatal: [localhost]: FAILED! => User 'vmware' doesn't have running process 'vmtoolsd -n vmusr'", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check process vmtoolsd nvm user is running task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux utilities check process status yml number fatal localhost failed user vmware doesn t have running process vmtoolsd nvm user", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5502, "name": "3746", "raw": "2022-08-31 15:04:05,031 | Failed at Play [gosc_perl_staticip] ************************\n2022-08-31 15:04:05,031 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:108\nfatal: [localhost]: FAILED! => ['VM hostname is localhost, expected hostname is gosc-static-vm-01', 'VM DNS domain name is , expected domain name is gosc.test.com', \"VM DNS servers are ['192.168.192.1'], not expected DNS servers ['192.168.1.1', '192.168.1.2']\", \"VM DNS domain search domains are  ['lan'] not expected search domains ['test.com', 'gosc.test.com']\", \"VM static IPv4 address is '192.168.192.201', expected IPv4 address is 192.168.192.101\", \"VM static IPv4 gateway is '192.168.1.1', expected IPv4 gateway is 192.168.192.1\", 'VM hwclockUTC is True, expected hwclockUTC is False']", "category": null, "processed": "timestamp failed at play go sc perl static ip timestamp task gos customization failed task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is localhost expected hostname is go sc static vm number vm dns domain name is expected domain name is go sc test com vm dns servers are ip address not expected dns servers ip address ip address vm dns domain search domains are lan not expected search domains test com go sc test com vm static ip v number address is ip address expected ip v number address is ip address vm static ip v number gateway is ip address expected ip v number gateway is ip address vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5503, "name": "3669", "raw": "2022-08-31 02:08:02,031 | Failed at Play [ovt_verify_install] ************************\n2022-08-31 02:08:02,031 | TASK [Create snapshot 'BaseSnapshot' on 'uos-server-20-PARAVIRTUAL_VMXNET3_EFI'] \ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.\n2022-08-31 02:08:05,031 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case ovt_verify_install", "category": null, "processed": "timestamp failed at play ovt verify install timestamp task create snapshot base snapshot on u os server number para virtual vmxnet number efi task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation common vm take snapshot yml number fatal localhost failed insufficient disk space on data store timestamp task exit testing when exit testing when fail is set true task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux setup test rescue yml number fatal localhost failed failed to run test case ovt verify install", "solution": "retry", "target": "nimbus", "version": 202209081034}, {"id": 5505, "name": "3637", "raw": "2022-08-30 04:23:08,030 | Failed at Play [ovt_verify_status] *************************\n2022-08-30 04:23:08,030 | TASK [Check GNOME configure file status] *******************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/enable_auto_login.yml:38\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'gdm_conf_path' is undefined\nThe error appears to be in '/home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/enable_auto_login.yml': line 38, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n- name: \"Check GNOME configure file status\"\n  ^ here", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check gnome configure file status task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux utilities enable auto login yml number fatal localhost failed the task includes an option with an un defined variable the error was gdm configuration path is un defined the error appears to be in home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux utilities enable auto login yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be name check gnome configure file status here", "solution": "deepdive", "target": "testcase", "version": 202209081034}, {"id": 5506, "name": "3642", "raw": "2022-08-30 03:21:24,030 | Failed at Play [ovt_verify_status] *************************\n2022-08-30 03:21:24,030 | TASK [Check process 'vmtoolsd -n vmusr' is running] ********\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/check_process_status.yml:34\nfatal: [localhost]: FAILED! => User 'vmware' doesn't have running process 'vmtoolsd -n vmusr'", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check process vmtoolsd nvm user is running task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux utilities check process status yml number fatal localhost failed user vmware doesn t have running process vmtoolsd nvm user", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5507, "name": "3647", "raw": "2022-08-30 04:58:40,030 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-08-30 04:58:40,030 | TASK [GOS customization failed] ****************************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:108\nfatal: [localhost]: FAILED! => [\"VM static IPv4 address is '192.168.192.161', expected IPv4 address is 192.168.192.101\", \"VM static IPv4 gateway is '192.168.1.1', expected IPv4 gateway is 192.168.192.1\"]", "category": null, "processed": "timestamp failed at play go sc cloud init static ip timestamp task gos customization failed task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm static ip v number address is ip address expected ip v number address is ip address vm static ip v number gateway is ip address expected ip v number gateway is ip address", "solution": "deepdive", "target": "product", "version": 202209081034}, {"id": 5508, "name": "3628", "raw": "2022-08-30 01:46:56,030 | Failed at Play [ovt_verify_status] *************************\n2022-08-30 01:46:56,030 | TASK [Check GNOME configure file status] *******************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/enable_auto_login.yml:38\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'gdm_conf_path' is undefined\nThe error appears to be in '/home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/enable_auto_login.yml': line 38, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n- name: \"Check GNOME configure file status\"\n  ^ here", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check gnome configure file status task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux utilities enable auto login yml number fatal localhost failed the task includes an option with an un defined variable the error was gdm configuration path is un defined the error appears to be in home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux utilities enable auto login yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be name check gnome configure file status here", "solution": "deepdive", "target": "testcase", "version": 202209081034}, {"id": 5509, "name": "3633", "raw": "2022-08-30 03:02:08,030 | Failed at Play [gosc_perl_staticip] ************************\n2022-08-30 03:02:08,030 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.", "category": null, "processed": "timestamp failed at play go sc perl static ip timestamp task customize linux guest os task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc execution yml number fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5510, "name": "3623", "raw": "2022-08-30 01:35:37,030 | Failed at Play [ovt_verify_install] ************************\n2022-08-30 01:35:37,030 | TASK [Rename snapshot 'BaseSnapshot' to 'BaseSnapshot-2022-08-30-01-35-36'] \ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_rename_snapshot.yml:11\nexception in /AnsiballZ_vmware_guest_snapshot.py when _ansiballz_main in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error\n2022-08-30 01:37:29,030 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case ovt_verify_install", "category": null, "processed": "timestamp failed at play ovt verify install timestamp task rename snapshot base snapshot to base snapshot timestamp number task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation common vm rename snapshot yml number exception in ansi ball z vmware guest snapshot python when ansi ball z main in soap adapter python when invoke method fatal localhost failed module failure see stdout stderr for the exact error timestamp task exit testing when exit testing when fail is set true task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux setup test rescue yml number fatal localhost failed failed to run test case ovt verify install", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5511, "name": "3596", "raw": "2022-08-29 10:00:25,029 | Failed at Play [ovt_verify_status] *************************\n2022-08-29 10:00:25,029 | TASK [Check GNOME configure file status] *******************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/enable_auto_login.yml:38\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'gdm_conf_path' is undefined\nThe error appears to be in '/home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/enable_auto_login.yml': line 38, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n- name: \"Check GNOME configure file status\"\n  ^ here", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check gnome configure file status task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux utilities enable auto login yml number fatal localhost failed the task includes an option with an un defined variable the error was gdm configuration path is un defined the error appears to be in home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux utilities enable auto login yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be name check gnome configure file status here", "solution": "deepdive", "target": "testcase", "version": 202209081034}, {"id": 5512, "name": "3601", "raw": "2022-08-29 11:28:19,029 | Failed at Play [gosc_perl_staticip] ************************\n2022-08-29 11:28:19,029 | TASK [Customize Linux guest OS] ****************************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.", "category": null, "processed": "timestamp failed at play go sc perl static ip timestamp task customize linux guest os task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc execution yml number fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5513, "name": "3597", "raw": "2022-08-29 10:13:46,029 | Failed at Play [device_list] *******************************\n2022-08-29 10:13:46,029 | TASK [Wait for VMware Tools collecting guest info] *********\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_65U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table", "category": null, "processed": "timestamp failed at play device list timestamp task wait for vmware tools collecting guest info task path home worker workspace ansible debian number x number b it para virtual vmxnet number bios ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed hardware configuration table", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5514, "name": "3583", "raw": "2022-08-29 04:28:26,029 | Failed at Play [e1000e_network_device_ops] *****************\n2022-08-29 04:28:26,029 | TASK [e1000e_network_device_ops][Try to ping IP] ***********\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04_ISO/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": null, "processed": "timestamp failed at play network device ops timestamp task network device ops try to ping ip task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "retry", "target": "targetvm", "version": 202209081034}, {"id": 5515, "name": "3567", "raw": "2022-08-26 04:39:30,026 | Failed at Play [nvme_disk_hot_extend_spec13] ***************\n2022-08-26 04:39:30,026 | TASK [nvme_disk_hot_extend_spec13][Execute powershell command '(Get-WmiObject Win32_SCSIController | where-object {$_.Name -like '*NVM Express*'} | measure).Count'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n2022-08-26 04:39:59,026 | TASK [nvme_disk_hot_extend_spec13][Guest OS unreachable] ***\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:39\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': \"Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\", 'skip_reason': 'Host localhost is unreachable', 'unreachable': True}", "category": null, "processed": "timestamp failed at play nvme disk hot extend spec number timestamp task nvme disk hot extend spec number execute powershell command get wmi object win number scsi controller where object name like nvm express measure count task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer timestamp task nvme disk hot extend spec number guest os un reachable task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost failed changed false message failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer skip reason host localhost is un reachable un reachable true", "solution": "retry", "target": "targetvm", "version": 202209081034}, {"id": 5516, "name": "3654", "raw": "2022-08-30 09:41:19,030 | Failed at Play [env_setup] *********************************\n2022-08-30 09:41:19,030 | TASK [Check VM 'uos-server-20-default' does not exist] *****\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/env_setup/env_setup.yml:25\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'uos-server-20-default' already exists. Please provide a new vm_name.", "category": null, "processed": "timestamp failed at play environment setup timestamp task check vm u os server number default does not exist task path home worker workspace ansible debian number x number b it number ga ide vmxnet number efi ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name u os server number default already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202209081034}, {"id": 5517, "name": "3575", "raw": "2022-08-26 07:56:19,026 | Failed at Play [deploy_vm_efi_ide_vmxnet3] *****************\n2022-08-26 07:56:19,026 | TASK [Download datastore file] *****************************\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/common/esxi_download_datastore_file.yml:29\nsocket.timeout: The read operation timed out\nfatal: [localhost]: FAILED! => failed to create temporary content file: The read operation timed out\n2022-08-26 07:56:52,026 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case deploy_vm_efi_ide_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm efi ide vmxnet number timestamp task download data store file task path home worker workspace ansible debian number x number b it number ga ide vmxnet number efi ansible vsphere gos validation common esxi download data store file yml number socket timeout the read operation timed out fatal localhost failed failed to create temporary content file the read operation timed out timestamp task exit testing when exit testing when fail is set true task path home worker workspace ansible debian number x number b it number ga ide vmxnet number efi ansible vsphere gos validation linux setup test rescue yml number fatal localhost failed failed to run test case deploy vm efi ide vmxnet number", "solution": "deepdive", "target": "testbed", "version": 202209081034}, {"id": 5518, "name": "3573", "raw": "2022-08-26 06:32:22,026 | Failed at Play [ovt_verify_install] ************************\n2022-08-26 06:32:22,026 | TASK [Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.117.16.64:22\n2022-08-26 06:34:56,026 | TASK [Exit testing when exit_testing_when_fail is set True] \ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/setup/test_rescue.yml:52\nfatal: [localhost]: FAILED! => Failed to run test case ovt_verify_install", "category": null, "processed": "timestamp failed at play ovt verify install timestamp task wait for port number to become open or contain specific keyword task path home worker workspace ansible debian number x number b it number ga ide vmxnet number efi ansible vsphere gos validation common vm wait ssh yml number fatal localhost failed timeout when waiting for search string openssh in ip address timestamp task exit testing when exit testing when fail is set true task path home worker workspace ansible debian number x number b it number ga ide vmxnet number efi ansible vsphere gos validation linux setup test rescue yml number fatal localhost failed failed to run test case ovt verify install", "solution": "deepdive", "target": "targetvm", "version": 202209081034}, {"id": 5520, "name": "3560", "raw": "2022-08-26 03:48:05,026 | Failed at Play [testbed_deploy_nimbus] *********************\n2022-08-26 03:48:05,026 | TASK [testbed_deploy_nimbus][Testbed deploy result is not PASS] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/newgos_testing_internal/testbed_deploy/parse_testbed_info.yml:10\nfatal: [localhost]: FAILED! => Get deployment result in testbedInfo.json is 'INVALID', not PASS", "category": null, "processed": "timestamp failed at play testbed deploy nimbus timestamp task testbed deploy nimbus testbed deploy result is not pass task path home worker workspace ansible cycle windows number new gos testing internal testbed deploy parse testbed info yml number fatal localhost failed get deployment result in testbed info json is invalid not pass", "solution": "retry", "target": "nimbus", "version": 202209081634}, {"id": 5521, "name": "3864", "raw": "2022-09-05 04:36:07,005 | Failed at Play [deploy_vm_efi_sata_e1000e] *****************\n2022-09-05 04:36:07,005 | TASK [deploy_vm_efi_sata_e1000e][Add virtual TPM device to VM] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_add_remove_vtpm.yml:14\nexception in /vmware_guest_tpm.py when vtpm_operation in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => Failed to configure vTPM device on virtual machine due to '('RuntimeFault.summary', None)'\n2022-09-05 04:36:14,005 | TASK [deploy_vm_efi_sata_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:57\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi sata timestamp task deploy vm efi sata add virtual tpm device to vm task path home worker workspace ansible windows number sata efi ansible vsphere gos validation common vm add remove vtpm yml number exception in vmware guest tpm python when vtpm operation in vmware python when wait for task fatal localhost failed failed to configure vtpm device on virtual machine due to runtime fault summary none timestamp task deploy vm efi sata testing exit due to failure task path home worker workspace ansible windows number sata efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi sata e number e", "solution": "deepdive", "target": "testbed", "version": 202209081634}, {"id": 5522, "name": "3862", "raw": "2022-09-05 04:36:25,005 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-09-05 04:36:25,005 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Add virtual TPM device to VM] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_add_remove_vtpm.yml:14\nexception in /vmware_guest_tpm.py when vtpm_operation in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => Failed to configure vTPM device on virtual machine due to '('RuntimeFault.summary', None)'\n2022-09-05 04:36:32,005 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:57\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task deploy vm efi lsi logic sas add virtual tpm device to vm task path home worker workspace ansible windows number ga lsi logic sas efi ansible vsphere gos validation common vm add remove vtpm yml number exception in vmware guest tpm python when vtpm operation in vmware python when wait for task fatal localhost failed failed to configure vtpm device on virtual machine due to runtime fault summary none timestamp task deploy vm efi lsi logic sas testing exit due to failure task path home worker workspace ansible windows number ga lsi logic sas efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e", "solution": "deepdive", "target": "testbed", "version": 202209081634}, {"id": 5523, "name": "3863", "raw": "2022-09-05 04:36:30,005 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2022-09-05 04:36:30,005 | TASK [deploy_vm_efi_paravirtual_e1000e][Add virtual TPM device to VM] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_add_remove_vtpm.yml:14\nexception in /vmware_guest_tpm.py when vtpm_operation in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => Failed to configure vTPM device on virtual machine due to '('RuntimeFault.summary', None)'\n2022-09-05 04:36:37,005 | TASK [deploy_vm_efi_paravirtual_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:57\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual timestamp task deploy vm efi para virtual add virtual tpm device to vm task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation common vm add remove vtpm yml number exception in vmware guest tpm python when vtpm operation in vmware python when wait for task fatal localhost failed failed to configure vtpm device on virtual machine due to runtime fault summary none timestamp task deploy vm efi para virtual testing exit due to failure task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual e number e", "solution": "deepdive", "target": "testbed", "version": 202209081634}, {"id": 5524, "name": "3670", "raw": "2022-08-31 02:17:20,031 | Failed at Play [ovt_verify_status] *************************\n2022-08-31 02:17:20,031 | TASK [Check process 'vmtoolsd -n vmusr' is running] ********\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/utils/check_process_status.yml:34\nfatal: [localhost]: FAILED! => User 'vmware' doesn't have running process 'vmtoolsd -n vmusr'", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check process vmtoolsd nvm user is running task path home worker workspace ansible debian number x number b it number ga ide vmxnet number efi ansible vsphere gos validation linux utilities check process status yml number fatal localhost failed user vmware doesn t have running process vmtoolsd nvm user", "solution": "deepdive", "target": "targetvm", "version": 202209081634}, {"id": 5525, "name": "3663", "raw": "2022-08-30 12:44:30,030 | Failed at Play [ovt_verify_status] *************************\n2022-08-30 12:44:30,030 | TASK [Check process 'vmtoolsd -n vmusr' is running] ********\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/utils/check_process_status.yml:34\nfatal: [localhost]: FAILED! => User 'vmware' doesn't have running process 'vmtoolsd -n vmusr'", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check process vmtoolsd nvm user is running task path home worker workspace ansible debian number x number b it number ga ide vmxnet number efi ansible vsphere gos validation linux utilities check process status yml number fatal localhost failed user vmware doesn t have running process vmtoolsd nvm user", "solution": "deepdive", "target": "targetvm", "version": 202209081634}, {"id": 5526, "name": "3657", "raw": "2022-08-30 10:32:36,030 | Failed at Play [ovt_verify_status] *************************\n2022-08-30 10:32:36,030 | TASK [Check process 'vmtoolsd -n vmusr' is running] ********\ntask path: /home/worker/workspace/Ansible_Debian_10.x_64bit_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/utils/check_process_status.yml:34\nfatal: [localhost]: FAILED! => User 'vmware' doesn't have running process 'vmtoolsd -n vmusr'", "category": null, "processed": "timestamp failed at play ovt verify status timestamp task check process vmtoolsd nvm user is running task path home worker workspace ansible debian number x number b it number ga ide vmxnet number efi ansible vsphere gos validation linux utilities check process status yml number fatal localhost failed user vmware doesn t have running process vmtoolsd nvm user", "solution": "deepdive", "target": "targetvm", "version": 202209081634}, {"id": 5533, "name": "log-3963", "raw": "2022-09-06 09:57:02,006 | Failed at Play [gosc_sanity_dhcp] **************************\n2022-09-06 09:57:02,006 | TASK [gosc_sanity_dhcp][Check VM 'test_vm' IP address] *****\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play go sc sanity dhcp timestamp task go sc sanity dhcp check vm test vm ip address task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5529, "name": "log-3949", "raw": "2022-09-06 09:36:32,006 | Failed at Play [gosc_sanity_dhcp] **************************\n2022-09-06 09:36:32,006 | TASK [gosc_sanity_dhcp][Check VM 'test_vm' IP address] *****\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play go sc sanity dhcp timestamp task go sc sanity dhcp check vm test vm ip address task path home worker workspace ansible windows number b it number ga sata bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5531, "name": "log-3952", "raw": "2022-09-06 07:07:18,006 | Failed at Play [check_ip_address] **************************\n2022-09-06 07:07:18,006 | TASK [check_ip_address][Check VM 'test_vm' IP address] *****\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play check ip address timestamp task check ip address check vm test vm ip address task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5532, "name": "log-3954", "raw": "2022-09-06 07:25:43,006 | Failed at Play [mouse_driver_vmtools] **********************\n2022-09-06 07:25:43,006 | TASK [mouse_driver_vmtools][Check VM 'test_vm' IP address] *\ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play mouse driver vm tools timestamp task mouse driver vm tools check vm test vm ip address task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5530, "name": "log-3951", "raw": "2022-09-06 06:50:40,006 | Failed at Play [wintools_complete_install_verify] **********\n2022-09-06 06:50:40,006 | TASK [wintools_complete_install_verify][Check VM 'test_vm' IP address] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify check vm test vm ip address task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5536, "name": "log-3982", "raw": "2022-09-07 05:22:52,007 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-07 05:22:52,007 | TASK [nvme_vhba_device_ops][Check VM 'test_vm' IP address] *\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops check vm test vm ip address task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5527, "name": "log-3914", "raw": "2022-09-06 01:18:25,006 | Failed at Play [wintools_uninstall_verify] *****************\n2022-09-06 01:18:25,006 | TASK [wintools_uninstall_verify][Check no problem device listed] \ntask path: /home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/windows/utils/win_check_problem_device.yml:16\nfatal: [localhost]: FAILED! => No 'No devices were found on the system' in output, please check listed problem devices.", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify check no problem device listed task path home worker workspace dw existing ansible vsphere gos validation windows utilities win check problem device yml number fatal localhost failed no no devices were found on the system in output please check listed problem devices", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5528, "name": "log-3938", "raw": "2022-09-06 06:57:55,006 | Failed at Play [wintools_complete_install_verify] **********\n2022-09-06 06:57:55,006 | TASK [wintools_complete_install_verify][Check VM 'test_vm' IP address] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify check vm test vm ip address task path home worker workspace ansible windows number b it number ga sata bios ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5534, "name": "log-4097", "raw": "2022-09-13 03:42:21,013 | Failed at Play [check_inbox_driver] ************************\n2022-09-13 03:42:21,013 | TASK [check_inbox_driver][Check inbox driver's filename is valid] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_9.x/ansible-vsphere-gos-validation/linux/check_inbox_driver/get_inbox_drivers.yml:141\nfailed: [localhost] => (item=vsock is (builtin)) => Invalid inbox driver vsock filename: (builtin) and vsock is (builtin)", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver check inbox driver s filename is valid task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation linux check inbox driver get inbox drivers yml number failed localhost item vsock is builtin invalid inbox driver vsock filename builtin and vsock is builtin", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5535, "name": "log-3974", "raw": "2022-09-07 04:46:17,007 | Failed at Play [wintools_complete_install_verify] **********\n2022-09-07 04:46:17,007 | TASK [wintools_complete_install_verify][Check VM 'test_vm' IP address] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm' IP Address", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify check vm test vm ip address task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm ip address", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5537, "name": "log-4069", "raw": "2022-09-09 04:23:05,009 | Failed at Play [gosc_sanity_dhcp] **************************\n2022-09-09 04:23:05,009 | TASK [gosc_sanity_dhcp][Customize Windows guest OS] ********\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/guest_customization/win_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.", "category": "", "processed": "timestamp failed at play go sc sanity dhcp timestamp task go sc sanity dhcp customize windows guest os task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows guest customization win go sc execution yml number fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5538, "name": "log-4058", "raw": "2022-09-09 02:33:46,009 | Failed at Play [e1000e_network_device_ops] *****************\n2022-09-09 02:33:46,009 | TASK [e1000e_network_device_ops][Get network interface 'ens35' status] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_11.x_64/ansible-vsphere-gos-validation/linux/utils/set_network_adapter_status.yml:62\nfatal: [localhost -> 10.186.86.173]: FAILED! =>  when ip", "category": "", "processed": "timestamp failed at play network device ops timestamp task network device ops get network interface status task path home worker workspace ansible regression debian number x number ansible vsphere gos validation linux utilities set network adapter status yml number fatal localhost ip address failed when ip", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5539, "name": "log-4059", "raw": "2022-09-09 02:37:24,009 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-09-09 02:37:24,009 | TASK [vmxnet3_network_device_ops][Get network interface 'ens35' status] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_11.x_64/ansible-vsphere-gos-validation/linux/utils/set_network_adapter_status.yml:62\nfatal: [localhost -> 10.186.86.173]: FAILED! =>  when ip", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops get network interface status task path home worker workspace ansible regression debian number x number ansible vsphere gos validation linux utilities set network adapter status yml number fatal localhost ip address failed when ip", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5540, "name": "log-4061", "raw": "2022-09-09 02:34:38,009 | Failed at Play [e1000e_network_device_ops] *****************\n2022-09-09 02:34:38,009 | TASK [e1000e_network_device_ops][Get network interface 'ens35' status] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_ISO/ansible-vsphere-gos-validation/linux/utils/set_network_adapter_status.yml:62\nfatal: [localhost -> 10.187.154.92]: FAILED! =>  when ip", "category": "", "processed": "timestamp failed at play network device ops timestamp task network device ops get network interface status task path home worker workspace ansible regression ubuntu number server iso ansible vsphere gos validation linux utilities set network adapter status yml number fatal localhost ip address failed when ip", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5541, "name": "log-4064", "raw": "2022-09-09 02:22:15,009 | Failed at Play [e1000e_network_device_ops] *****************\n2022-09-09 02:22:15,009 | TASK [e1000e_network_device_ops][Get network interface 'ens224' status] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_OVA/ansible-vsphere-gos-validation/linux/utils/set_network_adapter_status.yml:62\nfatal: [localhost -> 10.182.0.161]: FAILED! =>  when ip", "category": "", "processed": "timestamp failed at play network device ops timestamp task network device ops get network interface status task path home worker workspace ansible regression ubuntu number server ova ansible vsphere gos validation linux utilities set network adapter status yml number fatal localhost ip address failed when ip", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5542, "name": "log-4072", "raw": "2022-09-09 05:09:26,009 | Failed at Play [e1000e_network_device_ops] *****************\n2022-09-09 05:09:26,009 | TASK [e1000e_network_device_ops][Remove a network adapter from VM 'test_rockylinux8'] \ntask path: /home/worker/workspace/Ansible_Regression_RockyLinux_8.x/ansible-vsphere-gos-validation/common/vm_remove_network_adapter.yml:4\nhttp.client.HTTPException: 503 Service Unavailable\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "", "processed": "timestamp failed at play network device ops timestamp task network device ops remove a network adapter from vm test rocky linux number task path home worker workspace ansible regression rocky linux number x ansible vsphere gos validation common vm remove network adapter yml number http client http exception number service unavailable fatal localhost failed module failure see stdout stderr for the exact error", "solution": "retry", "target": "targetvm", "version": 202209151400}, {"id": 5543, "name": "log-4075", "raw": "2022-09-09 05:07:35,009 | Failed at Play [e1000e_network_device_ops] *****************\n2022-09-09 05:07:35,009 | TASK [e1000e_network_device_ops][Reconfigure network adapter to test_rockylinux9] \ntask path: /home/worker/workspace/Ansible_Regression_RockyLinux_9.x/ansible-vsphere-gos-validation/common/vm_configure_network_adapter.yml:5\nhttp.client.HTTPException: 503 Service Unavailable\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error", "category": "", "processed": "timestamp failed at play network device ops timestamp task network device ops re configure network adapter to test rocky linux number task path home worker workspace ansible regression rocky linux number x ansible vsphere gos validation common vm configure network adapter yml number http client http exception number service unavailable fatal localhost failed module failure see stdout stderr for the exact error", "solution": "retry", "target": "targetvm", "version": 202209151400}, {"id": 5544, "name": "log-4080", "raw": "2022-09-09 08:39:45,009 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-09-09 08:39:45,009 | TASK [vmxnet3_network_device_ops][Add a new 'vmxnet3' adapter in 'vSwitch2022-09-09-08-33-46_PG' to VM 'test_debian10_32'] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_10.x_32/ansible-vsphere-gos-validation/common/vm_add_network_adapter.yml:4\nexception in /vmware_guest_network.py when _nic_present in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => ('The available Memory resources in the parent resource pool are insufficient for the operation.', None)", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops add a new vmxnet number adapter in vswitch timestamp number page to vm test debian number task path home worker workspace ansible regression debian number x number ansible vsphere gos validation common vm add network adapter yml number exception in vmware guest network python when nic present in vmware python when wait for task fatal localhost failed the available memory resources in the parent resource pool are insufficient for the operation none", "solution": "retry", "target": "nimbus", "version": 202209151400}, {"id": 5545, "name": "log-4081", "raw": "2022-09-09 08:38:01,009 | Failed at Play [e1000e_network_device_ops] *****************\n2022-09-09 08:38:01,009 | TASK [e1000e_network_device_ops][Assert new e1000e adapter interface is removed] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_10.x_32/ansible-vsphere-gos-validation/linux/network_device_ops/network_status_validate.yml:155\nfatal: [localhost]: FAILED! => After hot removing the new network adapter, guest OS still can detect it", "category": "", "processed": "timestamp failed at play network device ops timestamp task network device ops assert new adapter interface is removed task path home worker workspace ansible regression debian number x number ansible vsphere gos validation linux network device ops network status validate yml number fatal localhost failed after hot removing the new network adapter guest os still can detect it", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5546, "name": "log-4082", "raw": "2022-09-09 08:42:23,009 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-09-09 08:42:23,009 | TASK [vmxnet3_network_device_ops][Waiting for network interface 'ens35' status up] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_10.x_32/ansible-vsphere-gos-validation/linux/utils/set_network_adapter_status.yml:62\nfatal: [localhost -> 10.212.28.189]: FAILED! =>  when ip", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops waiting for network interface status up task path home worker workspace ansible regression debian number x number ansible vsphere gos validation linux utilities set network adapter status yml number fatal localhost ip address failed when ip", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5547, "name": "log-4084", "raw": "2022-09-09 21:12:30,009 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-09-09 21:12:30,009 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20220909200020.log] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_9.x/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-09-09 21:13:01,009 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_9.x/ansible-vsphere-gos-validation/common/test_rescue.yml:57\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202209151400}, {"id": 5548, "name": "log-4099", "raw": "2022-09-13 09:50:35,013 | Failed at Play [wintools_uninstall_verify] *****************\n2022-09-13 09:50:35,013 | TASK [wintools_uninstall_verify][Check no problem device listed] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_check_problem_device.yml:16\nfatal: [localhost]: FAILED! => No 'No devices were found on the system' in output, please check listed problem devices.", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify check no problem device listed task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation windows utilities win check problem device yml number fatal localhost failed no no devices were found on the system in output please check listed problem devices", "solution": "deepdive", "target": "targetvm", "version": 202209151400}, {"id": 5549, "name": "log-4108", "raw": "2022-09-15 11:33:19,015 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-15 11:33:19,015 | TASK [nvdimm_cold_add_remove][Check new NVDIMM device backing vmdk file in PMem datastore] \ntask path: /home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/windows/nvdimm_cold_add_remove/cold_add_nvdimm_test.yml:70\nfatal: [localhost]: FAILED! => 'PMemDS' keyword is not in the backing vmdk file path of new NVDIMM device: ", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove check new nvdimm device backing vmdk file in pmem data store task path home worker workspace dw existing ansible vsphere gos validation windows nvdimm cold add remove cold add nvdimm test yml number fatal localhost failed pmem ds keyword is not in the backing vmdk file path of new nvdimm device", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5550, "name": "log-4126", "raw": "2022-09-15 12:01:00,015 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-15 12:01:00,015 | TASK [nvdimm_cold_add_remove][Set fact of persistent memory disk unique ID list] \ntask path: /home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/windows/utils/win_get_pmem_disk_list.yml:17\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'item' is undefined\nThe error appears to be in '/home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/windows/utils/win_get_pmem_disk_list.yml': line 17, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n    win_powershell_cmd: \"get-physicaldisk | where BusType -eq 'SCM' | select UniqueId | fl\"\n- name: \"Set fact of persistent memory disk unique ID list\"\n  ^ here", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove set fact of persistent memory disk unique id list task path home worker workspace dw existing ansible vsphere gos validation windows utilities win get pmem disk list yml number fatal localhost failed the task includes an option with an un defined variable the error was item is un defined the error appears to be in home worker workspace dw existing ansible vsphere gos validation windows utilities win get pmem disk list yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be win powershell command get physical disk where bus type eq scm select unique id fl name set fact of persistent memory disk unique id list here", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5551, "name": "log-4161", "raw": "2022-09-15 12:56:29,015 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-15 12:56:29,015 | TASK [nvdimm_cold_add_remove][Set fact of persistent memory disk unique ID list] \ntask path: /home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/windows/utils/win_get_pmem_disk_list.yml:17\nfailed: [localhost] => (item=None) => The task includes an option with an undefined variable. The error was: list object has no element 1\nThe error appears to be in '/home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/windows/utils/win_get_pmem_disk_list.yml': line 17, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n    win_powershell_cmd: \"get-physicaldisk | where BusType -eq 'SCM' | select UniqueId | fl\"\n- name: \"Set fact of persistent memory disk unique ID list\"\n  ^ here", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove set fact of persistent memory disk unique id list task path home worker workspace dw existing ansible vsphere gos validation windows utilities win get pmem disk list yml number failed localhost item none the task includes an option with an un defined variable the error was list object has no element number the error appears to be in home worker workspace dw existing ansible vsphere gos validation windows utilities win get pmem disk list yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be win powershell command get physical disk where bus type eq scm select unique id fl name set fact of persistent memory disk unique id list here", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5563, "name": "log-5158", "raw": "2022-09-21 10:40:34,021 | Failed at Play [wsl_test] **********************************\n2022-09-21 10:40:34,021 | TASK [wsl_test][Execute powershell command 'wsl --install -d Ubuntu | Out-File -FilePath C:\\temp\\wsl_install.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.184.97.236]: FAILED! => non-zero return code when wsl", "category": "", "processed": "timestamp failed at play wsl test timestamp task wsl test execute powershell command wsl installd ubuntu out file file path c temp wsl install text task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when wsl", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5552, "name": "log-5127", "raw": "2022-09-16 03:32:52,016 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-16 03:32:52,016 | TASK [nvdimm_cold_add_remove][Set fact of persistent memory disk unique ID list] \ntask path: /home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/windows/utils/win_get_pmem_disk_list.yml:17\nfailed: [localhost] => (item=None) => The task includes an option with an undefined variable. The error was: list object has no element 1\nThe error appears to be in '/home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/windows/utils/win_get_pmem_disk_list.yml': line 17, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n    win_powershell_cmd: \"(get-physicaldisk | where BusType -eq 'SCM' | select UniqueId | fl | out-string).trim()\"\n- name: \"Set fact of persistent memory disk unique ID list\"\n  ^ here", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove set fact of persistent memory disk unique id list task path home worker workspace dw existing ansible vsphere gos validation windows utilities win get pmem disk list yml number failed localhost item none the task includes an option with an un defined variable the error was list object has no element number the error appears to be in home worker workspace dw existing ansible vsphere gos validation windows utilities win get pmem disk list yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be win powershell command get physical disk where bus type eq scm select unique id fl out string trim name set fact of persistent memory disk unique id list here", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5621, "name": "log-5574", "raw": "2022-09-23 11:19:30,023 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-23 11:19:30,023 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type V:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.212.20.221]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type v test disk write text task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5553, "name": "log-5128", "raw": "2022-09-16 04:00:35,016 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-16 04:00:35,016 | TASK [nvdimm_cold_add_remove][Set fact of the file path to do disk write test] \ntask path: /home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/create_partition_raw_disk.yml:86\nfatal: [localhost]: FAILED! => The conditional check 'not (test_disk_controller_type == \"nvme\" and test_purpose == \"hot-extend\")' failed. The error was: error while evaluating conditional (not (test_disk_controller_type == \"nvme\" and test_purpose == \"hot-extend\")): 'test_disk_controller_type' is undefined\nThe error appears to be in '/home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/create_partition_raw_disk.yml': line 86, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n- block:\n    - name: \"Set fact of the file path to do disk write test\"\n      ^ here", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove set fact of the file path to do disk write test task path home worker workspace dw existing ansible vsphere gos validation windows v hba hot add remove create partition raw disk yml number fatal localhost failed the conditional check not test disk controller type nvme and test purpose hot extend failed the error was error while evaluating conditional not test disk controller type nvme and test purpose hot extend test disk controller type is un defined the error appears to be in home worker workspace dw existing ansible vsphere gos validation windows v hba hot add remove create partition raw disk yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be block name set fact of the file path to do disk write test here", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5554, "name": "log-5129", "raw": "2022-09-16 07:38:10,016 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-16 07:38:10,016 | TASK [nvdimm_cold_add_remove][Check no disk volume found in guest OS] \ntask path: /home/worker/workspace/dw_win11_70u3_existing/ansible-vsphere-gos-validation/windows/nvdimm_cold_add_remove/cold_remove_nvdimm_test.yml:72\nfatal: [localhost]: FAILED! => Get specified disk volume size not failed with keyword 'No MSFT_Volume objects found' in results.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove check no disk volume found in guest os task path home worker workspace dw existing ansible vsphere gos validation windows nvdimm cold add remove cold remove nvdimm test yml number fatal localhost failed get specified disk volume size not failed with keyword no msft volume objects found in results", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5555, "name": "log-5133", "raw": "2022-09-17 06:17:33,017 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-09-17 06:17:33,017 | TASK [vmxnet3_network_device_ops][Create/Update network configure file '{{ network_conf_path }}'] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_67U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/network_device_ops/apply_new_network_config.yml:48\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'network_conf_template' is undefined\nThe error appears to be in '/home/worker/workspace/Ansible_CentOS_8.x_67U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/network_device_ops/apply_new_network_config.yml': line 48, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n    # Create the network config file for new network interface\n    - name: \"Create/Update network configure file '{{ network_conf_path }}'\"\n      ^ here\nWe could be wrong, but this one looks like it might be an issue with\nmissing quotes. Always quote template expression brackets when they\nstart a value. For instance:\n    with_items:\n      - {{ foo }}\nShould be written as:\n    with_items:\n      - \"{{ foo }}\"", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops create update network configure file network configuration path task path home worker workspace ansible centos number x nvme vmxnet number efi ansible vsphere gos validation linux network device ops apply new network configuration yml number fatal localhost failed the task includes an option with an un defined variable the error was network configuration template is un defined the error appears to be in home worker workspace ansible centos number x nvme vmxnet number efi ansible vsphere gos validation linux network device ops apply new network configuration yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be create the network configuration file for new network interface name create update network configure file network configuration path here we could be wrong but this one looks like it might be an issue with missing quotes always quote template expression brackets when they start a value for instance with items foo should be written as with items foo", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5556, "name": "log-5142", "raw": "2022-09-17 09:10:09,017 | Failed at Play [wintools_uninstall_verify] *****************\n2022-09-17 09:10:09,017 | TASK [wintools_uninstall_verify][Add a new 'vmxnet3' adapter in 'vSwitch2022-09-17-09-09-57_PG' to VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_add_network_adapter.yml:13\nfatal: [localhost]: FAILED! => unable to find specified network_name/vlan_id (vSwitch2022-09-17-09-09-57_PG), check parameters", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify add a new vmxnet number adapter in vswitch timestamp number page to vm test vm task path home worker workspace ansible windows number lsi logic sas vmxnet number efi ansible vsphere gos validation common vm add network adapter yml number fatal localhost failed unable to find specified network name vlan id vswitch timestamp number page check parameters", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5557, "name": "log-5469", "raw": "2022-09-22 20:58:45,022 | Failed at Play [wintools_uninstall_verify] *****************\n2022-09-22 20:58:45,022 | TASK [wintools_uninstall_verify][Check no problem device listed] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/wintools_uninstall_verify/wintools_uninstall_verify.yml:74\nfatal: [localhost]: FAILED! => Problem devices were found on the system, please check listed problem devices: ['Microsoft PnP Utility', '', 'Instance ID:                PCI\\\\VEN_15AD&DEV_07B0&SUBSYS_07B015AD&REV_01\\\\3&218e0f40&0&20', 'Device Description:         vmxnet3 Ethernet Adapter', 'Class Name:                 Net', 'Class GUID:                 {4d36e972-e325-11ce-bfc1-08002be10318}', 'Manufacturer Name:          VMware, Inc.', 'Status:                     Problem', 'Problem Code:               32 (0x20) [CM_PROB_DISABLED_SERVICE]', 'Driver Name:                vmxnet3.inf', '']", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify check no problem device listed task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation windows win tools un install verify win tools un install verify yml number fatal localhost failed problem devices were found on the system please check listed problem devices microsoft pnp utility instance id pci ven dev number b number sub system hex id rev number hex id number device description vmxnet number ethernet adapter class name net class guid hex id manufacturer name vmware inc status problem problem code number hex id cm prob disabled service driver name vmxnet number inf", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5558, "name": "log-5146", "raw": "2022-09-17 15:06:28,017 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-09-17 15:06:28,017 | TASK [vmxnet3_network_device_ops][Create/Update network configure file '{{ network_conf_path }}'] \ntask path: /home/worker/workspace/Ansible_CentOS_8.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/linux/network_device_ops/apply_new_network_config.yml:48\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'network_conf_template' is undefined\nThe error appears to be in '/home/worker/workspace/Ansible_CentOS_8.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/linux/network_device_ops/apply_new_network_config.yml': line 48, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n    # Create the network config file for new network interface\n    - name: \"Create/Update network configure file '{{ network_conf_path }}'\"\n      ^ here\nWe could be wrong, but this one looks like it might be an issue with\nmissing quotes. Always quote template expression brackets when they\nstart a value. For instance:\n    with_items:\n      - {{ foo }}\nShould be written as:\n    with_items:\n      - \"{{ foo }}\"", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops create update network configure file network configuration path task path home worker workspace ansible centos number x number ga sata efi ansible vsphere gos validation linux network device ops apply new network configuration yml number fatal localhost failed the task includes an option with an un defined variable the error was network configuration template is un defined the error appears to be in home worker workspace ansible centos number x number ga sata efi ansible vsphere gos validation linux network device ops apply new network configuration yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be create the network configuration file for new network interface name create update network configure file network configuration path here we could be wrong but this one looks like it might be an issue with missing quotes always quote template expression brackets when they start a value for instance with items foo should be written as with items foo", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5559, "name": "log-5153", "raw": "2022-09-17 17:03:16,017 | Failed at Play [secureboot_enable_disable] *****************\n2022-09-17 17:03:16,017 | TASK [secureboot_enable_disable][Try to ping IP] ***********\ntask path: /home/worker/workspace/Ansible_CentOS_8.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": "", "processed": "timestamp failed at play secure boot enable disable timestamp task secure boot enable disable try to ping ip task path home worker workspace ansible centos number x number ga sata efi ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5560, "name": "log-5155", "raw": "2022-09-19 06:44:11,019 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-19 06:44:11,019 | TASK [nvdimm_cold_add_remove][include_tasks] ***************\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/nvdimm_cold_add_remove/nvdimm_cold_add_remove.yml:18\nfatal: [localhost]: FAILED! => {\n    \"msg\": \"The conditional check '(esxi_version is version('6.7', '<') or (vm_hardware_version_num | int < 14)\\n' failed. The error was: template error while templating string: unexpected '}', expected ')'. String: {% if (esxi_version is version('6.7', '<') or (vm_hardware_version_num | int < 14)\\n %} True {% else %} False {% endif %}\\n\\nThe error appears to be in '/home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/nvdimm_cold_add_remove/nvdimm_cold_add_remove.yml': line 18, column 11, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n            skip_test_no_vmtools: false\\n        - include_tasks: ../../common/skip_test_case.yml\\n          ^ here\\n\"\n}", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove include tasks task path home worker workspace ansible windows server lts c nvme efi ansible vsphere gos validation windows nvdimm cold add remove nvdimm cold add remove yml number fatal localhost failed message the conditional check esxi version is version number or vm hardware version number int number n failed the error was template error while templating string unexpected expected string if esxi version is version number or vm hardware version number int number n true else false end if n n the error appears to be in home worker workspace ansible windows server lts c nvme efi ansible vsphere gos validation windows nvdimm cold add remove nvdimm cold add remove yml line number column number but may n be elsewhere in the file depending on the exact syntax problem n n the offending line appears to be n n skip test no vm tools false n include tasks common skip test case yml n here n ", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5561, "name": "log-5156", "raw": "2022-09-20 12:15:51,020 | Failed at Play [wsl_test] **********************************\n2022-09-20 12:15:51,020 | TASK [wsl_test][Execute powershell command 'wsl --install -d Ubuntu | Out-File -FilePath C:\\temp\\wsl_install.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.185.111.143]: FAILED! => non-zero return code when wsl", "category": "", "processed": "timestamp failed at play wsl test timestamp task wsl test execute powershell command wsl installd ubuntu out file file path c temp wsl install text task path home worker workspace ansible windows server lts c main para virtual vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when wsl", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5562, "name": "log-5157", "raw": "2022-09-21 10:38:17,021 | Failed at Play [wsl_test] **********************************\n2022-09-21 10:38:17,021 | TASK [wsl_test][Execute powershell command 'wsl --install -d Ubuntu | Out-File -FilePath C:\\temp\\wsl_install.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.186.78.77]: FAILED! => non-zero return code when wsl", "category": "", "processed": "timestamp failed at play wsl test timestamp task wsl test execute powershell command wsl installd ubuntu out file file path c temp wsl install text task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when wsl", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5564, "name": "log-5351", "raw": "2022-09-22 16:38:21,022 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-09-22 16:38:21,022 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Wait for VMware Tools collecting guest info] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_9.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:22\nfatal: [localhost]: FAILED! => hardware configuration table\n2022-09-22 16:38:54,022 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_9.x/ansible-vsphere-gos-validation/common/test_rescue.yml:57\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number wait for vmware tools collecting guest info task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed hardware configuration table timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202210102130}, {"id": 5565, "name": "log-5162", "raw": "2022-09-21 15:20:31,021 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-09-21 15:20:31,021 | TASK [gosc_cloudinit_dhcp][Fetch file /var/log/vmware-imc/toolsDeployPkg.log from VM guest] \ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update_OVA/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware.py when connect_to_api in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Unable to log on to vCenter or ESXi API at 10.43.247.114:443  as Administrator@vsphere.local: Cannot complete login due to an incorrect user name or password.", "category": "", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task go sc cloud init dhcp fetch file var log vmware imc tools deploy package log from vm guest task path home worker workspace ansible cycle photon number x update ova ansible vsphere gos validation common vm guest file operation yml number exception in vmware python when connect to api in soap adapter python when invoke method fatal localhost failed unable to log on to vcenter or esxi api at ip address as administrator vsphere local can not complete login due to an incorrect user name or password", "solution": "retry", "target": "nimbus", "version": 202210102130}, {"id": 5566, "name": "log-5231", "raw": "2022-09-22 01:47:19,022 | Failed at Play [wintools_complete_install_verify] **********\n2022-09-22 01:47:19,022 | TASK [wintools_complete_install_verify][Check VMware tools install task status every 3 seconds] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/wintools_complete_install_verify/install_vmtools.yml:40\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'ansible_job_id'\nThe error appears to be in '/home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/wintools_complete_install_verify/install_vmtools.yml': line 40, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n    minutes: 2\n- name: \"Check VMware tools install task status every 3 seconds\"\n  ^ here", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify check vmware tools install task status every number seconds task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows win tools complete install verify install vm tools yml number fatal localhost failed the task includes an option with an un defined variable the error was dict object has no attribute ansible job id the error appears to be in home worker workspace ansible cycle windows server number ansible vsphere gos validation windows win tools complete install verify install vm tools yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be minutes number name check vmware tools install task status every number seconds here", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5567, "name": "log-5604", "raw": "2022-09-25 11:07:24,025 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-25 11:07:24,025 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\3&218E0F40&0&18''] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.212.7.233]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5568, "name": "log-5404", "raw": "2022-09-22 16:10:04,022 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-22 16:10:04,022 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type T:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.168.177.58]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type t test disk write text task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5575, "name": "log-5238", "raw": "2022-09-22 06:13:33,022 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-09-22 06:13:33,022 | TASK [deploy_vm_efi_paravirtual_vmxnet3][ansible.builtin.fail] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/linux/deploy_vm/ubuntu/prepare_ubuntu_iso_install.yml:17\nfatal: [localhost]: FAILED! => Ubuntu desktop is not supported by now\n2022-09-22 06:13:34,022 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'zyh-ubuntu22.04.1-autoinstall'] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM zyh-ubuntu22.04.1-autoinstall", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number ansible builtin fail task path home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation linux deploy vm ubuntu prepare ubuntu iso install yml number fatal localhost failed ubuntu desktop is not supported by now timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm zyh ubuntu version id auto install task path home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm zyh ubuntu version id auto install", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5576, "name": "log-5251", "raw": "2022-09-22 06:34:54,022 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-09-22 06:34:54,022 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Set fact of Ubuntu unattend install file] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/linux/deploy_vm/ubuntu/prepare_ubuntu_iso_install.yml:26\nfatal: [localhost]: FAILED! => The conditional check 'ubuntu_install_method == \"cloud-init\"' failed. The error was: error while evaluating conditional (ubuntu_install_method == \"cloud-init\"): 'ubuntu_install_method' is undefined\nThe error appears to be in '/home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/linux/deploy_vm/ubuntu/prepare_ubuntu_iso_install.yml': line 26, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n- name: \"Set fact of Ubuntu unattend install file\"\n  ^ here\n2022-09-22 06:34:55,022 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'zyh-ubuntu22.04.1-autoinstall'] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM zyh-ubuntu22.04.1-autoinstall", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number set fact of ubuntu un attend install file task path home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation linux deploy vm ubuntu prepare ubuntu iso install yml number fatal localhost failed the conditional check ubuntu install method cloud init failed the error was error while evaluating conditional ubuntu install method cloud init ubuntu install method is un defined the error appears to be in home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation linux deploy vm ubuntu prepare ubuntu iso install yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be name set fact of ubuntu un attend install file here timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm zyh ubuntu version id auto install task path home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm zyh ubuntu version id auto install", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5569, "name": "log-5239", "raw": "2022-09-22 03:16:45,022 | Failed at Play [wintools_complete_install_verify] **********\n2022-09-22 03:16:45,022 | TASK [wintools_complete_install_verify][Check VMware tools install task status every 3 seconds] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/wintools_complete_install_verify/install_vmtools.yml:40\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'ansible_job_id'\nThe error appears to be in '/home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/wintools_complete_install_verify/install_vmtools.yml': line 40, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n    minutes: 2\n- name: \"Check VMware tools install task status every 3 seconds\"\n  ^ here", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify check vmware tools install task status every number seconds task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows win tools complete install verify install vm tools yml number fatal localhost failed the task includes an option with an un defined variable the error was dict object has no attribute ansible job id the error appears to be in home worker workspace ansible cycle windows number ansible vsphere gos validation windows win tools complete install verify install vm tools yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be minutes number name check vmware tools install task status every number seconds here", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5570, "name": "log-5594", "raw": "2022-09-25 07:37:19,025 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-25 07:37:19,025 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type Y:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.43.255.153]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type y test disk write text task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5571, "name": "log-5608", "raw": "2022-09-26 02:50:23,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 02:50:23,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type X:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.43.255.153]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type x test disk write text task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5572, "name": "log-5556", "raw": "2022-09-23 04:24:16,023 | Failed at Play [wintools_uninstall_verify] *****************\n2022-09-23 04:24:16,023 | TASK [wintools_uninstall_verify][Check no problem device listed] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/wintools_uninstall_verify/wintools_uninstall_verify.yml:74\nfatal: [localhost]: FAILED! => Problem devices were found on the system, please check listed problem devices: ['Microsoft PnP Utility', '', 'Instance ID:                PCI\\\\VEN_15AD&DEV_07B0&SUBSYS_07B015AD&REV_01\\\\3&218e0f40&0&20', 'Device Description:         vmxnet3 Ethernet Adapter', 'Class Name:                 Net', 'Class GUID:                 {4d36e972-e325-11ce-bfc1-08002be10318}', 'Manufacturer Name:          VMware, Inc.', 'Status:                     Problem', 'Problem Code:               32 (0x20) [CM_PROB_DISABLED_SERVICE]', 'Driver Name:                vmxnet3.inf', '']", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify check no problem device listed task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows win tools un install verify win tools un install verify yml number fatal localhost failed problem devices were found on the system please check listed problem devices microsoft pnp utility instance id pci ven dev number b number sub system hex id rev number hex id number device description vmxnet number ethernet adapter class name net class guid hex id manufacturer name vmware inc status problem problem code number hex id cm prob disabled service driver name vmxnet number inf", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5573, "name": "log-5230", "raw": "2022-09-22 02:06:29,022 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-09-22 02:06:29,022 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VM 'test_vm_1663809978052' IP address] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_9.x/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'test_vm_1663809978052' IP Address\n2022-09-22 02:07:10,022 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_9.x/ansible-vsphere-gos-validation/common/test_rescue.yml:57\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vm test vm number ip address task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm test vm number ip address timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "targetvm", "version": 202210102130}, {"id": 5574, "name": "log-5619", "raw": "2022-09-26 07:51:42,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 07:51:42,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type Z:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.43.197.154]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type z test disk write text task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5577, "name": "log-5551", "raw": "2022-09-23 03:48:41,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 03:48:41,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.184.101.88]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number ga lsi logic sas efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5578, "name": "log-5461", "raw": "2022-09-22 20:50:45,022 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-22 20:50:45,022 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number ga lsi logic sas efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5579, "name": "log-5557", "raw": "2022-09-23 04:12:01,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 04:12:01,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&1EE266C4&0&00B0''] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.212.232.67]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number sata efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5580, "name": "log-5484", "raw": "2022-09-22 21:14:30,022 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-22 21:14:30,022 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number sata efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5581, "name": "log-5552", "raw": "2022-09-23 03:53:16,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 03:53:16,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_11_67U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.192.192.113]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number lsi logic sas efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5582, "name": "log-5559", "raw": "2022-09-23 04:20:34,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 04:20:34,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&1EE266C4&0&00B0''] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.185.42.3]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number ga ide efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5583, "name": "log-5460", "raw": "2022-09-22 20:50:41,022 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-22 20:50:41,022 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_67U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number lsi logic sas efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5584, "name": "log-5465", "raw": "2022-09-22 20:53:00,022 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-22 20:53:00,022 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number ga ide efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5585, "name": "log-5554", "raw": "2022-09-23 04:00:54,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 04:00:54,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.186.130.236]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number lsi logic sas vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5586, "name": "log-5457", "raw": "2022-09-22 20:49:19,022 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-22 20:49:19,022 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number lsi logic sas vmxnet number efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5587, "name": "log-5470", "raw": "2022-09-22 20:44:20,022 | Failed at Play [wintools_complete_install_verify] **********\n2022-09-22 20:44:20,022 | TASK [wintools_complete_install_verify][Check VMware tools install task status every 3 seconds] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/wintools_complete_install_verify/install_vmtools.yml:40\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'ansible_job_id'\nThe error appears to be in '/home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/wintools_complete_install_verify/install_vmtools.yml': line 40, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n    minutes: 2\n- name: \"Check VMware tools install task status every 3 seconds\"\n  ^ here", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify check vmware tools install task status every number seconds task path home worker workspace ansible windows number main para virtual vmxnet number efi ansible vsphere gos validation windows win tools complete install verify install vm tools yml number fatal localhost failed the task includes an option with an un defined variable the error was dict object has no attribute ansible job id the error appears to be in home worker workspace ansible windows number main para virtual vmxnet number efi ansible vsphere gos validation windows win tools complete install verify install vm tools yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be minutes number name check vmware tools install task status every number seconds here", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5588, "name": "log-5579", "raw": "2022-09-23 12:18:27,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 12:18:27,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.78.91.7]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5589, "name": "log-5492", "raw": "2022-09-22 21:30:53,022 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-22 21:30:53,022 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5590, "name": "log-5571", "raw": "2022-09-23 08:16:28,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 08:16:28,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\3&218E0F40&0&18''] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.185.176.195]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number task path home worker workspace ansible windows number main para virtual vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5591, "name": "log-5479", "raw": "2022-09-22 21:01:54,022 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-22 21:01:54,022 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number main para virtual vmxnet number efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5592, "name": "log-5467", "raw": "2022-09-22 20:49:29,022 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-22 20:49:29,022 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\3&218E0F40&0&18''] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.78.137.60]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5593, "name": "log-5468", "raw": "2022-09-22 20:51:17,022 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-22 20:51:17,022 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5594, "name": "log-5466", "raw": "2022-09-22 20:53:19,022 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-22 20:53:19,022 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number nvme efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5595, "name": "log-5532", "raw": "2022-09-23 00:53:08,023 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-23 00:53:08,023 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number b it number ga nvme bios ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5596, "name": "log-5499", "raw": "2022-09-22 23:28:25,022 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-22 23:28:25,022 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&1EE266C4&0&00B0''] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.78.186.61]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5597, "name": "log-5516", "raw": "2022-09-23 00:15:51,023 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-23 00:15:51,023 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number b it number ga ide efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5598, "name": "log-5587", "raw": "2022-09-23 15:43:49,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 15:43:49,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.212.186.253]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number b it para virtual bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5614, "name": "log-5534", "raw": "2022-09-22 23:31:55,022 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-22 23:31:55,022 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67U2_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number b it ide bios ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5599, "name": "log-5530", "raw": "2022-09-23 00:49:03,023 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-23 00:49:03,023 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number b it para virtual bios ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5600, "name": "log-5503", "raw": "2022-09-22 23:29:33,022 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-22 23:29:33,022 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&1EE266C4&0&00B0''] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67U2_SATA_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.184.94.53]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number b it sata efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5601, "name": "log-5517", "raw": "2022-09-23 00:14:34,023 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-23 00:14:34,023 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67U2_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number b it sata efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5602, "name": "log-5536", "raw": "2022-09-23 00:49:40,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 00:49:40,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\3&218E0F40&0&18''] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_MAIN_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.182.8.209]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number task path home worker workspace ansible windows number b it main para virtual efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5603, "name": "log-5522", "raw": "2022-09-23 00:23:19,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 00:23:19,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.185.224.152]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number b it lsi logic sas efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5604, "name": "log-5537", "raw": "2022-09-23 00:51:11,023 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-23 00:51:11,023 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_MAIN_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number b it main para virtual efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5627, "name": "log-5628", "raw": "2022-09-26 17:16:18,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 17:16:18,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type S:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.78.179.255]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command types test disk write text task path home worker workspace ansible windows server lts c number ga ide efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5605, "name": "log-5524", "raw": "2022-09-23 00:32:49,023 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-23 00:32:49,023 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number b it lsi logic sas efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5606, "name": "log-5515", "raw": "2022-09-23 00:15:26,023 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-23 00:15:26,023 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number b it main nvme efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5607, "name": "log-5572", "raw": "2022-09-23 08:26:00,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 08:26:00,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_11_67U3_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.161.100.145]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number para virtual vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5608, "name": "log-5454", "raw": "2022-09-22 20:48:57,022 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-22 20:48:57,022 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_67U3_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number para virtual vmxnet number efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5609, "name": "log-5500", "raw": "2022-09-22 23:28:58,022 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-22 23:28:58,022 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&1EE266C4&0&00B0''] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.161.121.182]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number b it number ga sata bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5610, "name": "log-5525", "raw": "2022-09-23 00:35:51,023 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-23 00:35:51,023 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number b it number ga sata bios ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5611, "name": "log-5328", "raw": "2022-09-22 14:24:04,022 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-22 14:24:04,022 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.161.164.104]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number b it number ga nvme bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5612, "name": "log-5602", "raw": "2022-09-25 11:05:03,025 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-25 11:05:03,025 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type S:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.43.255.153]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command types test disk write text task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5613, "name": "log-5533", "raw": "2022-09-22 23:29:33,022 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-22 23:29:33,022 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&1EE266C4&0&00B0''] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67U2_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.186.91.120]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number b it ide bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5615, "name": "log-5521", "raw": "2022-09-23 00:27:39,023 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-23 00:27:39,023 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because no persistent memory on ESXi host '0', or available size '0' is less than 16 MB.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace ansible windows number b it number ga lsi logic sas efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because no persistent memory on esxi host number or available size number is less than number mb", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5616, "name": "log-5558", "raw": "2022-09-23 04:10:00,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 04:10:00,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.212.44.65]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number nvme efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5617, "name": "log-5485", "raw": "2022-09-22 20:44:25,022 | Failed at Play [wintools_complete_install_verify] **********\n2022-09-22 20:44:25,022 | TASK [wintools_complete_install_verify][Check VMware tools install task status every 3 seconds] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/windows/wintools_complete_install_verify/install_vmtools.yml:40\nfatal: [localhost]: FAILED! => The task includes an option with an undefined variable. The error was: 'dict object' has no attribute 'ansible_job_id'\nThe error appears to be in '/home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/windows/wintools_complete_install_verify/install_vmtools.yml': line 40, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\nThe offending line appears to be:\n    minutes: 2\n- name: \"Check VMware tools install task status every 3 seconds\"\n  ^ here", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify check vmware tools install task status every number seconds task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation windows win tools complete install verify install vm tools yml number fatal localhost failed the task includes an option with an un defined variable the error was dict object has no attribute ansible job id the error appears to be in home worker workspace ansible windows number para virtual efi ansible vsphere gos validation windows win tools complete install verify install vm tools yml line number column number but may be elsewhere in the file depending on the exact syntax problem the offending line appears to be minutes number name check vmware tools install task status every number seconds here", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5618, "name": "log-5563", "raw": "2022-09-23 05:26:26,023 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2022-09-23 05:26:26,023 | TASK [deploy_vm_efi_nvme_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20220923041048.log] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-09-23 05:56:38,023 | TASK [deploy_vm_efi_nvme_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:57\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_nvme_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task deploy vm efi nvme vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible rhel number x nvme vmxnet number efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm efi nvme vmxnet number testing exit due to failure task path home worker workspace ansible rhel number x nvme vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi nvme vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5619, "name": "log-5520", "raw": "2022-09-23 00:25:51,023 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-23 00:25:51,023 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_67GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.184.84.112]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows number b it number ga lsi logic sas efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5620, "name": "log-5541", "raw": "2022-09-23 01:53:43,023 | Failed at Play [env_setup] *********************************\n2022-09-23 01:53:43,023 | TASK [env_setup][Get all registerd VMs and templates on 10.186.134.160] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.186.134.160:443 : [Errno 110] Connection timed out", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on ip address task path home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation common vm check exist yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5622, "name": "log-5590", "raw": "2022-09-25 07:02:39,025 | Failed at Play [env_setup] *********************************\n2022-09-25 07:02:39,025 | TASK [env_setup][Get all registerd VMs and templates on 10.212.39.10] \ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_7.x/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.212.39.10:443 : [Errno 110] Connection timed out", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on ip address task path home worker workspace ansible cycle rhel number x ansible vsphere gos validation common vm check exist yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out", "solution": "deepdive", "target": "testbed", "version": 202210102130}, {"id": 5623, "name": "log-5621", "raw": "2022-09-26 12:01:04,026 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-09-26 12:01:04,026 | TASK [nvdimm_cold_add_remove][Check the new PMem device is recognized by guest OS after cold add] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.04_Server_OVA/ansible-vsphere-gos-validation/linux/nvdimm_cold_add_remove/cold_add_nvdimm_test.yml:117\nfatal: [localhost]: FAILED! => Guest PMem device list before cold add is [], after cold add is [], the new added PMem device is not recognized.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove check the new pmem device is recognized by guest os after cold add task path home worker workspace ansible regression ubuntu number server ova ansible vsphere gos validation linux nvdimm cold add remove cold add nvdimm test yml number fatal localhost failed guest pmem device list before cold add is after cold add is the new added pmem device is not recognized", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5624, "name": "log-5617", "raw": "2022-09-26 06:37:46,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 06:37:46,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type W:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.168.194.180]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type w test disk write text task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5625, "name": "log-5626", "raw": "2022-09-26 17:12:11,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 17:12:11,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type X:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.186.95.158]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type x test disk write text task path home worker workspace ansible windows server lts c number ga sata bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5626, "name": "log-5627", "raw": "2022-09-26 17:24:08,026 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-26 17:24:08,026 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&1EE266C4&0&00B0''] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70GA_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.186.95.158]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows server lts c number ga sata bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5628, "name": "log-5630", "raw": "2022-09-26 17:17:02,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 17:17:02,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type X:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U1_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.186.201.71]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type x test disk write text task path home worker workspace ansible windows server lts c ide bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5629, "name": "log-5631", "raw": "2022-09-26 17:17:17,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 17:17:17,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type W:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.182.183.131]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type w test disk write text task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5630, "name": "log-5632", "raw": "2022-09-26 17:29:53,026 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-26 17:29:53,026 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&1EE266C4&0&00B0''] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U1_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.186.201.71]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows server lts c ide bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5631, "name": "log-5633", "raw": "2022-09-26 17:29:02,026 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-26 17:29:02,026 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\3&218E0F40&0&18''] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.182.183.131]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number task path home worker workspace ansible windows server lts c main lsi logic sas efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5632, "name": "log-5634", "raw": "2022-09-26 17:09:11,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 17:09:11,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type Y:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.161.100.207]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type y test disk write text task path home worker workspace ansible windows server lts c main nvme bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5633, "name": "log-5636", "raw": "2022-09-26 17:28:09,026 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-26 17:28:09,026 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.161.100.207]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows server lts c main nvme bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5634, "name": "log-5637", "raw": "2022-09-26 17:13:37,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 17:13:37,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type W:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.182.14.44]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type w test disk write text task path home worker workspace ansible windows server lts c para virtual bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5635, "name": "log-5639", "raw": "2022-09-26 17:32:14,026 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-26 17:32:14,026 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.182.14.44]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows server lts c para virtual bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5636, "name": "log-5641", "raw": "2022-09-26 17:18:26,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 17:18:26,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type W:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.185.98.202]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type w test disk write text task path home worker workspace ansible windows server lts c sata efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5637, "name": "log-5643", "raw": "2022-09-26 17:19:41,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 17:19:41,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type V:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.185.99.198]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type v test disk write text task path home worker workspace ansible windows server lts c nvme efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5638, "name": "log-5644", "raw": "2022-09-26 17:33:02,026 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-26 17:33:02,026 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.185.99.198]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows server lts c nvme efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5639, "name": "log-5646", "raw": "2022-09-26 18:39:46,026 | Failed at Play [wintools_uninstall_verify] *****************\n2022-09-26 18:39:46,026 | TASK [wintools_uninstall_verify][Add a new 'vmxnet3' adapter in 'vSwitch2022-09-26-18-39-34_PG' to VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_add_network_adapter.yml:13\nfatal: [localhost]: FAILED! => unable to find specified network_name/vlan_id (vSwitch2022-09-26-18-39-34_PG), check parameters", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify add a new vmxnet number adapter in vswitch timestamp number page to vm test vm task path home worker workspace ansible windows server lts c nvme efi ansible vsphere gos validation common vm add network adapter yml number fatal localhost failed unable to find specified network name vlan id vswitch timestamp number page check parameters", "solution": "deepdive", "target": "testcase", "version": 202210102130}, {"id": 5640, "name": "log-5647", "raw": "2022-09-26 17:25:33,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 17:25:33,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type V:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.212.15.240]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type v test disk write text task path home worker workspace ansible windows server lts c main para virtual vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5641, "name": "log-5648", "raw": "2022-09-26 17:38:41,026 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-26 17:38:41,026 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\3&218E0F40&0&18''] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.212.15.240]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number task path home worker workspace ansible windows server lts c main para virtual vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5642, "name": "log-5649", "raw": "2022-09-26 17:22:58,026 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-26 17:22:58,026 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type S:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.212.190.162]: FAILED! => non-zero return code when type", "category": "", "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command types test disk write text task path home worker workspace ansible windows server lts c lsi logic sas bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5643, "name": "log-5651", "raw": "2022-09-26 17:43:53,026 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-26 17:43:53,026 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\4&3B50545D&0&00B8''] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_70U3_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.212.190.162]: FAILED! => non-zero return code when pnputil", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number b number task path home worker workspace ansible windows server lts c lsi logic sas bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210102130}, {"id": 5644, "name": "5153", "raw": "2022-09-17 17:03:16,017 | Failed at Play [secureboot_enable_disable] *****************\n2022-09-17 17:03:16,017 | TASK [secureboot_enable_disable][Try to ping IP] ***********\ntask path: /home/worker/workspace/Ansible_CentOS_8.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": null, "processed": "timestamp failed at play secure boot enable disable timestamp task secure boot enable disable try to ping ip task path home worker workspace ansible centos number x number ga sata efi ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "deepdive", "target": "testcase", "version": 202210110644}, {"id": 5645, "name": "5153", "raw": "2022-09-17 17:03:16,017 | Failed at Play [secureboot_enable_disable] *****************\n2022-09-17 17:03:16,017 | TASK [secureboot_enable_disable][Try to ping IP] ***********\ntask path: /home/worker/workspace/Ansible_CentOS_8.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": null, "processed": "timestamp failed at play secure boot enable disable timestamp task secure boot enable disable try to ping ip task path home worker workspace ansible centos number x number ga sata efi ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "deepdive", "target": "testcase", "version": 202210110644}, {"id": 5646, "name": "5153", "raw": "2022-09-17 17:03:16,017 | Failed at Play [secureboot_enable_disable] *****************\n2022-09-17 17:03:16,017 | TASK [secureboot_enable_disable][Try to ping IP] ***********\ntask path: /home/worker/workspace/Ansible_CentOS_8.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:14\nfatal: [localhost]: FAILED! => non-zero return code when ping", "category": null, "processed": "timestamp failed at play secure boot enable disable timestamp task secure boot enable disable try to ping ip task path home worker workspace ansible centos number x number ga sata efi ansible vsphere gos validation common vm wait ping yml number fatal localhost failed nonzero return code when ping", "solution": "deepdive", "target": "targetvm", "version": 202210110644}, {"id": 5647, "name": "5590", "raw": "2022-09-25 07:02:39,025 | Failed at Play [env_setup] *********************************\n2022-09-25 07:02:39,025 | TASK [env_setup][Get all registerd VMs and templates on 10.212.39.10] \ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_7.x/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.212.39.10:443 : [Errno 110] Connection timed out", "category": null, "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on ip address task path home worker workspace ansible cycle rhel number x ansible vsphere gos validation common vm check exist yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out", "solution": "retry", "target": "nimbus", "version": 202210110644}, {"id": 5648, "name": "5594", "raw": "2022-09-25 07:37:19,025 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-09-25 07:37:19,025 | TASK [lsilogicsas_vhba_device_ops][Execute powershell command 'type Y:\\test_disk_write.txt'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.43.255.153]: FAILED! => non-zero return code when type", "category": null, "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops execute powershell command type y test disk write text task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when type", "solution": "deepdive", "target": "targetvm", "version": 202210110644}, {"id": 5649, "name": "5597", "raw": "2022-09-25 08:35:44,025 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-09-25 08:35:44,025 | TASK [nvme_vhba_device_ops][Execute powershell command 'pnputil /disable-device 'PCI\\VEN_15AD&DEV_07F0&SUBSYS_07F015AD&REV_00\\3&218E0F40&0&18''] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.212.7.233]: FAILED! => non-zero return code when pnputil", "category": null, "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops execute powershell command pnpu til disable device pci ven dev sub system hex id rev number hex id number task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when pnpu til", "solution": "deepdive", "target": "targetvm", "version": 202210110644}, {"id": 5650, "name": "5910", "raw": "2022-10-10 07:50:48,010 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2022-10-10 07:50:48,010 | TASK [lsilogic_vhba_device_ops][Check new disk is recognized by guest OS] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/hot_add_remove_disk.yml:39\nfatal: [localhost]: FAILED! => Guest OS failed to recognize the new hot-added lsilogic disk", "category": null, "processed": "timestamp failed at play lsi logic v hba device ops timestamp task lsi logic v hba device ops check new disk is recognized by guest os task path home worker workspace ansible ubuntu number server iso main nvme bios ansible vsphere gos validation linux v hba hot add remove hot add remove disk yml number fatal localhost failed guest os failed to recognize the new hot added lsi logic disk", "solution": "deepdive", "target": "targetvm", "version": 202210110644}, {"id": 5651, "name": "5911", "raw": "2022-10-10 08:13:33,010 | Failed at Play [deploy_vm_bios_lsilogicsas_e1000e] *********\n2022-10-10 08:13:33,010 | TASK [deploy_vm_bios_lsilogicsas_e1000e][Wait for message 'Ubuntu autoinstall is started at 2022-10-10-07-51-52' appear in VM log serial-20221010075750.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 150,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-10-10 08:14:22,010 | TASK [deploy_vm_bios_lsilogicsas_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_lsilogicsas_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios lsi logic sas timestamp task deploy vm bios lsi logic sas wait for message ubuntu auto install is started at timestamp number appear in vm log serial timestamp log task path home worker workspace ansible ubuntu number server iso lsi logic sas bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm bios lsi logic sas testing exit due to failure task path home worker workspace ansible ubuntu number server iso lsi logic sas bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios lsi logic sas e number e", "solution": "deepdive", "target": "targetvm", "version": 202210110644}, {"id": 5652, "name": "5912", "raw": "2022-10-10 07:21:32,010 | Failed at Play [memory_hot_add_basic] **********************\n2022-10-10 07:21:32,010 | TASK [memory_hot_add_basic][Copy calculate_total_memory.sh into guest] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/utils/memory_size_in_guest.yml:17\nfatal: [localhost]: FAILED! => Failed to get information on remote file (/tmp/calculate_total_memory.sh): Warning: Permanently added '10.185.188.34' (ED25519) to the list of known hosts.\nShared connection to 10.185.188.34 closed.", "category": null, "processed": "timestamp failed at play memory hot add basic timestamp task memory hot add basic copy calculate total memory shinto guest task path home worker workspace ansible ubuntu number server iso number ga ide vmxnet number efi ansible vsphere gos validation linux utilities memory size in guest yml number fatal localhost failed failed to get information on remote file tmp calculate total memory sh warning permanently added ip address to the list of known hosts shared connection to ip address closed", "solution": "retry", "target": "targetvm", "version": 202210110644}, {"id": 5653, "name": "5886", "raw": "2022-10-10 05:16:26,010 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-10-10 05:16:26,010 | TASK [cpu_multicores_per_socket][Change VM power state failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:69\nfatal: [localhost]: FAILED! => The attempted operation cannot be performed in the current state (Powered on). ", "category": null, "processed": "timestamp failed at play cpu multi cores per socket timestamp task cpu multi cores per socket change vm power state failure task path home worker workspace ansible ubuntu number server iso number ga ide vmxnet number efi ansible vsphere gos validation common vm set power state yml number fatal localhost failed the attempted operation can not be performed in the current state powered on", "solution": "retry", "target": "targetvm", "version": 202210110644}, {"id": 5654, "name": "5857", "raw": "2022-10-10 04:35:52,010 | Failed at Play [ovt_verify_install] ************************\n2022-10-10 04:35:52,010 | TASK [ovt_verify_install][Clean all cache and metadata] ****\ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70GA_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/utils/repo_update.yml:12\nfatal: [localhost -> 10.78.224.28]: FAILED! => non-zero return code when apt-get\n2022-10-10 04:36:33,010 | TASK [ovt_verify_install][Testing exit due to failure] *****\ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70GA_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install", "category": null, "processed": "timestamp failed at play ovt verify install timestamp task ovt verify install clean all cache and metadata task path home worker workspace ansible ubuntu number server iso number ga lsi logic sas vmxnet number efi ansible vsphere gos validation linux utilities repository update yml number fatal localhost ip address failed nonzero return code when apt get timestamp task ovt verify install testing exit due to failure task path home worker workspace ansible ubuntu number server iso number ga lsi logic sas vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case ovt verify install", "solution": "retry", "target": "targetvm", "version": 202210110644}, {"id": 5655, "name": "5914", "raw": "2022-10-10 04:56:38,010 | Failed at Play [memory_hot_add_basic] **********************\n2022-10-10 04:56:38,010 | TASK [memory_hot_add_basic][Wait for 24 memory blocks present under /sys/devices/system/memory/] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/utils/wait_for_memory_blocks.yml:10\nfatal: [localhost -> 10.168.189.245]: FAILED! =>  when ls", "category": null, "processed": "timestamp failed at play memory hot add basic timestamp task memory hot add basic wait for number memory blocks present under system devices system memory task path home worker workspace ansible ubuntu number server iso main para virtual vmxnet number efi ansible vsphere gos validation linux utilities wait for memory blocks yml number fatal localhost ip address failed when ls", "solution": "deepdive", "target": "targetvm", "version": 202210110644}, {"id": 5656, "name": "5935", "raw": "2022-10-12 03:19:15,012 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-10-12 03:19:15,012 | TASK [lsilogicsas_vhba_device_ops][Check file content] *****\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/create_new_file_read_write.yml:90\nfatal: [localhost]: FAILED! => The file content after guest restart is '', which is not the same as before guest restart: ' 'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. '  'This is the test line to write to the file. \nThis is the test line to write to the file. ' \n'", "category": null, "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops check file content task path home worker workspace ansible windows server lts c main para virtual vmxnet number efi ansible vsphere gos validation windows v hba hot add remove create new file read write yml number fatal localhost failed the file content after guest restart is which is not the same as before guest restart this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file this is the test line to write to the file ", "solution": "deepdive", "target": "targetvm", "version": 202210121247}, {"id": 5657, "name": "5946", "raw": "2022-10-12 12:23:44,012 | Failed at Play [deploy_vm_bios_nvme_e1000e] ****************\n2022-10-12 12:23:44,012 | TASK [deploy_vm_bios_nvme_e1000e][Wait for message 'Ubuntu autoinstall is started at 2022-10-12-10-32-53' appear in VM log serial-20221012120519.log] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 150,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-10-12 12:25:21,012 | TASK [deploy_vm_bios_nvme_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/zyh_ansible_ubuntu_existing_testbed_3.1/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_nvme_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios nvme timestamp task deploy vm bios nvme wait for message ubuntu auto install is started at timestamp number appear in vm log serial timestamp log task path home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm bios nvme testing exit due to failure task path home worker workspace zyh ansible ubuntu existing testbed number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios nvme e number e", "solution": "deepdive", "target": "targetvm", "version": 202210130649}, {"id": 5658, "name": "5925", "raw": "2022-10-11 06:52:43,011 | Failed at Play [deploy_vm_bios_lsilogicsas_e1000e] *********\n2022-10-11 06:52:43,011 | TASK [deploy_vm_bios_lsilogicsas_e1000e][Wait for message 'Ubuntu autoinstall is started at 2022-10-11-06-27-43' appear in VM log serial-20221011063637.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 150,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-10-11 06:53:36,011 | TASK [deploy_vm_bios_lsilogicsas_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_lsilogicsas_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios lsi logic sas timestamp task deploy vm bios lsi logic sas wait for message ubuntu auto install is started at timestamp number appear in vm log serial timestamp log task path home worker workspace ansible ubuntu number server iso lsi logic sas bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm bios lsi logic sas testing exit due to failure task path home worker workspace ansible ubuntu number server iso lsi logic sas bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios lsi logic sas e number e", "solution": "deepdive", "target": "targetvm", "version": 202210130649}, {"id": 5659, "name": "5932", "raw": "2022-10-11 09:42:28,011 | Failed at Play [lsilogicsas_vhba_device_ops] ***************\n2022-10-11 09:42:28,011 | TASK [lsilogicsas_vhba_device_ops][Check file content] *****\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/create_new_file_read_write.yml:84\nfatal: [localhost]: FAILED! => The file content is 'Online\n', which should be: 'This is the test line to write to the file. '", "category": null, "processed": "timestamp failed at play lsi logic sas v hba device ops timestamp task lsi logic sas v hba device ops check file content task path home worker workspace ansible windows server lts c main para virtual vmxnet number efi ansible vsphere gos validation windows v hba hot add remove create new file read write yml number fatal localhost failed the file content is online which should be this is the test line to write to the file", "solution": "deepdive", "target": "targetvm", "version": 202210130649}, {"id": 5660, "name": "5958", "raw": "2022-10-13 08:16:37,013 | Failed at Play [deploy_vmwarephoton_ova] *******************\n2022-10-13 08:16:37,013 | TASK [deploy_vmwarephoton_ova][Check VMware Tools is running and collects guest OS fullname successfully] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/vm_wait_guest_fullname.yml:40\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest OS fullname, tools running status is guestToolsNotRunning and guest OS fullname is \nerror message:\nIt's timed out for VMware Tools collecting guest OS fullname, tools running status is guestToolsNotRunning and guest OS fullname is\n2022-10-13 08:17:01,013 | TASK [deploy_vmwarephoton_ova][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova", "category": null, "processed": "timestamp failed at play deploy vmware photon ova timestamp task deploy vmware photon ova check vmware tools is running and collects guest os full name successfully task path home worker workspace ansible regression photon number ova ansible vsphere gos validation common vm wait guest full name yml number fatal localhost failed it s timed out for vmware tools collecting guest os full name tools running status is guest tools not running and guest os full name is error message it s timed out for vmware tools collecting guest os full name tools running status is guest tools not running and guest os full name is timestamp task deploy vmware photon ova testing exit due to failure task path home worker workspace ansible regression photon number ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vmware photon ova error message exit testing when exit testing when fail is set to true in test case deploy vmware photon ova", "solution": "deepdive", "target": "targetvm", "version": 202210181310}, {"id": 5661, "name": "6066", "raw": "2022-10-13 19:03:24,013 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-10-13 19:03:24,013 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_11.x_64/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:42\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address, tools running status is '' and guest IP addresses are ''\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address, tools running status is '' and guest IP addresses are ''\n2022-10-13 19:03:58,013 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_11.x_64/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible regression debian number x number ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address tools running status is and guest ip addresses are error message it s timed out for vmware tools collecting guest ip v number address tools running status is and guest ip addresses are timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible regression debian number x number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "nimbus", "version": 202210181310}, {"id": 5662, "name": "6027", "raw": "2022-10-13 15:17:55,013 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-10-13 15:17:55,013 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_10.x_32/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:42\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address, tools running status is '' and guest IP addresses are ''\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address, tools running status is '' and guest IP addresses are ''\n2022-10-13 15:18:36,013 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_10.x_32/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible regression debian number x number ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address tools running status is and guest ip addresses are error message it s timed out for vmware tools collecting guest ip v number address tools running status is and guest ip addresses are timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible regression debian number x number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "nimbus", "version": 202210181310}, {"id": 5663, "name": "5998", "raw": "2022-10-13 14:45:49,013 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-10-13 14:45:49,013 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_10.x_64/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:42\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address, tools running status is '' and guest IP addresses are ''\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address, tools running status is '' and guest IP addresses are ''\n2022-10-13 14:46:23,013 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_10.x_64/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible regression debian number x number ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address tools running status is and guest ip addresses are error message it s timed out for vmware tools collecting guest ip v number address tools running status is and guest ip addresses are timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible regression debian number x number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "nimbus", "version": 202210181310}, {"id": 5664, "name": "5955", "raw": "2022-10-13 04:10:31,013 | Failed at Play [env_setup] *********************************\n2022-10-13 04:10:31,013 | TASK [env_setup][Check variables for new VM settings] ******\ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_OVA/ansible-vsphere-gos-validation/env_setup/check_testing_vars.yml:57\nfatal: [localhost]: FAILED! => Invalid variables for new VM settings", "category": null, "processed": "timestamp failed at play environment setup timestamp task environment setup check variables for new vm settings task path home worker workspace ansible regression photon number ova ansible vsphere gos validation environment setup check testing vars yml number fatal localhost failed invalid variables for new vm settings", "solution": "deepdive", "target": "usererror", "version": 202210181310}, {"id": 5665, "name": "5959", "raw": "2022-10-13 13:24:45,013 | Failed at Play [deploy_vmwarephoton_ova] *******************\n2022-10-13 13:24:45,013 | TASK [deploy_vmwarephoton_ova][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_OVA/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:42\nfatal: [localhost]: FAILED! => {\n    \"msg\": \"The conditional check '(((not wait_ipv4) or (wait_ipv4 and wait_ipv4 in vm_guest_facts.instance.hw_eth0.ipaddresses | ansible.netcommon.ipv4))' failed. The error was: template error while templating string: unexpected '}', expected ')'. String: {% if (((not wait_ipv4) or (wait_ipv4 and wait_ipv4 in vm_guest_facts.instance.hw_eth0.ipaddresses | ansible.netcommon.ipv4)) %} True {% else %} False {% endif %}\"\n}\nerror message:\nThe conditional check '(((not wait_ipv4) or (wait_ipv4 and wait_ipv4 in vm_guest_facts.instance.hw_eth0.ipaddresses | ansible.netcommon.ipv4))' failed. The error was: template error while templating string: unexpected '}', expected ')'. String: {% if (((not wait_ipv4) or (wait_ipv4 and wait_ipv4 in vm_guest_facts.instance.hw_eth0.ipaddresses | ansible.netcommon.ipv4)) %} True {% else %} False {% endif %}\n2022-10-13 13:25:12,013 | TASK [deploy_vmwarephoton_ova][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_OVA/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova", "category": null, "processed": "timestamp failed at play deploy vmware photon ova timestamp task deploy vmware photon ova check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible regression photon number ova ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed message the conditional check not wait ip v number or wait ip v number and wait ip v number in vm guest facts instance hw eth number ip addresses ansible net common ip v number failed the error was template error while templating string unexpected expected string if not wait ip v number or wait ip v number and wait ip v number in vm guest facts instance hw eth number ip addresses ansible net common ip v number true else false end if error message the conditional check not wait ip v number or wait ip v number and wait ip v number in vm guest facts instance hw eth number ip addresses ansible net common ip v number failed the error was template error while templating string unexpected expected string if not wait ip v number or wait ip v number and wait ip v number in vm guest facts instance hw eth number ip addresses ansible net common ip v number true else false end if timestamp task deploy vmware photon ova testing exit due to failure task path home worker workspace ansible regression photon number ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vmware photon ova error message exit testing when exit testing when fail is set to true in test case deploy vmware photon ova", "solution": "deepdive", "target": "testcase", "version": 202210181310}, {"id": 5666, "name": "5930", "raw": "2022-10-11 09:25:52,011 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-10-11 09:25:52,011 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Download datastore file] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.10_ISO/ansible-vsphere-gos-validation/common/esxi_download_datastore_file.yml:36\nsocket.timeout: The read operation timed out\nfatal: [localhost]: FAILED! => failed to create temporary content file: The read operation timed out\n2022-10-11 09:25:53,011 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'test_ubuntu22.10_iso'] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.10_ISO/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_ubuntu22.10_iso", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number download data store file task path home worker workspace ansible regression ubuntu number iso ansible vsphere gos validation common esxi download data store file yml number socket timeout the read operation timed out fatal localhost failed failed to create temporary content file the read operation timed out timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm test ubuntu number iso task path home worker workspace ansible regression ubuntu number iso ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test ubuntu number iso", "solution": "retry", "target": "nimbus", "version": 202210181310}, {"id": 5667, "name": "6101", "raw": "2022-10-18 08:16:21,018 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-10-18 08:16:21,018 | TASK [nvdimm_cold_add_remove][Check VMware Tools running status is 'guestToolsRunning'] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/common/vm_wait_vmtools_status.yml:52\nfatal: [localhost]: FAILED! => It's timed out to wait for VMware Tools running status became 'guestToolsRunning'. current VMware Tools running status is ''\nerror message:\nIt's timed out to wait for VMware Tools running status became 'guestToolsRunning'. current VMware Tools running status is ''", "category": null, "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove check vmware tools running status is guest tools running task path home worker workspace ansible regression windows number ansible vsphere gos validation common vm wait vm tools status yml number fatal localhost failed it s timed out to wait for vmware tools running status became guest tools running current vmware tools running status is error message it s timed out to wait for vmware tools running status became guest tools running current vmware tools running status is", "solution": "deepdive", "target": "testcase", "version": 202210181310}, {"id": 5668, "name": "6102", "raw": "2022-10-18 08:34:52,018 | Failed at Play [memory_hot_add_basic] **********************\n2022-10-18 08:34:52,018 | TASK [memory_hot_add_basic][Check VMware Tools running status is 'guestToolsRunning'] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/common/vm_wait_vmtools_status.yml:52\nfatal: [localhost]: FAILED! => It's timed out to wait for VMware Tools running status became 'guestToolsRunning'. current VMware Tools running status is ''\nerror message:\nIt's timed out to wait for VMware Tools running status became 'guestToolsRunning'. current VMware Tools running status is ''", "category": null, "processed": "timestamp failed at play memory hot add basic timestamp task memory hot add basic check vmware tools running status is guest tools running task path home worker workspace ansible regression windows number ansible vsphere gos validation common vm wait vm tools status yml number fatal localhost failed it s timed out to wait for vmware tools running status became guest tools running current vmware tools running status is error message it s timed out to wait for vmware tools running status became guest tools running current vmware tools running status is", "solution": "deepdive", "target": "testcase", "version": 202210181310}, {"id": 5669, "name": "6093", "raw": "2022-10-17 13:53:03,017 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-10-17 13:53:03,017 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:42\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address, tools running status is 'guestToolsRunning' and guest IP addresses are '[]'\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address, tools running status is 'guestToolsRunning' and guest IP addresses are '[]'\n2022-10-17 13:53:33,017 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible regression rhel number x ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address tools running status is guest tools running and guest ip addresses are error message it s timed out for vmware tools collecting guest ip v number address tools running status is guest tools running and guest ip addresses are timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible regression rhel number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "nimbus", "version": 202210181310}, {"id": 5670, "name": "6103", "raw": "2022-10-18 08:41:45,018 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-10-18 08:41:45,018 | TASK [cpu_multicores_per_socket][Check VMware Tools running status is 'guestToolsRunning'] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/common/vm_wait_vmtools_status.yml:52\nfatal: [localhost]: FAILED! => It's timed out to wait for VMware Tools running status became 'guestToolsRunning'. current VMware Tools running status is ''\nerror message:\nIt's timed out to wait for VMware Tools running status became 'guestToolsRunning'. current VMware Tools running status is ''", "category": null, "processed": "timestamp failed at play cpu multi cores per socket timestamp task cpu multi cores per socket check vmware tools running status is guest tools running task path home worker workspace ansible regression windows number ansible vsphere gos validation common vm wait vm tools status yml number fatal localhost failed it s timed out to wait for vmware tools running status became guest tools running current vmware tools running status is error message it s timed out to wait for vmware tools running status became guest tools running current vmware tools running status is", "solution": "deepdive", "target": "testcase", "version": 202210181310}, {"id": 5676, "name": "6170", "raw": "2022-10-19 11:36:05,019 | Failed at Play [wintools_complete_install_verify] **********\n2022-10-19 11:36:05,019 | TASK [wintools_complete_install_verify][Configure VM CDROM to 'iso'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nfatal: [localhost]: FAILED! => Connection control operation failed for disk 'sata0:0'.", "category": null, "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify configure vm cdrom to iso task path home worker workspace ansible cycle windows server number ansible vsphere gos validation common vm configure cdrom yml number fatal localhost failed connection control operation failed for disk sata number number", "solution": "deepdive", "target": "targetvm", "version": 202210200116}, {"id": 5671, "name": "6181", "raw": "2022-10-19 16:47:36,019 | Failed at Play [deploy_vmwarephoton_ova] *******************\n2022-10-19 16:47:36,019 | TASK [deploy_vmwarephoton_ova][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_OVA/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:51\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address in 300 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '['fe80::250:56ff:feac:eb4e']'.\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 300 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '['fe80::250:56ff:feac:eb4e']'.\n2022-10-19 16:48:01,019 | TASK [deploy_vmwarephoton_ova][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_OVA/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova\n2022-10-19 16:48:14,019 | TASK [deploy_vmwarephoton_ova][Fetch file /tmp/cloud-init_2022-10-19-16-48-05.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_OVA/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Guest file /tmp/cloud-init_2022-10-19-16-48-05.tar.gz does not exist : File /tmp/cloud-init_2022-10-19-16-48-05.tar.gz was not found\nerror message:\nGuest file /tmp/cloud-init_2022-10-19-16-48-05.tar.gz does not exist : File /tmp/cloud-init_2022-10-19-16-48-05.tar.gz was not found", "category": null, "processed": "timestamp failed at play deploy vmware photon ova timestamp task deploy vmware photon ova check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible regression photon number ova ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are ip address error message it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are ip address timestamp task deploy vmware photon ova testing exit due to failure task path home worker workspace ansible regression photon number ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vmware photon ova error message exit testing when exit testing when fail is set to true in test case deploy vmware photon ova timestamp task deploy vmware photon ova fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible regression photon number ova ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed guest file tmp cloud init timestamp number tar gz does not exist file tmp cloud init timestamp number tar gz was not found error message guest file tmp cloud init timestamp number tar gz does not exist file tmp cloud init timestamp number tar gz was not found", "solution": "retry", "target": "nimbus", "version": 202210200116}, {"id": 5672, "name": "6185", "raw": "2022-10-19 19:54:37,019 | Failed at Play [deploy_vm_ovf] *****************************\n2022-10-19 19:54:37,019 | TASK [deploy_vm_ovf][Deploy VM from ovf template] **********\ntask path: /home/worker/workspace/Ansible_Windows_MS_Template_MAIN/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nexception in /vmware_deploy_ovf.py when run in /request.py when http_error_default\nfatal: [localhost]: FAILED! => HTTP Error 500: Internal Server Error Problem validating OVF import spec: Line 98: Invalid value 'hostonly' for element 'Connection'.\n2022-10-19 19:54:38,019 | TASK [deploy_vm_ovf][Testing exit due to failure] **********\ntask path: /home/worker/workspace/Ansible_Windows_MS_Template_MAIN/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_ovf", "category": null, "processed": "timestamp failed at play deploy vm ovf timestamp task deploy vm ovf deploy vm from ovf template task path home worker workspace ansible windows ms template main ansible vsphere gos validation common ovf deploy yml number exception in vmware deploy ovf python when run in request python when http error default fatal localhost failed http error number internal server error problem validating ovf import spec line number invalid value host only for element connection timestamp task deploy vm ovf testing exit due to failure task path home worker workspace ansible windows ms template main ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm ovf", "solution": "deepdive", "target": "testbed", "version": 202210200116}, {"id": 5673, "name": "6157", "raw": "2022-10-19 07:48:02,019 | Failed at Play [secureboot_enable_disable] *****************\n2022-10-19 07:48:02,019 | TASK [secureboot_enable_disable][Check VM 'UOS-Server-V20-1050a-PVSCSI_VMXNET3_EFI-HWv17' IP address] \ntask path: /home/worker/workspace/Ansible_UOS_Server_V20_70GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'UOS-Server-V20-1050a-PVSCSI_VMXNET3_EFI-HWv17' IP Address", "category": null, "processed": "timestamp failed at play secure boot enable disable timestamp task secure boot enable disable check vm u os server v number pvscsi vmxnet number efi ip address task path home worker workspace ansible u os server v number ga para virtual vmxnet number efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm u os server v number pvscsi vmxnet number efi ip address", "solution": "deepdive", "target": "targetvm", "version": 202210200116}, {"id": 5674, "name": "6168", "raw": "2022-10-19 11:55:38,019 | Failed at Play [memory_hot_add_basic] **********************\n2022-10-19 11:55:38,019 | TASK [memory_hot_add_basic][Skip testcase: memory_hot_add_basic, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04_ISO/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'memory_hot_add_basic' is blocked because memory hotadd test value list is empty", "category": null, "processed": "timestamp failed at play memory hot add basic timestamp task memory hot add basic skip test case memory hot add basic reason blocked task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common skip test case yml number fatal localhost failed test case memory hot add basic is blocked because memory hot add test value list is empty", "solution": "deepdive", "target": "testbed", "version": 202210200116}, {"id": 5675, "name": "6142", "raw": "2022-10-19 04:42:38,019 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-10-19 04:42:38,019 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221019032827.log] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04_ISO/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 720,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\n2022-10-19 04:42:48,019 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Fetch file /tmp/cloud-init_2022-10-19-04-42-40.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04_ISO/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task deploy vm efi para virtual vmxnet number fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root", "solution": "deepdive", "target": "targetvm", "version": 202210200116}, {"id": 5677, "name": "6152", "raw": "2022-10-19 07:44:53,019 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2022-10-19 07:44:53,019 | TASK [deploy_vm_efi_ide_e1000e][Datastore file operation] **\ntask path: /home/worker/workspace/Ansible_UOS_Server_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Linux/UOS/V20/1050/uniontechos-server-20-1050a-amd64.iso' is absent, cannot continue\n2022-10-19 07:44:54,019 | TASK [deploy_vm_efi_ide_e1000e][Get specified property info for VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_UOS_Server_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm", "category": null, "processed": "timestamp failed at play deploy vm efi ide timestamp task deploy vm efi ide data store file operation task path home worker workspace ansible u os server ide efi ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed file os linux u os v number union tech os server number amd number i so is absent can not continue timestamp task deploy vm efi ide get specified property info for vm test vm task path home worker workspace ansible u os server ide efi ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test vm", "solution": "deepdive", "target": "testbed", "version": 202210200116}, {"id": 5678, "name": "6158", "raw": "2022-10-19 07:57:24,019 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-10-19 07:57:24,019 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Datastore file operation] \ntask path: /home/worker/workspace/Ansible_UOS_Server_V20_70GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => File 'OS/Linux/UOS/V20/1050/uniontechos-server-20-1050a-amd64.iso' is absent, cannot continue\n2022-10-19 07:57:25,019 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'UOS-Server-V20-1050a-PVSCSI_VMXNET3_EFI-HWv17_1666166176263'] \ntask path: /home/worker/workspace/Ansible_UOS_Server_V20_70GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM UOS-Server-V20-1050a-PVSCSI_VMXNET3_EFI-HWv17_1666166176263", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number data store file operation task path home worker workspace ansible u os server v number ga para virtual vmxnet number efi ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed file os linux u os v number union tech os server number amd number i so is absent can not continue timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm u os server v number pvscsi vmxnet number efi number task path home worker workspace ansible u os server v number ga para virtual vmxnet number efi ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm u os server v number pvscsi vmxnet number efi number", "solution": "deepdive", "target": "testbed", "version": 202210200116}, {"id": 5680, "name": "log-6146", "raw": "2022-10-19 04:19:14,019 | Failed at Play [nvme_vhba_device_ops_spec13] ***************\n2022-10-19 04:19:14,019 | TASK [nvme_vhba_device_ops_spec13][Execute powershell command '(Get-WmiObject Win32_SCSIController | where-object {$_.Name -like '*NVM Express*'} | measure).Count'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n2022-10-19 04:19:41,019 | TASK [nvme_vhba_device_ops_spec13][Guest OS unreachable] ***\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:39\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': \"Failed to connect to the host via PSRP: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\", 'skip_reason': 'Host localhost is unreachable', 'unreachable': True}", "category": "", "processed": "timestamp failed at play nvme v hba device ops spec number timestamp task nvme v hba device ops spec number execute powershell command get wmi object win number scsi controller where object name like nvm express measure count task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost un reachable failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer timestamp task nvme v hba device ops spec number guest os un reachable task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost failed changed false message failed to connect to the host via psrp connection aborted connection reset error number connection reset by peer skip reason host localhost is un reachable un reachable true", "solution": "retry", "target": "targetvm", "version": 202212132000}, {"id": 5679, "name": "log-6093", "raw": "2022-10-17 13:53:03,017 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-10-17 13:53:03,017 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:42\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address, tools running status is 'guestToolsRunning' and guest IP addresses are '[]'\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address, tools running status is 'guestToolsRunning' and guest IP addresses are '[]'\n2022-10-17 13:53:33,017 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_RHEL_8.x/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible regression rhel number x ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address tools running status is guest tools running and guest ip addresses are error message it s timed out for vmware tools collecting guest ip v number address tools running status is guest tools running and guest ip addresses are timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible regression rhel number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5681, "name": "log-6165", "raw": "2022-10-19 11:52:50,019 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-10-19 11:52:50,019 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_ISO/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:47\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is '' and guest IP addresses are ''.\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is '' and guest IP addresses are ''.\n2022-10-19 11:53:26,019 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_4.0_ISO/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible regression photon number iso ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is and guest ip addresses are error message it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is and guest ip addresses are timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible regression photon number iso ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5682, "name": "log-6155", "raw": "2022-10-19 06:41:48,019 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-10-19 06:41:48,019 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_UOS_Server_V20_70GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is localhost, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is , expected domain name is gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible u os server v number ga para virtual vmxnet number efi ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is localhost expected hostname is go sc dhcp vm number vm dns domain name is expected domain name is go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5683, "name": "log-6201", "raw": "2022-10-20 06:10:52,020 | Failed at Play [env_setup] *********************************\n2022-10-20 06:10:52,020 | TASK [env_setup][Enable GuestIPHack on ESXi host 'w4-hs12-t0821.eng.vmware.com'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_Physical_80GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/esxi_enable_guest_ip_hack.yml:5\nfatal: [localhost]: UNREACHABLE! => Data could not be sent to remote host \"w4-hs12-t0821.eng.vmware.com\". Make sure this host can be reached over ssh: ssh: connect to host w4-hs12-t0821.eng.vmware.com port 22: Connection refused", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup enable guest ip hack on esxi host eng vmware com task path home worker workspace ansible windows server lts c physical number ga lsi logic sas efi ansible vsphere gos validation common esxi enable guest ip hack yml number fatal localhost un reachable data could not be sent to remote host eng vmware com make sure this host can be reached over ssh ssh connect to host eng vmware com port number connection refused", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5684, "name": "log-6203", "raw": "2022-10-20 06:24:25,020 | Failed at Play [ovt_verify_install] ************************\n2022-10-20 06:24:25,020 | TASK [ovt_verify_install][Create snapshot 'BaseSnapshot' on 'UOS-Server-V20-1050a-PVSCSI_VMXNET3_EFI-HWv17'] \ntask path: /home/worker/workspace/Ansible_UOS_Server_V20_70GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Insufficient disk space on datastore ''.\n2022-10-20 06:24:38,020 | TASK [ovt_verify_install][Testing exit due to failure] *****\ntask path: /home/worker/workspace/Ansible_UOS_Server_V20_70GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install", "category": "", "processed": "timestamp failed at play ovt verify install timestamp task ovt verify install create snapshot base snapshot on u os server v number pvscsi vmxnet number efi task path home worker workspace ansible u os server v number ga para virtual vmxnet number efi ansible vsphere gos validation common vm take snapshot yml number fatal localhost failed insufficient disk space on data store timestamp task ovt verify install testing exit due to failure task path home worker workspace ansible u os server v number ga para virtual vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case ovt verify install", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5685, "name": "log-6211", "raw": "2022-10-20 08:19:55,020 | Failed at Play [env_setup] *********************************\n2022-10-20 08:19:55,020 | TASK [env_setup][Enable GuestIPHack on ESXi host 'w4-hs12-t0821.eng.vmware.com'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_Physical_80GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/esxi_enable_guest_ip_hack.yml:5\nfatal: [localhost]: UNREACHABLE! => Data could not be sent to remote host \"w4-hs12-t0821.eng.vmware.com\". Make sure this host can be reached over ssh: ssh: connect to host w4-hs12-t0821.eng.vmware.com port 22: Connection refused\nerror message:\nData could not be sent to remote host \"w4-hs12-t0821.eng.vmware.com\". Make sure this host can be reached over ssh: ssh: connect to host w4-hs12-t0821.eng.vmware.com port 22: Connection refused", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup enable guest ip hack on esxi host eng vmware com task path home worker workspace ansible windows server lts c physical number ga lsi logic sas efi ansible vsphere gos validation common esxi enable guest ip hack yml number fatal localhost un reachable data could not be sent to remote host eng vmware com make sure this host can be reached over ssh ssh connect to host eng vmware com port number connection refused error message data could not be sent to remote host eng vmware com make sure this host can be reached over ssh ssh connect to host eng vmware com port number connection refused", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5686, "name": "log-6215", "raw": "2022-10-20 09:01:14,020 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-10-20 09:01:14,020 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Set default UEK release packaged in OracleLinux 9.0] \ntask path: /home/worker/workspace/Ansible_Regression_OracleLinux_9.x/ansible-vsphere-gos-validation/linux/utils/add_official_online_repo.yml:128\nfatal: [localhost]: FAILED! => template error while templating string: expected token 'end of statement block', got '-'. String: {%- if guest_os_ansible_distribution_major_ver == 8 -%}UEKR6\n{%- else -%}UEKR7{%- endif -}\nerror message:\ntemplate error while templating string: expected token 'end of statement block', got '-'. String: {%- if guest_os_ansible_distribution_major_ver == 8 -%}UEKR6\n{%- else -%}UEKR7{%- endif -}\n2022-10-20 09:02:00,020 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_OracleLinux_9.x/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number set default uek release packaged in oracle linux number task path home worker workspace ansible regression oracle linux number x ansible vsphere gos validation linux utilities add official online repository yml number fatal localhost failed template error while templating string expected token end of statement block got string if guest os ansible distribution major ver number uekr number else uekr number end if error message template error while templating string expected token end of statement block got string if guest os ansible distribution major ver number uekr number else uekr number end if timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible regression oracle linux number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5687, "name": "log-6219", "raw": "2022-10-20 10:28:43,020 | Failed at Play [deploy_vm_efi_lsilogicsas_e1000e] **********\n2022-10-20 10:28:43,020 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Guest OS connection failure] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_Physical_main_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_check_winrm.yml:26\nfatal: [localhost]: FAILED! => Guest winrm is not connectable in 1800 seconds.\nerror message:\nGuest winrm is not connectable in 1800 seconds.\n2022-10-20 10:30:11,020 | TASK [deploy_vm_efi_lsilogicsas_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_Physical_main_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi lsi logic sas timestamp task deploy vm efi lsi logic sas guest os connection failure task path home worker workspace ansible windows number b it physical main lsi logic sas efi ansible vsphere gos validation windows utilities win check win rm yml number fatal localhost failed guest win rm is not connectable in number seconds error message guest win rm is not connectable in number seconds timestamp task deploy vm efi lsi logic sas testing exit due to failure task path home worker workspace ansible windows number b it physical main lsi logic sas efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e error message exit testing when exit testing when fail is set to true in test case deploy vm efi lsi logic sas e number e", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5688, "name": "log-6665", "raw": "2022-11-09 12:10:30,009 | Failed at Play [check_os_fullname] *************************\n2022-11-09 12:10:30,009 | TASK [check_os_fullname][Skip testcase: check_os_fullname, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U1_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'check_os_fullname' is blocked because VMware tools installed: False, running: False\nerror message:\nTest case 'check_os_fullname' is blocked because VMware tools installed: False, running: False", "category": "", "processed": "timestamp failed at play check os full name timestamp task check os full name skip test case check os full name reason blocked task path home worker workspace ansible windows number b it sata bios ansible vsphere gos validation common skip test case yml number fatal localhost failed test case check os full name is blocked because vmware tools installed false running false error message test case check os full name is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202212132000}, {"id": 5689, "name": "log-6267", "raw": "2022-10-20 15:29:31,020 | Failed at Play [wintools_uninstall_verify] *****************\n2022-10-20 15:29:31,020 | TASK [wintools_uninstall_verify][Add a new 'vmxnet3' adapter in 'vSwitch2022-10-20-15-29-20_PG' to VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_80GA_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_add_network_adapter.yml:13\nfatal: [localhost]: FAILED! => unable to find specified network_name/vlan_id (vSwitch2022-10-20-15-29-20_PG), check parameters\nerror message:\nunable to find specified network_name/vlan_id (vSwitch2022-10-20-15-29-20_PG), check parameters", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify add a new vmxnet number adapter in vswitch timestamp number page to vm test vm task path home worker workspace ansible windows server lts c number ga nvme vmxnet number efi ansible vsphere gos validation common vm add network adapter yml number fatal localhost failed unable to find specified network name vlan id vswitch timestamp number page check parameters error message unable to find specified network name vlan id vswitch timestamp number page check parameters", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5690, "name": "log-6281", "raw": "2022-10-20 19:16:15,020 | Failed at Play [env_setup] *********************************\n2022-10-20 19:16:15,020 | TASK [env_setup][Get all registerd VMs and templates on gosv-vc-70update.eng.vmware.com] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_Physical_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nexception in /vmware.py when connect_to_api in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Unable to log on to vCenter or ESXi API at gosv-vc-70update.eng.vmware.com:443  as Administrator@vsphere.local: Cannot complete login due to an incorrect user name or password.\nerror message:\nUnable to log on to vCenter or ESXi API at gosv-vc-70update.eng.vmware.com:443  as Administrator@vsphere.local: Cannot complete login due to an incorrect user name or password.", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on go sv vc number update eng vmware com task path home worker workspace ansible windows server lts c physical lsi logic sas efi ansible vsphere gos validation common vm check exist yml number exception in vmware python when connect to api in soap adapter python when invoke method fatal localhost failed unable to log on to vcenter or esxi api at go sv vc number update eng vmware com number as administrator vsphere local can not complete login due to an incorrect user name or password error message unable to log on to vcenter or esxi api at go sv vc number update eng vmware com number as administrator vsphere local can not complete login due to an incorrect user name or password", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5691, "name": "log-6966", "raw": "2022-11-11 09:39:26,011 | Failed at Play [check_os_fullname] *************************\n2022-11-11 09:39:26,011 | TASK [check_os_fullname][Skip testcase: check_os_fullname, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'check_os_fullname' is blocked because VMware tools installed: False, running: False\nerror message:\nTest case 'check_os_fullname' is blocked because VMware tools installed: False, running: False", "category": "", "processed": "timestamp failed at play check os full name timestamp task check os full name skip test case check os full name reason blocked task path home worker workspace ansible windows number b it main nvme efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case check os full name is blocked because vmware tools installed false running false error message test case check os full name is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202212132000}, {"id": 5692, "name": "log-6354", "raw": "2022-10-21 04:30:20,021 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-10-21 04:30:20,021 | TASK [deploy_ubuntu_ova][Configure VM CDROM to 'client'] ***\ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_OVA_80GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nfatal: [localhost]: FAILED! => Connection control operation failed for disk 'sata0:0'.\nerror message:\nConnection control operation failed for disk 'sata0:0'.\n2022-10-21 04:30:37,021 | TASK [deploy_ubuntu_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_OVA_80GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova", "category": "", "processed": "timestamp failed at play deploy ubuntu ova timestamp task deploy ubuntu ova configure vm cdrom to client task path home worker workspace ansible ubuntu number server ova number ga para virtual vmxnet number efi ansible vsphere gos validation common vm configure cdrom yml number fatal localhost failed connection control operation failed for disk sata number error message connection control operation failed for disk sata number timestamp task deploy ubuntu ova testing exit due to failure task path home worker workspace ansible ubuntu number server ova number ga para virtual vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ubuntu ova error message exit testing when exit testing when fail is set to true in test case deploy ubuntu ova", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5693, "name": "log-6360", "raw": "2022-10-21 04:44:19,021 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-10-21 04:44:19,021 | TASK [deploy_ubuntu_ova][Configure VM CDROM to 'client'] ***\ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_OVA_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nfatal: [localhost]: FAILED! => Connection control operation failed for disk 'sata0:0'.\nerror message:\nConnection control operation failed for disk 'sata0:0'.\n2022-10-21 04:44:34,021 | TASK [deploy_ubuntu_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_OVA_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova", "category": "", "processed": "timestamp failed at play deploy ubuntu ova timestamp task deploy ubuntu ova configure vm cdrom to client task path home worker workspace ansible ubuntu number server ova main para virtual vmxnet number efi ansible vsphere gos validation common vm configure cdrom yml number fatal localhost failed connection control operation failed for disk sata number error message connection control operation failed for disk sata number timestamp task deploy ubuntu ova testing exit due to failure task path home worker workspace ansible ubuntu number server ova main para virtual vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ubuntu ova error message exit testing when exit testing when fail is set to true in test case deploy ubuntu ova", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5694, "name": "log-6362", "raw": "2022-10-21 04:48:45,021 | Failed at Play [env_setup] *********************************\n2022-10-21 04:48:45,021 | TASK [env_setup][Get all registerd VMs and templates on pek2-hs1-a0410.eng.vmware.com] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_Physical_main_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nexception in /vmware.py when connect_to_api in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Unable to log on to vCenter or ESXi API at pek2-hs1-a0410.eng.vmware.com:443  as root: Cannot complete login due to an incorrect user name or password.\nerror message:\nUnable to log on to vCenter or ESXi API at pek2-hs1-a0410.eng.vmware.com:443  as root: Cannot complete login due to an incorrect user name or password.", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on pe k number hs number eng vmware com task path home worker workspace ansible windows number b it physical main lsi logic sas efi ansible vsphere gos validation common vm check exist yml number exception in vmware python when connect to api in soap adapter python when invoke method fatal localhost failed unable to log on to vcenter or esxi api at pe k number hs number eng vmware com number as root can not complete login due to an incorrect user name or password error message unable to log on to vcenter or esxi api at pe k number hs number eng vmware com number as root can not complete login due to an incorrect user name or password", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5717, "name": "log-6583", "raw": "2022-11-03 05:58:07,003 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-11-03 05:58:07,003 | TASK [vmxnet3_network_device_ops][Deploy VM from ovf template] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nfatal: [localhost]: FAILED! => Failure validating OVF import spec: The virtual machine is not supported on the target datastore.\nerror message:\nFailure validating OVF import spec: The virtual machine is not supported on the target datastore.", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops deploy vm from ovf template task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common ovf deploy yml number fatal localhost failed failure validating ovf import spec the virtual machine is not supported on the target data store error message failure validating ovf import spec the virtual machine is not supported on the target data store", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5695, "name": "log-6365", "raw": "2022-10-21 05:01:51,021 | Failed at Play [env_setup] *********************************\n2022-10-21 05:01:51,021 | TASK [env_setup][Get all registerd VMs and templates on gosv-vc-70update.eng.vmware.com] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_Physical_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nexception in /vmware.py when connect_to_api in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Unable to log on to vCenter or ESXi API at gosv-vc-70update.eng.vmware.com:443  as Administrator@vsphere.local: Cannot complete login due to an incorrect user name or password.\nerror message:\nUnable to log on to vCenter or ESXi API at gosv-vc-70update.eng.vmware.com:443  as Administrator@vsphere.local: Cannot complete login due to an incorrect user name or password.", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on go sv vc number update eng vmware com task path home worker workspace ansible windows number b it physical lsi logic sas efi ansible vsphere gos validation common vm check exist yml number exception in vmware python when connect to api in soap adapter python when invoke method fatal localhost failed unable to log on to vcenter or esxi api at go sv vc number update eng vmware com number as administrator vsphere local can not complete login due to an incorrect user name or password error message unable to log on to vcenter or esxi api at go sv vc number update eng vmware com number as administrator vsphere local can not complete login due to an incorrect user name or password", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5696, "name": "log-6367", "raw": "2022-10-21 05:17:59,021 | Failed at Play [env_setup] *********************************\n2022-10-21 05:17:59,021 | TASK [env_setup][Get all registerd VMs and templates on gosv-vc-80ga.eng.vmware.com] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_Physical_80GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nexception in /vmware.py when connect_to_api in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Unable to log on to vCenter or ESXi API at gosv-vc-80ga.eng.vmware.com:443  as Administrator@vsphere.local: Cannot complete login due to an incorrect user name or password.\nerror message:\nUnable to log on to vCenter or ESXi API at gosv-vc-80ga.eng.vmware.com:443  as Administrator@vsphere.local: Cannot complete login due to an incorrect user name or password.", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on go sv vc number ga eng vmware com task path home worker workspace ansible windows number b it physical number ga lsi logic sas efi ansible vsphere gos validation common vm check exist yml number exception in vmware python when connect to api in soap adapter python when invoke method fatal localhost failed unable to log on to vcenter or esxi api at go sv vc number ga eng vmware com number as administrator vsphere local can not complete login due to an incorrect user name or password error message unable to log on to vcenter or esxi api at go sv vc number ga eng vmware com number as administrator vsphere local can not complete login due to an incorrect user name or password", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5697, "name": "log-6372", "raw": "2022-10-21 07:52:37,021 | Failed at Play [wintools_uninstall_verify] *****************\n2022-10-21 07:52:37,021 | TASK [wintools_uninstall_verify][Add a new 'vmxnet3' adapter in 'vSwitch2022-10-21-07-52-26_PG' to VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_add_network_adapter.yml:13\nfatal: [localhost]: FAILED! => unable to find specified network_name/vlan_id (vSwitch2022-10-21-07-52-26_PG), check parameters\nerror message:\nunable to find specified network_name/vlan_id (vSwitch2022-10-21-07-52-26_PG), check parameters", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify add a new vmxnet number adapter in vswitch timestamp number page to vm test vm task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation common vm add network adapter yml number fatal localhost failed unable to find specified network name vlan id vswitch timestamp number page check parameters error message unable to find specified network name vlan id vswitch timestamp number page check parameters", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5698, "name": "log-6408", "raw": "2022-10-21 08:48:40,021 | Failed at Play [check_os_fullname] *************************\n2022-10-21 08:48:40,021 | TASK [check_os_fullname][Skip testcase: check_os_fullname, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'check_os_fullname' is blocked because VMware tools installed: False, running: False\nerror message:\nTest case 'check_os_fullname' is blocked because VMware tools installed: False, running: False", "category": "", "processed": "timestamp failed at play check os full name timestamp task check os full name skip test case check os full name reason blocked task path home worker workspace ansible windows number b it number ga sata efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case check os full name is blocked because vmware tools installed false running false error message test case check os full name is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202212132000}, {"id": 5699, "name": "log-6991", "raw": "2022-11-11 14:03:07,011 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-11-11 14:03:07,011 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221111125655.log] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04_ISO/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-11 14:03:23,011 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Fetch file /tmp/cloud-init_2022-11-11-14-03-13.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04_ISO/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root\nerror message:\nInvalid guest login for user root", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm efi para virtual vmxnet number fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible cycle ubuntu number iso ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root error message invalid guest login for user root", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5700, "name": "log-6437", "raw": "2022-10-21 13:07:28,021 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-10-21 13:07:28,021 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check Oracle Linux 9.0 kernel UEK R7 is upgraded successfully] \ntask path: /home/worker/workspace/Ansible_Regression_OracleLinux_9.x/ansible-vsphere-gos-validation/linux/deploy_vm/deploy_vm_from_iso.yml:262\nfatal: [localhost]: FAILED! => Oracle Linux 9.0 UEK R7 upgrading failed. Before upgrade, the UEK R7 version is '9.0', after upgrade the UEK R7 version is '9.0'.\nerror message:\nOracle Linux 9.0 UEK R7 upgrading failed. Before upgrade, the UEK R7 version is '9.0', after upgrade the UEK R7 version is '9.0'.\n2022-10-21 13:07:56,021 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_OracleLinux_9.x/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check oracle linux number kernel uek is upgraded successfully task path home worker workspace ansible regression oracle linux number x ansible vsphere gos validation linux deploy vm deploy vm from iso yml number fatal localhost failed oracle linux number uek upgrading failed before upgrade the uek version is number after upgrade the uek version is number error message oracle linux number uek upgrading failed before upgrade the uek version is number after upgrade the uek version is number timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible regression oracle linux number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5701, "name": "log-6489", "raw": "2022-10-24 13:27:11,024 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-10-24 13:27:11,024 | TASK [deploy_ubuntu_ova][Configure VM CDROM to 'client'] ***\ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_OVA_70U1/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nfatal: [localhost]: FAILED! => Connection control operation failed for disk 'sata0:0'.\nerror message:\nConnection control operation failed for disk 'sata0:0'.\n2022-10-24 13:27:31,024 | TASK [deploy_ubuntu_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_OVA_70U1/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova", "category": "", "processed": "timestamp failed at play deploy ubuntu ova timestamp task deploy ubuntu ova configure vm cdrom to client task path home worker workspace ansible ubuntu number server ova ansible vsphere gos validation common vm configure cdrom yml number fatal localhost failed connection control operation failed for disk sata number error message connection control operation failed for disk sata number timestamp task deploy ubuntu ova testing exit due to failure task path home worker workspace ansible ubuntu number server ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ubuntu ova error message exit testing when exit testing when fail is set to true in test case deploy ubuntu ova", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5702, "name": "log-6494", "raw": "2022-10-25 06:03:29,025 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-10-25 06:03:29,025 | TASK [deploy_ubuntu_ova][Configure VM CDROM to 'client'] ***\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.10_OVA/ansible-vsphere-gos-validation/common/vm_configure_cdrom.yml:14\nfatal: [localhost]: FAILED! => Connection control operation failed for disk 'sata0:0'.\nerror message:\nConnection control operation failed for disk 'sata0:0'.\n2022-10-25 06:03:44,025 | TASK [deploy_ubuntu_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_22.10_OVA/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova", "category": "", "processed": "timestamp failed at play deploy ubuntu ova timestamp task deploy ubuntu ova configure vm cdrom to client task path home worker workspace ansible regression ubuntu number ova ansible vsphere gos validation common vm configure cdrom yml number fatal localhost failed connection control operation failed for disk sata number error message connection control operation failed for disk sata number timestamp task deploy ubuntu ova testing exit due to failure task path home worker workspace ansible regression ubuntu number ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ubuntu ova error message exit testing when exit testing when fail is set to true in test case deploy ubuntu ova", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5703, "name": "log-6493", "raw": "2022-10-25 05:41:06,025 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-10-25 05:41:06,025 | TASK [nvdimm_cold_add_remove][Skip testcase: nvdimm_cold_add_remove, reason: Blocked] \ntask path: /home/worker/workspace/zyh_test_ubuntu_existing_10.22/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'nvdimm_cold_add_remove' is blocked because ESXi PMem is less than 16MB. ESXi total persistent memory size is '0 MB' ESXi available persistent memory size is '0 MB'\nerror message:\nTest case 'nvdimm_cold_add_remove' is blocked because ESXi PMem is less than 16MB. ESXi total persistent memory size is '0 MB' ESXi available persistent memory size is '0 MB'", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove skip test case nvdimm cold add remove reason blocked task path home worker workspace zyh test ubuntu existing number ansible vsphere gos validation common skip test case yml number fatal localhost failed test case nvdimm cold add remove is blocked because esxi pmem is less than number mb esxi total persistent memory size is number mb esxi available persistent memory size is number mb error message test case nvdimm cold add remove is blocked because esxi pmem is less than number mb esxi total persistent memory size is number mb esxi available persistent memory size is number mb", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5718, "name": "log-6584", "raw": "2022-11-03 06:00:07,003 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-03 06:00:07,003 | TASK [gosc_perl_staticip][Deploy VM from ovf template] *****\ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nfatal: [localhost]: FAILED! => Failure validating OVF import spec: The virtual machine is not supported on the target datastore.\nerror message:\nFailure validating OVF import spec: The virtual machine is not supported on the target datastore.", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip deploy vm from ovf template task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common ovf deploy yml number fatal localhost failed failure validating ovf import spec the virtual machine is not supported on the target data store error message failure validating ovf import spec the virtual machine is not supported on the target data store", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5704, "name": "log-7078", "raw": "2022-11-17 08:19:41,017 | Failed at Play [deploy_vm_bios_sata_vmxnet3] ***************\n2022-11-17 08:19:41,017 | TASK [deploy_vm_bios_sata_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221117071619.log] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_70U1_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-17 08:20:28,017 | TASK [deploy_vm_bios_sata_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_70U1_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm bios sata vmxnet number timestamp task deploy vm bios sata vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible alma linux number x sata vmxnet number bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm bios sata vmxnet number testing exit due to failure task path home worker workspace ansible alma linux number x sata vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5705, "name": "log-6523", "raw": "2022-10-28 00:04:34,028 | Failed at Play [sata_vhba_device_ops] **********************\n2022-10-28 00:04:34,028 | TASK [sata_vhba_device_ops][Get VM disk controller info] ***\ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/common/vm_get_disk_controller_facts.yml:5\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.161.98.13:443 : [Errno 110] Connection timed out\nerror message:\nUnknown error while connecting to vCenter or ESXi API at 10.161.98.13:443 : [Errno 110] Connection timed out", "category": "", "processed": "timestamp failed at play sata v hba device ops timestamp task sata v hba device ops get vm disk controller info task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common vm get disk controller facts yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out error message unknown error while connecting to vcenter or esxi api at ip address error number connection timed out", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5706, "name": "log-6524", "raw": "2022-10-28 00:11:26,028 | Failed at Play [nvme_vhba_device_ops] **********************\n2022-10-28 00:11:26,028 | TASK [nvme_vhba_device_ops][Add new disk to VM via vmware_guest module] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/common/vm_hot_add_ctrl_disk.yml:28\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.161.98.13:443 : [Errno 110] Connection timed out\nerror message:\nUnknown error while connecting to vCenter or ESXi API at 10.161.98.13:443 : [Errno 110] Connection timed out", "category": "", "processed": "timestamp failed at play nvme v hba device ops timestamp task nvme v hba device ops add new disk to vm via vmware guest module task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common vm hot add ctrl disk yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out error message unknown error while connecting to vcenter or esxi api at ip address error number connection timed out", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5707, "name": "log-6547", "raw": "2022-10-28 14:30:24,028 | Failed at Play [gosc_sanity_staticip] **********************\n2022-10-28 14:30:24,028 | TASK [gosc_sanity_staticip][Customize Windows guest OS] ****\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/guest_customization/win_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.\nerror message:\nCustomization failed. For detailed information see warnings\n2022-10-28 14:31:48,028 | TASK [gosc_sanity_staticip][Fetch file C:\\Windows\\Temp\\vmware-imc\\guestcust.log from VM guest] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user Administrator\nerror message:\nInvalid guest login for user Administrator", "category": "", "processed": "timestamp failed at play go sc sanity static ip timestamp task go sc sanity static ip customize windows guest os task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows guest customization win go sc execution yml number fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out error message customization failed for detailed information see warnings timestamp task go sc sanity static ip fetch file c windows temp vmware imc guest cust log from vm guest task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user administrator error message invalid guest login for user administrator", "solution": "retry", "target": "targetvm", "version": 202212132000}, {"id": 5719, "name": "log-6585", "raw": "2022-11-03 06:07:09,003 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-11-03 06:07:09,003 | TASK [gosc_cloudinit_staticip][Deploy VM from ovf template] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nfatal: [localhost]: FAILED! => Failure validating OVF import spec: The virtual machine is not supported on the target datastore.\nerror message:\nFailure validating OVF import spec: The virtual machine is not supported on the target datastore.", "category": "", "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip deploy vm from ovf template task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common ovf deploy yml number fatal localhost failed failure validating ovf import spec the virtual machine is not supported on the target data store error message failure validating ovf import spec the virtual machine is not supported on the target data store", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5708, "name": "log-6529", "raw": "2022-10-28 03:43:36,028 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2022-10-28 03:43:36,028 | TASK [deploy_vm_efi_paravirtual_e1000e][Guest OS connection failure] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/utils/win_check_winrm.yml:26\nfatal: [localhost]: FAILED! => Guest winrm is not connectable in 1800 seconds.\nerror message:\nGuest winrm is not connectable in 1800 seconds.\n2022-10-28 03:44:17,028 | TASK [deploy_vm_efi_paravirtual_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_e1000e\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual timestamp task deploy vm efi para virtual guest os connection failure task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows utilities win check win rm yml number fatal localhost failed guest win rm is not connectable in number seconds error message guest win rm is not connectable in number seconds timestamp task deploy vm efi para virtual testing exit due to failure task path home worker workspace ansible cycle windows server number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual e number e error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual e number e", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5709, "name": "log-7022", "raw": "2022-11-13 11:50:08,013 | Failed at Play [wintools_complete_install_verify] **********\n2022-11-13 11:50:08,013 | TASK [wintools_complete_install_verify][Execute powershell command 'get-service -Name VMTools | foreach {$_.Status}'] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.212.14.35]: FAILED! => non-zero return code when get-service\nerror message:\nnon-zero return code: 1\nget-service : Cannot find any service with service name 'VMTools'.\nAt line:1 char:65\n+ ... ew-Object Text.UTF8Encoding $false; get-service -Name VMTools | forea ...\n+                                         ~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (VMTools:String) [Get-Service], ServiceCommandException\n    + FullyQualifiedErrorId : NoServiceFoundForGivenName,Microsoft.PowerShell.Commands.GetServiceCommand", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify execute powershell command get service name vm tools for each status task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when get service error message nonzero return code number get service can not find any service with service name vm tools at line number char number ew object text utf number encoding false get service name vm tools fore a category info object not found vm tools string get service service command exception fully qualified error id no service found forgiven name microsoft powershell commands get service command", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5710, "name": "log-6593", "raw": "2022-11-04 02:39:21,004 | Failed at Play [deploy_vm_bios_nvme_e1000e] ****************\n2022-11-04 02:39:21,004 | TASK [deploy_vm_bios_nvme_e1000e][Upload local file to ESXi datastore] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/esxi_upload_datastore_file.yml:11\nexception in /vsphere_copy.py when main in /request.py when http_error_default\nfatal: [localhost]: FAILED! => HTTP Error 503: Service Unavailable\nerror message:\nHTTP Error 503: Service Unavailable\n2022-11-04 02:39:23,004 | TASK [deploy_vm_bios_nvme_e1000e][Get specified property info for VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nexception in /vmware.py when connect_to_api in /connect.py when SmartConnect\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.168.208.107:443 : 10.168.208.107:443 is not a VIM server\nerror message:\nUnknown error while connecting to vCenter or ESXi API at 10.168.208.107:443 : 10.168.208.107:443 is not a VIM server", "category": "", "processed": "timestamp failed at play deploy vm bios nvme timestamp task deploy vm bios nvme upload local file to esxi data store task path home worker workspace ansible alma linux number x main nvme bios ansible vsphere gos validation common esxi upload data store file yml number exception in vsphere copy python when main in request python when http error default fatal localhost failed http error number service unavailable error message http error number service unavailable timestamp task deploy vm bios nvme get specified property info for vm test vm task path home worker workspace ansible alma linux number x main nvme bios ansible vsphere gos validation common vm get configuration yml number exception in vmware python when connect to api in connect python when smart connect fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address ip address is not a vim server error message unknown error while connecting to vcenter or esxi api at ip address ip address is not a vim server", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5711, "name": "log-6638", "raw": "2022-11-07 04:13:22,007 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-11-07 04:13:22,007 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Upload local file to ESXi datastore] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/esxi_upload_datastore_file.yml:11\nexception in /vsphere_copy.py when main in /request.py when http_error_default\nfatal: [localhost]: FAILED! => HTTP Error 503: Service Unavailable\nerror message:\nHTTP Error 503: Service Unavailable\n2022-11-07 04:13:23,007 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nexception in /vmware.py when connect_to_api in /connect.py when SmartConnect\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.185.26.164:443 : 10.185.26.164:443 is not a VIM server\nerror message:\nUnknown error while connecting to vCenter or ESXi API at 10.185.26.164:443 : 10.185.26.164:443 is not a VIM server", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number upload local file to esxi data store task path home worker workspace ansible alma linux number x main para virtual vmxnet number efi ansible vsphere gos validation common esxi upload data store file yml number exception in vsphere copy python when main in request python when http error default fatal localhost failed http error number service unavailable error message http error number service unavailable timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm test vm task path home worker workspace ansible alma linux number x main para virtual vmxnet number efi ansible vsphere gos validation common vm get configuration yml number exception in vmware python when connect to api in connect python when smart connect fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address ip address is not a vim server error message unknown error while connecting to vcenter or esxi api at ip address ip address is not a vim server", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5712, "name": "log-6571", "raw": "2022-11-02 04:14:33,002 | Failed at Play [deploy_vm_bios_nvme_e1000e] ****************\n2022-11-02 04:14:33,002 | TASK [deploy_vm_bios_nvme_e1000e][Datastore file operation] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => Failed to query for file 'almalinux_64GuestRHEL_8_server_with_GUI_ks-1667362461.iso'\nerror message:\nFailed to query for file 'almalinux_64GuestRHEL_8_server_with_GUI_ks-1667362461.iso'\n2022-11-02 04:14:34,002 | TASK [deploy_vm_bios_nvme_e1000e][Get specified property info for VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nexception in /vmware.py when connect_to_api in /connect.py when SmartConnect\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.168.184.181:443 : 10.168.184.181:443 is not a VIM server\nerror message:\nUnknown error while connecting to vCenter or ESXi API at 10.168.184.181:443 : 10.168.184.181:443 is not a VIM server", "category": "", "processed": "timestamp failed at play deploy vm bios nvme timestamp task deploy vm bios nvme data store file operation task path home worker workspace ansible alma linux number x main nvme bios ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed failed to query for file alma linux number guest rhel number server with gui ks hex id iso error message failed to query for file alma linux number guest rhel number server with gui ks hex id iso timestamp task deploy vm bios nvme get specified property info for vm test vm task path home worker workspace ansible alma linux number x main nvme bios ansible vsphere gos validation common vm get configuration yml number exception in vmware python when connect to api in connect python when smart connect fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address ip address is not a vim server error message unknown error while connecting to vcenter or esxi api at ip address ip address is not a vim server", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5713, "name": "log-6580", "raw": "2022-11-03 03:02:55,003 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2022-11-03 03:02:55,003 | TASK [deploy_vm_efi_ide_e1000e][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:51\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '['fe80::250:56ff:fe9e:67a6']'.\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '['fe80::250:56ff:fe9e:67a6']'.\n2022-11-03 03:03:35,003 | TASK [deploy_vm_efi_ide_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi ide timestamp task deploy vm efi ide check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible alma linux number x ide efi ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are ip address error message it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are ip address timestamp task deploy vm efi ide testing exit due to failure task path home worker workspace ansible alma linux number x ide efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e error message exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5714, "name": "log-6578", "raw": "2022-11-03 02:34:52,003 | Failed at Play [env_setup] *********************************\n2022-11-03 02:34:52,003 | TASK [env_setup][Get all registerd VMs and templates on 10.168.184.181] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nexception in /vmware.py when connect_to_api in /connect.py when SmartConnect\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.168.184.181:443 : 10.168.184.181:443 is not a VIM server\nerror message:\nUnknown error while connecting to vCenter or ESXi API at 10.168.184.181:443 : 10.168.184.181:443 is not a VIM server", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on ip address task path home worker workspace ansible alma linux number x main nvme bios ansible vsphere gos validation common vm check exist yml number exception in vmware python when connect to api in connect python when smart connect fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address ip address is not a vim server error message unknown error while connecting to vcenter or esxi api at ip address ip address is not a vim server", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5715, "name": "log-6579", "raw": "2022-11-03 02:54:06,003 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-11-03 02:54:06,003 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Datastore file operation] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:24\nfatal: [localhost]: FAILED! => Failed to query for file 'OS/Linux/AlmaLinux/9/9.1/Beta/x86_64/AlmaLinux-9.1-beta-1-x86_64-dvd.iso'\nerror message:\nFailed to query for file 'OS/Linux/AlmaLinux/9/9.1/Beta/x86_64/AlmaLinux-9.1-beta-1-x86_64-dvd.iso'\n2022-11-03 02:54:07,003 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_AlmaLinux_9.x_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nexception in /vmware.py when connect_to_api in /connect.py when SmartConnect\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.78.160.255:443 : 10.78.160.255:443 is not a VIM server\nerror message:\nUnknown error while connecting to vCenter or ESXi API at 10.78.160.255:443 : 10.78.160.255:443 is not a VIM server", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number data store file operation task path home worker workspace ansible alma linux number x main para virtual vmxnet number efi ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed failed to query for file os linux alma linux number beta x number alma linux number beta number x number dvd iso error message failed to query for file os linux alma linux number beta x number alma linux number beta number x number dvd iso timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm test vm task path home worker workspace ansible alma linux number x main para virtual vmxnet number efi ansible vsphere gos validation common vm get configuration yml number exception in vmware python when connect to api in connect python when smart connect fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address ip address is not a vim server error message unknown error while connecting to vcenter or esxi api at ip address ip address is not a vim server", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5716, "name": "log-6582", "raw": "2022-11-03 05:56:30,003 | Failed at Play [e1000e_network_device_ops] *****************\n2022-11-03 05:56:30,003 | TASK [e1000e_network_device_ops][Deploy VM from ovf template] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nfatal: [localhost]: FAILED! => Failure validating OVF import spec: The virtual machine is not supported on the target datastore.\nerror message:\nFailure validating OVF import spec: The virtual machine is not supported on the target datastore.", "category": "", "processed": "timestamp failed at play network device ops timestamp task network device ops deploy vm from ovf template task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common ovf deploy yml number fatal localhost failed failure validating ovf import spec the virtual machine is not supported on the target data store error message failure validating ovf import spec the virtual machine is not supported on the target data store", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5720, "name": "log-6586", "raw": "2022-11-03 06:10:03,003 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-03 06:10:03,003 | TASK [nvdimm_cold_add_remove][Configure VM NVDIMM device] **\ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/common/vm_add_remove_nvdimm.yml:11\nfatal: [localhost]: FAILED! => Failed to configure change tracking for NVDIMM device nvdimm0:0: The specified feature is not supported by this version\nerror message:\nFailed to configure change tracking for NVDIMM device nvdimm0:0: The specified feature is not supported by this version", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove configure vm nvdimm device task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common vm add remove nvdimm yml number fatal localhost failed failed to configure change tracking for nvdimm device nvdimm number the specified feature is not supported by this version error message failed to configure change tracking for nvdimm device nvdimm number the specified feature is not supported by this version", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5721, "name": "log-6596", "raw": "2022-11-04 06:54:46,004 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-11-04 06:54:46,004 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Copy unattend install config files to /home/worker/workspace/lli_Pardus_21.x_64bit_70GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/cache/unattend_iso_hdb8_6cd] \ntask path: /home/worker/workspace/lli_Pardus_21.x_64bit_70GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/deploy_vm/rebuild_unattend_install_iso.yml:122\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\nfailed: [localhost] => (item=/isolinux/gtk.cfg) => Could not find or access '/home/worker/workspace/lli_Pardus_21.x_64bit_70GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/cache/unattend_iso_hdb8_6cd/expand_Pardus-21.3-GNOME-amd64__lxl78vp/isolinux/gtk.cfg' on the Ansible Controller.\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\nerror message:\nCould not find or access '/home/worker/workspace/lli_Pardus_21.x_64bit_70GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/cache/unattend_iso_hdb8_6cd/expand_Pardus-21.3-GNOME-amd64__lxl78vp/isolinux/gtk.cfg' on the Ansible Controller.\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\n2022-11-04 06:54:49,004 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'test_vm'] \ntask path: /home/worker/workspace/lli_Pardus_21.x_64bit_70GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm\nerror message:\nUnable to gather information for non-existing VM test_vm", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number copy un attend install configuration files to home worker workspace ll i pard us number x number b it number ga para virtual vmxnet number efi ansible vsphere gos validation cache un attend iso hdb number cd task path home worker workspace ll i pard us number x number b it number ga para virtual vmxnet number efi ansible vsphere gos validation linux deploy vm rebuild un attend install iso yml number if you are using a module and expect the file to exist on the remote see the remote source option failed localhost item iso linux gtk configuration could not find or access home worker workspace ll i pard us number x number b it number ga para virtual vmxnet number efi ansible vsphere gos validation cache un attend iso hdb number cd expand pard us number gnome amd number lxl number vp iso linux gtk configuration on the ansible controller if you are using a module and expect the file to exist on the remote see the remote source option error message could not find or access home worker workspace ll i pard us number x number b it number ga para virtual vmxnet number efi ansible vsphere gos validation cache un attend iso hdb number cd expand pard us number gnome amd number lxl number vp iso linux gtk configuration on the ansible controller if you are using a module and expect the file to exist on the remote see the remote source option timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm test vm task path home worker workspace ll i pard us number x number b it number ga para virtual vmxnet number efi ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test vm error message unable to gather information for non existing vm test vm", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5722, "name": "log-6639", "raw": "2022-11-07 09:13:53,007 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2022-11-07 09:13:53,007 | TASK [deploy_vm_efi_ide_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221107081050.log] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-07 09:14:36,007 | TASK [deploy_vm_efi_ide_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi ide timestamp task deploy vm efi ide wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible rhel number x ide efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm efi ide testing exit due to failure task path home worker workspace ansible rhel number x ide efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e error message exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5723, "name": "log-6627", "raw": "2022-11-04 12:23:03,004 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-11-04 12:23:03,004 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221104112032.log] \ntask path: /home/worker/workspace/Ansible_Regression_UOS_Server_20/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-04 12:23:12,004 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_UOS_Server_20/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible regression u os server number ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible regression u os server number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5724, "name": "log-6601", "raw": "2022-11-04 08:10:48,004 | Failed at Play [ovt_verify_install] ************************\n2022-11-04 08:10:48,004 | TASK [ovt_verify_install][Set fact of guest OS display manager] \ntask path: /home/worker/workspace/Ansible_Regression_AlmaLinux_8.x/ansible-vsphere-gos-validation/linux/utils/check_guest_os_gui.yml:35\nfatal: [localhost]: FAILED! => template error while templating string: Unexpected end of template. Jinja was looking for the following tags: 'elif' or 'else' or 'endif'. The innermost block that needs to be closed is 'if'.. String: {%- if \"GNOME Display Manager\" in display_manager_status.stdout_lines[0] -%}gdm {%- if \"Light Display Manager\" in display_manager_status.stdout_lines[0] -%}lightdm {%- if \"X Display Manager\" in display_manager_status.stdout_lines[0] -%}xdm {%- if \"LXDE Display Manager\" in display_manager_status.stdout_lines[0] -%}lxdm {%- endif -%}\nerror message:\ntemplate error while templating string: Unexpected end of template. Jinja was looking for the following tags: 'elif' or 'else' or 'endif'. The innermost block that needs to be closed is 'if'.. String: {%- if \"GNOME Display Manager\" in display_manager_status.stdout_lines[0] -%}gdm {%- if \"Light Display Manager\" in display_manager_status.stdout_lines[0] -%}lightdm {%- if \"X Display Manager\" in display_manager_status.stdout_lines[0] -%}xdm {%- if \"LXDE Display Manager\" in display_manager_status.stdout_lines[0] -%}lxdm {%- endif -%}\n2022-11-04 08:10:59,004 | TASK [ovt_verify_install][Testing exit due to failure] *****\ntask path: /home/worker/workspace/Ansible_Regression_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install", "category": "", "processed": "timestamp failed at play ovt verify install timestamp task ovt verify install set fact of guest os display manager task path home worker workspace ansible regression alma linux number x ansible vsphere gos validation linux utilities check guest os gui yml number fatal localhost failed template error while templating string unexpected end of template jinja was looking for the following tags else if or else or end if the innermost block that needs to be closed is if string if gnome display manager in display manager status stdout lines number gdm if light display manager in display manager status stdout lines number light dm if x display manager in display manager status stdout lines number xdm if lx de display manager in display manager status stdout lines number lxdm end if error message template error while templating string unexpected end of template jinja was looking for the following tags else if or else or end if the innermost block that needs to be closed is if string if gnome display manager in display manager status stdout lines number gdm if light display manager in display manager status stdout lines number light dm if x display manager in display manager status stdout lines number xdm if lx de display manager in display manager status stdout lines number lxdm end if timestamp task ovt verify install testing exit due to failure task path home worker workspace ansible regression alma linux number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case ovt verify install error message exit testing when exit testing when fail is set to true in test case ovt verify install", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5725, "name": "log-6602", "raw": "2022-11-04 08:11:00,004 | Failed at Play [ovt_verify_install] ************************\n2022-11-04 08:11:00,004 | TASK [ovt_verify_install][Set fact of guest OS display manager] \ntask path: /home/worker/workspace/Ansible_Regression_Debian_10.x_32/ansible-vsphere-gos-validation/linux/utils/check_guest_os_gui.yml:35\nfatal: [localhost]: FAILED! => template error while templating string: Unexpected end of template. Jinja was looking for the following tags: 'elif' or 'else' or 'endif'. The innermost block that needs to be closed is 'if'.. String: {%- if \"GNOME Display Manager\" in display_manager_status.stdout_lines[0] -%}gdm {%- if \"Light Display Manager\" in display_manager_status.stdout_lines[0] -%}lightdm {%- if \"X Display Manager\" in display_manager_status.stdout_lines[0] -%}xdm {%- if \"LXDE Display Manager\" in display_manager_status.stdout_lines[0] -%}lxdm {%- endif -%}\nerror message:\ntemplate error while templating string: Unexpected end of template. Jinja was looking for the following tags: 'elif' or 'else' or 'endif'. The innermost block that needs to be closed is 'if'.. String: {%- if \"GNOME Display Manager\" in display_manager_status.stdout_lines[0] -%}gdm {%- if \"Light Display Manager\" in display_manager_status.stdout_lines[0] -%}lightdm {%- if \"X Display Manager\" in display_manager_status.stdout_lines[0] -%}xdm {%- if \"LXDE Display Manager\" in display_manager_status.stdout_lines[0] -%}lxdm {%- endif -%}\n2022-11-04 08:11:09,004 | TASK [ovt_verify_install][Testing exit due to failure] *****\ntask path: /home/worker/workspace/Ansible_Regression_Debian_10.x_32/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install", "category": "", "processed": "timestamp failed at play ovt verify install timestamp task ovt verify install set fact of guest os display manager task path home worker workspace ansible regression debian number x number ansible vsphere gos validation linux utilities check guest os gui yml number fatal localhost failed template error while templating string unexpected end of template jinja was looking for the following tags else if or else or end if the innermost block that needs to be closed is if string if gnome display manager in display manager status stdout lines number gdm if light display manager in display manager status stdout lines number light dm if x display manager in display manager status stdout lines number xdm if lx de display manager in display manager status stdout lines number lxdm end if error message template error while templating string unexpected end of template jinja was looking for the following tags else if or else or end if the innermost block that needs to be closed is if string if gnome display manager in display manager status stdout lines number gdm if light display manager in display manager status stdout lines number light dm if x display manager in display manager status stdout lines number xdm if lx de display manager in display manager status stdout lines number lxdm end if timestamp task ovt verify install testing exit due to failure task path home worker workspace ansible regression debian number x number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case ovt verify install error message exit testing when exit testing when fail is set to true in test case ovt verify install", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5726, "name": "log-6603", "raw": "2022-11-04 08:12:12,004 | Failed at Play [ovt_verify_install] ************************\n2022-11-04 08:12:12,004 | TASK [ovt_verify_install][Set fact of guest OS display manager] \ntask path: /home/worker/workspace/Ansible_Regression_OracleLinux_9.x/ansible-vsphere-gos-validation/linux/utils/check_guest_os_gui.yml:35\nfatal: [localhost]: FAILED! => template error while templating string: Unexpected end of template. Jinja was looking for the following tags: 'elif' or 'else' or 'endif'. The innermost block that needs to be closed is 'if'.. String: {%- if \"GNOME Display Manager\" in display_manager_status.stdout_lines[0] -%}gdm {%- if \"Light Display Manager\" in display_manager_status.stdout_lines[0] -%}lightdm {%- if \"X Display Manager\" in display_manager_status.stdout_lines[0] -%}xdm {%- if \"LXDE Display Manager\" in display_manager_status.stdout_lines[0] -%}lxdm {%- endif -%}\nerror message:\ntemplate error while templating string: Unexpected end of template. Jinja was looking for the following tags: 'elif' or 'else' or 'endif'. The innermost block that needs to be closed is 'if'.. String: {%- if \"GNOME Display Manager\" in display_manager_status.stdout_lines[0] -%}gdm {%- if \"Light Display Manager\" in display_manager_status.stdout_lines[0] -%}lightdm {%- if \"X Display Manager\" in display_manager_status.stdout_lines[0] -%}xdm {%- if \"LXDE Display Manager\" in display_manager_status.stdout_lines[0] -%}lxdm {%- endif -%}\n2022-11-04 08:12:21,004 | TASK [ovt_verify_install][Testing exit due to failure] *****\ntask path: /home/worker/workspace/Ansible_Regression_OracleLinux_9.x/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install", "category": "", "processed": "timestamp failed at play ovt verify install timestamp task ovt verify install set fact of guest os display manager task path home worker workspace ansible regression oracle linux number x ansible vsphere gos validation linux utilities check guest os gui yml number fatal localhost failed template error while templating string unexpected end of template jinja was looking for the following tags else if or else or end if the innermost block that needs to be closed is if string if gnome display manager in display manager status stdout lines number gdm if light display manager in display manager status stdout lines number light dm if x display manager in display manager status stdout lines number xdm if lx de display manager in display manager status stdout lines number lxdm end if error message template error while templating string unexpected end of template jinja was looking for the following tags else if or else or end if the innermost block that needs to be closed is if string if gnome display manager in display manager status stdout lines number gdm if light display manager in display manager status stdout lines number light dm if x display manager in display manager status stdout lines number xdm if lx de display manager in display manager status stdout lines number lxdm end if timestamp task ovt verify install testing exit due to failure task path home worker workspace ansible regression oracle linux number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case ovt verify install error message exit testing when exit testing when fail is set to true in test case ovt verify install", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5727, "name": "log-6615", "raw": "2022-11-04 09:45:33,004 | Failed at Play [e1000e_network_device_ops] *****************\n2022-11-04 09:45:33,004 | TASK [e1000e_network_device_ops][Deploy VM from ovf template] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nfatal: [localhost]: FAILED! => Datastore 'datastore2' could not be located on specified ESXi host or datacenter\nerror message:\nDatastore 'datastore2' could not be located on specified ESXi host or datacenter", "category": "", "processed": "timestamp failed at play network device ops timestamp task network device ops deploy vm from ovf template task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common ovf deploy yml number fatal localhost failed data store data store number could not be located on specified esxi host or data center error message data store data store number could not be located on specified esxi host or data center", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5728, "name": "log-6616", "raw": "2022-11-04 09:47:28,004 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-11-04 09:47:28,004 | TASK [vmxnet3_network_device_ops][Deploy VM from ovf template] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nfatal: [localhost]: FAILED! => Datastore 'datastore2' could not be located on specified ESXi host or datacenter\nerror message:\nDatastore 'datastore2' could not be located on specified ESXi host or datacenter", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops deploy vm from ovf template task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common ovf deploy yml number fatal localhost failed data store data store number could not be located on specified esxi host or data center error message data store data store number could not be located on specified esxi host or data center", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5729, "name": "log-6618", "raw": "2022-11-04 09:57:06,004 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-04 09:57:06,004 | TASK [gosc_perl_staticip][Deploy VM from ovf template] *****\ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nfatal: [localhost]: FAILED! => Datastore 'datastore2' could not be located on specified ESXi host or datacenter\nerror message:\nDatastore 'datastore2' could not be located on specified ESXi host or datacenter", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip deploy vm from ovf template task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common ovf deploy yml number fatal localhost failed data store data store number could not be located on specified esxi host or data center error message data store data store number could not be located on specified esxi host or data center", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5730, "name": "log-6620", "raw": "2022-11-04 10:13:08,004 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-11-04 10:13:08,004 | TASK [gosc_cloudinit_staticip][Deploy VM from ovf template] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_8.x/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nfatal: [localhost]: FAILED! => Datastore 'datastore2' could not be located on specified ESXi host or datacenter\nerror message:\nDatastore 'datastore2' could not be located on specified ESXi host or datacenter", "category": "", "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip deploy vm from ovf template task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common ovf deploy yml number fatal localhost failed data store data store number could not be located on specified esxi host or data center error message data store data store number could not be located on specified esxi host or data center", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5731, "name": "log-6621", "raw": "2022-11-04 12:00:08,004 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-11-04 12:00:08,004 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Regression_RockyLinux_8.x/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:51\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '['fd01:0:106:6:250:56ff:febc:e071']'.\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '['fd01:0:106:6:250:56ff:febc:e071']'.\n2022-11-04 12:00:18,004 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_RockyLinux_8.x/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible regression rocky linux number x ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are ip address error message it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are ip address timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible regression rocky linux number x ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5732, "name": "log-6922", "raw": "2022-11-11 02:33:20,011 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-11-11 02:33:20,011 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_Regression_UOS_Server_20/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc1-10-168-212-161, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\", \"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM hostname is sc1-10-168-212-161, expected hostname is gosc-dhcp-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\nVM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible regression u os server number ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com error message vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5733, "name": "log-6923", "raw": "2022-11-11 02:38:18,011 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-11 02:38:18,011 | TASK [gosc_perl_staticip][GOS customization failed] ********\ntask path: /home/worker/workspace/Ansible_Regression_UOS_Server_20/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc1-10-168-212-161, expected hostname is gosc-static-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', 'VM hwclockUTC is True, expected hwclockUTC is False']\nerror message:\nVM hostname is sc1-10-168-212-161, expected hostname is gosc-static-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM hwclockUTC is True, expected hwclockUTC is False", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip gos customization failed task path home worker workspace ansible regression u os server number ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false error message vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5734, "name": "log-6637", "raw": "2022-11-06 20:00:11,006 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-11-06 20:00:11,006 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Create a new VM 'test_vm' on server '10.182.49.237'] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_9.x/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\nerror message:\nFailed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\n2022-11-06 20:00:12,006 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Cycle_OracleLinux_9.x/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_vm\nerror message:\nUnable to gather information for non-existing VM test_vm", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number create a new vm test vm on server ip address task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common vm create yml number fatal localhost failed failed to create a virtual machine a specified parameter was not correct configuration spec guest id error message failed to create a virtual machine a specified parameter was not correct configuration spec guest id timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm test vm task path home worker workspace ansible cycle oracle linux number x ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test vm error message unable to gather information for non existing vm test vm", "solution": "deepdive", "target": "usererror", "version": 202212132000}, {"id": 5735, "name": "log-6640", "raw": "2022-11-08 01:38:40,008 | Failed at Play [env_setup] *********************************\n2022-11-08 01:38:40,008 | TASK [env_setup][Get all registerd VMs and templates on 10.168.173.107] \ntask path: /home/worker/workspace/Ansible_Regression_UOS_Server_20_1050e/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.168.173.107:443 : [Errno 113] No route to host\nerror message:\nUnknown error while connecting to vCenter or ESXi API at 10.168.173.107:443 : [Errno 113] No route to host", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on ip address task path home worker workspace ansible regression u os server number ansible vsphere gos validation common vm check exist yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number no route to host error message unknown error while connecting to vcenter or esxi api at ip address error number no route to host", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5736, "name": "log-6647", "raw": "2022-11-09 03:47:20,009 | Failed at Play [ovt_verify_install] ************************\n2022-11-09 03:47:20,009 | TASK [ovt_verify_install][Skip testcase: ovt_verify_install, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Regression_UOS_Server_20_1050e/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case is blocked because VMware tools installed: False, running: False\nerror message:\nTest case is blocked because VMware tools installed: False, running: False\n2022-11-09 03:47:29,009 | TASK [ovt_verify_install][Testing exit due to failure] *****\ntask path: /home/worker/workspace/Ansible_Regression_UOS_Server_20_1050e/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case ovt_verify_install", "category": "", "processed": "timestamp failed at play ovt verify install timestamp task ovt verify install skip test case ovt verify install reason blocked task path home worker workspace ansible regression u os server number ansible vsphere gos validation common skip test case yml number fatal localhost failed test case is blocked because vmware tools installed false running false error message test case is blocked because vmware tools installed false running false timestamp task ovt verify install testing exit due to failure task path home worker workspace ansible regression u os server number ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case ovt verify install error message exit testing when exit testing when fail is set to true in test case ovt verify install", "solution": "deepdive", "target": "usererror", "version": 202212132000}, {"id": 5737, "name": "log-6658", "raw": "2022-11-09 13:22:24,009 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-09 13:22:24,009 | TASK [nvdimm_cold_add_remove][Execute powershell command 'diskpart /s C:\\test_diskpart\\diskpart.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.168.211.133]: FAILED! => non-zero return code when diskpart\nerror message:\nnon-zero return code: 1\nMicrosoft DiskPart version 10.0.19041.964\nCopyright (C) Microsoft Corporation.\nOn computer: DESKTOP-RDGLVQ5\nThe disk you specified is not valid.\nThere is no disk selected.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove execute powershell command diskpart s c test diskpart diskpart text task path home worker workspace ansible windows number b it number ga sata efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when diskpart error message nonzero return code number microsoft diskpart version version id number copyright c microsoft corporation on computer desktop rdglvq number the disk you specified is not valid there is no disk selected", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5738, "name": "log-6660", "raw": "2022-11-09 13:23:49,009 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-09 13:23:49,009 | TASK [nvdimm_cold_add_remove][Execute powershell command 'diskpart /s C:\\test_diskpart\\diskpart.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U1_LSILOGICSAS_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.185.99.98]: FAILED! => non-zero return code when diskpart\nerror message:\nnon-zero return code: 1\nMicrosoft DiskPart version 10.0.19041.964\nCopyright (C) Microsoft Corporation.\nOn computer: DESKTOP-IL0380P\nThe disk you specified is not valid.\nThere is no disk selected.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove execute powershell command diskpart s c test diskpart diskpart text task path home worker workspace ansible windows number b it lsi logic sas bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when diskpart error message nonzero return code number microsoft diskpart version version id number copyright c microsoft corporation on computer desktop il number p the disk you specified is not valid there is no disk selected", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5739, "name": "log-6669", "raw": "2022-11-09 13:54:15,009 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-09 13:54:15,009 | TASK [nvdimm_cold_add_remove][Execute powershell command 'diskpart /s C:\\test_diskpart\\diskpart.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.78.176.108]: FAILED! => non-zero return code when diskpart\nerror message:\nnon-zero return code: 1\nMicrosoft DiskPart version 10.0.25236.1000\nCopyright (C) Microsoft Corporation.\nOn computer: DESKTOP-D60N1M4\nThe disk you specified is not valid.\nThere is no disk selected.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove execute powershell command diskpart s c test diskpart diskpart text task path home worker workspace ansible windows number ga lsi logic sas efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when diskpart error message nonzero return code number microsoft diskpart version version id number copyright c microsoft corporation on computer desktopd number n number m number the disk you specified is not valid there is no disk selected", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5740, "name": "log-6716", "raw": "2022-11-09 21:22:32,009 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-09 21:22:32,009 | TASK [nvdimm_cold_add_remove][Execute powershell command 'diskpart /s C:\\test_diskpart\\diskpart.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U1_SATA_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.185.1.170]: FAILED! => non-zero return code when diskpart\nerror message:\nnon-zero return code: 1\nMicrosoft DiskPart version 10.0.19041.964\nCopyright (C) Microsoft Corporation.\nOn computer: DESKTOP-QBOO95A\nThe disk you specified is not valid.\nThere is no disk selected.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove execute powershell command diskpart s c test diskpart diskpart text task path home worker workspace ansible windows number b it sata bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when diskpart error message nonzero return code number microsoft diskpart version version id number copyright c microsoft corporation on computer desktop q boo number a the disk you specified is not valid there is no disk selected", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5741, "name": "log-6681", "raw": "2022-11-09 14:02:41,009 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-09 14:02:41,009 | TASK [nvdimm_cold_add_remove][Execute powershell command 'diskpart /s C:\\test_diskpart\\diskpart.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.168.200.157]: FAILED! => non-zero return code when diskpart\nerror message:\nnon-zero return code: 1\nMicrosoft DiskPart version 10.0.25236.1000\nCopyright (C) Microsoft Corporation.\nOn computer: DESKTOP-K3KHUB8\nThe disk you specified is not valid.\nThere is no disk selected.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove execute powershell command diskpart s c test diskpart diskpart text task path home worker workspace ansible windows number nvme vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when diskpart error message nonzero return code number microsoft diskpart version version id number copyright c microsoft corporation on computer desktop k number k hub number the disk you specified is not valid there is no disk selected", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5742, "name": "log-6685", "raw": "2022-11-09 14:15:26,009 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-09 14:15:26,009 | TASK [nvdimm_cold_add_remove][Execute powershell command 'diskpart /s C:\\test_diskpart\\diskpart.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.182.6.32]: FAILED! => non-zero return code when diskpart\nerror message:\nnon-zero return code: 1\nMicrosoft DiskPart version 10.0.25236.1000\nCopyright (C) Microsoft Corporation.\nOn computer: DESKTOP-G6OCG6J\nThe disk you specified is not valid.\nThere is no disk selected.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove execute powershell command diskpart s c test diskpart diskpart text task path home worker workspace ansible windows number ga ide vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when diskpart error message nonzero return code number microsoft diskpart version version id number copyright c microsoft corporation on computer desktop g number ocg number j the disk you specified is not valid there is no disk selected", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5743, "name": "log-6689", "raw": "2022-11-09 16:22:44,009 | Failed at Play [gosc_sanity_staticip] **********************\n2022-11-09 16:22:44,009 | TASK [gosc_sanity_staticip][Customize Windows guest OS] ****\ntask path: /home/worker/workspace/Ansible_Windows_11_80GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/guest_customization/win_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.\nerror message:\nCustomization failed. For detailed information see warnings\n2022-11-09 16:23:42,009 | TASK [gosc_sanity_staticip][Fetch file C:\\Windows\\Temp\\vmware-imc\\guestcust.log from VM guest] \ntask path: /home/worker/workspace/Ansible_Windows_11_80GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user Administrator\nerror message:\nInvalid guest login for user Administrator", "category": "", "processed": "timestamp failed at play go sc sanity static ip timestamp task go sc sanity static ip customize windows guest os task path home worker workspace ansible windows number ga lsi logic sas efi ansible vsphere gos validation windows guest customization win go sc execution yml number fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out error message customization failed for detailed information see warnings timestamp task go sc sanity static ip fetch file c windows temp vmware imc guest cust log from vm guest task path home worker workspace ansible windows number ga lsi logic sas efi ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user administrator error message invalid guest login for user administrator", "solution": "retry", "target": "targetvm", "version": 202212132000}, {"id": 5744, "name": "log-6693", "raw": "2022-11-09 17:48:14,009 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-09 17:48:14,009 | TASK [nvdimm_cold_add_remove][Execute powershell command 'diskpart /s C:\\test_diskpart\\diskpart.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70GA_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:18\nfatal: [localhost -> 10.78.136.211]: FAILED! => non-zero return code when diskpart\nerror message:\nnon-zero return code: 1\nMicrosoft DiskPart version 10.0.19041.964\nCopyright (C) Microsoft Corporation.\nOn computer: DESKTOP-55EH23A\nThe disk you specified is not valid.\nThere is no disk selected.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove execute powershell command diskpart s c test diskpart diskpart text task path home worker workspace ansible windows number b it number ga ide bios ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when diskpart error message nonzero return code number microsoft diskpart version version id number copyright c microsoft corporation on computer desktop number eh number a the disk you specified is not valid there is no disk selected", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5745, "name": "log-6736", "raw": "2022-11-10 09:43:34,010 | Failed at Play [wintools_uninstall_verify] *****************\n2022-11-10 09:43:34,010 | TASK [wintools_uninstall_verify][Add a new 'vmxnet3' adapter in 'vSwitch2022-11-10-09-43-22_PG' to VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Windows_11_80GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_add_network_adapter.yml:13\nfatal: [localhost]: FAILED! => unable to find specified network_name/vlan_id (vSwitch2022-11-10-09-43-22_PG), check parameters\nerror message:\nunable to find specified network_name/vlan_id (vSwitch2022-11-10-09-43-22_PG), check parameters", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify add a new vmxnet number adapter in vswitch timestamp number page to vm test vm task path home worker workspace ansible windows number ga lsi logic sas efi ansible vsphere gos validation common vm add network adapter yml number fatal localhost failed unable to find specified network name vlan id vswitch timestamp number page check parameters error message unable to find specified network name vlan id vswitch timestamp number page check parameters", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5746, "name": "log-6773", "raw": "2022-11-10 13:24:55,010 | Failed at Play [deploy_vm_bios_lsilogicsas_vmxnet3] ********\n2022-11-10 13:24:55,010 | TASK [deploy_vm_bios_lsilogicsas_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221110122128.log] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-10 13:25:49,010 | TASK [deploy_vm_bios_lsilogicsas_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_lsilogicsas_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_lsilogicsas_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm bios lsi logic sas vmxnet number timestamp task deploy vm bios lsi logic sas vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible sled lsi logic sas vmxnet number bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm bios lsi logic sas vmxnet number testing exit due to failure task path home worker workspace ansible sled lsi logic sas vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios lsi logic sas vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios lsi logic sas vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5747, "name": "log-6737", "raw": "2022-11-10 10:13:50,010 | Failed at Play [deploy_vmwarephoton_ova] *******************\n2022-11-10 10:13:50,010 | TASK [deploy_vmwarephoton_ova][Check VMware Tools is running and collects guest OS fullname successfully] \ntask path: /home/worker/workspace/Ansible_Photon_4.x_OVA_70U1/ansible-vsphere-gos-validation/common/vm_wait_guest_fullname.yml:46\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest OS fullname in 300 seconds. Current VMware Tools running status is 'guestToolsNotRunning', and guest OS fullname is ''.\nerror message:\nIt's timed out for VMware Tools collecting guest OS fullname in 300 seconds. Current VMware Tools running status is 'guestToolsNotRunning', and guest OS fullname is ''.\n2022-11-10 10:13:59,010 | TASK [deploy_vmwarephoton_ova][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Photon_4.x_OVA_70U1/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vmwarephoton_ova", "category": "", "processed": "timestamp failed at play deploy vmware photon ova timestamp task deploy vmware photon ova check vmware tools is running and collects guest os full name successfully task path home worker workspace ansible photon number x ova ansible vsphere gos validation common vm wait guest full name yml number fatal localhost failed it s timed out for vmware tools collecting guest os full name in number seconds current vmware tools running status is guest tools not running and guest os full name is error message it s timed out for vmware tools collecting guest os full name in number seconds current vmware tools running status is guest tools not running and guest os full name is timestamp task deploy vmware photon ova testing exit due to failure task path home worker workspace ansible photon number x ova ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vmware photon ova error message exit testing when exit testing when fail is set to true in test case deploy vmware photon ova", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5748, "name": "log-6720", "raw": "2022-11-10 04:03:54,010 | Failed at Play [deploy_ubuntu_ova] *************************\n2022-11-10 04:03:54,010 | TASK [deploy_ubuntu_ova][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_OVA_80GA/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:51\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address in 300 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '['fd01:0:106:6:250:56ff:fea6:26bf', 'fe80::250:56ff:fea6:26bf']'.\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 300 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '['fd01:0:106:6:250:56ff:fea6:26bf', 'fe80::250:56ff:fea6:26bf']'.\n2022-11-10 04:04:04,010 | TASK [deploy_ubuntu_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_OVA_80GA/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_ubuntu_ova", "category": "", "processed": "timestamp failed at play deploy ubuntu ova timestamp task deploy ubuntu ova check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible ubuntu number server ova number ga ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are ip address ip address error message it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are ip address ip address timestamp task deploy ubuntu ova testing exit due to failure task path home worker workspace ansible ubuntu number server ova number ga ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy ubuntu ova error message exit testing when exit testing when fail is set to true in test case deploy ubuntu ova", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5784, "name": "log-6813", "raw": "2022-11-10 14:34:05,010 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-10 14:34:05,010 | TASK [nvdimm_cold_add_remove][Execute powershell command 'diskpart /s C:\\test_diskpart\\diskpart.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:22\nfatal: [localhost -> 10.212.51.31]: FAILED! => non-zero return code when diskpart\nerror message:\nnon-zero return code: 1\nMicrosoft DiskPart version 10.0.22621.1\nCopyright (C) Microsoft Corporation.\nOn computer: DESKTOP-04OI9MK\nThe disk you specified is not valid.\nThere is no disk selected.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove execute powershell command diskpart s c test diskpart diskpart text task path home worker workspace ansible windows number ga ide vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when diskpart error message nonzero return code number microsoft diskpart version version id number copyright c microsoft corporation on computer desktop number oi number mk the disk you specified is not valid there is no disk selected", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5749, "name": "log-6732", "raw": "2022-11-10 07:32:12,010 | Failed at Play [deploy_vm_bios_sata_vmxnet3] ***************\n2022-11-10 07:32:12,010 | TASK [deploy_vm_bios_sata_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221110062923.log] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_80GA_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-10 07:32:42,010 | TASK [deploy_vm_bios_sata_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_80GA_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm bios sata vmxnet number timestamp task deploy vm bios sata vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible u os server number ga sata vmxnet number bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm bios sata vmxnet number testing exit due to failure task path home worker workspace ansible u os server number ga sata vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5750, "name": "log-6772", "raw": "2022-11-10 13:21:38,010 | Failed at Play [deploy_vm_bios_paravirtual_e1000e] *********\n2022-11-10 13:21:38,010 | TASK [deploy_vm_bios_paravirtual_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221110121848.log] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-10 13:22:25,010 | TASK [deploy_vm_bios_paravirtual_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_paravirtual_e1000e\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_paravirtual_e1000e", "category": "", "processed": "timestamp failed at play deploy vm bios para virtual timestamp task deploy vm bios para virtual wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible u os server number para virtual bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm bios para virtual testing exit due to failure task path home worker workspace ansible u os server number para virtual bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios para virtual e number e error message exit testing when exit testing when fail is set to true in test case deploy vm bios para virtual e number e", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5751, "name": "log-6733", "raw": "2022-11-10 07:42:54,010 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-11-10 07:42:54,010 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221110064024.log] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-10 07:43:27,010 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible u os server number main para virtual vmxnet number efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible u os server number main para virtual vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5752, "name": "log-6771", "raw": "2022-11-10 13:19:57,010 | Failed at Play [deploy_vm_bios_sata_vmxnet3] ***************\n2022-11-10 13:19:57,010 | TASK [deploy_vm_bios_sata_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221110121723.log] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U1_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-10 13:20:54,010 | TASK [deploy_vm_bios_sata_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U1_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm bios sata vmxnet number timestamp task deploy vm bios sata vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible u os server number sata vmxnet number bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm bios sata vmxnet number testing exit due to failure task path home worker workspace ansible u os server number sata vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5753, "name": "log-6730", "raw": "2022-11-10 06:38:47,010 | Failed at Play [deploy_vm_efi_nvme_vmxnet3] ****************\n2022-11-10 06:38:47,010 | TASK [deploy_vm_efi_nvme_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221110053519.log] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-10 06:39:52,010 | TASK [deploy_vm_efi_nvme_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_nvme_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_nvme_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi nvme vmxnet number timestamp task deploy vm efi nvme vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible u os server number nvme vmxnet number efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm efi nvme vmxnet number testing exit due to failure task path home worker workspace ansible u os server number nvme vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi nvme vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi nvme vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5754, "name": "log-6731", "raw": "2022-11-10 06:42:08,010 | Failed at Play [deploy_vm_bios_nvme_e1000e] ****************\n2022-11-10 06:42:08,010 | TASK [deploy_vm_bios_nvme_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221110053909.log] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-10 06:42:46,010 | TASK [deploy_vm_bios_nvme_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_nvme_e1000e\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_nvme_e1000e", "category": "", "processed": "timestamp failed at play deploy vm bios nvme timestamp task deploy vm bios nvme wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible u os server number main nvme bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm bios nvme testing exit due to failure task path home worker workspace ansible u os server number main nvme bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios nvme e number e error message exit testing when exit testing when fail is set to true in test case deploy vm bios nvme e number e", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5755, "name": "log-6849", "raw": "2022-11-10 16:50:46,010 | Failed at Play [wintools_uninstall_verify] *****************\n2022-11-10 16:50:46,010 | TASK [wintools_uninstall_verify][Check no problem device listed after VMware Tools uninstall] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_SATA_E1000E_EFI/ansible-vsphere-gos-validation/windows/wintools_uninstall_verify/verify_uninstall.yml:26\nfatal: [localhost]: FAILED! => Problem devices found in guest after VMware Tools uninstall, please check listed problem devices: [{'device_name': 'vmxnet3 Ethernet Adapter', 'problem_code': '32'}]\nerror message:\nProblem devices found in guest after VMware Tools uninstall, please check listed problem devices: [{'device_name': 'vmxnet3 Ethernet Adapter', 'problem_code': '32'}]", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify check no problem device listed after vmware tools un install task path home worker workspace ansible windows number sata efi ansible vsphere gos validation windows win tools un install verify verify un install yml number fatal localhost failed problem devices found in guest after vmware tools un install please check listed problem devices device name vmxnet number ethernet adapter problem code number error message problem devices found in guest after vmware tools un install please check listed problem devices device name vmxnet number ethernet adapter problem code number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5756, "name": "log-6742", "raw": "2022-11-10 10:43:48,010 | Failed at Play [deploy_vm_efi_ide_vmxnet3] *****************\n2022-11-10 10:43:48,010 | TASK [deploy_vm_efi_ide_vmxnet3][Wait for message 'Ubuntu autoinstall is started at 2022-11-10-10-19-31' appear in VM log serial-20221110102742.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70U1_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 150,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-10 10:43:58,010 | TASK [deploy_vm_efi_ide_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70U1_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi ide vmxnet number timestamp task deploy vm efi ide vmxnet number wait for message ubuntu auto install is started at timestamp number appear in vm log serial timestamp log task path home worker workspace ansible ubuntu number server iso ide vmxnet number efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm efi ide vmxnet number testing exit due to failure task path home worker workspace ansible ubuntu number server iso ide vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi ide vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi ide vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5757, "name": "log-6863", "raw": "2022-11-10 17:08:58,010 | Failed at Play [wintools_complete_install_verify] **********\n2022-11-10 17:08:58,010 | TASK [wintools_complete_install_verify][Execute powershell command 'get-service -Name VMTools | foreach {$_.Status}'] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:22\nfatal: [localhost -> 10.212.226.75]: FAILED! => non-zero return code when get-service\nerror message:\nnon-zero return code: 1\nget-service : Cannot find any service with service name 'VMTools'.\nAt line:1 char:65\n+ ... ew-Object Text.UTF8Encoding $false; get-service -Name VMTools | forea ...\n+                                         ~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (VMTools:String) [Get-Service], ServiceCommandException\n    + FullyQualifiedErrorId : NoServiceFoundForGivenName,Microsoft.PowerShell.Commands.GetServiceCommand", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify execute powershell command get service name vm tools for each status task path home worker workspace ansible windows number nvme vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when get service error message nonzero return code number get service can not find any service with service name vm tools at line number char number ew object text utf number encoding false get service name vm tools fore a category info object not found vm tools string get service service command exception fully qualified error id no service found forgiven name microsoft powershell commands get service command", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5758, "name": "log-6745", "raw": "2022-11-10 10:47:06,010 | Failed at Play [deploy_vm_bios_lsilogicsas_vmxnet3] ********\n2022-11-10 10:47:06,010 | TASK [deploy_vm_bios_lsilogicsas_vmxnet3][Wait for message 'Ubuntu autoinstall is started at 2022-11-10-10-20-14' appear in VM log serial-20221110103048.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 150,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-10 10:47:18,010 | TASK [deploy_vm_bios_lsilogicsas_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.10_Server_ISO_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_lsilogicsas_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_lsilogicsas_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm bios lsi logic sas vmxnet number timestamp task deploy vm bios lsi logic sas vmxnet number wait for message ubuntu auto install is started at timestamp number appear in vm log serial timestamp log task path home worker workspace ansible ubuntu number server iso lsi logic sas vmxnet number bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm bios lsi logic sas vmxnet number testing exit due to failure task path home worker workspace ansible ubuntu number server iso lsi logic sas vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios lsi logic sas vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios lsi logic sas vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5759, "name": "log-6867", "raw": "2022-11-10 17:13:12,010 | Failed at Play [check_os_fullname] *************************\n2022-11-10 17:13:12,010 | TASK [check_os_fullname][Skip testcase: check_os_fullname, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'check_os_fullname' is blocked because VMware tools installed: False, running: False\nerror message:\nTest case 'check_os_fullname' is blocked because VMware tools installed: False, running: False", "category": "", "processed": "timestamp failed at play check os full name timestamp task check os full name skip test case check os full name reason blocked task path home worker workspace ansible windows number nvme vmxnet number efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case check os full name is blocked because vmware tools installed false running false error message test case check os full name is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202212132000}, {"id": 5760, "name": "log-6750", "raw": "2022-11-10 09:45:48,010 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-10 09:45:48,010 | TASK [nvdimm_cold_add_remove][Execute powershell command 'diskpart /s C:\\test_diskpart\\diskpart.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:22\nfatal: [localhost -> 10.212.226.75]: FAILED! => non-zero return code when diskpart\nerror message:\nnon-zero return code: 1\nMicrosoft DiskPart version 10.0.22621.1\nCopyright (C) Microsoft Corporation.\nOn computer: DESKTOP-JRSIORM\nThe disk you specified is not valid.\nThere is no disk selected.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove execute powershell command diskpart s c test diskpart diskpart text task path home worker workspace ansible windows number nvme vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when diskpart error message nonzero return code number microsoft diskpart version version id number copyright c microsoft corporation on computer desktop jrsiorm the disk you specified is not valid there is no disk selected", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5761, "name": "log-6933", "raw": "2022-11-11 08:51:28,011 | Failed at Play [deploy_vm_bios_ide_vmxnet3] ****************\n2022-11-11 08:51:28,011 | TASK [deploy_vm_bios_ide_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221111074842.log] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-11 08:51:58,011 | TASK [deploy_vm_bios_ide_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm bios ide vmxnet number timestamp task deploy vm bios ide vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible u os server number ga ide vmxnet number bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm bios ide vmxnet number testing exit due to failure task path home worker workspace ansible u os server number ga ide vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios ide vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios ide vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5762, "name": "log-6757", "raw": "2022-11-10 10:57:24,010 | Failed at Play [deploy_vm_efi_sata_e1000e] *****************\n2022-11-10 10:57:24,010 | TASK [deploy_vm_efi_sata_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221110095354.log] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-10 10:58:17,010 | TASK [deploy_vm_efi_sata_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_e1000e\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi sata timestamp task deploy vm efi sata wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible u os server number ga sata efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm efi sata testing exit due to failure task path home worker workspace ansible u os server number ga sata efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi sata e number e error message exit testing when exit testing when fail is set to true in test case deploy vm efi sata e number e", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5763, "name": "log-6853", "raw": "2022-11-10 16:52:51,010 | Failed at Play [wintools_uninstall_verify] *****************\n2022-11-10 16:52:51,010 | TASK [wintools_uninstall_verify][Check no problem device listed after VMware Tools uninstall] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/windows/wintools_uninstall_verify/verify_uninstall.yml:26\nfatal: [localhost]: FAILED! => Problem devices found in guest after VMware Tools uninstall, please check listed problem devices: [{'device_name': 'vmxnet3 Ethernet Adapter', 'problem_code': '32'}]\nerror message:\nProblem devices found in guest after VMware Tools uninstall, please check listed problem devices: [{'device_name': 'vmxnet3 Ethernet Adapter', 'problem_code': '32'}]", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify check no problem device listed after vmware tools un install task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation windows win tools un install verify verify un install yml number fatal localhost failed problem devices found in guest after vmware tools un install please check listed problem devices device name vmxnet number ethernet adapter problem code number error message problem devices found in guest after vmware tools un install please check listed problem devices device name vmxnet number ethernet adapter problem code number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5764, "name": "log-6995", "raw": "2022-11-11 14:59:48,011 | Failed at Play [wintools_uninstall_verify] *****************\n2022-11-11 14:59:48,011 | TASK [wintools_uninstall_verify][Check no problem device listed after VMware Tools uninstall] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/windows/wintools_uninstall_verify/verify_uninstall.yml:26\nfatal: [localhost]: FAILED! => Problem devices found in guest after VMware Tools uninstall, please check listed problem devices: [{'device_name': 'vmxnet3 Ethernet Adapter', 'problem_code': '32'}]\nerror message:\nProblem devices found in guest after VMware Tools uninstall, please check listed problem devices: [{'device_name': 'vmxnet3 Ethernet Adapter', 'problem_code': '32'}]", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify check no problem device listed after vmware tools un install task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation windows win tools un install verify verify un install yml number fatal localhost failed problem devices found in guest after vmware tools un install please check listed problem devices device name vmxnet number ethernet adapter problem code number error message problem devices found in guest after vmware tools un install please check listed problem devices device name vmxnet number ethernet adapter problem code number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5765, "name": "log-6845", "raw": "2022-11-10 16:47:42,010 | Failed at Play [wintools_uninstall_verify] *****************\n2022-11-10 16:47:42,010 | TASK [wintools_uninstall_verify][Check no problem device listed after VMware Tools uninstall] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/wintools_uninstall_verify/verify_uninstall.yml:26\nfatal: [localhost]: FAILED! => Problem devices found in guest after VMware Tools uninstall, please check listed problem devices: [{'device_name': 'vmxnet3 Ethernet Adapter', 'problem_code': '32'}]\nerror message:\nProblem devices found in guest after VMware Tools uninstall, please check listed problem devices: [{'device_name': 'vmxnet3 Ethernet Adapter', 'problem_code': '32'}]", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify check no problem device listed after vmware tools un install task path home worker workspace ansible windows number ga lsi logic sas efi ansible vsphere gos validation windows win tools un install verify verify un install yml number fatal localhost failed problem devices found in guest after vmware tools un install please check listed problem devices device name vmxnet number ethernet adapter problem code number error message problem devices found in guest after vmware tools un install please check listed problem devices device name vmxnet number ethernet adapter problem code number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5790, "name": "log-6894", "raw": "2022-11-11 00:13:10,011 | Failed at Play [deploy_vm_bios_ide_vmxnet3] ****************\n2022-11-11 00:13:10,011 | TASK [deploy_vm_bios_ide_vmxnet3][Add IDE boot disk] *******\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_create_with_ide_disk.yml:49\nfatal: [localhost -> 10.185.77.82]: FAILED! => non-zero return code when vim-cmd\nerror message:\nnon-zero return code: 1\nReconfigure failed: (vim.fault.NoDiskSpace) {\n   faultCause = (vmodl.MethodFault) null, \n   faultMessage = (vmodl.LocalizableMessage) [\n      (vmodl.LocalizableMessage) {\n         key = \"vob.fssvec.SetFileAttributes.file.failed\", \n         arg = <unset>, \n         message = \"File system specific implementation of SetFileAttributes[file] failed\"\n      }\n   ], \n   file = \"[datastore2] test_vm_1668125531232/test_vm_1668125531232_1.vmdk\", \n   datastore = \"datastore2\"\n   msg = \"Insufficient disk space on datastore 'datastore2'.\"\n}\n2022-11-11 00:13:13,011 | TASK [deploy_vm_bios_ide_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm bios ide vmxnet number timestamp task deploy vm bios ide vmxnet number add ide boot disk task path home worker workspace ansible u os server number ga ide vmxnet number bios ansible vsphere gos validation common vm create with ide disk yml number fatal localhost ip address failed nonzero return code when vim command error message nonzero return code number re configure failed vim fault no disk space fault cause vmodl method fault null fault message vmodl localizable message vmodl localizable message key vob fss vec set file attributes file failed arg un set message file system specific implementation of set file attributes file failed file data store number test vm number test vm number vmdk data store data store number message insufficient disk space on data store data store number timestamp task deploy vm bios ide vmxnet number testing exit due to failure task path home worker workspace ansible u os server number ga ide vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios ide vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios ide vmxnet number", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5766, "name": "log-6917", "raw": "2022-11-11 01:37:58,011 | Failed at Play [deploy_vm_bios_ide_vmxnet3] ****************\n2022-11-11 01:37:58,011 | TASK [deploy_vm_bios_ide_vmxnet3][Wait for port 22 to become open or contain specific keyword] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_ssh.yml:19\nfatal: [localhost]: FAILED! => Timeout when waiting for search string OpenSSH in 10.185.65.0:22\nerror message:\nTimeout when waiting for search string OpenSSH in 10.185.65.0:22\n2022-11-11 01:38:47,011 | TASK [deploy_vm_bios_ide_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm bios ide vmxnet number timestamp task deploy vm bios ide vmxnet number wait for port number to become open or contain specific keyword task path home worker workspace ansible u os server number ga ide vmxnet number bios ansible vsphere gos validation common vm wait ssh yml number fatal localhost failed timeout when waiting for search string openssh in ip address error message timeout when waiting for search string openssh in ip address timestamp task deploy vm bios ide vmxnet number testing exit due to failure task path home worker workspace ansible u os server number ga ide vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios ide vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios ide vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5767, "name": "log-6895", "raw": "2022-11-11 00:14:38,011 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-11-11 00:14:38,011 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_80GA_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc2-10-186-232-100, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\", \"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM hostname is sc2-10-186-232-100, expected hostname is gosc-dhcp-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\nVM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible u os server number ga sata vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com error message vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5768, "name": "log-6896", "raw": "2022-11-11 00:21:01,011 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-11 00:21:01,011 | TASK [gosc_perl_staticip][GOS customization failed] ********\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_80GA_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc2-10-186-232-100, expected hostname is gosc-static-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', 'VM hwclockUTC is True, expected hwclockUTC is False']\nerror message:\nVM hostname is sc2-10-186-232-100, expected hostname is gosc-static-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM hwclockUTC is True, expected hwclockUTC is False", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip gos customization failed task path home worker workspace ansible u os server number ga sata vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false error message vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5769, "name": "log-6897", "raw": "2022-11-11 00:15:36,011 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-11-11 00:15:36,011 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc1-10-78-143-20, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\", \"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM hostname is sc1-10-78-143-20, expected hostname is gosc-dhcp-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\nVM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible u os server number main nvme bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com error message vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5770, "name": "log-6898", "raw": "2022-11-11 00:22:32,011 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-11 00:22:32,011 | TASK [gosc_perl_staticip][GOS customization failed] ********\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc1-10-78-143-20, expected hostname is gosc-static-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', 'VM hwclockUTC is True, expected hwclockUTC is False']\nerror message:\nVM hostname is sc1-10-78-143-20, expected hostname is gosc-static-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM hwclockUTC is True, expected hwclockUTC is False", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip gos customization failed task path home worker workspace ansible u os server number main nvme bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false error message vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5771, "name": "log-6903", "raw": "2022-11-11 00:40:07,011 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-11-11 00:40:07,011 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc1-10-182-183-224, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\", \"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM hostname is sc1-10-182-183-224, expected hostname is gosc-dhcp-vm-01\nVM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\nVM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible u os server number ide efi ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com error message vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5772, "name": "log-6907", "raw": "2022-11-11 00:46:39,011 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-11 00:46:39,011 | TASK [gosc_perl_staticip][GOS customization failed] ********\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc1-10-182-183-224, expected hostname is gosc-static-vm-01', 'VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['192.168.192.1'], not expected DNS servers ['192.168.1.1', '192.168.1.2']\", \"VM DNS domain search domains are  ['lan', 'eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\", 'VM hwclockUTC is True, expected hwclockUTC is False']\nerror message:\nVM hostname is sc1-10-182-183-224, expected hostname is gosc-static-vm-01\nVM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['192.168.192.1'], not expected DNS servers ['192.168.1.1', '192.168.1.2']\nVM DNS domain search domains are  ['lan', 'eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\nVM hwclockUTC is True, expected hwclockUTC is False", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip gos customization failed task path home worker workspace ansible u os server number ide efi ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address not expected dns servers ip address ip address vm dns domain search domains are lan eng vmware com not expected search domains test com go sc test com vm hw clock utc is true expected hw clock utc is false error message vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address not expected dns servers ip address ip address vm dns domain search domains are lan eng vmware com not expected search domains test com go sc test com vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5773, "name": "log-6905", "raw": "2022-11-11 00:42:05,011 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-11-11 00:42:05,011 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_80GA_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc1-10-168-179-193, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\", \"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM hostname is sc1-10-168-179-193, expected hostname is gosc-dhcp-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\nVM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible u os server number ga para virtual efi ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com error message vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5796, "name": "log-6944", "raw": "2022-11-11 07:51:32,011 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-11 07:51:32,011 | TASK [nvdimm_cold_add_remove][Execute powershell command 'diskpart /s C:\\test_diskpart\\diskpart.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U1_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:22\nfatal: [localhost -> 10.182.130.56]: FAILED! => non-zero return code when diskpart\nerror message:\nnon-zero return code: 1\nMicrosoft DiskPart version 10.0.22621.1\nCopyright (C) Microsoft Corporation.\nOn computer: DESKTOP-R3LSPN2\nThe disk you specified is not valid.\nThere is no disk selected.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove execute powershell command diskpart s c test diskpart diskpart text task path home worker workspace ansible windows number nvme vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when diskpart error message nonzero return code number microsoft diskpart version version id number copyright c microsoft corporation on computer desktop r number lsp n number the disk you specified is not valid there is no disk selected", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5774, "name": "log-6909", "raw": "2022-11-11 00:47:52,011 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-11 00:47:52,011 | TASK [gosc_perl_staticip][GOS customization failed] ********\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_80GA_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc1-10-168-179-193, expected hostname is gosc-static-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', 'VM hwclockUTC is True, expected hwclockUTC is False']\nerror message:\nVM hostname is sc1-10-168-179-193, expected hostname is gosc-static-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM hwclockUTC is True, expected hwclockUTC is False", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip gos customization failed task path home worker workspace ansible u os server number ga para virtual efi ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false error message vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5775, "name": "log-6908", "raw": "2022-11-11 00:43:15,011 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-11-11 00:43:15,011 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc1-10-78-117-66, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\", \"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM hostname is sc1-10-78-117-66, expected hostname is gosc-dhcp-vm-01\nVM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\nVM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible u os server number main para virtual vmxnet number efi ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com error message vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5776, "name": "log-6911", "raw": "2022-11-11 00:49:17,011 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-11 00:49:17,011 | TASK [gosc_perl_staticip][GOS customization failed] ********\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc1-10-78-117-66, expected hostname is gosc-static-vm-01', 'VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com', 'VM hwclockUTC is True, expected hwclockUTC is False']\nerror message:\nVM hostname is sc1-10-78-117-66, expected hostname is gosc-static-vm-01\nVM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com\nVM hwclockUTC is True, expected hwclockUTC is False", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip gos customization failed task path home worker workspace ansible u os server number main para virtual vmxnet number efi ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false error message vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5777, "name": "log-6790", "raw": "2022-11-10 13:45:21,010 | Failed at Play [nvdimm_cold_add_remove] ********************\n2022-11-10 13:45:21,010 | TASK [nvdimm_cold_add_remove][Execute powershell command 'diskpart /s C:\\test_diskpart\\diskpart.txt'] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:22\nfatal: [localhost -> 10.182.48.75]: FAILED! => non-zero return code when diskpart\nerror message:\nnon-zero return code: 1\nMicrosoft DiskPart version 10.0.22621.1\nCopyright (C) Microsoft Corporation.\nOn computer: DESKTOP-2TLDQVR\nThe disk you specified is not valid.\nThere is no disk selected.", "category": "", "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove execute powershell command diskpart s c test diskpart diskpart text task path home worker workspace ansible windows number ga lsi logic sas efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when diskpart error message nonzero return code number microsoft diskpart version version id number copyright c microsoft corporation on computer desktop number tldqvr the disk you specified is not valid there is no disk selected", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5778, "name": "log-6910", "raw": "2022-11-11 00:43:07,011 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-11-11 00:43:07,011 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc2-10-212-28-192, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\", \"VM DNS domain search domains are  ['eng.vmware.com', 'vmware.com', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM hostname is sc2-10-212-28-192, expected hostname is gosc-dhcp-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\nVM DNS domain search domains are  ['eng.vmware.com', 'vmware.com', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible u os server number ga sata efi ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are eng vmware com vmware com nimbus eng vmware com not expected search domains test com go sc test com error message vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are eng vmware com vmware com nimbus eng vmware com not expected search domains test com go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5779, "name": "log-6913", "raw": "2022-11-11 00:49:52,011 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-11 00:49:52,011 | TASK [gosc_perl_staticip][GOS customization failed] ********\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc2-10-212-28-192, expected hostname is gosc-static-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['192.168.192.1'], not expected DNS servers ['192.168.1.1', '192.168.1.2']\", \"VM DNS domain search domains are  ['lan', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\", 'VM hwclockUTC is True, expected hwclockUTC is False']\nerror message:\nVM hostname is sc2-10-212-28-192, expected hostname is gosc-static-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['192.168.192.1'], not expected DNS servers ['192.168.1.1', '192.168.1.2']\nVM DNS domain search domains are  ['lan', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\nVM hwclockUTC is True, expected hwclockUTC is False", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip gos customization failed task path home worker workspace ansible u os server number ga sata efi ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address not expected dns servers ip address ip address vm dns domain search domains are lan nimbus eng vmware com not expected search domains test com go sc test com vm hw clock utc is true expected hw clock utc is false error message vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address not expected dns servers ip address ip address vm dns domain search domains are lan nimbus eng vmware com not expected search domains test com go sc test com vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5780, "name": "log-6914", "raw": "2022-11-11 00:47:36,011 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-11-11 00:47:36,011 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc2-10-43-250-189, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\", \"VM DNS domain search domains are  ['eng.vmware.com', 'vmware.com', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM hostname is sc2-10-43-250-189, expected hostname is gosc-dhcp-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\nVM DNS domain search domains are  ['eng.vmware.com', 'vmware.com', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible u os server number nvme vmxnet number efi ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are eng vmware com vmware com nimbus eng vmware com not expected search domains test com go sc test com error message vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are eng vmware com vmware com nimbus eng vmware com not expected search domains test com go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5781, "name": "log-6915", "raw": "2022-11-11 00:55:21,011 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-11 00:55:21,011 | TASK [gosc_perl_staticip][GOS customization failed] ********\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc2-10-43-250-189, expected hostname is gosc-static-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', 'VM hwclockUTC is True, expected hwclockUTC is False']\nerror message:\nVM hostname is sc2-10-43-250-189, expected hostname is gosc-static-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM hwclockUTC is True, expected hwclockUTC is False", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip gos customization failed task path home worker workspace ansible u os server number nvme vmxnet number efi ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false error message vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5782, "name": "log-6856", "raw": "2022-11-10 17:08:16,010 | Failed at Play [wintools_complete_install_verify] **********\n2022-11-10 17:08:16,010 | TASK [wintools_complete_install_verify][Execute powershell command 'get-service -Name VMTools | foreach {$_.Status}'] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:22\nfatal: [localhost -> 10.212.51.31]: FAILED! => non-zero return code when get-service\nerror message:\nnon-zero return code: 1\nget-service : Cannot find any service with service name 'VMTools'.\nAt line:1 char:65\n+ ... ew-Object Text.UTF8Encoding $false; get-service -Name VMTools | forea ...\n+                                         ~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (VMTools:String) [Get-Service], ServiceCommandException\n    + FullyQualifiedErrorId : NoServiceFoundForGivenName,Microsoft.PowerShell.Commands.GetServiceCommand", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify execute powershell command get service name vm tools for each status task path home worker workspace ansible windows number ga ide vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when get service error message nonzero return code number get service can not find any service with service name vm tools at line number char number ew object text utf number encoding false get service name vm tools fore a category info object not found vm tools string get service service command exception fully qualified error id no service found forgiven name microsoft powershell commands get service command", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5783, "name": "log-6949", "raw": "2022-11-11 09:39:54,011 | Failed at Play [check_os_fullname] *************************\n2022-11-11 09:39:54,011 | TASK [check_os_fullname][Skip testcase: check_os_fullname, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'check_os_fullname' is blocked because VMware tools installed: False, running: False\nerror message:\nTest case 'check_os_fullname' is blocked because VMware tools installed: False, running: False", "category": "", "processed": "timestamp failed at play check os full name timestamp task check os full name skip test case check os full name reason blocked task path home worker workspace ansible windows number ga ide vmxnet number efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case check os full name is blocked because vmware tools installed false running false error message test case check os full name is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202212132000}, {"id": 5785, "name": "log-6899", "raw": "2022-11-11 00:15:50,011 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-11-11 00:15:50,011 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U1_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc2-10-184-81-13, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\", \"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM hostname is sc2-10-184-81-13, expected hostname is gosc-dhcp-vm-01\nVM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\nVM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible u os server number sata vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com error message vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5786, "name": "log-6900", "raw": "2022-11-11 00:24:29,011 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-11 00:24:29,011 | TASK [gosc_perl_staticip][GOS customization failed] ********\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U1_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc2-10-184-81-13, expected hostname is gosc-static-vm-01', 'VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com', 'VM hwclockUTC is True, expected hwclockUTC is False']\nerror message:\nVM hostname is sc2-10-184-81-13, expected hostname is gosc-static-vm-01\nVM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com\nVM hwclockUTC is True, expected hwclockUTC is False", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip gos customization failed task path home worker workspace ansible u os server number sata vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false error message vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5787, "name": "log-6820", "raw": "2022-11-10 12:27:28,010 | Failed at Play [wintools_complete_install_verify] **********\n2022-11-10 12:27:28,010 | TASK [wintools_complete_install_verify][Execute powershell command 'get-service -Name VMTools | foreach {$_.Status}'] \ntask path: /home/worker/workspace/Ansible_Windows_11_80GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/windows/utils/win_execute_cmd.yml:22\nfatal: [localhost -> 10.212.127.102]: FAILED! => non-zero return code when get-service\nerror message:\nnon-zero return code: 1\nget-service : Cannot find any service with service name 'VMTools'.\nAt line:1 char:65\n+ ... ew-Object Text.UTF8Encoding $false; get-service -Name VMTools | forea ...\n+                                         ~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (VMTools:String) [Get-Service], ServiceCommandException\n    + FullyQualifiedErrorId : NoServiceFoundForGivenName,Microsoft.PowerShell.Commands.GetServiceCommand", "category": "", "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify execute powershell command get service name vm tools for each status task path home worker workspace ansible windows number ga para virtual vmxnet number efi ansible vsphere gos validation windows utilities win execute command yml number fatal localhost ip address failed nonzero return code when get service error message nonzero return code number get service can not find any service with service name vm tools at line number char number ew object text utf number encoding false get service name vm tools fore a category info object not found vm tools string get service service command exception fully qualified error id no service found forgiven name microsoft powershell commands get service command", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5788, "name": "log-6822", "raw": "2022-11-10 12:34:04,010 | Failed at Play [check_os_fullname] *************************\n2022-11-10 12:34:04,010 | TASK [check_os_fullname][Skip testcase: check_os_fullname, reason: Blocked] \ntask path: /home/worker/workspace/Ansible_Windows_11_80GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/skip_test_case.yml:20\nfatal: [localhost]: FAILED! => Test case 'check_os_fullname' is blocked because VMware tools installed: False, running: False\nerror message:\nTest case 'check_os_fullname' is blocked because VMware tools installed: False, running: False", "category": "", "processed": "timestamp failed at play check os full name timestamp task check os full name skip test case check os full name reason blocked task path home worker workspace ansible windows number ga para virtual vmxnet number efi ansible vsphere gos validation common skip test case yml number fatal localhost failed test case check os full name is blocked because vmware tools installed false running false error message test case check os full name is blocked because vmware tools installed false running false", "solution": "deepdive", "target": "usererror", "version": 202212132000}, {"id": 5789, "name": "log-6892", "raw": "2022-11-11 00:01:03,011 | Failed at Play [env_setup] *********************************\n2022-11-11 00:01:03,011 | TASK [env_setup][Get all registerd VMs and templates on 10.168.217.2] \ntask path: /home/worker/workspace/qiz_test_fedora/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.168.217.2:443 : [Errno 113] No route to host\nerror message:\nUnknown error while connecting to vCenter or ESXi API at 10.168.217.2:443 : [Errno 113] No route to host", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on ip address task path home worker workspace qiz test fedora ansible vsphere gos validation common vm check exist yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number no route to host error message unknown error while connecting to vcenter or esxi api at ip address error number no route to host", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5791, "name": "log-6918", "raw": "2022-11-11 01:58:22,011 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-11-11 01:58:22,011 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc2-10-212-42-157, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\", \"VM DNS domain search domains are  ['eng.vmware.com', 'vmware.com', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM hostname is sc2-10-212-42-157, expected hostname is gosc-dhcp-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['10.172.40.1', '10.172.40.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\nVM DNS domain search domains are  ['eng.vmware.com', 'vmware.com', 'nimbus.eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible u os server number para virtual bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are eng vmware com vmware com nimbus eng vmware com not expected search domains test com go sc test com error message vm hostname is sc number expected hostname is go sc dhcp vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are eng vmware com vmware com nimbus eng vmware com not expected search domains test com go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5792, "name": "log-6919", "raw": "2022-11-11 02:04:58,011 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-11 02:04:58,011 | TASK [gosc_perl_staticip][GOS customization failed] ********\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc2-10-212-42-157, expected hostname is gosc-static-vm-01', 'VM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com', 'VM hwclockUTC is True, expected hwclockUTC is False']\nerror message:\nVM hostname is sc2-10-212-42-157, expected hostname is gosc-static-vm-01\nVM DNS domain name is nimbus.eng.vmware.com, expected domain name is gosc.test.com\nVM hwclockUTC is True, expected hwclockUTC is False", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip gos customization failed task path home worker workspace ansible u os server number para virtual bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false error message vm hostname is sc number expected hostname is go sc static vm number vm dns domain name is nimbus eng vmware com expected domain name is go sc test com vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5793, "name": "log-6929", "raw": "2022-11-11 08:40:41,011 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-11-11 08:40:41,011 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update_ISO/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:51\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '['fe80::250:56ff:fe93:bd46']'.\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '['fe80::250:56ff:fe93:bd46']'.\n2022-11-11 08:41:28,011 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update_ISO/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible cycle photon number x update iso ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are ip address error message it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are ip address timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible cycle photon number x update iso ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5794, "name": "log-6936", "raw": "2022-11-11 07:42:03,011 | Failed at Play [vmxnet3_network_device_ops] ****************\n2022-11-11 07:42:03,011 | TASK [vmxnet3_network_device_ops][Revert snapshot failed] **\ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:47\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed\nerror message:\nRevert to snapshot 'BaseSnapshot' failed", "category": "", "processed": "timestamp failed at play vmxnet number network device ops timestamp task vmxnet number network device ops revert snapshot failed task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed error message revert to snapshot base snapshot failed", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5795, "name": "log-6939", "raw": "2022-11-11 08:07:52,011 | Failed at Play [cpu_multicores_per_socket] *****************\n2022-11-11 08:07:52,011 | TASK [cpu_multicores_per_socket][Get ESXi host specified property] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/common/esxi_get_property.yml:8\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.184.105.237:443 : [Errno 110] Connection timed out\nerror message:\nUnknown error while connecting to vCenter or ESXi API at 10.184.105.237:443 : [Errno 110] Connection timed out", "category": "", "processed": "timestamp failed at play cpu multi cores per socket timestamp task cpu multi cores per socket get esxi host specified property task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation common esxi get property yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out error message unknown error while connecting to vcenter or esxi api at ip address error number connection timed out", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5797, "name": "log-6959", "raw": "2022-11-11 10:50:20,011 | Failed at Play [env_setup] *********************************\n2022-11-11 10:50:20,011 | TASK [env_setup][Get all registerd VMs and templates on 10.184.105.237] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nexception in /vmware.py when connect_to_api in /socket.py when create_connection\nfatal: [localhost]: FAILED! => Unknown error while connecting to vCenter or ESXi API at 10.184.105.237:443 : [Errno 110] Connection timed out\nerror message:\nUnknown error while connecting to vCenter or ESXi API at 10.184.105.237:443 : [Errno 110] Connection timed out", "category": "", "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on ip address task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation common vm check exist yml number exception in vmware python when connect to api in socket python when create connection fatal localhost failed unknown error while connecting to vcenter or esxi api at ip address error number connection timed out error message unknown error while connecting to vcenter or esxi api at ip address error number connection timed out", "solution": "retry", "target": "nimbus", "version": 202212132000}, {"id": 5798, "name": "log-6946", "raw": "2022-11-11 09:45:41,011 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2022-11-11 09:45:41,011 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update_ISO/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:51\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is '' and guest IP addresses are ''.\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is '' and guest IP addresses are ''.\n2022-11-11 09:46:39,011 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Cycle_Photon_3.x_Update_ISO/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible cycle photon number x update iso ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is and guest ip addresses are error message it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is and guest ip addresses are timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible cycle photon number x update iso ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5799, "name": "log-6960", "raw": "2022-11-11 10:33:50,011 | Failed at Play [gosc_perl_dhcp] ****************************\n2022-11-11 10:33:50,011 | TASK [gosc_perl_dhcp][GOS customization failed] ************\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc-rdops-vm14-dhcp-161-159, expected hostname is gosc-dhcp-vm-01', 'VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\", \"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM hostname is sc-rdops-vm14-dhcp-161-159, expected hostname is gosc-dhcp-vm-01\nVM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['10.166.1.1', '10.166.1.2', '10.142.7.2'], not expected DNS servers ['10.10.1.1', '10.10.1.2']\nVM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": "", "processed": "timestamp failed at play go sc perl dhcp timestamp task go sc perl dhcp gos customization failed task path home worker workspace ansible u os server number ga ide vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is scr do ps dhcp number expected hostname is go sc dhcp vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com error message vm hostname is scr do ps dhcp number expected hostname is go sc dhcp vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address ip address ip address not expected dns servers ip address ip address vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com not expected search domains test com go sc test com", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5800, "name": "log-6961", "raw": "2022-11-11 10:38:57,011 | Failed at Play [gosc_perl_staticip] ************************\n2022-11-11 10:38:57,011 | TASK [gosc_perl_staticip][GOS customization failed] ********\ntask path: /home/worker/workspace/Ansible_UOS_Server_20_70GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['VM hostname is sc-rdops-vm14-dhcp-161-159, expected hostname is gosc-static-vm-01', 'VM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com', \"VM DNS servers are ['192.168.192.1'], not expected DNS servers ['192.168.1.1', '192.168.1.2']\", \"VM DNS domain search domains are  ['lan', 'eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\", 'VM hwclockUTC is True, expected hwclockUTC is False']\nerror message:\nVM hostname is sc-rdops-vm14-dhcp-161-159, expected hostname is gosc-static-vm-01\nVM DNS domain name is eng.vmware.com, expected domain name is gosc.test.com\nVM DNS servers are ['192.168.192.1'], not expected DNS servers ['192.168.1.1', '192.168.1.2']\nVM DNS domain search domains are  ['lan', 'eng.vmware.com'] not expected search domains ['test.com', 'gosc.test.com']\nVM hwclockUTC is True, expected hwclockUTC is False", "category": "", "processed": "timestamp failed at play go sc perl static ip timestamp task go sc perl static ip gos customization failed task path home worker workspace ansible u os server number ga ide vmxnet number bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm hostname is scr do ps dhcp number expected hostname is go sc static vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address not expected dns servers ip address ip address vm dns domain search domains are lan eng vmware com not expected search domains test com go sc test com vm hw clock utc is true expected hw clock utc is false error message vm hostname is scr do ps dhcp number expected hostname is go sc static vm number vm dns domain name is eng vmware com expected domain name is go sc test com vm dns servers are ip address not expected dns servers ip address ip address vm dns domain search domains are lan eng vmware com not expected search domains test com go sc test com vm hw clock utc is true expected hw clock utc is false", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5801, "name": "log-6993", "raw": "2022-11-11 14:36:02,011 | Failed at Play [wintools_uninstall_verify] *****************\n2022-11-11 14:36:02,011 | TASK [wintools_uninstall_verify][Add a new 'vmxnet3' adapter in 'vSwitch2022-11-11-14-35-47_PG' to VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Windows_11_70GA_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_add_network_adapter.yml:13\nfatal: [localhost]: FAILED! => unable to find specified network_name/vlan_id (vSwitch2022-11-11-14-35-47_PG), check parameters\nerror message:\nunable to find specified network_name/vlan_id (vSwitch2022-11-11-14-35-47_PG), check parameters", "category": "", "processed": "timestamp failed at play win tools un install verify timestamp task win tools un install verify add a new vmxnet number adapter in vswitch timestamp number page to vm test vm task path home worker workspace ansible windows number ga ide vmxnet number efi ansible vsphere gos validation common vm add network adapter yml number fatal localhost failed unable to find specified network name vlan id vswitch timestamp number page check parameters error message unable to find specified network name vlan id vswitch timestamp number page check parameters", "solution": "deepdive", "target": "testbed", "version": 202212132000}, {"id": 5802, "name": "log-6997", "raw": "2022-11-11 14:41:49,011 | Failed at Play [nvme_vhba_device_ops_spec13] ***************\n2022-11-11 14:41:49,011 | TASK [nvme_vhba_device_ops_spec13][Hot add or remove VM disk controller] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/common/vm_hot_add_remove_disk_ctrl.yml:12\nexception in /vmware_guest_controller.py when configure_disk_controllers in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => ('The guest operating system did not respond to a hot-remove request for device nvme0 in a timely manner.', None)\nerror message:\n('The guest operating system did not respond to a hot-remove request for device nvme0 in a timely manner.', None)", "category": "", "processed": "timestamp failed at play nvme v hba device ops spec number timestamp task nvme v hba device ops spec number hot add or remove vm disk controller task path home worker workspace ansible cycle windows server number ansible vsphere gos validation common vm hot add remove disk ctrl yml number exception in vmware guest controller python when configure disk controllers in vmware python when wait for task fatal localhost failed the guest operating system did not respond to a hot remove request for device nvme number in a timely manner none error message the guest operating system did not respond to a hot remove request for device nvme number in a timely manner none", "solution": "retry", "target": "targetvm", "version": 202212132000}, {"id": 5803, "name": "log-7042", "raw": "2022-11-15 03:52:26,015 | Failed at Play [check_inbox_driver] ************************\n2022-11-15 03:52:26,015 | TASK [check_inbox_driver][Collect filtered guest information for '10.117.16.51'] \ntask path: /home/worker/workspace/lli_Pardus_21.x_64bit_70U1_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/get_system_info.yml:24\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.117.16.51' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.117.16.51: Permission denied (publickey,password).\nerror message:\nInvalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.117.16.51' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.117.16.51: Permission denied (publickey,password).", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver collect filtered guest information for ip address task path home worker workspace ll i pard us number x number b it para virtual vmxnet number efi ansible vsphere gos validation common get system info yml number fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key password error message invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key password", "solution": "retry", "target": "targetvm", "version": 202212132000}, {"id": 5804, "name": "log-7049", "raw": "2022-11-15 07:14:19,015 | Failed at Play [check_inbox_driver] ************************\n2022-11-15 07:14:19,015 | TASK [check_inbox_driver][Dump inbox drivers versions] *****\ntask path: /home/worker/workspace/lli_Pardus_21.x_64bit_70U1_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/check_inbox_driver/check_inbox_driver.yml:80\nfatal: [localhost]: FAILED! => Destination directory /home/worker/workspace/lli_Pardus_21.x_64bit_70U1_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/logs/Pardus-21.3-XFCE/2022-11-15-05-56-14/check_inbox_driver/pardus-gnu does not exist\nerror message:\nDestination directory /home/worker/workspace/lli_Pardus_21.x_64bit_70U1_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/logs/Pardus-21.3-XFCE/2022-11-15-05-56-14/check_inbox_driver/pardus-gnu does not exist", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver dump inbox drivers versions task path home worker workspace ll i pard us number x number b it para virtual vmxnet number efi ansible vsphere gos validation linux check inbox driver check inbox driver yml number fatal localhost failed destination directory home worker workspace ll i pard us number x number b it para virtual vmxnet number efi ansible vsphere gos validation logs pard us number xfce timestamp number check inbox driver pard us gnu does not exist error message destination directory home worker workspace ll i pard us number x number b it para virtual vmxnet number efi ansible vsphere gos validation logs pard us number xfce timestamp number check inbox driver pard us gnu does not exist", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5805, "name": "log-7056", "raw": "2022-11-15 08:07:46,015 | Failed at Play [check_inbox_driver] ************************\n2022-11-15 08:07:46,015 | TASK [check_inbox_driver][Dump inbox drivers versions] *****\ntask path: /home/worker/workspace/lli_Pardus_21.x_GNOME_70U1_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/check_inbox_driver/check_inbox_driver.yml:80\nfatal: [localhost]: FAILED! => Destination directory /home/worker/workspace/lli_Pardus_21.x_GNOME_70U1_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/logs/Pardus-21.3-GNOME/2022-11-15-08-06-01/check_inbox_driver/pardus-gnu does not exist\nerror message:\nDestination directory /home/worker/workspace/lli_Pardus_21.x_GNOME_70U1_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/logs/Pardus-21.3-GNOME/2022-11-15-08-06-01/check_inbox_driver/pardus-gnu does not exist", "category": "", "processed": "timestamp failed at play check inbox driver timestamp task check inbox driver dump inbox drivers versions task path home worker workspace ll i pard us number x gnome para virtual vmxnet number efi ansible vsphere gos validation linux check inbox driver check inbox driver yml number fatal localhost failed destination directory home worker workspace ll i pard us number x gnome para virtual vmxnet number efi ansible vsphere gos validation logs pard us number gnome timestamp number check inbox driver pard us gnu does not exist error message destination directory home worker workspace ll i pard us number x gnome para virtual vmxnet number efi ansible vsphere gos validation logs pard us number gnome timestamp number check inbox driver pard us gnu does not exist", "solution": "deepdive", "target": "testcase", "version": 202212132000}, {"id": 5810, "name": "log-7074", "raw": "2022-11-16 08:19:36,016 | Failed at Play [wsl_test] **********************************\n2022-11-16 08:19:36,016 | TASK [wsl_test][Create snapshot 'BaseSnapshot' on 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Windows_11_MAIN_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:4\nfatal: [localhost]: FAILED! => Detected an invalid snapshot configuration.\nerror message:\nDetected an invalid snapshot configuration.", "category": "", "processed": "timestamp failed at play wsl test timestamp task wsl test create snapshot base snapshot on test vm task path home worker workspace ansible windows number main nvme efi ansible vsphere gos validation common vm take snapshot yml number fatal localhost failed detected an invalid snapshot configuration error message detected an invalid snapshot configuration", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5806, "name": "log-7069", "raw": "2022-11-15 19:29:20,015 | Failed at Play [deploy_vm_bios_sata_vmxnet3] ***************\n2022-11-15 19:29:20,015 | TASK [deploy_vm_bios_sata_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221115182607.log] \ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70U1_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-15 19:30:12,015 | TASK [deploy_vm_bios_sata_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70U1_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm bios sata vmxnet number timestamp task deploy vm bios sata vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible rocky linux number x sata vmxnet number bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm bios sata vmxnet number testing exit due to failure task path home worker workspace ansible rocky linux number x sata vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5807, "name": "log-7068", "raw": "2022-11-15 18:22:45,015 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2022-11-15 18:22:45,015 | TASK [deploy_vm_efi_ide_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221115171922.log] \ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-15 18:23:30,015 | TASK [deploy_vm_efi_ide_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_RockyLinux_8.x_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e", "category": "", "processed": "timestamp failed at play deploy vm efi ide timestamp task deploy vm efi ide wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible rocky linux number x ide efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm efi ide testing exit due to failure task path home worker workspace ansible rocky linux number x ide efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e error message exit testing when exit testing when fail is set to true in test case deploy vm efi ide e number e", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5808, "name": "log-7075", "raw": "2022-11-16 09:13:07,016 | Failed at Play [deploy_vm_bios_sata_vmxnet3] ***************\n2022-11-16 09:13:07,016 | TASK [deploy_vm_bios_sata_vmxnet3][Wait for message 'Autoinstall is completed.' appear in VM log serial-20221116080949.log] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U1_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:35\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2022-11-16 09:13:56,016 | TASK [deploy_vm_bios_sata_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U1_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm bios sata vmxnet number timestamp task deploy vm bios sata vmxnet number wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible rhel number x sata vmxnet number bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm bios sata vmxnet number testing exit due to failure task path home worker workspace ansible rhel number x sata vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5809, "name": "log-7072", "raw": "2022-11-16 01:52:30,016 | Failed at Play [deploy_vm_ovf] *****************************\n2022-11-16 01:52:30,016 | TASK [deploy_vm_ovf][Deploy VM from ovf template] **********\ntask path: /home/worker/workspace/Ansible_Windows_MS_Template_70U3/ansible-vsphere-gos-validation/common/ovf_deploy.yml:4\nexception in /vmware_deploy_ovf.py when run in /request.py when do_open\nfatal: [localhost]: FAILED! => <urlopen error The write operation timed out> Problem validating OVF import spec: Line 98: Invalid value 'hostonly' for element 'Connection'.\nerror message:\n<urlopen error The write operation timed out>\n2022-11-16 01:52:32,016 | TASK [deploy_vm_ovf][Testing exit due to failure] **********\ntask path: /home/worker/workspace/Ansible_Windows_MS_Template_70U3/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_ovf\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_ovf", "category": "", "processed": "timestamp failed at play deploy vm ovf timestamp task deploy vm ovf deploy vm from ovf template task path home worker workspace ansible windows ms template ansible vsphere gos validation common ovf deploy yml number exception in vmware deploy ovf python when run in request python when do open fatal localhost failed url open error the write operation timed out problem validating ovf import spec line number invalid value host only for element connection error message url open error the write operation timed out timestamp task deploy vm ovf testing exit due to failure task path home worker workspace ansible windows ms template ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm ovf error message exit testing when exit testing when fail is set to true in test case deploy vm ovf", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5811, "name": "log-7123", "raw": "2022-11-22 14:08:07,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-11-22 14:08:07,022 | TASK [gosc_cloudinit_dhcp][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.187.150.90]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task go sc cloud init dhcp update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x ide efi ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5812, "name": "log-7127", "raw": "2022-11-22 14:10:01,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-11-22 14:10:01,022 | TASK [gosc_cloudinit_staticip][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70U1_IDE_E1000E_EFI/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.187.150.90]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x ide efi ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5813, "name": "log-7132", "raw": "2022-11-22 14:12:37,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-11-22 14:12:37,022 | TASK [gosc_cloudinit_dhcp][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70GA_LSILOGIC_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.78.219.10]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task go sc cloud init dhcp update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x number ga lsi logic vmxnet number bios ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5814, "name": "log-7122", "raw": "2022-11-22 13:59:15,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-11-22 13:59:15,022 | TASK [gosc_cloudinit_staticip][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70GA_LSILOGIC_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.78.219.10]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x number ga lsi logic vmxnet number bios ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5815, "name": "log-7126", "raw": "2022-11-22 14:08:57,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-11-22 14:08:57,022 | TASK [gosc_cloudinit_dhcp][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_80GA_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.184.92.252]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task go sc cloud init dhcp update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x number ga sata vmxnet number bios ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5816, "name": "log-7133", "raw": "2022-11-22 14:11:18,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-11-22 14:11:18,022 | TASK [gosc_cloudinit_staticip][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_80GA_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.184.92.252]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x number ga sata vmxnet number bios ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5817, "name": "log-7124", "raw": "2022-11-22 14:08:31,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-11-22 14:08:31,022 | TASK [gosc_cloudinit_dhcp][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_80GA_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.212.225.36]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task go sc cloud init dhcp update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x number ga para virtual efi ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5818, "name": "log-7129", "raw": "2022-11-22 14:10:45,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-11-22 14:10:45,022 | TASK [gosc_cloudinit_staticip][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_80GA_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.212.225.36]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x number ga para virtual efi ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5819, "name": "log-7130", "raw": "2022-11-22 14:11:45,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-11-22 14:11:45,022 | TASK [gosc_cloudinit_dhcp][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.161.67.5]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task go sc cloud init dhcp update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x number ga sata efi ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5820, "name": "log-7121", "raw": "2022-11-22 13:59:10,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-11-22 13:59:10,022 | TASK [gosc_cloudinit_staticip][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70GA_SATA_E1000E_EFI/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.161.67.5]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x number ga sata efi ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5821, "name": "log-7099", "raw": "2022-11-22 11:40:32,022 | Failed at Play [deploy_vm_bios_lsilogicsas_vmxnet3] ********\n2022-11-22 11:40:32,022 | TASK [deploy_vm_bios_lsilogicsas_vmxnet3][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:51\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '[]'.\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 600 seconds. Current VMware Tools running status is 'guestToolsRunning' and guest IP addresses are '[]'.\n2022-11-22 11:41:09,022 | TASK [deploy_vm_bios_lsilogicsas_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_lsilogicsas_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_lsilogicsas_vmxnet3", "category": "", "processed": "timestamp failed at play deploy vm bios lsi logic sas vmxnet number timestamp task deploy vm bios lsi logic sas vmxnet number check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible oracle linux number x lsi logic sas vmxnet number bios ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are error message it s timed out for vmware tools collecting guest ip v number address in number seconds current vmware tools running status is guest tools running and guest ip addresses are timestamp task deploy vm bios lsi logic sas vmxnet number testing exit due to failure task path home worker workspace ansible oracle linux number x lsi logic sas vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios lsi logic sas vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios lsi logic sas vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5822, "name": "log-7128", "raw": "2022-11-22 14:10:21,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-11-22 14:10:21,022 | TASK [gosc_cloudinit_dhcp][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.212.12.33]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task go sc cloud init dhcp update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x para virtual bios ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5823, "name": "log-7120", "raw": "2022-11-22 13:58:53,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-11-22 13:58:53,022 | TASK [gosc_cloudinit_staticip][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70U3_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.212.12.33]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x para virtual bios ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5824, "name": "log-7125", "raw": "2022-11-22 14:09:21,022 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2022-11-22 14:09:21,022 | TASK [gosc_cloudinit_dhcp][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.161.106.53]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init dhcp timestamp task go sc cloud init dhcp update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x nvme vmxnet number efi ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5825, "name": "log-7131", "raw": "2022-11-22 14:11:26,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2022-11-22 14:11:26,022 | TASK [gosc_cloudinit_staticip][Update service cloud-init-local, enabled: True, state: started] \ntask path: /home/worker/workspace/Ansible_OracleLinux_8.x_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/utils/service_operation.yml:17\nfatal: [localhost -> 10.161.106.53]: FAILED! => Unable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.\nerror message:\nUnable to start service cloud-init-local: Job for cloud-init-local.service failed because the control process exited with error code.\nSee \"systemctl status cloud-init-local.service\" and \"journalctl -xe\" for details.", "category": "", "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip update service cloud init local enabled true state started task path home worker workspace ansible oracle linux number x nvme vmxnet number efi ansible vsphere gos validation linux utilities service operation yml number fatal localhost ip address failed unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details error message unable to start service cloud init local job for cloud init local service failed because the control process exited with error code see systemctl status cloud init local service and journal ctl xe for details", "solution": "deepdive", "target": "targetvm", "version": 202212132000}, {"id": 5826, "name": "8195", "raw": "2023-02-01 08:32:21,001 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2023-02-01 08:32:21,001 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Create a new VM 'test_rockylinux8' on server '10.182.189.103'] \ntask path: /home/worker/workspace/Ansible_Regression_RockyLinux_8.x/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\nerror message:\nFailed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\n2023-02-01 08:32:22,001 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'test_rockylinux8'] \ntask path: /home/worker/workspace/Ansible_Regression_RockyLinux_8.x/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_rockylinux8\nerror message:\nUnable to gather information for non-existing VM test_rockylinux8", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number create a new vm test rocky linux number on server ip address task path home worker workspace ansible regression rocky linux number x ansible vsphere gos validation common vm create yml number fatal localhost failed failed to create a virtual machine a specified parameter was not correct configuration spec guest id error message failed to create a virtual machine a specified parameter was not correct configuration spec guest id timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm test rocky linux number task path home worker workspace ansible regression rocky linux number x ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test rocky linux number error message unable to gather information for non existing vm test rocky linux number", "solution": "deepdive", "target": "usererror", "version": 202302070551}, {"id": 5827, "name": "8197", "raw": "2023-02-01 08:33:25,001 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2023-02-01 08:33:25,001 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Create a new VM 'test_almalinux8' on server '10.186.200.221'] \ntask path: /home/worker/workspace/Ansible_Regression_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\nerror message:\nFailed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\n2023-02-01 08:33:27,001 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'test_almalinux8'] \ntask path: /home/worker/workspace/Ansible_Regression_AlmaLinux_8.x/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_almalinux8\nerror message:\nUnable to gather information for non-existing VM test_almalinux8", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number create a new vm test alma linux number on server ip address task path home worker workspace ansible regression alma linux number x ansible vsphere gos validation common vm create yml number fatal localhost failed failed to create a virtual machine a specified parameter was not correct configuration spec guest id error message failed to create a virtual machine a specified parameter was not correct configuration spec guest id timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm test alma linux number task path home worker workspace ansible regression alma linux number x ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test alma linux number error message unable to gather information for non existing vm test alma linux number", "solution": "deepdive", "target": "usererror", "version": 202302070551}, {"id": 5828, "name": "8198", "raw": "2023-02-01 08:35:31,001 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2023-02-01 08:35:31,001 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Create a new VM 'test_almalinux9' on server '10.212.96.106'] \ntask path: /home/worker/workspace/Ansible_Regression_AlmaLinux_9.x/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\nerror message:\nFailed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\n2023-02-01 08:35:33,001 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'test_almalinux9'] \ntask path: /home/worker/workspace/Ansible_Regression_AlmaLinux_9.x/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_almalinux9\nerror message:\nUnable to gather information for non-existing VM test_almalinux9", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number create a new vm test alma linux number on server ip address task path home worker workspace ansible regression alma linux number x ansible vsphere gos validation common vm create yml number fatal localhost failed failed to create a virtual machine a specified parameter was not correct configuration spec guest id error message failed to create a virtual machine a specified parameter was not correct configuration spec guest id timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm test alma linux number task path home worker workspace ansible regression alma linux number x ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test alma linux number error message unable to gather information for non existing vm test alma linux number", "solution": "deepdive", "target": "usererror", "version": 202302070551}, {"id": 5829, "name": "8196", "raw": "2023-02-01 08:32:40,001 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2023-02-01 08:32:40,001 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Create a new VM 'test_rockylinux9' on server '10.161.184.174'] \ntask path: /home/worker/workspace/Ansible_Regression_RockyLinux_9.x/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\nerror message:\nFailed to create a virtual machine : A specified parameter was not correct: configSpec.guestId\n2023-02-01 08:32:41,001 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Get specified property info for VM 'test_rockylinux9'] \ntask path: /home/worker/workspace/Ansible_Regression_RockyLinux_9.x/ansible-vsphere-gos-validation/common/vm_get_config.yml:4\nfatal: [localhost]: FAILED! => Unable to gather information for non-existing VM test_rockylinux9\nerror message:\nUnable to gather information for non-existing VM test_rockylinux9", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number create a new vm test rocky linux number on server ip address task path home worker workspace ansible regression rocky linux number x ansible vsphere gos validation common vm create yml number fatal localhost failed failed to create a virtual machine a specified parameter was not correct configuration spec guest id error message failed to create a virtual machine a specified parameter was not correct configuration spec guest id timestamp task deploy vm efi para virtual vmxnet number get specified property info for vm test rocky linux number task path home worker workspace ansible regression rocky linux number x ansible vsphere gos validation common vm get configuration yml number fatal localhost failed unable to gather information for non existing vm test rocky linux number error message unable to gather information for non existing vm test rocky linux number", "solution": "deepdive", "target": "usererror", "version": 202302070551}, {"id": 5830, "name": "8247", "raw": "2023-02-06 13:01:50,006 | Failed at Play [wintools_complete_install_verify] **********\n2023-02-06 13:01:50,006 | TASK [wintools_complete_install_verify][Check VMware Tools install task started] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_Server_2022_64/ansible-vsphere-gos-validation/windows/wintools_complete_install_verify/install_vmtools.yml:38\nfatal: [localhost]: FAILED! => The result of above VMware Tools installation task executed in guest OS is not returned.\nerror message:\nThe result of above VMware Tools installation task executed in guest OS is not returned.", "category": null, "processed": "timestamp failed at play win tools complete install verify timestamp task win tools complete install verify check vmware tools install task started task path home worker workspace ansible cycle windows server number ansible vsphere gos validation windows win tools complete install verify install vm tools yml number fatal localhost failed the result of above vmware tools installation task executed in guest os is not returned error message the result of above vmware tools installation task executed in guest os is not returned", "solution": "retry", "target": "targetvm", "version": 202302070551}, {"id": 5832, "name": "8258", "raw": "2023-02-06 14:54:08,006 | Failed at Play [check_quiesce_snapshot] ********************\n2023-02-06 14:54:08,006 | TASK [check_quiesce_snapshot][Check VM snapshot took is quiesced snapshot] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_10_32/ansible-vsphere-gos-validation/common/vm_take_snapshot.yml:36\nfatal: [localhost]: FAILED! => Snapshot 'quiesce_2023-02-06-14-53-06' quiesced status is false.\nerror message:\nSnapshot 'quiesce_2023-02-06-14-53-06' quiesced status is false.", "category": null, "processed": "timestamp failed at play check quiesce snapshot timestamp task check quiesce snapshot check vm snapshot took is quiesced snapshot task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm take snapshot yml number fatal localhost failed snapshot quiesce timestamp number quiesced status is false error message snapshot quiesce timestamp number quiesced status is false", "solution": "deepdive", "target": "targetvm", "version": 202302071150}, {"id": 5831, "name": "8268", "raw": "2023-02-06 18:19:19,006 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2023-02-06 18:19:19,006 | TASK [gosc_cloudinit_dhcp][Get installed packages on Ubuntu] \ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_20.04_Server_ISO/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:36\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.78.166.10' (ECDSA) to the list of known hosts.\nroot@10.78.166.10: Permission denied (publickey,keyboard-interactive).\nerror message:\nInvalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.78.166.10' (ECDSA) to the list of known hosts.\nroot@10.78.166.10: Permission denied (publickey,keyboard-interactive).", "category": null, "processed": "timestamp failed at play go sc cloud init dhcp timestamp task go sc cloud init dhcp get installed packages on ubuntu task path home worker workspace ansible cycle ubuntu number server iso ansible vsphere gos validation linux utilities install un install package yml number fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts root ip address permission denied public key keyboard interactive error message invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address ecdsa to the list of known hosts root ip address permission denied public key keyboard interactive", "solution": "retry", "target": "targetvm", "version": 202302071150}, {"id": 5847, "name": "3", "raw": "2023-03-03 07:44:46,003 | Failed at Play [stat_hosttime] *****************************\n2023-03-03 07:44:46,003 | TASK [stat_hosttime][Assert VM has valid IPv4 address for VM 'test_windows11_22538'] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/common/vm_get_primary_nic.yml:15\nfatal: [localhost]: FAILED! => Not found valid IPv4 address for VM 'test_windows11_22538'\nerror message:\nNot found valid IPv4 address for VM 'test_windows11_22538'", "category": null, "processed": "timestamp failed at play stat host time timestamp task stat host time assert vm has valid ip v number address for vm test windows number task path home worker workspace ansible regression windows number ansible vsphere gos validation common vm get primary nic yml number fatal localhost failed not found valid ip v number address for vm test windows number error message not found valid ip v number address for vm test windows number number", "solution": "deepdive", "target": "targetvm", "version": 202304102150}, {"id": 5842, "name": "2", "raw": "2023-04-06 03:01:13,006 | Failed at Play [17_e1000e_network_device_ops] **************\n2023-04-06 03:01:13,006 | TASK [17_e1000e_network_device_ops][Add a new 'e1000e' adapter in 'vSwitch2023-04-06-01-36-55_PG' to VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Cycle_RHEL_8.x/ansible-vsphere-gos-validation/common/vm_add_network_adapter.yml:13\nexception in /vmware_guest_network.py when _nic_present in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => ('The available Memory resources in the parent resource pool are insufficient for the operation.', None)\nerror message:\n('The available Memory resources in the parent resource pool are insufficient for the operation.', None)", "category": null, "processed": "timestamp failed at play number network device ops timestamp task number network device ops add a new adapter in vswitch timestamp number page to vm test vm task path home worker workspace ansible cycle rhel number x ansible vsphere gos validation common vm add network adapter yml number exception in vmware guest network python when nic present in vmware python when wait for task fatal localhost failed the available memory resources in the parent resource pool are insufficient for the operation none error message the available memory resources in the parent resource pool are insufficient for the operation none", "solution": "retry", "target": "nimbus", "version": 202304102150}, {"id": 5836, "name": "2", "raw": "2023-04-04 09:13:53,004 | Failed at Play [12_memory_hot_add_basic] *******************\n2023-04-04 09:13:53,004 | TASK [12_memory_hot_add_basic][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:49\nfatal: [localhost]: FAILED! => [\"It's timed out for VMware Tools collecting guest IPv4 address in 900 seconds.\", \"VMware Tools running status is 'guestToolsRunning'.\", \"VM's IP address in guest info is '2620:124:6020:c70a:88f6:aa6e:d086:c9ef'.\", \"VM's all IP addresses in guest info are '['2620:124:6020:c70a:88f6:aa6e:d086:c9ef', '2620:124:6020:c70a:250:56ff:feb2:370f', 'fe80::250:56ff:feb2:370f']'.\"]\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 900 seconds.\nVMware Tools running status is 'guestToolsRunning'.\nVM's IP address in guest info is '2620:124:6020:c70a:88f6:aa6e:d086:c9ef'.\nVM's all IP addresses in guest info are '['2620:124:6020:c70a:88f6:aa6e:d086:c9ef', '2620:124:6020:c70a:250:56ff:feb2:370f', 'fe80::250:56ff:feb2:370f']'.", "category": null, "processed": "timestamp failed at play number memory hot add basic timestamp task number memory hot add basic check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible sles main nvme bios ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds vmware tools running status is guest tools running vm sip address in guest info is ip address vm s all ip addresses in guest info are ip address ip address ip address error message it s timed out for vmware tools collecting guest ip v number address in number seconds vmware tools running status is guest tools running vm sip address in guest info is ip address vm s all ip addresses in guest info are ip address ip address ip address", "solution": "retry", "target": "nimbus", "version": 202304102150}, {"id": 5837, "name": "2", "raw": "2023-03-31 10:13:24,031 | Failed at Play [env_setup] *********************************\n2023-03-31 10:13:24,031 | TASK [env_setup][Check VM 'Ansible_win11_nested_virt_80ga' does not exist] \ntask path: /home/worker/workspace/Ansible_Windows_11_Physical_80GA_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/env_setup/env_setup.yml:23\nfatal: [localhost]: FAILED! => Cann't deploy VM as a VM with same name 'Ansible_win11_nested_virt_80ga' already exists. Please provide a new vm_name.\nerror message:\nCann't deploy VM as a VM with same name 'Ansible_win11_nested_virt_80ga' already exists. Please provide a new vm_name.", "category": null, "processed": "timestamp failed at play environment setup timestamp task environment setup check vm ansible nested virt number ga does not exist task path home worker workspace ansible windows number physical number ga para virtual efi ansible vsphere gos validation environment setup environment setup yml number fatal localhost failed can n t deploy vm as a vm with same name ansible nested virt number ga already exists please provide a new vm name error message can n t deploy vm as a vm with same name ansible nested virt number ga already exists please provide a new vm name", "solution": "deepdive", "target": "usererror", "version": 202304102150}, {"id": 5838, "name": "2", "raw": "2023-03-31 10:34:27,031 | Failed at Play [1_deploy_vm] *******************************\n2023-03-31 10:34:27,031 | TASK [1_deploy_vm][Change VM power state failure] **********\ntask path: /home/worker/workspace/Ansible_Windows_11_Physical_80GA_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:69\nfatal: [localhost]: FAILED! => License key has expired. \nerror message:\nLicense key has expired.\n2023-03-31 10:34:44,031 | TASK [1_deploy_vm][Testing exit due to failure] ************\ntask path: /home/worker/workspace/Ansible_Windows_11_Physical_80GA_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:87\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_e1000e\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_e1000e", "category": null, "processed": "timestamp failed at play number deploy vm timestamp task number deploy vm change vm power state failure task path home worker workspace ansible windows number physical number ga para virtual efi ansible vsphere gos validation common vm set power state yml number fatal localhost failed license key has expired error message license key has expired timestamp task number deploy vm testing exit due to failure task path home worker workspace ansible windows number physical number ga para virtual efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm efi para virtual e number e error message exit testing because exit testing when fail is set to true in test case deploy vm efi para virtual e number e", "solution": "deepdive", "target": "testbed", "version": 202304102150}, {"id": 5839, "name": "2", "raw": "2023-03-23 09:52:31,023 | Failed at Play [env_setup] *********************************\n2023-03-23 09:52:31,023 | TASK [env_setup][Get all registerd VMs and templates on wdc-gosv-80ga.eng.vmware.com] \ntask path: /home/worker/workspace/Ansible_Windows_10_64bit_70U3_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_check_exist.yml:14\nfatal: [localhost]: FAILED! => Failed to find folder specified by /ansible_test/vm\nerror message:\nFailed to find folder specified by /ansible_test/vm", "category": null, "processed": "timestamp failed at play environment setup timestamp task environment setup get all register dv ms and templates on wdcgosv number ga eng vmware com task path home worker workspace ansible windows number b it ide efi ansible vsphere gos validation common vm check exist yml number fatal localhost failed failed to find folder specified by ansible test vm error message failed to find folder specified by ansible test vm", "solution": "deepdive", "target": "testbed", "version": 202304102150}, {"id": 5840, "name": "2", "raw": "2023-04-06 14:05:46,006 | Failed at Play [01_deploy_vm] ******************************\n2023-04-06 14:05:46,006 | TASK [01_deploy_vm][Wait for message 'Autoinstall is completed.' appear in VM log serial-20230406130243.log] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:40\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2023-04-06 14:06:32,006 | TASK [01_deploy_vm][Testing exit due to failure] ***********\ntask path: /home/worker/workspace/Ansible_SLED_15SP4_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:87\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_lsilogicsas_vmxnet3\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_lsilogicsas_vmxnet3\n2023-04-06 14:12:19,006 | TASK [01_deploy_vm][Check VMware Tools is running and collects guest OS fullname successfully] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_guest_fullname.yml:46\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest OS fullname in 300 seconds. Current VMware Tools running status is 'guestToolsNotRunning', and guest OS fullname is ''.\nerror message:\nIt's timed out for VMware Tools collecting guest OS fullname in 300 seconds. Current VMware Tools running status is 'guestToolsNotRunning', and guest OS fullname is ''.", "category": null, "processed": "timestamp failed at play number deploy vm timestamp task number deploy vm wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible sled lsi logic sas vmxnet number bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task number deploy vm testing exit due to failure task path home worker workspace ansible sled lsi logic sas vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm bios lsi logic sas vmxnet number error message exit testing because exit testing when fail is set to true in test case deploy vm bios lsi logic sas vmxnet number timestamp task number deploy vm check vmware tools is running and collects guest os full name successfully task path home worker workspace ansible sled lsi logic sas vmxnet number bios ansible vsphere gos validation common vm wait guest full name yml number fatal localhost failed it s timed out for vmware tools collecting guest os full name in number seconds current vmware tools running status is guest tools not running and guest os full name is error message it s timed out for vmware tools collecting guest os full name in number seconds current vmware tools running status is guest tools not running and guest os full name is", "solution": "deepdive", "target": "targetvm", "version": 202304102150}, {"id": 5841, "name": "2", "raw": "2023-03-24 09:31:32,024 | Failed at Play [env_setup] *********************************\n2023-03-24 09:31:32,024 | TASK [env_setup][Check VM only has one network adapter and it should be connected] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_Upgrade2019_MAIN_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/env_setup/check_vm_settings.yml:10\nfatal: [localhost]: FAILED! => VM doesn't meet test requirement, which must have one network adapter and it should be connected. Current VM's network adapters are '{'0': {'mac_address': '00:50:56:b6:f7:e8', 'label': 'Network adapter 1', 'unit_number': 7, 'wake_onlan': True, 'allow_guest_ctl': True, 'connected': True, 'start_connected': True, 'network_name': 'VM Network', 'vlan_id': 0, 'switch': 'vSwitch0', 'device_type': 'e1000e', 'mac_addr': '00:50:56:b6:f7:e8', 'name': 'VM Network'}, '1': {'mac_address': '00:50:56:b6:91:39', 'label': 'Network adapter 2', 'unit_number': 8, 'wake_onlan': False, 'allow_guest_ctl': True, 'connected': False, 'start_connected': False, 'network_name': 'vSwitch2023-03-24-09-26-47_PG', 'vlan_id': 0, 'switch': 'vSwitch2023-03-24-09-26-47', 'device_type': 'vmxnet3', 'mac_addr': '00:50:56:b6:91:39', 'name': 'vSwitch2023-03-24-09-26-47_PG'}}'.\nerror message:\nVM doesn't meet test requirement, which must have one network adapter and it should be connected. Current VM's network adapters are '{'0': {'mac_address': '00:50:56:b6:f7:e8', 'label': 'Network adapter 1', 'unit_number': 7, 'wake_onlan': True, 'allow_guest_ctl': True, 'connected': True, 'start_connected': True, 'network_name': 'VM Network', 'vlan_id': 0, 'switch': 'vSwitch0', 'device_type': 'e1000e', 'mac_addr': '00:50:56:b6:f7:e8', 'name': 'VM Network'}, '1': {'mac_address': '00:50:56:b6:91:39', 'label': 'Network adapter 2', 'unit_number': 8, 'wake_onlan': False, 'allow_guest_ctl': True, 'connected': False, 'start_connected': False, 'network_name': 'vSwitch2023-03-24-09-26-47_PG', 'vlan_id': 0, 'switch': 'vSwitch2023-03-24-09-26-47', 'device_type': 'vmxnet3', 'mac_addr': '00:50:56:b6:91:39', 'name': 'vSwitch2023-03-24-09-26-47_PG'}}'.", "category": null, "processed": "timestamp failed at play environment setup timestamp task environment setup check vm only has one network adapter and it should be connected task path home worker workspace ansible windows server lts c main para virtual vmxnet number bios ansible vsphere gos validation environment setup check vm settings yml number fatal localhost failed vm doesn t meet test requirement which must have one network adapter and it should be connected current vm s network adapters are number mac address mac address label network adapter number unit number wake on lan true allow guest ctl true connected true start connected true network name vm network vlan id number switch vswitch number device type mac address mac address name vm network number mac address mac address label network adapter number unit number wake on lan false allow guest ctl true connected false start connected false network name vswitch timestamp number page vlan id number switch vswitch timestamp number device type vmxnet number mac address mac address name vswitch timestamp number page error message vm doesn t meet test requirement which must have one network adapter and it should be connected current vm s network adapters are number mac address mac address label network adapter number unit number wake on lan true allow guest ctl true connected true start connected true network name vm network vlan id number switch vswitch number device type mac address mac address name vm network number mac address mac address label network adapter number unit number wake on lan false allow guest ctl true connected false start connected false network name vswitch timestamp number page vlan id number switch vswitch timestamp number device type vmxnet number mac address mac address name vswitch timestamp number page", "solution": "deepdive", "target": "usererror", "version": 202304102150}, {"id": 5846, "name": "3", "raw": "2023-03-23 10:05:18,023 | Failed at Play [deploy_vm_efi_paravirtual_e1000e] **********\n2023-03-23 10:05:18,023 | TASK [deploy_vm_efi_paravirtual_e1000e][Datastore file operation] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:28\nfatal: [localhost]: FAILED! => File 'OS/Other/Windows/Windows11/v22H2/GA/en-us_windows_11_business_editions_version_22h2_x64_dvd_17a08ce3.iso' is absent, cannot continue\nerror message:\nFile 'OS/Other/Windows/Windows11/v22H2/GA/en-us_windows_11_business_editions_version_22h2_x64_dvd_17a08ce3.iso' is absent, cannot continue\n2023-03-23 10:05:19,023 | TASK [deploy_vm_efi_paravirtual_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:77\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_e1000e\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual timestamp task deploy vm efi para virtual data store file operation task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed file os other windows windows number v number h number ga en us windows number business editions version number h number x number dvd hex id iso is absent can not continue error message file os other windows windows number v number h number ga en us windows number business editions version number h number x number dvd hex id iso is absent can not continue timestamp task deploy vm efi para virtual testing exit due to failure task path home worker workspace ansible windows number para virtual efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm efi para virtual e number e error message exit testing because exit testing when fail is set to true in test case deploy vm efi para virtual e number e", "solution": "deepdive", "target": "testbed", "version": 202304102150}, {"id": 5851, "name": "3", "raw": "2023-03-09 07:15:08,009 | Failed at Play [gosc_sanity_staticip] **********************\n2023-03-09 07:15:08,009 | TASK [gosc_sanity_staticip][Revert snapshot failed] ********\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_Physical_VBS_80GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:47\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed\nerror message:\nRevert to snapshot 'BaseSnapshot' failed", "category": null, "processed": "timestamp failed at play go sc sanity static ip timestamp task go sc sanity static ip revert snapshot failed task path home worker workspace ansible windows server lts c physical vbs number ga lsi logic sas efi ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed error message revert to snapshot base snapshot failed", "solution": "deepdive", "target": "targetvm", "version": 202304102150}, {"id": 5843, "name": "2", "raw": "2023-04-06 09:08:18,006 | Failed at Play [01_ovt_verify_install] *********************\n2023-04-06 09:08:18,006 | TASK [01_ovt_verify_install][Assert command is executed successfully] \ntask path: /home/worker/workspace/Ansible_Cycle_Photon_5.0_ISO/ansible-vsphere-gos-validation/linux/open_vm_tools/uninstall_ovt.yml:22\nfatal: [localhost]: FAILED! => Failed to uninstall open-vm-tools by executing command: tdnf --enablerepo=photon --enablerepo=photon-updates -y remove open-vm-tools\nerror message:\nFailed to uninstall open-vm-tools by executing command: tdnf --enablerepo=photon --enablerepo=photon-updates -y remove open-vm-tools\n2023-04-06 09:08:53,006 | TASK [01_ovt_verify_install][Testing exit due to failure] **\ntask path: /home/worker/workspace/Ansible_Cycle_Photon_5.0_ISO/ansible-vsphere-gos-validation/common/test_rescue.yml:87\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case ovt_verify_install\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case ovt_verify_install", "category": null, "processed": "timestamp failed at play number ovt verify install timestamp task number ovt verify install assert command is executed successfully task path home worker workspace ansible cycle photon number iso ansible vsphere gos validation linux open vm tools un install ovt yml number fatal localhost failed failed to un install open vm tools by executing command tdnf enable repository photon enable repository photon updates y remove open vm tools error message failed to un install open vm tools by executing command tdnf enable repository photon enable repository photon updates y remove open vm tools timestamp task number ovt verify install testing exit due to failure task path home worker workspace ansible cycle photon number iso ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case ovt verify install error message exit testing because exit testing when fail is set to true in test case ovt verify install", "solution": "deepdive", "target": "targetvm", "version": 202304102150}, {"id": 5848, "name": "3", "raw": "2023-03-01 08:03:44,001 | Failed at Play [deploy_vm_bios_nvme_e1000e] ****************\n2023-03-01 08:03:44,001 | TASK [deploy_vm_bios_nvme_e1000e][Datastore file operation] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.04_Desktop_ISO_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:28\nfatal: [localhost]: FAILED! => Failed to query for file 'OS/Linux/Ubuntu/22/22.04.2/ubuntu-22.04.2-desktop-amd64.iso'\nerror message:\nFailed to query for file 'OS/Linux/Ubuntu/22/22.04.2/ubuntu-22.04.2-desktop-amd64.iso'\n2023-03-01 08:03:45,001 | TASK [deploy_vm_bios_nvme_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.04_Desktop_ISO_MAIN_NVME_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_nvme_e1000e\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_nvme_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios nvme timestamp task deploy vm bios nvme data store file operation task path home worker workspace ansible ubuntu number desktop iso main nvme bios ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed failed to query for file os linux ubuntu number version id ubuntu version id desktop amd number i so error message failed to query for file os linux ubuntu number version id ubuntu version id desktop amd number i so timestamp task deploy vm bios nvme testing exit due to failure task path home worker workspace ansible ubuntu number desktop iso main nvme bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios nvme e number e error message exit testing when exit testing when fail is set to true in test case deploy vm bios nvme e number e", "solution": "deepdive", "target": "testbed", "version": 202304102150}, {"id": 5844, "name": "2", "raw": "2023-04-03 21:12:46,003 | Failed at Play [16_secureboot_enable_disable] **************\n2023-04-03 21:12:46,003 | TASK [16_secureboot_enable_disable][Set VM boot options] ***\ntask path: /home/worker/workspace/Ansible_Cycle_AlmaLinux_9.x/ansible-vsphere-gos-validation/common/vm_set_boot_options.yml:4\nexception in /vmware_guest_boot_manager.py when ensure in /vmware.py when wait_for_task\nfatal: [localhost]: FAILED! => Failed to perform reconfigure virtual machine test_vm for boot order due to: ('An error occurred while communicating with the remote host.', None)\nerror message:\nFailed to perform reconfigure virtual machine test_vm for boot order due to: ('An error occurred while communicating with the remote host.', None)", "category": null, "processed": "timestamp failed at play number secure boot enable disable timestamp task number secure boot enable disable set vm boot options task path home worker workspace ansible cycle alma linux number x ansible vsphere gos validation common vm set boot options yml number exception in vmware guest boot manager python when ensure in vmware python when wait for task fatal localhost failed failed to perform re configure virtual machine test vm for boot order due to an error occurred while communicating with the remote host none error message failed to perform re configure virtual machine test vm for boot order due to an error occurred while communicating with the remote host none", "solution": "deepdive", "target": "testbed", "version": 202304102150}, {"id": 5849, "name": "3", "raw": "2023-03-28 14:19:50,028 | Failed at Play [gosc_sanity_staticip] **********************\n2023-03-28 14:19:50,028 | TASK [gosc_sanity_staticip][Customize Windows guest OS] ****\ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/windows/guest_customization/win_gosc_execution.yml:4\nfatal: [localhost]: FAILED! => Customization failed. For detailed information see warnings Waiting for customization result event timed out.\nerror message:\nCustomization failed. For detailed information see warnings\n2023-03-28 14:20:57,028 | TASK [gosc_sanity_staticip][Fetch file C:\\Windows\\Temp\\vmware-imc\\guestcust.log from VM guest] \ntask path: /home/worker/workspace/Ansible_Cycle_Windows_11_64/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user Administrator\nerror message:\nInvalid guest login for user Administrator", "category": null, "processed": "timestamp failed at play go sc sanity static ip timestamp task go sc sanity static ip customize windows guest os task path home worker workspace ansible cycle windows number ansible vsphere gos validation windows guest customization win go sc execution yml number fatal localhost failed customization failed for detailed information see warnings waiting for customization result event timed out error message customization failed for detailed information see warnings timestamp task go sc sanity static ip fetch file c windows temp vmware imc guest cust log from vm guest task path home worker workspace ansible cycle windows number ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user administrator error message invalid guest login for user administrator", "solution": "deepdive", "target": "targetvm", "version": 202304102150}, {"id": 5845, "name": "3", "raw": "2023-03-31 09:58:58,031 | Failed at Play [env_cleanup] *******************************\n2023-03-31 09:58:58,031 | TASK [env_cleanup][Remove portgroup vSwitch2023-03-31-04-31-44_PG on vSwitch vSwitch2023-03-31-04-31-44] \ntask path: /home/worker/workspace/Ansible_Windows_11_Physical_VBS_70U3_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/esxi_remove_portgroup.yml:9\nexception in /vmware_portgroup.py when main in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Unable to communicate with the remote host, since it is disconnected.\nerror message:\nUnable to communicate with the remote host, since it is disconnected.", "category": null, "processed": "timestamp failed at play environment cleanup timestamp task environment cleanup remove port group vswitch timestamp number page on vswitch vswitch timestamp number task path home worker workspace ansible windows number physical vbs para virtual vmxnet number efi ansible vsphere gos validation common esxi remove port group yml number exception in vmware port group python when main in soap adapter python when invoke method fatal localhost failed unable to communicate with the remote host since it is disconnected error message unable to communicate with the remote host since it is disconnected", "solution": "deepdive", "target": "testbed", "version": 202304102150}, {"id": 5850, "name": "3", "raw": "2023-03-01 12:02:18,001 | Failed at Play [deploy_vm_bios_sata_vmxnet3] ***************\n2023-03-01 12:02:18,001 | TASK [deploy_vm_bios_sata_vmxnet3][Create a new VM 'test_vm' on server '10.212.24.174'] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.04_Desktop_ISO_70GA_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A component of the virtual machine is not accessible on the host.\nerror message:\nFailed to create a virtual machine : A component of the virtual machine is not accessible on the host.\n2023-03-01 12:02:20,001 | TASK [deploy_vm_bios_sata_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.04_Desktop_ISO_70GA_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_sata_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm bios sata vmxnet number timestamp task deploy vm bios sata vmxnet number create a new vm test vm on server ip address task path home worker workspace ansible ubuntu number desktop iso number ga sata vmxnet number bios ansible vsphere gos validation common vm create yml number fatal localhost failed failed to create a virtual machine a component of the virtual machine is not accessible on the host error message failed to create a virtual machine a component of the virtual machine is not accessible on the host timestamp task deploy vm bios sata vmxnet number testing exit due to failure task path home worker workspace ansible ubuntu number desktop iso number ga sata vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm bios sata vmxnet number", "solution": "deepdive", "target": "testbed", "version": 202304102150}, {"id": 5852, "name": "1", "raw": "2023-04-06 09:42:11,006 | Failed at Play [1_cpu_hot_add_basic] ***********************\n2023-04-06 09:42:11,006 | TASK [1_cpu_hot_add_basic][Check 3 CPUs are present] *******\ntask path: /home/worker/workspace/Ansible_Cycle_Ubuntu_23.04_Server_ISO/ansible-vsphere-gos-validation/linux/utils/wait_for_cpus_online.yml:20\nfatal: [localhost]: FAILED! => It's timed out to wait for 3 CPUs being present in 50 seconds, only '2'' CPUs are present.\nerror message:\nIt's timed out to wait for 3 CPUs being present in 50 seconds, only '2'' CPUs are present.", "category": null, "processed": "timestamp failed at play number cpu hot add basic timestamp task number cpu hot add basic check number cpus are present task path home worker workspace ansible cycle ubuntu number server iso ansible vsphere gos validation linux utilities wait for cpus online yml number fatal localhost failed it s timed out to wait for number cpus being present in number seconds only number cpus are present error message it s timed out to wait for number cpus being present in number seconds only number cpus are present", "solution": "deepdive", "target": "product", "version": 202304110353}, {"id": 5853, "name": "1", "raw": "2023-03-24 19:56:31,024 | Failed at Play [deploy_vm_bios_ide_e1000e] *****************\n2023-03-24 19:56:31,024 | TASK [deploy_vm_bios_ide_e1000e][Wait for message 'gdm.service' appear in VM log serial-20230324185350.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_20.04_Desktop_ISO_70GA_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:40\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2023-03-24 19:57:14,024 | TASK [deploy_vm_bios_ide_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_20.04_Desktop_ISO_70GA_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:91\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_e1000e\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_e1000e\n2023-03-24 20:02:55,024 | TASK [deploy_vm_bios_ide_e1000e][Check VMware Tools is running and collects guest OS fullname successfully] \ntask path: /home/worker/workspace/Ansible_Ubuntu_20.04_Desktop_ISO_70GA_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_wait_guest_fullname.yml:46\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest OS fullname in 300 seconds. Current VMware Tools running status is 'guestToolsNotRunning', and guest OS fullname is ''.\nerror message:\nIt's timed out for VMware Tools collecting guest OS fullname in 300 seconds. Current VMware Tools running status is 'guestToolsNotRunning', and guest OS fullname is ''.", "category": null, "processed": "timestamp failed at play deploy vm bios ide timestamp task deploy vm bios ide wait for message gdm service appear in vm log serial timestamp log task path home worker workspace ansible ubuntu number desktop iso number ga ide bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm bios ide testing exit due to failure task path home worker workspace ansible ubuntu number desktop iso number ga ide bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm bios ide e number e error message exit testing because exit testing when fail is set to true in test case deploy vm bios ide e number e timestamp task deploy vm bios ide check vmware tools is running and collects guest os full name successfully task path home worker workspace ansible ubuntu number desktop iso number ga ide bios ansible vsphere gos validation common vm wait guest full name yml number fatal localhost failed it s timed out for vmware tools collecting guest os full name in number seconds current vmware tools running status is guest tools not running and guest os full name is error message it s timed out for vmware tools collecting guest os full name in number seconds current vmware tools running status is guest tools not running and guest os full name is", "solution": "deepdive", "target": "targetvm", "version": 202304110353}, {"id": 5854, "name": "1", "raw": "2023-03-25 13:35:32,025 | Failed at Play [lsilogic_vhba_device_ops] ******************\n2023-03-25 13:35:32,025 | TASK [lsilogic_vhba_device_ops][Get installed packages on Ubuntu] \ntask path: /home/worker/workspace/Ansible_Ubuntu_20.04_Desktop_ISO_70GA_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/linux/utils/install_uninstall_package.yml:36\nfatal: [localhost]: UNREACHABLE! => Invalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.193.0.187' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.193.0.187: Permission denied (publickey,password).\nerror message:\nInvalid/incorrect username/password. Skipping remaining 5 retries to prevent account lockout: Warning: Permanently added '10.193.0.187' (ED25519) to the list of known hosts.\nPermission denied, please try again.\nPermission denied, please try again.\nroot@10.193.0.187: Permission denied (publickey,password).", "category": null, "processed": "timestamp failed at play lsi logic v hba device ops timestamp task lsi logic v hba device ops get installed packages on ubuntu task path home worker workspace ansible ubuntu number desktop iso number ga ide bios ansible vsphere gos validation linux utilities install un install package yml number fatal localhost un reachable invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key password error message invalid incorrect username password skipping remaining number retries to prevent account lockout warning permanently added ip address to the list of known hosts permission denied please try again permission denied please try again root ip address permission denied public key password", "solution": "retry", "target": "testbed", "version": 202304110353}, {"id": 5855, "name": "1", "raw": "2023-03-25 08:58:21,025 | Failed at Play [pvrdma_network_device_ops] *****************\n2023-03-25 08:58:21,025 | TASK [pvrdma_network_device_ops][Check RDMA ping result from client VM to server VM] \ntask path: /home/worker/workspace/Ansible_Ubuntu_20.04_Desktop_ISO_70GA_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/linux/network_device_ops/pvrdma_network_device_ops.yml:149\nfatal: [localhost]: FAILED! => Failed to run RDMA ping from client VM 'test_vm_client_20230325085110' to server VM 'test_vm'. Hit error 'cma event RDMA_CM_EVENT_UNREACHABLE, error -110'\nerror message:\nFailed to run RDMA ping from client VM 'test_vm_client_20230325085110' to server VM 'test_vm'. Hit error 'cma event RDMA_CM_EVENT_UNREACHABLE, error -110'", "category": null, "processed": "timestamp failed at play pvrdma network device ops timestamp task pvrdma network device ops check remote direct memory access ping result from client vm to server vm task path home worker workspace ansible ubuntu number desktop iso number ga ide bios ansible vsphere gos validation linux network device ops pvrdma network device ops yml number fatal localhost failed failed to run remote direct memory access ping from client vm test vm client timestamp to server vm test vm hit error cma event remote direct memory access cm event un reachable error number error message failed to run remote direct memory access ping from client vm test vm client timestamp to server vm test vm hit error cma event remote direct memory access cm event un reachable error number", "solution": "deepdive", "target": "targetvm", "version": 202304110353}, {"id": 5856, "name": "1", "raw": "2023-03-23 03:46:43,023 | Failed at Play [env_setup] *********************************\n2023-03-23 03:46:43,023 | TASK [env_setup][Check VM only has one network adapter and it should be connected] \ntask path: /home/worker/workspace/Ansible_Ubuntu_20.04_Server_ISO_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/env_setup/check_vm_settings.yml:10\nfatal: [localhost]: FAILED! => VM doesn't meet test requirement, which must have one network adapter and it should be connected. Current VM's network adapters are '{'0': {'mac_address': '00:50:56:af:ab:57', 'label': 'Network adapter 1', 'unit_number': 7, 'wake_onlan': True, 'allow_guest_ctl': True, 'connected': True, 'start_connected': True, 'network_name': 'VM Network', 'vlan_id': 0, 'switch': 'vSwitch0', 'device_type': 'vmxnet3', 'mac_addr': '00:50:56:af:ab:57', 'name': 'VM Network'}, '1': {'mac_address': '00:50:56:af:c7:ee', 'label': 'Network adapter 2', 'unit_number': 8, 'wake_onlan': False, 'allow_guest_ctl': True, 'connected': True, 'start_connected': True, 'network_name': 'DPortGroup2023-03-22-20-41-44', 'switch': 'DSwitch2023-03-22-20-41-44', 'device_type': 'pvrdma', 'device_protocol': 'rocev2', 'mac_addr': '00:50:56:af:c7:ee', 'name': 'DPortGroup2023-03-22-20-41-44'}}'.\nerror message:\nVM doesn't meet test requirement, which must have one network adapter and it should be connected. Current VM's network adapters are '{'0': {'mac_address': '00:50:56:af:ab:57', 'label': 'Network adapter 1', 'unit_number': 7, 'wake_onlan': True, 'allow_guest_ctl': True, 'connected': True, 'start_connected': True, 'network_name': 'VM Network', 'vlan_id': 0, 'switch': 'vSwitch0', 'device_type': 'vmxnet3', 'mac_addr': '00:50:56:af:ab:57', 'name': 'VM Network'}, '1': {'mac_address': '00:50:56:af:c7:ee', 'label': 'Network adapter 2', 'unit_number': 8, 'wake_onlan': False, 'allow_guest_ctl': True, 'connected': True, 'start_connected': True, 'network_name': 'DPortGroup2023-03-22-20-41-44', 'switch': 'DSwitch2023-03-22-20-41-44', 'device_type': 'pvrdma', 'device_protocol': 'rocev2', 'mac_addr': '00:50:56:af:c7:ee', 'name': 'DPortGroup2023-03-22-20-41-44'}}'.", "category": null, "processed": "timestamp failed at play environment setup timestamp task environment setup check vm only has one network adapter and it should be connected task path home worker workspace ansible ubuntu number server iso lsi logic sas vmxnet number bios ansible vsphere gos validation environment setup check vm settings yml number fatal localhost failed vm doesn t meet test requirement which must have one network adapter and it should be connected current vm s network adapters are number mac address mac address label network adapter number unit number wake on lan true allow guest ctl true connected true start connected true network name vm network vlan id number switch vswitch number device type vmxnet number mac address mac address name vm network number mac address mac address label network adapter number unit number wake on lan false allow guest ctl true connected true start connected true network name d port group timestamp number switch ds witch timestamp number device type pvrdma device protocol roce v number mac address mac address name d port group timestamp number error message vm doesn t meet test requirement which must have one network adapter and it should be connected current vm s network adapters are number mac address mac address label network adapter number unit number wake on lan true allow guest ctl true connected true start connected true network name vm network vlan id number switch vswitch number device type vmxnet number mac address mac address name vm network number mac address mac address label network adapter number unit number wake on lan false allow guest ctl true connected true start connected true network name d port group timestamp number switch ds witch timestamp number device type pvrdma device protocol roce v number mac address mac address name d port group timestamp number", "solution": "deepdive", "target": "usererror", "version": 202304110353}, {"id": 5860, "name": "1", "raw": "2023-03-31 11:54:44,031 | Failed at Play [3_lsilogic_vhba_device_ops] ****************\n2023-03-31 11:54:44,031 | TASK [3_lsilogic_vhba_device_ops][Wait for device to be present] \ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Server_OVA_80GA/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:147\nfatal: [localhost]: UNREACHABLE! => Failed to create temporary directory. In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1680263684.8301709-8934-27032237102412 `\" && echo ansible-tmp-1680263684.8301709-8934-27032237102412=\"` echo /tmp/ansible-tmp-1680263684.8301709-8934-27032237102412 `\" ), exited with result 1\nerror message:\nFailed to create temporary directory. In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1680263684.8301709-8934-27032237102412 `\" && echo ansible-tmp-1680263684.8301709-8934-27032237102412=\"` echo /tmp/ansible-tmp-1680263684.8301709-8934-27032237102412 `\" ), exited with result 1\n2023-03-31 11:54:45,031 | TASK [3_lsilogic_vhba_device_ops][Guest OS unreachable] ****\ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Server_OVA_80GA/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:160\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': 'Failed to create temporary directory. In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1680263684.8301709-8934-27032237102412 `\" && echo ansible-tmp-1680263684.8301709-8934-27032237102412=\"` echo /tmp/ansible-tmp-1680263684.8301709-8934-27032237102412 `\" ), exited with result 1', 'unreachable': True}\nerror message:\nFailed to create temporary directory. In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1680263684.8301709-8934-27032237102412 `\" && echo ansible-tmp-1680263684.8301709-8934-27032237102412=\"` echo /tmp/ansible-tmp-1680263684.8301709-8934-27032237102412 `\" ), exited with result 1", "category": null, "processed": "timestamp failed at play number lsi logic v hba device ops timestamp task number lsi logic v hba device ops wait for device to be present task path home worker workspace ansible ubuntu number server ova number ga ansible vsphere gos validation linux v hba hot add remove wait device list changed yml number fatal localhost un reachable failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo tmp ansible tmp hex id number timestamp exited with result number error message failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo tmp ansible tmp hex id number timestamp exited with result number timestamp task number lsi logic v hba device ops guest os un reachable task path home worker workspace ansible ubuntu number server ova number ga ansible vsphere gos validation linux v hba hot add remove wait device list changed yml number fatal localhost failed changed false message failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo tmp ansible tmp hex id number timestamp exited with result number un reachable true error message failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo tmp ansible tmp hex id number timestamp exited with result number", "solution": "deepdive", "target": "targetvm", "version": 202304110353}, {"id": 5861, "name": "1", "raw": "2023-03-31 07:27:57,031 | Failed at Play [01_deploy_vm] ******************************\n2023-03-31 07:27:57,031 | TASK [01_deploy_vm][Wait for message 'gdm.service' appear in VM log serial-20230331062508.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Desktop_ISO_80GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:40\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2023-03-31 07:28:41,031 | TASK [01_deploy_vm][Testing exit due to failure] ***********\ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Desktop_ISO_80GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:87\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_vmxnet3\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_vmxnet3\n2023-03-31 07:34:25,031 | TASK [01_deploy_vm][Check VMware Tools is running and collects guest OS fullname successfully] \ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Desktop_ISO_80GA_IDE_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_guest_fullname.yml:46\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest OS fullname in 300 seconds. Current VMware Tools running status is 'guestToolsNotRunning', and guest OS fullname is ''.\nerror message:\nIt's timed out for VMware Tools collecting guest OS fullname in 300 seconds. Current VMware Tools running status is 'guestToolsNotRunning', and guest OS fullname is ''.", "category": null, "processed": "timestamp failed at play number deploy vm timestamp task number deploy vm wait for message gdm service appear in vm log serial timestamp log task path home worker workspace ansible ubuntu number desktop iso number ga ide vmxnet number bios ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task number deploy vm testing exit due to failure task path home worker workspace ansible ubuntu number desktop iso number ga ide vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm bios ide vmxnet number error message exit testing because exit testing when fail is set to true in test case deploy vm bios ide vmxnet number timestamp task number deploy vm check vmware tools is running and collects guest os full name successfully task path home worker workspace ansible ubuntu number desktop iso number ga ide vmxnet number bios ansible vsphere gos validation common vm wait guest full name yml number fatal localhost failed it s timed out for vmware tools collecting guest os full name in number seconds current vmware tools running status is guest tools not running and guest os full name is error message it s timed out for vmware tools collecting guest os full name in number seconds current vmware tools running status is guest tools not running and guest os full name is", "solution": "deepdive", "target": "targetvm", "version": 202304110353}, {"id": 5857, "name": "1", "raw": "2023-03-23 15:57:51,023 | Failed at Play [gosc_cloudinit_staticip] *******************\n2023-03-23 15:57:51,023 | TASK [gosc_cloudinit_staticip][Check GOSC complete message] \ntask path: /home/worker/workspace/Ansible_Ubuntu_20.04_Desktop_ISO_70U1_IDE_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/guest_customization/wait_gosc_complete_msg.yml:46\nfatal: [localhost]: FAILED! => It's timed out to wait for GOSC complete message 'Cloud-init .*finished at.*Datasource DataSourceOVF \\[seed=vmware-tools\\]' present in /var/log/cloud-init.log in 100 seconds.\nerror message:\nIt's timed out to wait for GOSC complete message 'Cloud-init .*finished at.*Datasource DataSourceOVF \\[seed=vmware-tools\\]' present in /var/log/cloud-init.log in 100 seconds.", "category": null, "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip check go sc complete message task path home worker workspace ansible ubuntu number desktop iso ide vmxnet number efi ansible vsphere gos validation linux guest customization wait go sc complete message yml number fatal localhost failed it s timed out to wait for go sc complete message cloud init finished at data source data source ovf seed vmware tools present in var log cloud init log in number seconds error message it s timed out to wait for go sc complete message cloud init finished at data source data source ovf seed vmware tools present in var log cloud init log in number seconds", "solution": "deepdive", "target": "targetvm", "version": 202304110353}, {"id": 5862, "name": "1", "raw": "2023-03-31 11:52:23,031 | Failed at Play [2_gosc_cloudinit_staticip] *****************\n2023-03-31 11:52:23,031 | TASK [2_gosc_cloudinit_staticip][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Server_OVA_80GA/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:49\nfatal: [localhost]: FAILED! => [\"It's timed out for VMware Tools collecting guest IPv4 address in 300 seconds.\", \"VMware Tools running status is 'guestToolsRunning'.\", \"VM's IP address in guest info is ''.\", \"VM's all IP addresses in guest info are '[]'.\"]\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 300 seconds.\nVMware Tools running status is 'guestToolsRunning'.\nVM's IP address in guest info is ''.\nVM's all IP addresses in guest info are '[]'.", "category": null, "processed": "timestamp failed at play number go sc cloud init static ip timestamp task number go sc cloud init static ip check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible ubuntu number server ova number ga ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds vmware tools running status is guest tools running vm sip address in guest info is vm s all ip addresses in guest info are error message it s timed out for vmware tools collecting guest ip v number address in number seconds vmware tools running status is guest tools running vm sip address in guest info is vm s all ip addresses in guest info are", "solution": "deepdive", "target": "targetvm", "version": 202304110353}, {"id": 5858, "name": "1", "raw": "2023-03-01 11:28:32,001 | Failed at Play [deploy_vm_bios_ide_e1000e] *****************\n2023-03-01 11:28:32,001 | TASK [deploy_vm_bios_ide_e1000e][Create a new VM 'test_vm' on server '10.186.1.224'] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.04_Desktop_ISO_70GA_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/vm_create.yml:30\nfatal: [localhost]: FAILED! => Failed to create a virtual machine : A component of the virtual machine is not accessible on the host.\nerror message:\nFailed to create a virtual machine : A component of the virtual machine is not accessible on the host.\n2023-03-01 11:28:33,001 | TASK [deploy_vm_bios_ide_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.04_Desktop_ISO_70GA_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_e1000e\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios ide timestamp task deploy vm bios ide create a new vm test vm on server ip address task path home worker workspace ansible ubuntu number desktop iso number ga ide bios ansible vsphere gos validation common vm create yml number fatal localhost failed failed to create a virtual machine a component of the virtual machine is not accessible on the host error message failed to create a virtual machine a component of the virtual machine is not accessible on the host timestamp task deploy vm bios ide testing exit due to failure task path home worker workspace ansible ubuntu number desktop iso number ga ide bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm bios ide e number e error message exit testing when exit testing when fail is set to true in test case deploy vm bios ide e number e", "solution": "deepdive", "target": "testbed", "version": 202304110353}, {"id": 5859, "name": "1", "raw": "2023-03-22 22:21:44,022 | Failed at Play [pvrdma_network_device_ops] *****************\n2023-03-22 22:21:44,022 | TASK [pvrdma_network_device_ops][Remove a network adapter from VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_Ubuntu_20.04_Server_ISO_70U1_LSILOGICSAS_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_remove_network_adapter.yml:4\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error\nerror message:\nMODULE FAILURE\nSee stdout/stderr for the exact error\npyVmomi.VmomiSupport.GenericVmConfigFault: (vim.fault.GenericVmConfigFault) {\n   dynamicType = <unset>,\n   dynamicProperty = (vmodl.DynamicProperty) [],\n   msg = \"The guest operating system did not respond to a hot-remove request for device 'ethernet1' in a timely manner.\",\n   faultCause = <unset>,\n   faultMessage = (vmodl.LocalizableMessage) [\n      (vmodl.LocalizableMessage) {\n         dynamicType = <unset>,\n         dynamicProperty = (vmodl.DynamicProperty) [],\n         key = 'msg.vigor.hotRemoveStillExists',\n         arg = (vmodl.KeyAnyValue) [\n            (vmodl.KeyAnyValue) {\n               dynamicType = <unset>,\n               dynamicProperty = (vmodl.DynamicProperty) [],\n               key = '1',\n               value = 'ethernet1'\n            }\n         ],\n         message = \"The guest operating system did not respond to a hot-remove request for device 'ethernet1' in a timely manner.\"\n      }\n   ],\n   reason = \"The guest operating system did not respond to a hot-remove request for device 'ethernet1' in a timely manner.\"\n}\nThe above exception was the direct cause of the following exception:\nansible_collections.community.vmware.plugins.module_utils.vmware.TaskError: (\"The guest operating system did not respond to a hot-remove request for device 'ethernet1' in a timely manner.\", None)", "category": null, "processed": "timestamp failed at play pvrdma network device ops timestamp task pvrdma network device ops remove a network adapter from vm test vm task path home worker workspace ansible ubuntu number server iso lsi logic sas vmxnet number bios ansible vsphere gos validation common vm remove network adapter yml number fatal localhost failed module failure see stdout stderr for the exact error error message module failure see stdout stderr for the exact error python vmomi vmomi support generic vm configuration fault vim fault generic vm configuration fault dynamic type un set dynamic property vmodl dynamic property message the guest operating system did not respond to a hot remove request for device ethernet number in a timely manner fault cause un set fault message vmodl localizable message vmodl localizable message dynamic type un set dynamic property vmodl dynamic property key message vigor hot remove still exists arg vmodl key any value vmodl key any value dynamic type un set dynamic property vmodl dynamic property key number value ethernet number message the guest operating system did not respond to a hot remove request for device ethernet number in a timely manner reason the guest operating system did not respond to a hot remove request for device ethernet number in a timely manner the above exception was the direct cause of the following exception ansible collections community vmware plugins module utilities vmware task error the guest operating system did not respond to a hot remove request for device ethernet number in a timely manner none", "solution": "retry", "target": "targetvm", "version": 202304110353}, {"id": 5890, "name": "4", "raw": "2023-03-23 18:07:54,023 | Failed at Play [pvrdma_network_device_ops] *****************\n2023-03-23 18:07:54,023 | TASK [pvrdma_network_device_ops][Check new RDMA device exists] \ntask path: /home/worker/workspace/Ansible_RHEL_8.x_80GA_PARAVIRTUAL_E1000E_EFI/ansible-vsphere-gos-validation/linux/network_device_ops/check_and_reload_pvrdma.yml:22\nfatal: [localhost]: FAILED! => No new RDMA device is detected on VM test_vm\nerror message:\nNo new RDMA device is detected on VM test_vm", "category": null, "processed": "timestamp failed at play pvrdma network device ops timestamp task pvrdma network device ops check new remote direct memory access device exists task path home worker workspace ansible rhel number x number ga para virtual efi ansible vsphere gos validation linux network device ops check and reload pvrdma yml number fatal localhost failed no new remote direct memory access device is detected on vm test vm error message no new remote direct memory access device is detected on vm test vm", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5885, "name": "4", "raw": "2023-03-23 19:59:21,023 | Failed at Play [check_os_fullname] *************************\n2023-03-23 19:59:21,023 | TASK [check_os_fullname][Check guest family in VM's guest info with VMware Tools 12.1.5 on ESXi 7.0.3] \ntask path: /home/worker/workspace/Ansible_RHEL_9.x_70U3_NVME_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/check_os_fullname/validate_os_fullname.yml:99\nfatal: [localhost]: FAILED! => VM's guest family in guest info is 'otherGuestFamily',\n not expected 'linuxGuest'.\nerror message:\nVM's guest family in guest info is 'otherGuestFamily',\n not expected 'linuxGuest'.", "category": null, "processed": "timestamp failed at play check os full name timestamp task check os full name check guest family in vm s guest info with vmware tools version id on esxi version id task path home worker workspace ansible rhel number x nvme vmxnet number efi ansible vsphere gos validation linux check os full name validate os full name yml number fatal localhost failed vm s guest family in guest info is other guest family not expected linux guest error message vm s guest family in guest info is other guest family not expected linux guest", "solution": "deepdive", "target": "product", "version": 202304110953}, {"id": 5896, "name": "5", "raw": "2023-03-03 07:42:15,003 | Failed at Play [check_os_fullname] *************************\n2023-03-03 07:42:15,003 | TASK [check_os_fullname][Assert VM has valid IPv4 address for VM 'test_windows11_22538'] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/common/vm_get_primary_nic.yml:15\nfatal: [localhost]: FAILED! => Not found valid IPv4 address for VM 'test_windows11_22538'\nerror message:\nNot found valid IPv4 address for VM 'test_windows11_22538'", "category": null, "processed": "timestamp failed at play check os full name timestamp task check os full name assert vm has valid ip v number address for vm test windows number task path home worker workspace ansible regression windows number ansible vsphere gos validation common vm get primary nic yml number fatal localhost failed not found valid ip v number address for vm test windows number error message not found valid ip v number address for vm test windows number number", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5901, "name": "5", "raw": "2023-03-09 07:11:57,009 | Failed at Play [nvdimm_cold_add_remove] ********************\n2023-03-09 07:11:57,009 | TASK [nvdimm_cold_add_remove][Revert snapshot failed] ******\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_Physical_VBS_80GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:47\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed\nerror message:\nRevert to snapshot 'BaseSnapshot' failed", "category": null, "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove revert snapshot failed task path home worker workspace ansible windows server lts c physical vbs number ga lsi logic sas efi ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed error message revert to snapshot base snapshot failed", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5898, "name": "5", "raw": "2023-03-03 07:48:24,003 | Failed at Play [nvdimm_cold_add_remove] ********************\n2023-03-03 07:48:24,003 | TASK [nvdimm_cold_add_remove][Assert VM has valid IPv4 address for VM 'test_windows11_22538'] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/common/vm_get_primary_nic.yml:15\nfatal: [localhost]: FAILED! => Not found valid IPv4 address for VM 'test_windows11_22538'\nerror message:\nNot found valid IPv4 address for VM 'test_windows11_22538'", "category": null, "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove assert vm has valid ip v number address for vm test windows number task path home worker workspace ansible regression windows number ansible vsphere gos validation common vm get primary nic yml number fatal localhost failed not found valid ip v number address for vm test windows number error message not found valid ip v number address for vm test windows number number", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5886, "name": "4", "raw": "2023-03-24 05:42:29,024 | Failed at Play [deploy_vm_efi_lsilogicsas_vmxnet3] *********\n2023-03-24 05:42:29,024 | TASK [deploy_vm_efi_lsilogicsas_vmxnet3][Datastore file operation] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/common/esxi_check_delete_datastore_file.yml:28\nfatal: [localhost]: FAILED! => File 'OS/Other/Windows/Windows11/v21H2/en-us_windows_11_business_editions_x64_dvd_3a304c08.iso' is absent, cannot continue\nerror message:\nFile 'OS/Other/Windows/Windows11/v21H2/en-us_windows_11_business_editions_x64_dvd_3a304c08.iso' is absent, cannot continue\n2023-03-24 05:42:31,024 | TASK [deploy_vm_efi_lsilogicsas_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Windows_11_70U3_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:91\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_vmxnet3\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_lsilogicsas_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm efi lsi logic sas vmxnet number timestamp task deploy vm efi lsi logic sas vmxnet number data store file operation task path home worker workspace ansible windows number lsi logic sas vmxnet number efi ansible vsphere gos validation common esxi check delete data store file yml number fatal localhost failed file os other windows windows number v number h number en us windows number business editions x number dvd hex id iso is absent can not continue error message file os other windows windows number v number h number en us windows number business editions x number dvd hex id iso is absent can not continue timestamp task deploy vm efi lsi logic sas vmxnet number testing exit due to failure task path home worker workspace ansible windows number lsi logic sas vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm efi lsi logic sas vmxnet number error message exit testing because exit testing when fail is set to true in test case deploy vm efi lsi logic sas vmxnet number", "solution": "deepdive", "target": "usererror", "version": 202304110953}, {"id": 5887, "name": "4", "raw": "2023-03-20 13:53:41,020 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2023-03-20 13:53:41,020 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:49\nfatal: [localhost]: FAILED! => [\"It's timed out for VMware Tools collecting guest IPv4 address in 600 seconds.\", \"VMware Tools running status is 'guestToolsRunning'.\", \"VM's IP address in guest info is '2620:124:6020:c302:e798:9e56:5b3b:b0ee'.\", \"VM's all IP addresses in guest info are '['2620:124:6020:c302:e798:9e56:5b3b:b0ee', '2620:124:6020:c302:250:56ff:fe9f:af72', 'fe80::250:56ff:fe9f:af72']'.\"]\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 600 seconds.\nVMware Tools running status is 'guestToolsRunning'.\nVM's IP address in guest info is '2620:124:6020:c302:e798:9e56:5b3b:b0ee'.\nVM's all IP addresses in guest info are '['2620:124:6020:c302:e798:9e56:5b3b:b0ee', '2620:124:6020:c302:250:56ff:fe9f:af72', 'fe80::250:56ff:fe9f:af72']'.\n2023-03-20 13:54:26,020 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_MAIN_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:77\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible sles main para virtual vmxnet number efi ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds vmware tools running status is guest tools running vm sip address in guest info is ip address vm s all ip addresses in guest info are ip address ip address ip address error message it s timed out for vmware tools collecting guest ip v number address in number seconds vmware tools running status is guest tools running vm sip address in guest info is ip address vm s all ip addresses in guest info are ip address ip address ip address timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible sles main para virtual vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing because exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "nimbus", "version": 202304110953}, {"id": 5888, "name": "4", "raw": "2023-03-24 12:30:55,024 | Failed at Play [deploy_vm_bios_ide_e1000e] *****************\n2023-03-24 12:30:55,024 | TASK [deploy_vm_bios_ide_e1000e][Upload local file to ESXi datastore] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.04_Desktop_ISO_70GA_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/esxi_upload_datastore_file.yml:11\nssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:2384)\nurllib.error.URLError: <urlopen error EOF occurred in violation of protocol (_ssl.c:2384)>\nTypeError: 'URLError' object is not subscriptable\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error\nerror message:\nMODULE FAILURE\nSee stdout/stderr for the exact error\nssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:2384)\nurllib.error.URLError: <urlopen error EOF occurred in violation of protocol (_ssl.c:2384)>\nTypeError: 'URLError' object is not subscriptable\n2023-03-24 12:30:57,024 | TASK [deploy_vm_bios_ide_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Ubuntu_22.04_Desktop_ISO_70GA_IDE_E1000E_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:91\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_e1000e\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_ide_e1000e", "category": null, "processed": "timestamp failed at play deploy vm bios ide timestamp task deploy vm bios ide upload local file to esxi data store task path home worker workspace ansible ubuntu number desktop iso number ga ide bios ansible vsphere gos validation common esxi upload data store file yml number ssl ssl eof error eof occurred in violation of protocol ssl c number url library error url error url open error eof occurred in violation of protocol ssl c number type error url error object is not sub scriptable fatal localhost failed module failure see stdout stderr for the exact error error message module failure see stdout stderr for the exact error ssl ssl eof error eof occurred in violation of protocol ssl c number url library error url error url open error eof occurred in violation of protocol ssl c number type error url error object is not sub scriptable timestamp task deploy vm bios ide testing exit due to failure task path home worker workspace ansible ubuntu number desktop iso number ga ide bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm bios ide e number e error message exit testing because exit testing when fail is set to true in test case deploy vm bios ide e number e", "solution": "retry", "target": "nimbus", "version": 202304110953}, {"id": 5889, "name": "4", "raw": "2023-03-22 02:30:48,022 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2023-03-22 02:30:48,022 | TASK [deploy_vm_efi_ide_e1000e][Add IDE boot disk] *********\ntask path: /home/worker/workspace/Ansible_Photon_5.x_ISO_80GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_create_with_ide_disk.yml:49\nfatal: [localhost -> 10.182.5.68]: FAILED! => non-zero return code when vim-cmd\nerror message:\nnon-zero return code: 1\nReconfigure failed: (vim.fault.NoDiskSpace) {\n   faultCause = (vmodl.MethodFault) null, \n   faultMessage = (vmodl.LocalizableMessage) [\n      (vmodl.LocalizableMessage) {\n         key = \"vob.fssvec.SetFileAttributes.file.failed\", \n         arg = <unset>, \n         message = \"File system specific implementation of SetFileAttributes[file] failed\"\n      }\n   ], \n   file = \"[datastore2] test_vm_1679451863797/test_vm_1679451863797_1.vmdk\", \n   datastore = \"datastore2\"\n   msg = \"Insufficient disk space on datastore 'datastore2'.\"\n}\n2023-03-22 02:30:50,022 | TASK [deploy_vm_efi_ide_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Photon_5.x_ISO_80GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:77\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e", "category": null, "processed": "timestamp failed at play deploy vm efi ide timestamp task deploy vm efi ide add ide boot disk task path home worker workspace ansible photon number x iso number ga ide efi ansible vsphere gos validation common vm create with ide disk yml number fatal localhost ip address failed nonzero return code when vim command error message nonzero return code number re configure failed vim fault no disk space fault cause vmodl method fault null fault message vmodl localizable message vmodl localizable message key vob fss vec set file attributes file failed arg un set message file system specific implementation of set file attributes file failed file data store number test vm number test vm number vmdk data store data store number message insufficient disk space on data store data store number timestamp task deploy vm efi ide testing exit due to failure task path home worker workspace ansible photon number x iso number ga ide efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm efi ide e number e error message exit testing because exit testing when fail is set to true in test case deploy vm efi ide e number e", "solution": "retry", "target": "nimbus", "version": 202304110953}, {"id": 5906, "name": "7", "raw": "2023-03-20 05:20:07,020 | Failed at Play [pvrdma_network_device_ops] *****************\n2023-03-20 05:20:07,020 | TASK [pvrdma_network_device_ops][Check RDMA ping result from client VM to server VM] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_70GA_LSILOGICSAS_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/network_device_ops/pvrdma_network_device_ops.yml:149\nfatal: [localhost]: FAILED! => Failed to run RDMA ping from client VM 'test_vm_client_20230320051421' to server VM 'test_vm'. Hit error 'cma event RDMA_CM_EVENT_REJECTED, error 8'\nerror message:\nFailed to run RDMA ping from client VM 'test_vm_client_20230320051421' to server VM 'test_vm'. Hit error 'cma event RDMA_CM_EVENT_REJECTED, error 8'", "category": null, "processed": "timestamp failed at play pvrdma network device ops timestamp task pvrdma network device ops check remote direct memory access ping result from client vm to server vm task path home worker workspace ansible sles number ga lsi logic sas vmxnet number efi ansible vsphere gos validation linux network device ops pvrdma network device ops yml number fatal localhost failed failed to run remote direct memory access ping from client vm test vm client timestamp to server vm test vm hit error cma event remote direct memory access cm event rejected error number error message failed to run remote direct memory access ping from client vm test vm client timestamp to server vm test vm hit error cma event remote direct memory access cm event rejected error number", "solution": "retry", "target": "targetvm", "version": 202304110953}, {"id": 5894, "name": "5", "raw": "2023-03-03 07:53:20,003 | Failed at Play [check_os_fullname] *************************\n2023-03-03 07:53:20,003 | TASK [check_os_fullname][Assert VM has valid IPv4 address for VM 'test_ubuntu20.04.5_server_iso'] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_20.04_Server_ISO/ansible-vsphere-gos-validation/common/vm_get_primary_nic.yml:15\nfatal: [localhost]: FAILED! => Not found valid IPv4 address for VM 'test_ubuntu20.04.5_server_iso'\nerror message:\nNot found valid IPv4 address for VM 'test_ubuntu20.04.5_server_iso'\n2023-03-03 07:54:04,003 | TASK [check_os_fullname][Check guest file path is provided] \ntask path: /home/worker/workspace/Ansible_Regression_Ubuntu_20.04_Server_ISO/ansible-vsphere-gos-validation/linux/utils/get_file_stat_info.yml:11\nfatal: [localhost]: FAILED! => 'guest_file_path' must be set with a file path in guest OS\nerror message:\n'guest_file_path' must be set with a file path in guest OS", "category": null, "processed": "timestamp failed at play check os full name timestamp task check os full name assert vm has valid ip v number address for vm test ubuntu version id server iso task path home worker workspace ansible regression ubuntu number server iso ansible vsphere gos validation common vm get primary nic yml number fatal localhost failed not found valid ip v number address for vm test ubuntu version id server iso error message not found valid ip v number address for vm test ubuntu version id server iso timestamp task check os full name check guest file path is provided task path home worker workspace ansible regression ubuntu number server iso ansible vsphere gos validation linux utilities get file stat info yml number fatal localhost failed guest file path must be set with a file path in guest os error message guest file path must be set with a file path in guest os", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5899, "name": "5", "raw": "2023-03-22 02:16:29,022 | Failed at Play [gosc_cloudinit_staticip] *******************\n2023-03-22 02:16:29,022 | TASK [gosc_cloudinit_staticip][Check GOSC complete message] \ntask path: /home/worker/workspace/Ansible_Cycle_Photon_4.0_ISO/ansible-vsphere-gos-validation/linux/guest_customization/wait_gosc_complete_msg.yml:46\nfatal: [localhost]: FAILED! => It's timed out to wait for GOSC complete message 'Cloud-init .*finished at.*Datasource DataSourceOVF \\[seed=vmware-tools\\]' present in /var/log/cloud-init.log in 100 seconds.\nerror message:\nIt's timed out to wait for GOSC complete message 'Cloud-init .*finished at.*Datasource DataSourceOVF \\[seed=vmware-tools\\]' present in /var/log/cloud-init.log in 100 seconds.", "category": null, "processed": "timestamp failed at play go sc cloud init static ip timestamp task go sc cloud init static ip check go sc complete message task path home worker workspace ansible cycle photon number iso ansible vsphere gos validation linux guest customization wait go sc complete message yml number fatal localhost failed it s timed out to wait for go sc complete message cloud init finished at data source data source ovf seed vmware tools present in var log cloud init log in number seconds error message it s timed out to wait for go sc complete message cloud init finished at data source data source ovf seed vmware tools present in var log cloud init log in number seconds", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5904, "name": "6", "raw": "2023-03-09 07:18:19,009 | Failed at Play [wsl_distro_install_uninstall] **************\n2023-03-09 07:18:19,009 | TASK [wsl_distro_install_uninstall][Revert snapshot failed] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_Physical_VBS_80GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:47\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed\nerror message:\nRevert to snapshot 'BaseSnapshot' failed", "category": null, "processed": "timestamp failed at play wsl distro install un install timestamp task wsl distro install un install revert snapshot failed task path home worker workspace ansible windows server lts c physical vbs number ga lsi logic sas efi ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed error message revert to snapshot base snapshot failed", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5909, "name": "7", "raw": "2023-03-23 18:25:33,023 | Failed at Play [pvrdma_network_device_ops] *****************\n2023-03-23 18:25:33,023 | TASK [pvrdma_network_device_ops][Remove a network adapter from VM 'test_vm'] \ntask path: /home/worker/workspace/Ansible_RHEL_8.x_70U1_SATA_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_remove_network_adapter.yml:4\nfatal: [localhost]: FAILED! => MODULE FAILURE\nSee stdout/stderr for the exact error\nerror message:\nMODULE FAILURE\nSee stdout/stderr for the exact error\npyVmomi.VmomiSupport.GenericVmConfigFault: (vim.fault.GenericVmConfigFault) {\n   dynamicType = <unset>,\n   dynamicProperty = (vmodl.DynamicProperty) [],\n   msg = \"The guest operating system did not respond to a hot-remove request for device 'ethernet1' in a timely manner.\",\n   faultCause = <unset>,\n   faultMessage = (vmodl.LocalizableMessage) [\n      (vmodl.LocalizableMessage) {\n         dynamicType = <unset>,\n         dynamicProperty = (vmodl.DynamicProperty) [],\n         key = 'msg.vigor.hotRemoveStillExists',\n         arg = (vmodl.KeyAnyValue) [\n            (vmodl.KeyAnyValue) {\n               dynamicType = <unset>,\n               dynamicProperty = (vmodl.DynamicProperty) [],\n               key = '1',\n               value = 'ethernet1'\n            }\n         ],\n         message = \"The guest operating system did not respond to a hot-remove request for device 'ethernet1' in a timely manner.\"\n      }\n   ],\n   reason = \"The guest operating system did not respond to a hot-remove request for device 'ethernet1' in a timely manner.\"\n}\nThe above exception was the direct cause of the following exception:\nansible_collections.community.vmware.plugins.module_utils.vmware.TaskError: (\"The guest operating system did not respond to a hot-remove request for device 'ethernet1' in a timely manner.\", None)", "category": null, "processed": "timestamp failed at play pvrdma network device ops timestamp task pvrdma network device ops remove a network adapter from vm test vm task path home worker workspace ansible rhel number x sata vmxnet number bios ansible vsphere gos validation common vm remove network adapter yml number fatal localhost failed module failure see stdout stderr for the exact error error message module failure see stdout stderr for the exact error python vmomi vmomi support generic vm configuration fault vim fault generic vm configuration fault dynamic type un set dynamic property vmodl dynamic property message the guest operating system did not respond to a hot remove request for device ethernet number in a timely manner fault cause un set fault message vmodl localizable message vmodl localizable message dynamic type un set dynamic property vmodl dynamic property key message vigor hot remove still exists arg vmodl key any value vmodl key any value dynamic type un set dynamic property vmodl dynamic property key number value ethernet number message the guest operating system did not respond to a hot remove request for device ethernet number in a timely manner reason the guest operating system did not respond to a hot remove request for device ethernet number in a timely manner the above exception was the direct cause of the following exception ansible collections community vmware plugins module utilities vmware task error the guest operating system did not respond to a hot remove request for device ethernet number in a timely manner none", "solution": "retry", "target": "targetvm", "version": 202304110953}, {"id": 5891, "name": "4", "raw": "2023-03-03 07:17:26,003 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2023-03-03 07:17:26,003 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Assert VM has valid IPv4 address for VM 'test_photon3_iso'] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/common/vm_get_primary_nic.yml:15\nfatal: [localhost]: FAILED! => Not found valid IPv4 address for VM 'test_photon3_iso'\nerror message:\nNot found valid IPv4 address for VM 'test_photon3_iso'\n2023-03-03 07:18:10,003 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Regression_Photon_3.0_ISO/ansible-vsphere-gos-validation/common/test_rescue.yml:59\nfatal: [localhost]: FAILED! => Exit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing when 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number assert vm has valid ip v number address for vm test photon number iso task path home worker workspace ansible regression photon number iso ansible vsphere gos validation common vm get primary nic yml number fatal localhost failed not found valid ip v number address for vm test photon number iso error message not found valid ip v number address for vm test photon number iso timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible regression photon number iso ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5892, "name": "4", "raw": "2023-03-21 04:32:54,021 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2023-03-21 04:32:54,021 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Download datastore file] \ntask path: /home/worker/workspace/Ansible_Cycle_Debian_10.x_64bit/ansible-vsphere-gos-validation/common/esxi_download_datastore_file.yml:36\nTimeoutError: The read operation timed out\nfatal: [localhost]: FAILED! => failed to create temporary content file: The read operation timed out\nerror message:\nfailed to create temporary content file: The read operation timed out\n2023-03-21 04:32:55,021 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Cycle_Debian_10.x_64bit/ansible-vsphere-gos-validation/common/test_rescue.yml:77\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number download data store file task path home worker workspace ansible cycle debian number x number b it ansible vsphere gos validation common esxi download data store file yml number timeout error the read operation timed out fatal localhost failed failed to create temporary content file the read operation timed out error message failed to create temporary content file the read operation timed out timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible cycle debian number x number b it ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing because exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "retry", "target": "testbed", "version": 202304110953}, {"id": 5897, "name": "5", "raw": "2023-03-20 06:59:59,020 | Failed at Play [nvdimm_cold_add_remove] ********************\n2023-03-20 06:59:59,020 | TASK [nvdimm_cold_add_remove][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_SLES_15SP4_70U3_NVME_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:49\nfatal: [localhost]: FAILED! => [\"It's timed out for VMware Tools collecting guest IPv4 address in 900 seconds.\", \"VMware Tools running status is 'guestToolsRunning'.\", \"VM's IP address in guest info is '2620:124:6020:c308:0:a:0:63f'.\", \"VM's all IP addresses in guest info are '['2620:124:6020:c308:0:a:0:63f', '2620:124:6020:c308:8e1b:eac0:4aaf:5f59', '2620:124:6020:c308:250:56ff:fe9f:23f9', 'fe80::250:56ff:fe9f:23f9']'.\"]\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 900 seconds.\nVMware Tools running status is 'guestToolsRunning'.\nVM's IP address in guest info is '2620:124:6020:c308:0:a:0:63f'.\nVM's all IP addresses in guest info are '['2620:124:6020:c308:0:a:0:63f', '2620:124:6020:c308:8e1b:eac0:4aaf:5f59', '2620:124:6020:c308:250:56ff:fe9f:23f9', 'fe80::250:56ff:fe9f:23f9']'.", "category": null, "processed": "timestamp failed at play nvdimm cold add remove timestamp task nvdimm cold add remove check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible sles nvme efi ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds vmware tools running status is guest tools running vm sip address in guest info is ip address vm s all ip addresses in guest info are ip address ip address ip address ip address error message it s timed out for vmware tools collecting guest ip v number address in number seconds vmware tools running status is guest tools running vm sip address in guest info is ip address vm s all ip addresses in guest info are ip address ip address ip address ip address", "solution": "retry", "target": "nimbus", "version": 202304110953}, {"id": 5902, "name": "6", "raw": "2023-03-09 07:17:19,009 | Failed at Play [vbs_enable_disable] ************************\n2023-03-09 07:17:19,009 | TASK [vbs_enable_disable][Revert snapshot failed] **********\ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_Physical_VBS_80GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_revert_snapshot.yml:47\nfatal: [localhost]: FAILED! => Revert to snapshot 'BaseSnapshot' failed\nerror message:\nRevert to snapshot 'BaseSnapshot' failed", "category": null, "processed": "timestamp failed at play vbs enable disable timestamp task vbs enable disable revert snapshot failed task path home worker workspace ansible windows server lts c physical vbs number ga lsi logic sas efi ansible vsphere gos validation common vm revert snapshot yml number fatal localhost failed revert to snapshot base snapshot failed error message revert to snapshot base snapshot failed", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5907, "name": "7", "raw": "2023-03-27 09:39:06,027 | Failed at Play [deploy_vm_efi_paravirtual_vmxnet3] *********\n2023-03-27 09:39:06,027 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Check VM 'suvp_win11_v21h2_70u3' IP address] \ntask path: /home/worker/workspace/Ansible_SUVP_Windows_11_Physical_70U3_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_get_ip.yml:36\nfatal: [localhost]: FAILED! => Failed to get VM 'suvp_win11_v21h2_70u3' IP Address\nerror message:\nFailed to get VM 'suvp_win11_v21h2_70u3' IP Address\n2023-03-27 09:39:31,027 | TASK [deploy_vm_efi_paravirtual_vmxnet3][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_SUVP_Windows_11_Physical_70U3_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:91\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_paravirtual_vmxnet3", "category": null, "processed": "timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task deploy vm efi para virtual vmxnet number check vm suvp v number h number ip address task path home worker workspace ansible suvp windows number physical para virtual vmxnet number efi ansible vsphere gos validation common vm get ip yml number fatal localhost failed failed to get vm suvp v number h number ip address error message failed to get vm suvp v number h number ip address timestamp task deploy vm efi para virtual vmxnet number testing exit due to failure task path home worker workspace ansible suvp windows number physical para virtual vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number error message exit testing because exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5893, "name": "4", "raw": "2023-03-22 04:28:46,022 | Failed at Play [deploy_vm_efi_ide_e1000e] ******************\n2023-03-22 04:28:46,022 | TASK [deploy_vm_efi_ide_e1000e][Wait for message 'Autoinstall is completed.' appear in VM log serial-20230322032551.log] \ntask path: /home/worker/workspace/Ansible_Photon_5.x_ISO_80GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:40\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2023-03-22 04:29:30,022 | TASK [deploy_vm_efi_ide_e1000e][Testing exit due to failure] \ntask path: /home/worker/workspace/Ansible_Photon_5.x_ISO_80GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:77\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_ide_e1000e\n2023-03-22 04:35:18,022 | TASK [deploy_vm_efi_ide_e1000e][Check VMware Tools is running and collects guest OS fullname successfully] \ntask path: /home/worker/workspace/Ansible_Photon_5.x_ISO_80GA_IDE_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_wait_guest_fullname.yml:46\nfatal: [localhost]: FAILED! => It's timed out for VMware Tools collecting guest OS fullname in 300 seconds. Current VMware Tools running status is 'guestToolsNotRunning', and guest OS fullname is ''.\nerror message:\nIt's timed out for VMware Tools collecting guest OS fullname in 300 seconds. Current VMware Tools running status is 'guestToolsNotRunning', and guest OS fullname is ''.", "category": null, "processed": "timestamp failed at play deploy vm efi ide timestamp task deploy vm efi ide wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible photon number x iso number ga ide efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task deploy vm efi ide testing exit due to failure task path home worker workspace ansible photon number x iso number ga ide efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm efi ide e number e error message exit testing because exit testing when fail is set to true in test case deploy vm efi ide e number e timestamp task deploy vm efi ide check vmware tools is running and collects guest os full name successfully task path home worker workspace ansible photon number x iso number ga ide efi ansible vsphere gos validation common vm wait guest full name yml number fatal localhost failed it s timed out for vmware tools collecting guest os full name in number seconds current vmware tools running status is guest tools not running and guest os full name is error message it s timed out for vmware tools collecting guest os full name in number seconds current vmware tools running status is guest tools not running and guest os full name is", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5903, "name": "6", "raw": "2023-03-03 07:48:55,003 | Failed at Play [e1000e_network_device_ops] *****************\n2023-03-03 07:48:55,003 | TASK [e1000e_network_device_ops][Assert VM has valid IPv4 address for VM 'test_windows11_22538'] \ntask path: /home/worker/workspace/Ansible_Regression_Windows_11_64/ansible-vsphere-gos-validation/common/vm_get_primary_nic.yml:15\nfatal: [localhost]: FAILED! => Not found valid IPv4 address for VM 'test_windows11_22538'\nerror message:\nNot found valid IPv4 address for VM 'test_windows11_22538'", "category": null, "processed": "timestamp failed at play network device ops timestamp task network device ops assert vm has valid ip v number address for vm test windows number task path home worker workspace ansible regression windows number ansible vsphere gos validation common vm get primary nic yml number fatal localhost failed not found valid ip v number address for vm test windows number error message not found valid ip v number address for vm test windows number number", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5908, "name": "7", "raw": "2023-04-10 06:43:02,010 | Failed at Play [5_wsl_distro_install_uninstall] ************\n2023-04-10 06:43:02,010 | TASK [5_wsl_distro_install_uninstall][Check WSL version setting result] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_Physical_70U3_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/windows/wsl_distro_install_uninstall/install_wsl.yml:102\nfatal: [localhost]: FAILED! => Setting WSL default version to 2 failed. Please check log file wsl_set_default_version_to_2.txt.\nerror message:\nSetting WSL default version to 2 failed. Please check log file wsl_set_default_version_to_2.txt.", "category": null, "processed": "timestamp failed at play number wsl distro install un install timestamp task number wsl distro install un install check wsl version setting result task path home worker workspace ansible windows server lts c physical lsi logic sas efi ansible vsphere gos validation windows wsl distro install un install install wsl yml number fatal localhost failed setting wsl default version to number failed please check log file wsl set default version to number text error message setting wsl default version to number failed please check log file wsl set default version to number text", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5895, "name": "5", "raw": "2023-03-21 09:31:21,021 | Failed at Play [deploy_photon_ova] *************************\n2023-03-21 09:31:21,021 | TASK [deploy_photon_ova][Check ping IP address 10.78.226.240 is successful] \ntask path: /home/worker/workspace/Ansible_Photon_5.x_OVA_80GA/ansible-vsphere-gos-validation/common/vm_wait_ping.yml:28\nfatal: [localhost]: FAILED! => Pinging IP address 10.78.226.240 failed in 300 seconds.\nerror message:\nPinging IP address 10.78.226.240 failed in 300 seconds.\n2023-03-21 09:31:49,021 | TASK [deploy_photon_ova][Testing exit due to failure] ******\ntask path: /home/worker/workspace/Ansible_Photon_5.x_OVA_80GA/ansible-vsphere-gos-validation/common/test_rescue.yml:77\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_photon_ova\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_photon_ova\n2023-03-21 09:32:06,021 | TASK [deploy_photon_ova][Fetch file /tmp/cloud-init_2023-03-21-09-31-56.tar.gz from VM guest] \ntask path: /home/worker/workspace/Ansible_Photon_5.x_OVA_80GA/ansible-vsphere-gos-validation/common/vm_guest_file_operation.yml:91\nexception in /vmware_guest_file_operation.py when fetch in /SoapAdapter.py when InvokeMethod\nfatal: [localhost]: FAILED! => Invalid guest login for user root\nerror message:\nInvalid guest login for user root", "category": null, "processed": "timestamp failed at play deploy photon ova timestamp task deploy photon ova check ping ip address ip address is successful task path home worker workspace ansible photon number x ova number ga ansible vsphere gos validation common vm wait ping yml number fatal localhost failed pinging ip address ip address failed in number seconds error message pinging ip address ip address failed in number seconds timestamp task deploy photon ova testing exit due to failure task path home worker workspace ansible photon number x ova number ga ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy photon ova error message exit testing because exit testing when fail is set to true in test case deploy photon ova timestamp task deploy photon ova fetch file tmp cloud init timestamp number tar gz from vm guest task path home worker workspace ansible photon number x ova number ga ansible vsphere gos validation common vm guest file operation yml number exception in vmware guest file operation python when fetch in soap adapter python when invoke method fatal localhost failed invalid guest login for user root error message invalid guest login for user root", "solution": "deepdive", "target": "targetvm", "version": 202304110953}, {"id": 5900, "name": "5", "raw": "2023-03-06 06:33:36,006 | Failed at Play [gosc_cloudinit_dhcp] ***********************\n2023-03-06 06:33:36,006 | TASK [gosc_cloudinit_dhcp][GOS customization failed] *******\ntask path: /home/worker/workspace/Ansible_Regression_AmazonLinux_2_OVA/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => [\"VM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com', 'gosc.test.com'] not expected search domains ['test.com', 'gosc.test.com']\"]\nerror message:\nVM DNS domain search domains are  ['nimbus.eng.vmware.com', 'eng.vmware.com', 'vmware.com', 'gosc.test.com'] not expected search domains ['test.com', 'gosc.test.com']", "category": null, "processed": "timestamp failed at play go sc cloud init dhcp timestamp task go sc cloud init dhcp gos customization failed task path home worker workspace ansible regression amazon linux number ova ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com go sc test com not expected search domains test com go sc test com error message vm dns domain search domains are nimbus eng vmware com eng vmware com vmware com go sc test com not expected search domains test com go sc test com", "solution": "deepdive", "target": "product", "version": 202304110953}, {"id": 5905, "name": "6", "raw": "2023-03-09 06:14:54,009 | Failed at Play [wsl_distro_install_uninstall] **************\n2023-03-09 06:14:54,009 | TASK [wsl_distro_install_uninstall][Change VM power state failure] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_Physical_VBS_80GA_LSILOGICSAS_E1000E_EFI/ansible-vsphere-gos-validation/common/vm_set_power_state.yml:69\nfatal: [localhost]: FAILED! => Unable to communicate with the remote host, since it is disconnected. \nerror message:\nUnable to communicate with the remote host, since it is disconnected.", "category": null, "processed": "timestamp failed at play wsl distro install un install timestamp task wsl distro install un install change vm power state failure task path home worker workspace ansible windows server lts c physical vbs number ga lsi logic sas efi ansible vsphere gos validation common vm set power state yml number fatal localhost failed unable to communicate with the remote host since it is disconnected error message unable to communicate with the remote host since it is disconnected", "solution": "retry", "target": "nimbus", "version": 202304110953}, {"id": 5910, "name": "7", "raw": "2023-04-08 20:43:38,008 | Failed at Play [1_nvme_vhba_device_ops_spec13] *************\n2023-04-08 20:43:38,008 | TASK [1_nvme_vhba_device_ops_spec13][Verify disk number increases in guest OS] \ntask path: /home/worker/workspace/Ansible_Windows_Server_LTSC_MAIN_LSILOGICSAS_VMXNET3_EFI_HW17/ansible-vsphere-gos-validation/windows/vhba_hot_add_remove/hotadd_vm_disk_existing_ctrl.yml:46\nfatal: [localhost]: FAILED! => Disk number not increase 2 after hotadding disks to new controller and existing controller, before hotadd: 1, after hotadd: 2\nerror message:\nDisk number not increase 2 after hotadding disks to new controller and existing controller, before hotadd: 1, after hotadd: 2", "category": null, "processed": "timestamp failed at play number nvme v hba device ops spec number timestamp task number nvme v hba device ops spec number verify disk number increases in guest os task path home worker workspace ansible windows server lts c main lsi logic sas vmxnet number efi ansible vsphere gos validation windows v hba hot add remove hot add vm disk existing ctrl yml number fatal localhost failed disk number not increase number after hot adding disks to new controller and existing controller before hot add number after hot add number error message disk number not increase number after hot adding disks to new controller and existing controller before hot add number after hot add number", "solution": "deepdive", "target": "product", "version": 202304110953}, {"id": 5915, "name": "42", "raw": "2023-04-13 06:20:34,013 | Failed at Play [17_e1000e_network_device_ops] **************\n2023-04-13 06:20:34,013 | TASK [17_e1000e_network_device_ops][Connect network interface 'ens35'] \ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Desktop_ISO_80GA_PARAVIRTUAL_VMXNET3_EFI/ansible-vsphere-gos-validation/linux/utils/set_network_adapter_status.yml:18\nfatal: [localhost]: UNREACHABLE! => Failed to connect to the host via ssh: ssh: connect to host 10.218.130.33 port 22: No route to host\nerror message:\nFailed to connect to the host via ssh: ssh: connect to host 10.218.130.33 port 22: No route to host", "category": null, "processed": "timestamp failed at play number network device ops timestamp task number network device ops connect network interface task path home worker workspace ansible ubuntu number desktop iso number ga para virtual vmxnet number efi ansible vsphere gos validation linux utilities set network adapter status yml number fatal localhost un reachable failed to connect to the host via ssh ssh connect to host ip address port number no route to host error message failed to connect to the host via ssh ssh connect to host ip address port number no route to host", "solution": "retry", "target": "targetvm", "version": 202304140402}, {"id": 5911, "name": "42", "raw": "2023-04-13 05:12:17,013 | Failed at Play [25_lsilogic_vhba_device_ops] ***************\n2023-04-13 05:12:17,013 | TASK [25_lsilogic_vhba_device_ops][Wait for device to be present] \ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Desktop_ISO_MAIN_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:147\nfatal: [localhost]: UNREACHABLE! => Failed to create temporary directory. In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1681362737.8157394-13757-22481210705733 `\" && echo ansible-tmp-1681362737.8157394-13757-22481210705733=\"` echo /tmp/ansible-tmp-1681362737.8157394-13757-22481210705733 `\" ), exited with result 1\nerror message:\nFailed to create temporary directory. In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1681362737.8157394-13757-22481210705733 `\" && echo ansible-tmp-1681362737.8157394-13757-22481210705733=\"` echo /tmp/ansible-tmp-1681362737.8157394-13757-22481210705733 `\" ), exited with result 1\n2023-04-13 05:12:18,013 | TASK [25_lsilogic_vhba_device_ops][Guest OS unreachable] ***\ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Desktop_ISO_MAIN_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:160\nfatal: [localhost]: FAILED! => {'changed': False, 'msg': 'Failed to create temporary directory. In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1681362737.8157394-13757-22481210705733 `\" && echo ansible-tmp-1681362737.8157394-13757-22481210705733=\"` echo /tmp/ansible-tmp-1681362737.8157394-13757-22481210705733 `\" ), exited with result 1', 'unreachable': True}\nerror message:\nFailed to create temporary directory. In some cases, you may have been able to authenticate and did not have permissions on the target directory. Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. Failed command was: ( umask 77 && mkdir -p \"` echo /tmp `\"&& mkdir \"` echo /tmp/ansible-tmp-1681362737.8157394-13757-22481210705733 `\" && echo ansible-tmp-1681362737.8157394-13757-22481210705733=\"` echo /tmp/ansible-tmp-1681362737.8157394-13757-22481210705733 `\" ), exited with result 1", "category": null, "processed": "timestamp failed at play number lsi logic v hba device ops timestamp task number lsi logic v hba device ops wait for device to be present task path home worker workspace ansible ubuntu number desktop iso main para virtual bios ansible vsphere gos validation linux v hba hot add remove wait device list changed yml number fatal localhost un reachable failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo tmp ansible tmp hex id number timestamp exited with result number error message failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo tmp ansible tmp hex id number timestamp exited with result number timestamp task number lsi logic v hba device ops guest os un reachable task path home worker workspace ansible ubuntu number desktop iso main para virtual bios ansible vsphere gos validation linux v hba hot add remove wait device list changed yml number fatal localhost failed changed false message failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo tmp ansible tmp hex id number timestamp exited with result number un reachable true error message failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo tmp mkdir echo tmp ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo tmp ansible tmp hex id number timestamp exited with result number", "solution": "deepdive", "target": "targetvm", "version": 202304140402}, {"id": 5912, "name": "42", "raw": "2023-04-13 05:29:25,013 | Failed at Play [1_deploy_vm] *******************************\n2023-04-13 05:29:25,013 | TASK [1_deploy_vm][Wait for message 'gdm.service' appear in VM log serial-20230413042559.log] \ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Desktop_ISO_MAIN_SATA_VMXNET3_EFI/ansible-vsphere-gos-validation/common/vm_wait_log_msg.yml:40\nfatal: [localhost]: FAILED! => {\n    \"attempts\": 120,\n    \"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\",\n    \"changed\": false\n}\nerror message:\n2023-04-13 05:30:09,013 | TASK [1_deploy_vm][Testing exit due to failure] ************\ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Desktop_ISO_MAIN_SATA_VMXNET3_EFI/ansible-vsphere-gos-validation/common/test_rescue.yml:87\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_vmxnet3\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_efi_sata_vmxnet3", "category": null, "processed": "timestamp failed at play number deploy vm timestamp task number deploy vm wait for message gdm service appear in vm log serial timestamp log task path home worker workspace ansible ubuntu number desktop iso main sata vmxnet number efi ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false error message timestamp task number deploy vm testing exit due to failure task path home worker workspace ansible ubuntu number desktop iso main sata vmxnet number efi ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm efi sata vmxnet number error message exit testing because exit testing when fail is set to true in test case deploy vm efi sata vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202304140402}, {"id": 5913, "name": "42", "raw": "2023-04-07 08:52:23,007 | Failed at Play [01_deploy_vm] ******************************\n2023-04-07 08:52:23,007 | TASK [01_deploy_vm][Check VMware Tools is running and collects guest IPv4 address successfully] \ntask path: /home/worker/workspace/Ansible_SLED_15SP4_70U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/vm_wait_guest_ip.yml:49\nfatal: [localhost]: FAILED! => [\"It's timed out for VMware Tools collecting guest IPv4 address in 600 seconds.\", \"VMware Tools running status is 'guestToolsRunning'.\", \"VM's IP address in guest info is ''.\", \"VM's all IP addresses in guest info are '[]'.\"]\nerror message:\nIt's timed out for VMware Tools collecting guest IPv4 address in 600 seconds.\nVMware Tools running status is 'guestToolsRunning'.\nVM's IP address in guest info is ''.\nVM's all IP addresses in guest info are '[]'.\n2023-04-07 08:53:05,007 | TASK [01_deploy_vm][Testing exit due to failure] ***********\ntask path: /home/worker/workspace/Ansible_SLED_15SP4_70U3_PARAVIRTUAL_VMXNET3_BIOS/ansible-vsphere-gos-validation/common/test_rescue.yml:87\nfatal: [localhost]: FAILED! => Exit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_paravirtual_vmxnet3\nerror message:\nExit testing because 'exit_testing_when_fail' is set to True in test case deploy_vm_bios_paravirtual_vmxnet3", "category": null, "processed": "timestamp failed at play number deploy vm timestamp task number deploy vm check vmware tools is running and collects guest ip v number address successfully task path home worker workspace ansible sled para virtual vmxnet number bios ansible vsphere gos validation common vm wait guest ip yml number fatal localhost failed it s timed out for vmware tools collecting guest ip v number address in number seconds vmware tools running status is guest tools running vm sip address in guest info is vm s all ip addresses in guest info are error message it s timed out for vmware tools collecting guest ip v number address in number seconds vmware tools running status is guest tools running vm sip address in guest info is vm s all ip addresses in guest info are timestamp task number deploy vm testing exit due to failure task path home worker workspace ansible sled para virtual vmxnet number bios ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing because exit testing when fail is set to true in test case deploy vm bios para virtual vmxnet number error message exit testing because exit testing when fail is set to true in test case deploy vm bios para virtual vmxnet number", "solution": "deepdive", "target": "targetvm", "version": 202304140402}, {"id": 5914, "name": "42", "raw": "2023-04-13 05:03:42,013 | Failed at Play [23_gosc_cloudinit_staticip] ****************\n2023-04-13 05:03:42,013 | TASK [23_gosc_cloudinit_staticip][GOS customization failed] \ntask path: /home/worker/workspace/Ansible_Ubuntu_23.04_Desktop_ISO_MAIN_PARAVIRTUAL_E1000E_BIOS/ansible-vsphere-gos-validation/linux/guest_customization/linux_gosc_verify.yml:114\nfatal: [localhost]: FAILED! => ['Not found GOSC completed state keyword in vmware.log', \"VM DNS servers are ['192.168.192.1'], not expected DNS servers ['192.168.1.1', '192.168.1.2']\", \"VM DNS domain search domains are  [] not expected search domains ['test.com', 'gosc.test.com']\", \"VM static IPv4 address is '192.168.192.109', expected IPv4 address is 192.168.192.101\", \"VM static IPv4 gateway is '192.168.1.1', expected IPv4 gateway is 192.168.192.1\"]\nerror message:\nNot found GOSC completed state keyword in vmware.log\nVM DNS servers are ['192.168.192.1'], not expected DNS servers ['192.168.1.1', '192.168.1.2']\nVM DNS domain search domains are  [] not expected search domains ['test.com', 'gosc.test.com']\nVM static IPv4 address is '192.168.192.109', expected IPv4 address is 192.168.192.101\nVM static IPv4 gateway is '192.168.1.1', expected IPv4 gateway is 192.168.192.1", "category": null, "processed": "timestamp failed at play number go sc cloud init static ip timestamp task number go sc cloud init static ip gos customization failed task path home worker workspace ansible ubuntu number desktop iso main para virtual bios ansible vsphere gos validation linux guest customization linux go sc verify yml number fatal localhost failed not found go sc completed state keyword in vmware log vm dns servers are ip address not expected dns servers ip address ip address vm dns domain search domains are not expected search domains test com go sc test com vm static ip v number address is ip address expected ip v number address is ip address vm static ip v number gateway is ip address expected ip v number gateway is ip address error message not found go sc completed state keyword in vmware log vm dns servers are ip address not expected dns servers ip address ip address vm dns domain search domains are not expected search domains test com go sc test com vm static ip v number address is ip address expected ip v number address is ip address vm static ip v number gateway is ip address expected ip v number gateway is ip address", "solution": "deepdive", "target": "targetvm", "version": 202304140402}, {"id": 5916, "name": "43", "raw": "2023-04-13 15:59:45,013 | Failed at Play [25_lsilogic_vhba_device_ops] ***************\n2023-04-13 15:59:45,013 | TASK [25_lsilogic_vhba_device_ops][Rescan all scsi devices] \ntask path: /home/worker/workspace/Ansible_Photon_5.x_OVA_70GA/ansible-vsphere-gos-validation/linux/vhba_hot_add_remove/wait_device_list_changed.yml:36\nfatal: [localhost]: FAILED! => Failed to connect to the host via ssh: ssh: connect to host 10.78.164.165 port 22: Connection refused\nerror message:\nFailed to connect to the host via ssh: ssh: connect to host 10.78.164.165 port 22: Connection refused", "category": null, "processed": "timestamp failed at play number lsi logic v hba device ops timestamp task number lsi logic v hba device ops re scan all scsi devices task path home worker workspace ansible photon number x ova number ga ansible vsphere gos validation linux v hba hot add remove wait device list changed yml number fatal localhost failed failed to connect to the host via ssh ssh connect to host ip address port number connection refused error message failed to connect to the host via ssh ssh connect to host ip address port number connection refused", "solution": "deepdive", "target": "targetvm", "version": 202304140402}]