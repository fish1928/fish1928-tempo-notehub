{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fa7b86-5db7-4bac-83e5-a9a817e135fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, EarlyStoppingCallback, DistilBertForMaskedLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "DEVICE_DEFAULT = 'cuda'\n",
    "\n",
    "\n",
    "def get_ts():\n",
    "    return datetime.utcnow().replace(microsecond=0).isoformat()\n",
    "# end\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids.reshape(-1)\n",
    "    preds = pred.predictions.argmax(-1).reshape(-1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=preds)\n",
    "    precision = precision_score(y_true=labels, y_pred=preds, zero_division=1, average='macro')\n",
    "    recall = recall_score(y_true=labels, y_pred=preds, zero_division=1, average='macro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, zero_division=1, average='macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "# end\n",
    "\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_length=512\n",
    "output_dir = 'results'\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "labels_outdomain = ['testcase', 'targetvm', 'nimbus', 'testbed', 'usererror', 'product', 'infra']\n",
    "labels_indomain = ['test','machine','nimbus','environment','user', 'product', 'infrastructure']\n",
    "tokenids_indomain = tokenizer.convert_tokens_to_ids(labels_indomain)\n",
    "dict_labelout_tokenid = {labelout:tokenid for labelout, tokenid in zip(labels_outdomain, tokenids_indomain)}\n",
    "dict_tokenid_labelout = {tokenid:labelout for labelout, tokenid in zip(labels_outdomain, tokenids_indomain)}\n",
    "\n",
    "\n",
    "\n",
    "train_samples = [['timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number', 'is [MASK] problem']]\n",
    "valid_samples = [['timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number', 'is [MASK] problem']]\n",
    "\n",
    "train_labels = [['timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number', 'is target problem']]\n",
    "valid_labels = [['timestamp failed at play deploy vm efi para virtual vmxnet number timestamp task wait for message auto install is completed appear in vm log serial timestamp log task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common vm wait log message yml number fatal localhost failed attempts number censored the output has been hidden due to the fact that no log true was specified for this result changed false timestamp task testing exit due to failure task path home worker workspace ansible cycle photon number x update ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi para virtual vmxnet number', 'is target problem']]\n",
    "\n",
    "\n",
    "train_encodings = tokenizer.batch_encode_plus(train_samples, truncation=True, padding=True, max_length=max_length,\n",
    "                                              return_tensors='pt')\n",
    "valid_encodings = tokenizer.batch_encode_plus(valid_samples, truncation=True, padding=True, max_length=max_length,\n",
    "                                              return_tensors='pt')\n",
    "\n",
    "train_labels_e = tokenizer.batch_encode_plus(train_labels, truncation=True, padding=True, max_length=max_length,\n",
    "                                              return_tensors='pt')\n",
    "\n",
    "valid_labels_e = tokenizer.batch_encode_plus(valid_labels, truncation=True, padding=True, max_length=max_length,\n",
    "                                              return_tensors='pt')\n",
    "\n",
    "train_dataset = SimpleDataset(train_encodings, train_labels_e.input_ids.tolist())\n",
    "valid_dataset = SimpleDataset(valid_encodings, valid_labels_e.input_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fccf755f-9a87-4d0a-b165-5a73e42752fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2335, 15464,  2361,  3478,  2012,  2377, 21296,  1058,  2213,\n",
       "          1041,  8873, 11498,  7484,  1058, 22984,  7159,  2193,  2335, 15464,\n",
       "          2361,  4708,  3524,  2005,  4471,  8285, 16500,  2003,  2949,  3711,\n",
       "          1999,  1058,  2213,  8833,  7642,  2335, 15464,  2361,  8833,  4708,\n",
       "          4130,  2188,  7309,  2573, 15327,  2019, 19307,  5402, 26383,  2193,\n",
       "          1060, 10651,  2019, 19307,  5443, 27921,  2063,  2175,  2015, 27354,\n",
       "          2691,  1058,  2213,  3524,  8833,  4471,  1061, 19968,  2193, 10611,\n",
       "          2334, 15006,  2102,  3478,  4740,  2193,  8292, 29577,  2098,  1996,\n",
       "          6434,  2038,  2042,  5023,  2349,  2000,  1996,  2755,  2008,  2053,\n",
       "          8833,  2995,  2001,  9675,  2005,  2023,  2765,  2904,  6270,  2335,\n",
       "         15464,  2361,  4708,  5604,  6164,  2349,  2000,  4945,  4708,  4130,\n",
       "          2188,  7309,  2573, 15327,  2019, 19307,  5402, 26383,  2193,  1060,\n",
       "         10651,  2019, 19307,  5443, 27921,  2063,  2175,  2015, 27354,  2691,\n",
       "          3231,  5343,  1061, 19968,  2193, 10611,  2334, 15006,  2102,  3478,\n",
       "          6164,  5604,  2043,  6164,  5604,  2043,  8246,  2003,  2275,  2000,\n",
       "          2995,  1999,  3231,  2553, 21296,  1058,  2213,  1041,  8873, 11498,\n",
       "          7484,  1058, 22984,  7159,  2193,   102,  2003,  4539,  3291,   102]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_e.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eafa7fe-b12f-4f0a-9de4-4863a8ea21d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17T11:46:13] start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-25c7f41f52e9>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.396641</td>\n",
       "      <td>0.976471</td>\n",
       "      <td>0.966292</td>\n",
       "      <td>0.966292</td>\n",
       "      <td>0.932584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=0.4800097346305847, metrics={'train_runtime': 2.0784, 'train_samples_per_second': 0.481, 'train_steps_per_second': 0.481, 'total_flos': 44014446360.0, 'train_loss': 0.4800097346305847, 'epoch': 1.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,  # output directory\n",
    "    num_train_epochs=1,  # total number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,  # batch size for evaluation\n",
    "    warmup_steps=0,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir='./logs',  # directory for storing logs\n",
    "    # load the best model when finished training (default metric is loss)    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "    logging_steps=1,  # log & save weights each logging_steps\n",
    "    evaluation_strategy=\"epoch\",  # evaluate each `logging_steps`\n",
    "    learning_rate=2e-5,\n",
    "    save_strategy='no',\n",
    "    metric_for_best_model='f1'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # the instantiated Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=train_dataset,  # training dataset\n",
    "    eval_dataset=valid_dataset,  # evaluation dataset\n",
    "    compute_metrics=compute_metrics  # the callback that computes metrics of interest\n",
    ")\n",
    "\n",
    "print('[{}] start training...'.format(get_ts()))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59070db3-3d1f-43ac-90fa-c4ec59fb1aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
