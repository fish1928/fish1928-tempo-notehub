{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a36c05-aa20-4909-a5e9-a172529e22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from os.path import exists\n",
    "import torch.nn as nn\n",
    "# from torch.nn.functional import log_softmax, pad, one_hot\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "### utils.py ###\n",
    "\n",
    "class Dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        for k, v in self.items():\n",
    "            if k in other and other[k]:\n",
    "                self[k] += other[k]\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        return self\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "# Takes the file paths as arguments\n",
    "def parse_csv_file_to_json(path_file_csv):\n",
    "    # create a dictionary\n",
    "    elements = []\n",
    "\n",
    "    # Open a csv reader called DictReader\n",
    "    with open(path_file_csv, encoding='utf-8') as file_csv:\n",
    "    #with open(path_file_csv) as file_csv:\n",
    "        reader_csv = csv.DictReader(file_csv)\n",
    "\n",
    "        # Convert each row into a dictionary\n",
    "        # and add it to data\n",
    "        for dict_head_value in reader_csv:\n",
    "            element = {}\n",
    "\n",
    "            for head, value in dict_head_value.items():\n",
    "                #print(value)\n",
    "                if value and (value[0] in [\"[\", \"{\"]):\n",
    "                    #element[head] = eval(value)\n",
    "                    element[head] = value\n",
    "                else:\n",
    "                    element[head] = value\n",
    "\n",
    "            elements.append(element)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    return elements\n",
    "# end\n",
    "\n",
    "### utils.py ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### core.py ###\n",
    "\n",
    "\"Produce N identical layers.\"\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    \"Take in model size and number of heads.\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    # end\n",
    "\n",
    "\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # print('jinyuj: scores: {}, mask: {}'.format(scores.shape, mask.shape))\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # end\n",
    "        p_attn = scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        # end\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "    # end\n",
    "\n",
    "\n",
    "    \"Implements Figure 2\"\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = self.attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)\n",
    "    # end\n",
    "# end class\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A residual connection followed by a layer norm.\n",
    "Note for code simplicity the norm is first as opposed to last.\n",
    "\"\"\"\n",
    "class ResidualLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, size, dropout=0.1, eps=1e-6):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.norm = torch.nn.LayerNorm(size, eps)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    # end\n",
    "\n",
    "    \"Apply residual connection to any sublayer with the same size.\"\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    # end\n",
    "# end class\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleIDEmbeddings(nn.Module):\n",
    "    def __init__(self, size_vocab, dim_hidden, id_pad):\n",
    "        super(SimpleIDEmbeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(size_vocab, dim_hidden, padding_idx=id_pad)\n",
    "        self.dim_hidden = dim_hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.lut(x)\n",
    "        return result * math.sqrt(self.dim_hidden)\n",
    "    # end\n",
    "\n",
    "    def get_shape(self):\n",
    "        return (self.lut.num_embeddings, self.lut.embedding_dim)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\"Implement the PE function.\"\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_positional, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.dim_positional = dim_positional\n",
    "        pe = torch.zeros(max_len, dim_positional)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, dim_positional, 2) * -(math.log(10000.0) / dim_positional)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).to('cuda')\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return x\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleEmbedder(nn.Module):    # no segment embedder as we do not need that\n",
    "    def __init__(self, size_vocab=None, dim_hidden=128, dropout=0.1, id_pad=0):\n",
    "        super(SimpleEmbedder, self).__init__()\n",
    "        self.size_vocab = size_vocab\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.id_pad = id_pad\n",
    "\n",
    "        self.embedder = nn.Sequential(\n",
    "            SimpleIDEmbeddings(size_vocab, dim_hidden, id_pad),\n",
    "            PositionalEncoding(dim_hidden),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "    # end\n",
    "\n",
    "    def forward(self, ids_input):   # (batch, seqs_with_padding)\n",
    "        return self.embedder(ids_input)\n",
    "    # end\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.size_vocab\n",
    "    # end\n",
    "# end\n",
    "\n",
    "### core.py ###\n",
    "\n",
    "\n",
    "\n",
    "class SimpleEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, dim_feedforward, n_head, dropout=0.1):\n",
    "        super(SimpleEncoderLayer, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        self.layer_attention = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_feedforward = PositionwiseFeedForward(dim_hidden, dim_feedforward, dropout)\n",
    "        self.layers_residual = clones(ResidualLayer(dim_hidden, dropout), 2)\n",
    "    # end\n",
    "\n",
    "    def forward(self, embeddings, masks, *args):\n",
    "        embeddings = self.layers_residual[0](embeddings, lambda embeddings: self.layer_attention(embeddings, embeddings, embeddings, masks))\n",
    "        return self.layers_residual[1](embeddings, self.layer_feedforward)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class SimpleDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, dim_feedforward, n_head, dropout=0.1):\n",
    "        super(SimpleDecoderLayer, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        self.layer_attention_decoder = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_attention_encoder = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_feedforward = PositionwiseFeedForward(dim_hidden, dim_feedforward, dropout)\n",
    "        self.layers_residual = clones(ResidualLayer(dim_hidden, dropout), 3)\n",
    "\n",
    "    def forward(self, embeddings, masks_encoder, output_encoder, masks_decoder, *args):\n",
    "        embeddings = self.layers_residual[0](embeddings, lambda embeddings: self.layer_attention_decoder(embeddings, embeddings, embeddings, masks_decoder))\n",
    "        embeddings = self.layers_residual[1](embeddings, lambda embeddings: self.layer_attention_encoder(embeddings, output_encoder, output_encoder, masks_encoder))\n",
    "        return self.layers_residual[2](embeddings, self.layer_feedforward)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleTransformerStack(nn.Module):\n",
    "\n",
    "    def __init__(self, obj_layer, n_layers):\n",
    "        super(SimpleTransformerStack, self).__init__()\n",
    "        self.layers = clones(obj_layer, n_layers)\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(obj_layer.dim_hidden)\n",
    "        self.keys_cache = ['output']\n",
    "        self.cache = Dotdict({\n",
    "            'output': None\n",
    "        })\n",
    "    # end\n",
    "\n",
    "    def forward(self, embedding_encoder=None, masks_encoder=None, output_encoder=None, embedding_decoder=None, masks_decoder=None ,noncache=False, **kwargs):  # input -> (batch, len_seq, vocab)\n",
    "\n",
    "        if output_encoder is not None and embedding_decoder is not None and masks_decoder is not None:\n",
    "            embeddings = embedding_decoder\n",
    "        else:\n",
    "            embeddings = embedding_encoder\n",
    "        # end\n",
    "\n",
    "        for layer in self.layers:\n",
    "            embeddings = layer(embeddings, masks_encoder, output_encoder, masks_decoder)\n",
    "        # end\n",
    "\n",
    "        output = self.norm(embeddings)\n",
    "\n",
    "        if not noncache:\n",
    "            self.cache.output = output\n",
    "        # end\n",
    "\n",
    "        return output\n",
    "    # end\n",
    "\n",
    "    # def get_vocab_size(self):\n",
    "    #     return self.embedder.embedder_token.shape[-1]\n",
    "    # # end\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleEncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder_encoder, embedder_decoder, pooling=False):\n",
    "        super(SimpleEncoderDecoder, self).__init__()\n",
    "\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        self.embedder_encoder = embedder_encoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.embedder_decoder = embedder_decoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.keys_cache = ['output_encoder_pooled']\n",
    "        self.cache = Dotdict({\n",
    "            'output_encoder_pooled': None\n",
    "        })\n",
    "    # end\n",
    "\n",
    "    def forward(self, ids_encoder=None, masks_encoder=None, ids_decoder=None, masks_decoder=None, nocache=False, **kwargs):\n",
    "        \n",
    "        output_encoder = self.embed_and_encode(ids_encoder=ids_encoder, masks_encoder=masks_encoder, nocache=nocache)\n",
    "        output = output_encoder\n",
    "        \n",
    "        if self.pooling:\n",
    "            output_encoder_refilled = output_encoder.masked_fill(masks_encoder.transpose(-1,-2)==False, 0)\n",
    "            output_encoder_pooled = torch.mean(output_encoder_refilled, dim=-2)\n",
    "            self.cache.output_encoder_pooled = output_encoder_pooled\n",
    "            \n",
    "            output_encoder_pooled_expanded = output_encoder_pooled.unsqueeze(-2).expand(output_encoder.shape)\n",
    "            output = output_encoder_pooled_expanded\n",
    "        # end\n",
    "        \n",
    "        if self.embedder_decoder and self.decoder:\n",
    "            output_decoder = self.embed_and_decode(ids_decoder=ids_decoder, masks_encoder=masks_encoder, output_encoder=output, masks_decoder=masks_decoder, nocache=nocache)\n",
    "            output = output_decoder\n",
    "        # end if\n",
    "        \n",
    "        return output\n",
    "    # end\n",
    "    \n",
    "    def embed_and_encode(self, ids_encoder=None, masks_encoder=None, nocache=False, **kwargs):\n",
    "        self.encoder.clear_cache()\n",
    "        \n",
    "        embedding_encoder = self.embedder_encoder(ids_encoder)\n",
    "        output_encoder = self.encoder(\n",
    "            embedding_encoder=embedding_encoder,\n",
    "            masks_encoder=masks_encoder,\n",
    "            nocache=nocache\n",
    "        )\n",
    "        \n",
    "        return output_encoder\n",
    "    # end\n",
    "\n",
    "    \n",
    "    def embed_and_decode(self, ids_decoder=None, masks_encoder=None, output_encoder=None, masks_decoder=None, nocache=False, **kwargs):\n",
    "        self.decoder.clear_cache()\n",
    "        \n",
    "        embedding_decoder = self.embedder_decoder(ids_decoder)\n",
    "        output_decoder = self.decoder(\n",
    "            masks_encoder=masks_encoder,\n",
    "            output_encoder=output_encoder,    #(len_seq, dim_hidden) -> (1, dim_hidden)\n",
    "            embedding_decoder=embedding_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            nocache=nocache\n",
    "        )\n",
    "\n",
    "        return output_decoder\n",
    "    # end\n",
    "    \n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.encoder.clear_cache()\n",
    "        \n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "        \n",
    "        if self.decoder:\n",
    "            self.decoder.clear_cache()\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "\n",
    "    def get_vocab_size(self, name_embedder):\n",
    "        embedder = getattr(self, f'embedder_{name_embedder}')\n",
    "        return embedder.get_vocab_size()\n",
    "    # end\n",
    "\n",
    "# end\n",
    "\n",
    "class LinearAndNorm(nn.Module):\n",
    "    def __init__(self, dim_in = None, dim_out = None, dropout=0.1, eps_norm=1e-12):\n",
    "        super(LinearAndNorm, self).__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(dim_in, dim_out)\n",
    "        self.norm = torch.nn.LayerNorm(dim_out, eps_norm)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    # end\n",
    "\n",
    "    def forward(self, seqs_in):\n",
    "        return self.dropout(self.norm(self.linear(seqs_in).relu()))\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    DEVICE = 'cuda'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if v is not None and type(v) is not bool:\n",
    "                self.kwargs[k] = v.to(Batch.DEVICE)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.kwargs\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class Collator_Base:\n",
    "\n",
    "    def __init__(self, tokenizer, size_seq_max, need_masked=0.3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size_seq_max = size_seq_max\n",
    "        self.need_masked = need_masked\n",
    "\n",
    "        index_special_token_2_id = {k: v for k, v in zip(tokenizer.all_special_tokens, tokenizer.all_special_ids)}\n",
    "\n",
    "        self.id_pad = index_special_token_2_id['[PAD]']\n",
    "        self.id_mask = index_special_token_2_id['[MASK]']\n",
    "        self.id_cls = index_special_token_2_id['[CLS]']\n",
    "        self.id_sep = index_special_token_2_id['[SEP]']\n",
    "        self.id_unk = index_special_token_2_id['[UNK]']\n",
    "\n",
    "        self.regex_special_token = re.compile(r'\\[(PAD|MASK|CLS|SEP|EOL|UNK)\\]')\n",
    "    # end\n",
    "\n",
    "    def _preprocess(self, line):\n",
    "        line = re.sub(self.regex_special_token, r'<\\1>', line)\n",
    "        line = re.sub(r'''('|\"|`){2}''', '', line)\n",
    "        line = re.sub(r'\\.{2,3}', '', line)\n",
    "        line = re.sub(r' {2,}', ' ', line)\n",
    "        line = line.lstrip().rstrip()\n",
    "        return line\n",
    "    # end\n",
    "\n",
    "    # return masks_attention?, return masks_segment?\n",
    "    def pad_sequences(self, sequences, size_seq_max, need_diagonal=False,\n",
    "                      need_masked=0):\n",
    "        id_pad = self.id_pad\n",
    "        id_mask = self.id_mask\n",
    "\n",
    "        sequences_padded = []\n",
    "        sequences_masked_padded = []\n",
    "\n",
    "        for sequence in sequences:\n",
    "            len_seq = len(sequence)\n",
    "\n",
    "            count_pad = size_seq_max - len_seq\n",
    "\n",
    "            sequence = torch.LongTensor(sequence)\n",
    "            sequence_padded = torch.cat((sequence, torch.LongTensor([id_pad] * count_pad)))\n",
    "            sequences_padded.append(sequence_padded)\n",
    "\n",
    "            if need_masked:\n",
    "                index_masked = list(range(1, len_seq - 1))\n",
    "                random.shuffle(index_masked)\n",
    "                anchor_mask = int(need_masked * (len_seq - 2)) or 1\n",
    "                index_masked = torch.LongTensor(index_masked[:anchor_mask])\n",
    "                # index_masked = torch.LongTensor(index_masked[:int(need_masked * (len_seq-2))])\n",
    "\n",
    "                sequence_masked = sequence.detach().clone()\n",
    "                sequence_masked.index_fill_(0, index_masked, id_mask)\n",
    "                sequence_masked_padded = torch.cat((sequence_masked, torch.LongTensor([id_pad] * count_pad)))\n",
    "\n",
    "                sequences_masked_padded.append(sequence_masked_padded)\n",
    "            # end\n",
    "        #   # end for\n",
    "\n",
    "        inputs = torch.stack(sequences_padded)  # (batch, size_seq_max)\n",
    "        if need_masked:\n",
    "            inputs_masked_padded = torch.stack(sequences_masked_padded)\n",
    "        # end\n",
    "\n",
    "        masks_segment = (inputs != self.id_pad).unsqueeze(-2)  # (nbatch, 1, seq)\n",
    "        masks_attention = self.make_std_mask(inputs, self.id_pad) if need_diagonal else masks_segment\n",
    "\n",
    "        if need_masked:\n",
    "            masks_masked = (inputs_masked_padded != id_mask).unsqueeze(-2)\n",
    "            masks_attention = masks_attention & masks_masked\n",
    "            return inputs_masked_padded, masks_attention, masks_segment, inputs  # (inputs, masks_attention, masks_segment, labels)\n",
    "        else:\n",
    "            return inputs, masks_attention, masks_segment, None\n",
    "        # end\n",
    "\n",
    "    # end\n",
    "\n",
    "    def subsequent_mask(self, size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "            torch.uint8\n",
    "        )\n",
    "        return subsequent_mask == 0\n",
    "\n",
    "    def make_std_mask(self, tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & self.subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Collator_SC(Collator_Base):\n",
    "\n",
    "    def __call__(self, list_corpus_source):\n",
    "\n",
    "        tokens_input_encoder = []\n",
    "        tokens_input_decoder = []\n",
    "        tokens_label_decoder = []\n",
    "        labels_similarity = []\n",
    "        labels_sc = []\n",
    "\n",
    "        for corpus_source in list_corpus_source:  # (line0, line1, sim), output of zip remove single case\n",
    "            if len(corpus_source) == 3:  # (line0, line1, sim)\n",
    "                corpus_line = [corpus_source[0], corpus_source[1]]\n",
    "                labels_similarity.append(corpus_source[2])\n",
    "            elif len(corpus_source) == 2:  # (line, label_sc)\n",
    "                corpus_line = [corpus_source[0]]\n",
    "                labels_sc.append(corpus_source[1])\n",
    "            else:\n",
    "                corpus_line = [corpus_source[0]]\n",
    "            # end\n",
    "\n",
    "            for line in corpus_line:\n",
    "                tokens = self.tokenizer.encode(self._preprocess(line), add_special_tokens=False)\n",
    "\n",
    "                # TODO: check edge\n",
    "                if len(tokens) > self.size_seq_max - 2:\n",
    "                    tokens = tokens[:self.size_seq_max - 2]\n",
    "                # end\n",
    "\n",
    "                tokens_input_encoder.append([self.id_cls] + tokens + [self.id_sep])\n",
    "                tokens_input_decoder.append([self.id_cls] + tokens)\n",
    "                tokens_label_decoder.append(tokens + [self.id_sep])\n",
    "            # end\n",
    "\n",
    "        # end\n",
    "\n",
    "        inputs_encoder, masks_encoder, segments_encoder, labels_encoder = self.pad_sequences(tokens_input_encoder,\n",
    "                                                                                             self.size_seq_max,\n",
    "                                                                                             need_masked=self.need_masked)\n",
    "        inputs_decoder, masks_decoder, segments_decoder, _ = self.pad_sequences(tokens_input_decoder, self.size_seq_max,\n",
    "                                                                                need_diagonal=True)\n",
    "        labels_decoder, masks_label, segments_label, _ = self.pad_sequences(tokens_label_decoder, self.size_seq_max)\n",
    "        # labels_similarity = torch.Tensor(labels_similarity).unsqueeze(0).transpose(0,1)\n",
    "        labels_similarity = torch.Tensor(labels_similarity)\n",
    "        labels_sc = torch.LongTensor(labels_sc)\n",
    "\n",
    "        return Batch(\n",
    "            ids_encoder=inputs_encoder,  # contains [mask]s\n",
    "            masks_encoder=masks_encoder,\n",
    "            labels_encoder=labels_encoder,  # doesn't contain [mask]\n",
    "            segments_encoder=segments_encoder,\n",
    "            ids_decoder=inputs_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            labels_decoder=labels_decoder,\n",
    "            segments_label=segments_label,\n",
    "            labels_similarity=labels_similarity,\n",
    "            labels_sc=labels_sc\n",
    "        )\n",
    "\n",
    "    # end\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51933c37-2474-4ea8-8819-d67e8061c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def GOSV(path_base, filename_base, postfix, index_label_2_id, split=0.1):\n",
    "    filename = f'{filename_base}{postfix}'\n",
    "    path_file = os.path.join(path_base, filename)\n",
    "    contents = parse_csv_file_to_json(path_file)\n",
    "    \n",
    "    list_corpus = [(content['processed'], index_label_2_id[content['target']]) for content in contents]\n",
    "    \n",
    "    indexs_all = list(range(len(list_corpus)))\n",
    "    random.shuffle(indexs_all)\n",
    "    \n",
    "    index_split = int(split * len(list_corpus))\n",
    "    \n",
    "    indexs_eval = indexs_all[:index_split]\n",
    "    indexs_train = indexs_all[index_split:]\n",
    "    \n",
    "    list_corpus_eval = [list_corpus[i_e] for i_e in indexs_eval]\n",
    "    list_corpus_train = [list_corpus[i_t] for i_t in indexs_train]\n",
    "    \n",
    "    return list_corpus_train, list_corpus_eval, None\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c03c357-ab7a-4589-a7a0-b87c2a3572b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoderHead_MLM(nn.Module):\n",
    "\n",
    "    @classmethod\n",
    "    def get_info_accuracy_template(cls):\n",
    "        return Dotdict({\n",
    "            'corrects_segmented': 0,\n",
    "            'corrects_masked': 0,\n",
    "            'num_segmented': 0,\n",
    "            'num_masked': 0 \n",
    "        })\n",
    "    # end\n",
    "    \n",
    "    def __init__(self, model, size_vocab, dim_hidden=128, dropout=0.1):\n",
    "        super(SimpleEncoderHead_MLM, self).__init__()\n",
    "        \n",
    "        self.ffn = LinearAndNorm(dim_in=dim_hidden, dim_out=dim_hidden, dropout=dropout)\n",
    "        self.extractor = torch.nn.Linear(dim_hidden, size_vocab, bias=False)\n",
    "        self.extractor.weight = nn.Parameter(model.embedder_encoder.embedder[0].lut.weight)\n",
    "        \n",
    "        self.keys_cache = ['labels_mlm', 'masks_encoder', 'segments_encoder', 'output']\n",
    "        self.cache = Dotdict({\n",
    "            'labels_mlm': None,\n",
    "            'masks_encoder': None,\n",
    "            'segments_encoder': None,\n",
    "            'output': None\n",
    "        })\n",
    "        \n",
    "        self.func_loss = torch.nn.CrossEntropyLoss()\n",
    "    # end\n",
    "\n",
    "\n",
    "    def forward(self, model, labels_encoder=None, segments_encoder=None, masks_encoder=None, nocache=False, **kwargs):   # labels_input -> (batch, seq, labels)\n",
    "        output_encoder = model.encoder.cache.output\n",
    "        output_ffn = self.ffn(output_encoder)\n",
    "        output_mlm = self.extractor(output_ffn) # output_mlm = prediction_logits\n",
    "\n",
    "        if not nocache:\n",
    "            self.cache.labels_mlm = labels_encoder\n",
    "            self.cache.masks_encoder = masks_encoder\n",
    "            self.cache.segments_encoder = segments_encoder\n",
    "            self.cache.output = output_mlm\n",
    "        # end\n",
    "\n",
    "        return output_mlm\n",
    "    # end\n",
    "    \n",
    "    def get_loss(self):\n",
    "        \n",
    "        labels_mlm = self.cache.labels_mlm\n",
    "        masks_encoder = self.cache.masks_encoder\n",
    "        segments_encoder = self.cache.segments_encoder\n",
    "        output_mlm = self.cache.output\n",
    "        \n",
    "        info_acc = SimpleEncoderHead_MLM.get_info_accuracy_template()\n",
    "        \n",
    "        segments_encoder_2d = segments_encoder.transpose(-1,-2)[:,:,0]\n",
    "        hidden_mlm_segmented = output_mlm.masked_select(segments_encoder_2d.unsqueeze(-1)).reshape(-1, output_mlm.shape[-1]) # should be (segmented_all_batchs, size_vocab)\n",
    "        \n",
    "        loss_segments = self.func_loss(hidden_mlm_segmented, labels_mlm.masked_select(segments_encoder_2d))\n",
    "        info_acc.corrects_segmented = torch.sum(hidden_mlm_segmented.argmax(-1) == labels_mlm.masked_select(segments_encoder_2d)).cpu().item()\n",
    "        info_acc.num_segmented = hidden_mlm_segmented.shape[0]\n",
    "        \n",
    "        masks_masked = torch.logical_xor(masks_encoder, segments_encoder) & segments_encoder # True is masked\n",
    "        masks_masked_perbatch = masks_masked[:,0,:]\n",
    "        hidden_mlm_masked = output_mlm.masked_select(masks_masked_perbatch.unsqueeze(-1)).reshape(-1, output_mlm.shape[-1])\n",
    "\n",
    "        if hidden_mlm_masked.shape[0] != 0:\n",
    "            loss_masked = self.func_loss(hidden_mlm_masked, labels_mlm.masked_select(masks_masked_perbatch))       \n",
    "            info_acc.corrects_masked = torch.sum(hidden_mlm_masked.argmax(-1) == labels_mlm.masked_select(masks_masked_perbatch)).cpu().item()\n",
    "            info_acc.num_masked = hidden_mlm_masked.shape[0]\n",
    "        else:\n",
    "            loss_masked = 0\n",
    "            info_acc.corrects_masked = 0\n",
    "            info_acc.num_masked = 1\n",
    "        # end\n",
    "        \n",
    "        loss_mlm = loss_segments + loss_masked * 3\n",
    "        \n",
    "        return loss_mlm, info_acc\n",
    "    # end\n",
    "    \n",
    "    \n",
    "    def clear_cache(self):\n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b941e33c-9c6a-495b-8978-cc8b54e13120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoderHead_S2S(nn.Module):\n",
    "\n",
    "    @classmethod\n",
    "    def get_info_accuracy_template(cls):\n",
    "        return Dotdict({\n",
    "            'corrects_segmented': 0,\n",
    "            'num_segmented': 0 \n",
    "        })\n",
    "    # end\n",
    "\n",
    "\n",
    "    def __init__(self, model, size_vocab, dim_hidden=128, dropout=0.1):\n",
    "        super(SimpleDecoderHead_S2S, self).__init__()\n",
    "        \n",
    "        self.ffn = LinearAndNorm(dim_in=dim_hidden, dim_out=dim_hidden, dropout=dropout)\n",
    "        self.extractor = torch.nn.Linear(dim_hidden, size_vocab, bias=False)\n",
    "        self.extractor.weight = nn.Parameter(model.embedder_decoder.embedder[0].lut.weight)\n",
    "\n",
    "        self.func_loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.keys_cache = ['output', 'labels_s2s', 'segments_decoder']\n",
    "        self.cache = Dotdict({\n",
    "            'output': None,\n",
    "            'labels_s2s': None,\n",
    "            'segments_decoder': None\n",
    "        })\n",
    "\n",
    "    # end\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, model, labels_decoder=None, segments_label=None, nocache=False, **kwargs):   # labels_input -> (batch, seq, labels)\n",
    "        output_decoder = model.decoder.cache.output\n",
    "        output_ffn = self.ffn(output_decoder)\n",
    "        output_s2s = self.extractor(output_ffn)   # output_mlm = prediction_logits\n",
    "        \n",
    "        if not nocache:\n",
    "            self.cache.segments_label = segments_label\n",
    "            self.cache.labels_s2s =  labels_decoder\n",
    "            self.cache.output = output_s2s\n",
    "        # end\n",
    "\n",
    "        return output_s2s\n",
    "    # end\n",
    "\n",
    "\n",
    "    def get_loss(self):\n",
    "        labels_s2s = self.cache.labels_s2s\n",
    "        output_s2s = self.cache.output\n",
    "        info_acc = SimpleDecoderHead_S2S.get_info_accuracy_template()\n",
    "        \n",
    "        segments_label = self.cache.segments_label\n",
    "        segments_label_2d = segments_label.transpose(-1,-2)[:,:,0]\n",
    "        hidden_s2s_segmented = output_s2s.masked_select(segments_label_2d.unsqueeze(-1)).reshape(-1, output_s2s.shape[-1])\n",
    "\n",
    "        loss_segments = self.func_loss(hidden_s2s_segmented, labels_s2s.masked_select(segments_label_2d))\n",
    "        info_acc.corrects_segmented = torch.sum(hidden_s2s_segmented.argmax(-1) == labels_s2s.masked_select(segments_label_2d)).cpu().item()\n",
    "        info_acc.num_segmented = hidden_s2s_segmented.shape[0]\n",
    "        \n",
    "        return loss_segments * 4, info_acc\n",
    "    # end\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        pass\n",
    "    # end\n",
    "\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "    # end\n",
    "# end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dee83a2-f82b-427b-bbf4-cb74b50212b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoderHead_AveragePooling_SC(nn.Module):  # SC-> SequenceClassification\n",
    "\n",
    "    @classmethod\n",
    "    def get_info_accuracy_template(cls):\n",
    "        return Dotdict({\n",
    "            'corrects': 0,\n",
    "            'num': 0 \n",
    "        })\n",
    "    # end\n",
    "    \n",
    "    def __init__(self, num_labels, dim_hidden=128, dropout=0.1):\n",
    "        super(SimpleEncoderHead_AveragePooling_SC, self).__init__()\n",
    "        \n",
    "        self.ffn = LinearAndNorm(dim_in=dim_hidden, dim_out=dim_hidden, dropout=dropout)\n",
    "        self.classifier = torch.nn.Linear(dim_hidden, num_labels, bias=False)\n",
    "        \n",
    "        self.keys_cache = ['labels_sc', 'output']\n",
    "        self.cache = Dotdict({\n",
    "            'labels_sc': None,\n",
    "            'output': None\n",
    "        })\n",
    "        \n",
    "        self.func_loss = torch.nn.CrossEntropyLoss()\n",
    "    # end\n",
    "\n",
    "\n",
    "    def forward(self, model, labels_sc=None, nocache=False, **kwargs):   # labels_input -> (batch, seq, labels)\n",
    "        output_encoder_pooled = model.cache.output_encoder_pooled\n",
    "        output_ffn = self.ffn(output_encoder_pooled)\n",
    "        output_sc = self.classifier(output_ffn) # output_sc = prediction_logits\n",
    "\n",
    "        if not nocache:\n",
    "            self.cache.labels_sc = labels_sc\n",
    "            self.cache.output = output_sc\n",
    "        # end\n",
    "\n",
    "        return output_sc\n",
    "    # end\n",
    "    \n",
    "    def get_loss(self):\n",
    "        \n",
    "        labels_sc = self.cache.labels_sc\n",
    "        output_sc = self.cache.output\n",
    "        \n",
    "        info_acc = SimpleEncoderHead_AveragePooling_SC.get_info_accuracy_template()\n",
    "        \n",
    "        loss_sc = self.func_loss(output_sc, labels_sc)\n",
    "        info_acc.corrects = torch.sum(output_sc.argmax(-1) == labels_sc).cpu().item()\n",
    "        info_acc.num = output_sc.shape[0]\n",
    "        \n",
    "        return loss_sc, info_acc\n",
    "    # end\n",
    "    \n",
    "    \n",
    "    def clear_cache(self):\n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe4c3e2f-17aa-44ad-aa98-43ed56627067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadManager(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HeadManager, self).__init__()\n",
    "        self.index_name_head = set()\n",
    "    # end\n",
    "\n",
    "    def register(self, head):\n",
    "        name_head = head.__class__.__name__\n",
    "        setattr(self, name_head, head)\n",
    "        self.index_name_head.add(name_head)\n",
    "        return self\n",
    "    # end\n",
    "\n",
    "    def forward(self, model, **kwargs):\n",
    "        for name in self.index_name_head:\n",
    "            head = getattr(self, name)\n",
    "            head.forward(model, **kwargs)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def get_head(self, klass):\n",
    "        return getattr(self, klass.__name__)\n",
    "    # end\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for name_head in self.index_name_head:\n",
    "            getattr(self, name_head).clear_cache()\n",
    "        # end\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Trainer(nn.Module):\n",
    "    def __init__(self, model=None, manager=None):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.manager = manager\n",
    "    # end\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        self.clear_cache()\n",
    "        \n",
    "        self.model.forward(**kwargs)\n",
    "        self.manager.forward(self.model, **kwargs)\n",
    "    # end\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        self.model.clear_cache() if self.model else None\n",
    "        self.manager.clear_cache() if self.manager else None\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class ModelLoader:\n",
    "    def __init__(self, path_checkpoints='./checkpoints'):\n",
    "        self.dict_name_item = {}\n",
    "        self.path_checkpoints = path_checkpoints\n",
    "    # end\n",
    "    \n",
    "    def add_item(self, item, name=None):\n",
    "        if not name:\n",
    "            name = item.__class__.__name__\n",
    "        # end\n",
    "        \n",
    "        self.dict_name_item[name] = item\n",
    "        return self\n",
    "    # end\n",
    "    \n",
    "    \n",
    "    def update_checkpoint(self, name_checkpoint, name_checkpoint_previous=None):  # epoch_n\n",
    "        if not self.dict_name_item:\n",
    "            print(f'[ALERT] no item added, skip saving checkpoint.')\n",
    "            return\n",
    "        # end\n",
    "        \n",
    "        if name_checkpoint_previous:\n",
    "            result = self._delete_checkpoint_folder(name_checkpoint_previous)\n",
    "            if result:\n",
    "                print(f'[INFO] {name_checkpoint_previous} is cleared.')\n",
    "            else:\n",
    "                print(f'[ALERT] {name_checkpoint_previous} fail to be cleared.')\n",
    "            # end\n",
    "        # end\n",
    "        \n",
    "        folder_checkpoint = self._create_checkpoint_folder(name_checkpoint)\n",
    "        for name_item, item in self.dict_name_item.items():\n",
    "            path_checkpoint_item = os.path.join(folder_checkpoint, f'{name_item}.pt')\n",
    "            torch.save(item.state_dict(), path_checkpoint_item)\n",
    "            \n",
    "            size_file_saved_MB = os.path.getsize(path_checkpoint_item) / 1024 / 1024\n",
    "            print(f'[INFO] {name_item} is saved, {size_file_saved_MB} MB')\n",
    "        # end\n",
    "        \n",
    "        print(f'[INFO] {name_checkpoint} is saved')\n",
    "    # end\n",
    "\n",
    "    \n",
    "    def load_item_state(self, name_checkpoint, instance_item, name_item=None):\n",
    "        \n",
    "        if not name_item:\n",
    "            name_item = instance_item.__class__.__name__\n",
    "        # end\n",
    "        \n",
    "        if name_checkpoint is None:\n",
    "            print('[ALERT] ignoring loading {} due to no checkpoint'.format(name_item))\n",
    "            return\n",
    "        # end\n",
    "        \n",
    "        \n",
    "        path_checkpoint_item = os.path.join(self.path_checkpoints, name_checkpoint, f'{name_item}.pt')\n",
    "        if not os.path.exists(path_checkpoint_item):\n",
    "            print(f'[ERROR] {path_checkpoint_item} not exists')\n",
    "            return\n",
    "        # end\n",
    "        if issubclass(instance_item.__class__, torch.nn.Module):\n",
    "            instance_item.load_state_dict(torch.load(path_checkpoint_item), strict=False)\n",
    "        else:\n",
    "            instance_item.load_state_dict(torch.load(path_checkpoint_item))\n",
    "        # end\n",
    "        \n",
    "        print(f'[INFO] {name_item} loaded for {name_checkpoint}.')\n",
    "        return instance_item\n",
    "    # end\n",
    "    \n",
    "    \n",
    "    def list_items(self):\n",
    "        return list(self.dict_name_item.keys())\n",
    "    # end\n",
    "    \n",
    "    def _create_checkpoint_folder(self, name_checkpoint):\n",
    "        path_folder_target = os.path.join(self.path_checkpoints, name_checkpoint)\n",
    "        Path(path_folder_target).mkdir(parents=True, exist_ok=True)\n",
    "        return path_folder_target\n",
    "    # end\n",
    "    \n",
    "    def _delete_checkpoint_folder(self, name_checkpoint_previous):\n",
    "        path_folder_target = os.path.join(self.path_checkpoints, name_checkpoint_previous)\n",
    "        if os.path.exists(path_folder_target):\n",
    "            shutil.rmtree(path_folder_target, ignore_errors=True)\n",
    "        # end\n",
    "        return (not os.path.exists(path_folder_target))\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947695c3-1322-4748-8174-a1f1003cc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Builder:\n",
    "    \n",
    "    @classmethod\n",
    "    def build_model_with_mlm_sc_s2s(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer, num_labels):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "        \n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder, pooling=True)\n",
    "        head_mlm = SimpleEncoderHead_MLM(model, size_vocab, dim_hidden)\n",
    "        head_sc = SimpleEncoderHead_AveragePooling_SC(num_labels, dim_hidden)\n",
    "        head_s2s = SimpleDecoderHead_S2S(model, size_vocab, dim_hidden)\n",
    "\n",
    "        manager = HeadManager().register(head_mlm).register(head_sc).register(head_s2s)\n",
    "        trainer = Trainer(model=model, manager=manager)\n",
    "\n",
    "        return trainer\n",
    "    # end\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model_with_mlm_sc_s2s(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer, num_labels, name_checkpoint):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "        \n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder, pooling=True)\n",
    "        head_mlm = SimpleEncoderHead_MLM(model, size_vocab, dim_hidden)\n",
    "        head_sc = SimpleEncoderHead_AveragePooling_SC(num_labels, dim_hidden)\n",
    "        head_s2s = SimpleDecoderHead_S2S(model, size_vocab, dim_hidden)\n",
    "        \n",
    "        loader = ModelLoader()\n",
    "        loader.add_item(model).load_item_state(name_checkpoint, model)\n",
    "        loader.add_item(head_s2s).load_item_state(name_checkpoint, head_s2s)\n",
    "        loader.add_item(head_sc).load_item_state(name_checkpoint, head_sc)\n",
    "        loader.add_item(head_mlm).load_item_state(name_checkpoint, head_mlm)\n",
    "\n",
    "        manager = HeadManager().register(head_mlm).register(head_sc).register(head_s2s)\n",
    "        trainer = Trainer(model=model, manager=manager)\n",
    "\n",
    "        return trainer, loader\n",
    "    # end\n",
    "\n",
    "# end\n",
    "\n",
    "def train_a_batch(batch, trainer, optimizer=None, scheduler=None):\n",
    "    trainer.train()\n",
    "    trainer.forward(**batch())\n",
    "    \n",
    "    loss_s2s, info_acc_s2s = trainer.manager.get_head(SimpleDecoderHead_S2S).get_loss()\n",
    "    loss_mlm, info_acc_mlm = trainer.manager.get_head(SimpleEncoderHead_MLM).get_loss()\n",
    "    loss_sc, info_acc_sc = trainer.manager.get_head(SimpleEncoderHead_AveragePooling_SC).get_loss()\n",
    "\n",
    "    # crossentropy loss\n",
    "    loss_all = loss_mlm + loss_sc + loss_s2s\n",
    "    loss_all_value = loss_all.item()\n",
    "    \n",
    "    loss_all.backward()\n",
    "    \n",
    "    if optimizer:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    # end\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    # end\n",
    "    \n",
    "    trainer.clear_cache()\n",
    "    return loss_all_value, Dotdict({'mlm': info_acc_mlm, 'sc': info_acc_sc, 's2s': info_acc_s2s})\n",
    "# end\n",
    "\n",
    "\n",
    "def evaluate_a_batch(batch, trainer, *args, **kwargs):\n",
    "    trainer.eval()\n",
    "    with torch.no_grad():\n",
    "        trainer.forward(**batch())\n",
    "    # end\n",
    "    \n",
    "    loss_s2s, info_acc_s2s = trainer.manager.get_head(SimpleDecoderHead_S2S).get_loss()\n",
    "    loss_mlm, info_acc_mlm = trainer.manager.get_head(SimpleEncoderHead_MLM).get_loss()\n",
    "    loss_sc, info_acc_sc = trainer.manager.get_head(SimpleEncoderHead_AveragePooling_SC).get_loss()\n",
    "    \n",
    "    # crossentropy loss\n",
    "    loss_all = loss_mlm + loss_sc + loss_s2s\n",
    "    loss_all_value = loss_all.item()\n",
    "    \n",
    "    trainer.clear_cache()\n",
    "    return loss_all_value, Dotdict({'mlm': info_acc_mlm, 'sc': info_acc_sc, 's2s': info_acc_s2s})\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f395a35-ebba-4f21-8016-1a7bcaebc61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    folder_data, folder_output, version_data, version_data_last, postfix_train, postfix_test,\n",
    "    tokenizer, collator, index_label_2_labelid, index_labelid_2_label,\n",
    "    epochs, seq_max, batch_size, dim_hidden, dim_feedforward, n_head, n_layer,\n",
    "    lr_base_optimizer, betas_optimizer, eps_optimizer, warmup\n",
    "):\n",
    "\n",
    "    trainer, loader = Builder.load_model_with_mlm_sc_s2s(tokenizer.vocab_size, dim_hidden, dim_feedforward, n_head, n_layer, num_labels, str(version_data_last))\n",
    "    trainer = trainer.to('cuda')\n",
    "\n",
    "    train_source, valid_source, _ = GOSV(folder_data, version_data, postfix_train, index_label_2_labelid, split=0.1)\n",
    "    test_source, _, _ = GOSV(folder_data, version_data, postfix_test, index_label_2_labelid, split=0)\n",
    "    \n",
    "\n",
    "    dataloader_train = DataLoader(train_source, batch_size, shuffle=False, collate_fn=collator)\n",
    "    dataloader_eval = DataLoader(valid_source, batch_size, shuffle=False, collate_fn=collator)\n",
    "    dataloader_test = DataLoader(test_source, 1, shuffle=False, collate_fn=collator)\n",
    "\n",
    "\n",
    "    for p in trainer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(trainer.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.96)\n",
    "    \n",
    "    ### start train/eval epochs ####################################\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        info_acc_heads_train = Dotdict({\n",
    "            'mlm': SimpleEncoderHead_MLM.get_info_accuracy_template(),\n",
    "            'sc': SimpleEncoderHead_AveragePooling_SC.get_info_accuracy_template(),\n",
    "            's2s': SimpleDecoderHead_S2S.get_info_accuracy_template(),\n",
    "        })\n",
    "\n",
    "\n",
    "        info_acc_heads_eval = Dotdict({\n",
    "            'mlm': SimpleEncoderHead_MLM.get_info_accuracy_template(),\n",
    "            'sc': SimpleEncoderHead_AveragePooling_SC.get_info_accuracy_template(),\n",
    "            's2s': SimpleDecoderHead_S2S.get_info_accuracy_template(),\n",
    "        })\n",
    "\n",
    "        # train phase\n",
    "        \n",
    "        losss_per_e = []\n",
    "        for i, batch in enumerate(tqdm(dataloader_train)):\n",
    "            loss_current, info_acc_heads_batch = train_a_batch(batch, trainer, optimizer, None)\n",
    "            info_acc_heads_train += info_acc_heads_batch\n",
    "\n",
    "            losss_per_e.append(loss_current)\n",
    "            # if i % 200 == 0:\n",
    "            #     print('Epoch: {} Batch: {}, loss: {}, rate: {}, acc_mlm: {}, acc_sc: {}, acc_s2s: {}'.format(\n",
    "            #         e, i, loss_current, optimizer.param_groups[0]['lr'],\n",
    "            #         info_acc_heads_batch.mlm.corrects_masked / info_acc_heads_batch.mlm.num_masked,\n",
    "            #         info_acc_heads_batch.sc.corrects / info_acc_heads_batch.sc.num,\n",
    "            #         info_acc_heads_batch.s2s.corrects_segmented / info_acc_heads_batch.s2s.num_segmented,\n",
    "            #     ), end='\\r')\n",
    "            # # end\n",
    "        # end\n",
    "\n",
    "        loss_average_per_e = sum(losss_per_e) / len(losss_per_e)\n",
    "        print('[{}] Epoch: {} training ends. Status: Average loss: {}, Average MLM accuracy: {}, Average SC accuracy: {}, Average S2S accuracy: {}'.format(\n",
    "            datetime.utcnow(), e, loss_average_per_e,\n",
    "            info_acc_heads_train.mlm.corrects_masked / info_acc_heads_train.mlm.num_masked,\n",
    "            info_acc_heads_train.sc.corrects / info_acc_heads_train.sc.num,\n",
    "            info_acc_heads_train.s2s.corrects_segmented / info_acc_heads_train.s2s.num_segmented,\n",
    "        ))\n",
    "\n",
    "        if e % 2 == 0:\n",
    "            lr_scheduler.step() # schedule per 2 epoch\n",
    "        # end\n",
    "\n",
    "\n",
    "        # eval phase start\n",
    "        losss_per_e = []\n",
    "        for i, batch in enumerate(tqdm(dataloader_eval)):\n",
    "            loss_current, info_acc_heads_batch = evaluate_a_batch(batch, trainer)\n",
    "            info_acc_heads_eval += info_acc_heads_batch\n",
    "\n",
    "            losss_per_e.append(loss_current)\n",
    "        # end\n",
    "\n",
    "        loss_average_per_e = sum(losss_per_e) / len(losss_per_e)\n",
    "        # print('[{}] Epoch: {} Evalutation ends. Status: Average loss: {}, Average MLM accuracy: {}, Average SC accuracy: {}'.format(\n",
    "        print('[{}] Epoch: {} Evalutation ends. Status: Average loss: {}, Average MLM accuracy: {}, Average SC accuracy: {}, Average S2S accuracy: {}'.format(        \n",
    "            datetime.utcnow(), e, loss_average_per_e,\n",
    "            info_acc_heads_eval.mlm.corrects_masked / info_acc_heads_eval.mlm.num_masked,\n",
    "            info_acc_heads_eval.sc.corrects / info_acc_heads_eval.sc.num,\n",
    "            info_acc_heads_eval.s2s.corrects_segmented / info_acc_heads_eval.s2s.num_segmented,\n",
    "        ))\n",
    "        # eval phase end\n",
    "    # end\n",
    "    ### end train/eval epochs ####################################\n",
    "    \n",
    "    \n",
    "    ### start test  ##############################################\n",
    "    trainer.eval()\n",
    "    \n",
    "    list_corpus_test = []\n",
    "    for i, batch in enumerate(tqdm(dataloader_test)):\n",
    "        with torch.no_grad():\n",
    "            trainer.forward(**batch())\n",
    "        # end\n",
    "\n",
    "        label_sc = trainer.manager.get_head(SimpleEncoderHead_AveragePooling_SC).cache.labels_sc.squeeze(0).detach().cpu() # (batch, label)  -> (label)\n",
    "        logit_sc = trainer.manager.get_head(SimpleEncoderHead_AveragePooling_SC).cache.output.squeeze(0).detach().cpu()  #(batch, seq, num_label) -> (seq, num_label)\n",
    "        \n",
    "        pred_sc = logit_sc.argmax(-1) # (seq)\n",
    "        conf_sc = torch.index_select(logit_sc.softmax(-1), -1, pred_sc)\n",
    "        list_corpus_test.append((pred_sc.item(), label_sc.item(), conf_sc.item()))\n",
    "        \n",
    "        trainer.clear_cache()\n",
    "    # end for\n",
    "    \n",
    "    path_file_output = os.path.join(folder_output, f'{version_data}.json')\n",
    "    with open(path_file_output, 'w+') as file:\n",
    "        file.write(json.dumps(list_corpus_test))\n",
    "    # end\n",
    "        \n",
    "    ### end   test  ##############################################\n",
    "    \n",
    "    loader.update_checkpoint(str(version_data), str(version_data_last))\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11380c74-80e2-4b1d-8546-b752508f4e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] ./checkpoints/None/SimpleEncoderDecoder.pt not exists\n",
      "[ERROR] ./checkpoints/None/SimpleDecoderHead_S2S.pt not exists\n",
      "[ERROR] ./checkpoints/None/SimpleEncoderHead_AveragePooling_SC.pt not exists\n",
      "[ERROR] ./checkpoints/None/SimpleEncoderHead_MLM.pt not exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 11/628 [00:11<10:04,  1.02it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|| 628/628 [10:16<00:00,  1.02it/s]\n",
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:16:25.986731] Epoch: 0 training ends. Status: Average loss: 25.033128858371903, Average MLM accuracy: 0.19527302458228712, Average SC accuracy: 0.8044034818228366, Average S2S accuracy: 0.6023284848746004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 70/70 [00:23<00:00,  2.96it/s]\n",
      "  0%|          | 0/628 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:16:49.656024] Epoch: 0 Evalutation ends. Status: Average loss: 16.24704031263079, Average MLM accuracy: 0.2377125636794145, Average SC accuracy: 0.9795186891961085, Average S2S accuracy: 0.6995704327666962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 628/628 [10:13<00:00,  1.02it/s]\n",
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:27:02.809814] Epoch: 1 training ends. Status: Average loss: 14.037578323084837, Average MLM accuracy: 0.3286699060727239, Average SC accuracy: 0.97610513739546, Average S2S accuracy: 0.7024094500153653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 70/70 [00:23<00:00,  2.94it/s]\n",
      "  0%|          | 0/628 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:27:26.595550] Epoch: 1 Evalutation ends. Status: Average loss: 11.120327636173792, Average MLM accuracy: 0.5079823491425701, Average SC accuracy: 0.982078853046595, Average S2S accuracy: 0.7147652590778204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 628/628 [10:14<00:00,  1.02it/s]\n",
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:37:41.429520] Epoch: 2 training ends. Status: Average loss: 10.449114535264908, Average MLM accuracy: 0.5311676586378995, Average SC accuracy: 0.9838425214769301, Average S2S accuracy: 0.7134718510059975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 70/70 [00:23<00:00,  2.92it/s]\n",
      "  0%|          | 0/628 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:38:05.391087] Epoch: 2 Evalutation ends. Status: Average loss: 8.746621942520141, Average MLM accuracy: 0.6305517686733156, Average SC accuracy: 0.9882232462877624, Average S2S accuracy: 0.7251766260876863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 628/628 [10:12<00:00,  1.02it/s]\n",
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:48:18.306486] Epoch: 3 training ends. Status: Average loss: 8.938873216604732, Average MLM accuracy: 0.6128076660776838, Average SC accuracy: 0.9860613301473516, Average S2S accuracy: 0.719010705768823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 70/70 [00:23<00:00,  2.96it/s]\n",
      "  0%|          | 0/628 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:48:41.990679] Epoch: 3 Evalutation ends. Status: Average loss: 7.8436979498182025, Average MLM accuracy: 0.6762215684867618, Average SC accuracy: 0.9882232462877624, Average S2S accuracy: 0.725968624283399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 628/628 [10:15<00:00,  1.02it/s]\n",
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:58:57.153093] Epoch: 4 training ends. Status: Average loss: 8.238358928139801, Average MLM accuracy: 0.6536625437862302, Average SC accuracy: 0.9866302554474597, Average S2S accuracy: 0.722348533385647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 70/70 [00:23<00:00,  2.92it/s]\n",
      "  0%|          | 0/628 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 06:59:21.111819] Epoch: 4 Evalutation ends. Status: Average loss: 7.257947404044015, Average MLM accuracy: 0.7138731434311545, Average SC accuracy: 0.9866871479774706, Average S2S accuracy: 0.73154408178037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 628/628 [10:16<00:00,  1.02it/s]\n",
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:09:37.794725] Epoch: 5 training ends. Status: Average loss: 7.825895150755621, Average MLM accuracy: 0.6808080690659971, Average SC accuracy: 0.9875974284576435, Average S2S accuracy: 0.7245144075993836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 70/70 [00:23<00:00,  2.93it/s]\n",
      "  2%|         | 2/84 [00:00<00:06, 13.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:10:01.687680] Epoch: 5 Evalutation ends. Status: Average loss: 6.973699930735997, Average MLM accuracy: 0.7353088900050226, Average SC accuracy: 0.982078853046595, Average S2S accuracy: 0.7356089731822067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 84/84 [00:04<00:00, 20.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] None is cleared.\n",
      "[INFO] SimpleEncoderDecoder is saved, 614.9739637374878 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC is saved, 2.2815237045288086 MB\n",
      "[INFO] SimpleEncoderHead_MLM is saved, 91.68078327178955 MB\n",
      "[INFO] 202206171000 is saved\n",
      "[INFO] finish 202206171000\n",
      "[INFO] SimpleEncoderDecoder loaded for 202206171000.\n",
      "[INFO] SimpleDecoderHead_S2S loaded for 202206171000.\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC loaded for 202206171000.\n",
      "[INFO] SimpleEncoderHead_MLM loaded for 202206171000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [01:04<00:00,  1.02it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:11:35.838679] Epoch: 0 training ends. Status: Average loss: 59.6023976875074, Average MLM accuracy: 0.16470473781496253, Average SC accuracy: 0.3263850795392211, Average S2S accuracy: 0.26012727689686804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8/8 [00:02<00:00,  3.15it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:11:38.380978] Epoch: 0 Evalutation ends. Status: Average loss: 40.02669334411621, Average MLM accuracy: 0.16782773907536416, Average SC accuracy: 0.5643564356435643, Average S2S accuracy: 0.4970347991496028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [01:04<00:00,  1.02it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:12:42.874850] Epoch: 1 training ends. Status: Average loss: 33.30804683222915, Average MLM accuracy: 0.17013884063207926, Average SC accuracy: 0.7158529895776193, Average S2S accuracy: 0.5068716419866335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8/8 [00:02<00:00,  3.11it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:12:45.446562] Epoch: 1 Evalutation ends. Status: Average loss: 29.37020993232727, Average MLM accuracy: 0.16972767574414185, Average SC accuracy: 0.8366336633663366, Average S2S accuracy: 0.4994218790794823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [01:04<00:00,  1.02it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:13:50.056387] Epoch: 2 training ends. Status: Average loss: 27.24579464305531, Average MLM accuracy: 0.17073645296235043, Average SC accuracy: 0.9149753154141524, Average S2S accuracy: 0.5596129275324335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8/8 [00:02<00:00,  3.15it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:13:52.597400] Epoch: 2 Evalutation ends. Status: Average loss: 25.235288381576538, Average MLM accuracy: 0.1669411019632679, Average SC accuracy: 0.9752475247524752, Average S2S accuracy: 0.601059266718884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [01:04<00:00,  1.02it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:14:57.058759] Epoch: 3 training ends. Status: Average loss: 24.069485288677793, Average MLM accuracy: 0.17369671869136796, Average SC accuracy: 0.9775095995611629, Average S2S accuracy: 0.6278502162232997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8/8 [00:02<00:00,  3.08it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:14:59.656623] Epoch: 3 Evalutation ends. Status: Average loss: 22.667009353637695, Average MLM accuracy: 0.16922102596580113, Average SC accuracy: 0.9900990099009901, Average S2S accuracy: 0.6512625414941628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [01:04<00:00,  1.03it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:16:03.986083] Epoch: 4 training ends. Status: Average loss: 21.9983447970766, Average MLM accuracy: 0.17254318791433296, Average SC accuracy: 0.9901261656609983, Average S2S accuracy: 0.6608529353950989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8/8 [00:02<00:00,  3.04it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:16:06.616489] Epoch: 4 Evalutation ends. Status: Average loss: 20.794263124465942, Average MLM accuracy: 0.1694743508549715, Average SC accuracy: 0.9851485148514851, Average S2S accuracy: 0.6761030920144717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 66/66 [01:04<00:00,  1.03it/s]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:17:10.742905] Epoch: 5 training ends. Status: Average loss: 20.05579844388095, Average MLM accuracy: 0.17768543354689867, Average SC accuracy: 0.9928688974218322, Average S2S accuracy: 0.6802393198794391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8/8 [00:02<00:00,  3.02it/s]\n",
      " 11%|         | 3/28 [00:00<00:01, 18.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:17:13.390028] Epoch: 5 Evalutation ends. Status: Average loss: 19.042710065841675, Average MLM accuracy: 0.18720709309689676, Average SC accuracy: 0.9900990099009901, Average S2S accuracy: 0.692066689045541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:01<00:00, 26.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 202206171000 is cleared.\n",
      "[INFO] SimpleEncoderDecoder is saved, 614.9739637374878 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC is saved, 2.2815237045288086 MB\n",
      "[INFO] SimpleEncoderHead_MLM is saved, 91.68078327178955 MB\n",
      "[INFO] 202207021500 is saved\n",
      "[INFO] finish 202207021500\n",
      "[INFO] SimpleEncoderDecoder loaded for 202207021500.\n",
      "[INFO] SimpleDecoderHead_S2S loaded for 202207021500.\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC loaded for 202207021500.\n",
      "[INFO] SimpleEncoderHead_MLM loaded for 202207021500.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:39<00:00,  1.01s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:18:21.838091] Epoch: 0 training ends. Status: Average loss: 66.78886951544347, Average MLM accuracy: 0.1652065810687438, Average SC accuracy: 0.5185185185185185, Average S2S accuracy: 0.2057820199166121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.08it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:18:23.463014] Epoch: 0 Evalutation ends. Status: Average loss: 52.458499908447266, Average MLM accuracy: 0.17586694975230008, Average SC accuracy: 0.8, Average S2S accuracy: 0.18496626039650574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:38<00:00,  1.01it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:19:02.014596] Epoch: 1 training ends. Status: Average loss: 43.205277858636315, Average MLM accuracy: 0.17606495600318564, Average SC accuracy: 0.7583333333333333, Average S2S accuracy: 0.36878467316770613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.10it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:19:03.627831] Epoch: 1 Evalutation ends. Status: Average loss: 35.38463668823242, Average MLM accuracy: 0.17144373673036092, Average SC accuracy: 0.75, Average S2S accuracy: 0.5283778835591358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:38<00:00,  1.01it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:19:42.193675] Epoch: 2 training ends. Status: Average loss: 31.94781097998986, Average MLM accuracy: 0.17719158524504186, Average SC accuracy: 0.9712962962962963, Average S2S accuracy: 0.5270838358430102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.15it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:19:43.782467] Epoch: 2 Evalutation ends. Status: Average loss: 29.102899932861327, Average MLM accuracy: 0.17781316348195328, Average SC accuracy: 1.0, Average S2S accuracy: 0.5283778835591358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:38<00:00,  1.01it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:20:22.322076] Epoch: 3 training ends. Status: Average loss: 28.118008393507736, Average MLM accuracy: 0.17592898350847885, Average SC accuracy: 0.9981481481481481, Average S2S accuracy: 0.5298691751949738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.07it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:20:23.952240] Epoch: 3 Evalutation ends. Status: Average loss: 26.552506256103516, Average MLM accuracy: 0.17179759377211606, Average SC accuracy: 1.0, Average S2S accuracy: 0.54836009834179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:38<00:00,  1.00it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:21:02.949474] Epoch: 4 training ends. Status: Average loss: 25.78124877734062, Average MLM accuracy: 0.17649229812940698, Average SC accuracy: 1.0, Average S2S accuracy: 0.5822163261086799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.15it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:21:04.536321] Epoch: 4 Evalutation ends. Status: Average loss: 24.560700607299804, Average MLM accuracy: 0.1735668789808917, Average SC accuracy: 1.0, Average S2S accuracy: 0.613328451116807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:38<00:00,  1.02it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:21:42.831584] Epoch: 5 training ends. Status: Average loss: 23.844105744973206, Average MLM accuracy: 0.17857073483421068, Average SC accuracy: 1.0, Average S2S accuracy: 0.6207573825850247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.29it/s]\n",
      " 12%|        | 3/25 [00:00<00:00, 29.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:21:44.351515] Epoch: 5 Evalutation ends. Status: Average loss: 22.487924575805664, Average MLM accuracy: 0.17604387827317763, Average SC accuracy: 1.0, Average S2S accuracy: 0.6423078934979338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 25/25 [00:01<00:00, 24.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 202207021500 is cleared.\n",
      "[INFO] SimpleEncoderDecoder is saved, 614.9739637374878 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC is saved, 2.2815237045288086 MB\n",
      "[INFO] SimpleEncoderHead_MLM is saved, 91.68078327178955 MB\n",
      "[INFO] 202207041600 is saved\n",
      "[INFO] finish 202207041600\n",
      "[INFO] SimpleEncoderDecoder loaded for 202207041600.\n",
      "[INFO] SimpleDecoderHead_S2S loaded for 202207041600.\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC loaded for 202207041600.\n",
      "[INFO] SimpleEncoderHead_MLM loaded for 202207041600.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24/24 [00:23<00:00,  1.04it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:22:33.690855] Epoch: 0 training ends. Status: Average loss: 71.41076978047688, Average MLM accuracy: 0.1658314511575381, Average SC accuracy: 0.3148148148148148, Average S2S accuracy: 0.228483403936426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:00<00:00,  3.11it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:22:34.656024] Epoch: 0 Evalutation ends. Status: Average loss: 61.87786356608073, Average MLM accuracy: 0.1792841305036427, Average SC accuracy: 0.3888888888888889, Average S2S accuracy: 0.28400374181478016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24/24 [00:22<00:00,  1.05it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:22:57.455600] Epoch: 1 training ends. Status: Average loss: 54.297985553741455, Average MLM accuracy: 0.17550112930547712, Average SC accuracy: 0.5015432098765432, Average S2S accuracy: 0.2978756622292536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:00<00:00,  3.31it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:22:58.363674] Epoch: 1 Evalutation ends. Status: Average loss: 46.72112274169922, Average MLM accuracy: 0.164079822616408, Average SC accuracy: 0.3888888888888889, Average S2S accuracy: 0.3453695042095416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24/24 [00:23<00:00,  1.04it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:23:21.456938] Epoch: 2 training ends. Status: Average loss: 41.117675145467125, Average MLM accuracy: 0.17832439299830605, Average SC accuracy: 0.5462962962962963, Average S2S accuracy: 0.487921146580347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:00<00:00,  3.23it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:23:22.386781] Epoch: 2 Evalutation ends. Status: Average loss: 36.37058766682943, Average MLM accuracy: 0.16376306620209058, Average SC accuracy: 0.4583333333333333, Average S2S accuracy: 0.5180542563143125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24/24 [00:22<00:00,  1.05it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:23:45.288946] Epoch: 3 training ends. Status: Average loss: 33.37723231315613, Average MLM accuracy: 0.17761857707509882, Average SC accuracy: 0.7484567901234568, Average S2S accuracy: 0.5265672325322398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:00<00:00,  3.29it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:23:46.201991] Epoch: 3 Evalutation ends. Status: Average loss: 31.255734125773113, Average MLM accuracy: 0.17770034843205576, Average SC accuracy: 0.875, Average S2S accuracy: 0.5180542563143125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24/24 [00:22<00:00,  1.05it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:24:08.964649] Epoch: 4 training ends. Status: Average loss: 29.781593322753906, Average MLM accuracy: 0.17401891586674195, Average SC accuracy: 0.9290123456790124, Average S2S accuracy: 0.5265984574872238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:00<00:00,  3.29it/s]\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:24:09.878967] Epoch: 4 Evalutation ends. Status: Average loss: 28.835294087727863, Average MLM accuracy: 0.17421602787456447, Average SC accuracy: 0.9722222222222222, Average S2S accuracy: 0.5180542563143125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 24/24 [00:22<00:00,  1.05it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:24:32.653132] Epoch: 5 training ends. Status: Average loss: 27.82017993927002, Average MLM accuracy: 0.17588932806324112, Average SC accuracy: 0.9953703703703703, Average S2S accuracy: 0.5265464158955837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:00<00:00,  3.29it/s]\n",
      " 10%|         | 4/39 [00:00<00:01, 32.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:24:33.566048] Epoch: 5 Evalutation ends. Status: Average loss: 27.10377375284831, Average MLM accuracy: 0.16693063034526448, Average SC accuracy: 1.0, Average S2S accuracy: 0.5221702525724976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:01<00:00, 25.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 202207041600 is cleared.\n",
      "[INFO] SimpleEncoderDecoder is saved, 614.9739637374878 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC is saved, 2.2815237045288086 MB\n",
      "[INFO] SimpleEncoderHead_MLM is saved, 91.68078327178955 MB\n",
      "[INFO] 202207151000 is saved\n",
      "[INFO] finish 202207151000\n",
      "[INFO] SimpleEncoderDecoder loaded for 202207151000.\n",
      "[INFO] SimpleDecoderHead_S2S loaded for 202207151000.\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC loaded for 202207151000.\n",
      "[INFO] SimpleEncoderHead_MLM loaded for 202207151000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 51/51 [00:50<00:00,  1.02it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:26:18.390864] Epoch: 0 training ends. Status: Average loss: 61.68713737936581, Average MLM accuracy: 0.17266115866026815, Average SC accuracy: 0.5366713681241185, Average S2S accuracy: 0.3117329740299538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6/6 [00:02<00:00,  2.92it/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:26:20.445335] Epoch: 0 Evalutation ends. Status: Average loss: 43.39048703511556, Average MLM accuracy: 0.17208734157886701, Average SC accuracy: 0.7707006369426752, Average S2S accuracy: 0.5257806172950599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 51/51 [00:50<00:00,  1.02it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:27:10.633714] Epoch: 1 training ends. Status: Average loss: 34.74462808347216, Average MLM accuracy: 0.18044492818153335, Average SC accuracy: 0.9245416078984485, Average S2S accuracy: 0.5272906806496647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6/6 [00:02<00:00,  2.92it/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:27:12.686941] Epoch: 1 Evalutation ends. Status: Average loss: 29.107173919677734, Average MLM accuracy: 0.1809436555199267, Average SC accuracy: 1.0, Average S2S accuracy: 0.5272203725366688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 51/51 [00:50<00:00,  1.01it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:28:03.296279] Epoch: 2 training ends. Status: Average loss: 27.364798265344955, Average MLM accuracy: 0.17628918682695954, Average SC accuracy: 0.9964739069111425, Average S2S accuracy: 0.534445941541859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6/6 [00:02<00:00,  2.99it/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:28:05.307246] Epoch: 2 Evalutation ends. Status: Average loss: 25.138713518778484, Average MLM accuracy: 0.18048557031607879, Average SC accuracy: 1.0, Average S2S accuracy: 0.5753621884279673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 51/51 [00:50<00:00,  1.02it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:28:55.375761] Epoch: 3 training ends. Status: Average loss: 24.01051367965399, Average MLM accuracy: 0.1793400286944046, Average SC accuracy: 0.9992947813822285, Average S2S accuracy: 0.6068838376714027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6/6 [00:02<00:00,  2.92it/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:28:57.429852] Epoch: 3 Evalutation ends. Status: Average loss: 22.30602200826009, Average MLM accuracy: 0.1803328752481295, Average SC accuracy: 1.0, Average S2S accuracy: 0.626608476558985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 51/51 [00:50<00:00,  1.01it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:29:47.750545] Epoch: 4 training ends. Status: Average loss: 21.2986167084937, Average MLM accuracy: 0.18265472715579084, Average SC accuracy: 1.0, Average S2S accuracy: 0.6534295150863642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6/6 [00:02<00:00,  2.90it/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:29:49.823285] Epoch: 4 Evalutation ends. Status: Average loss: 19.534364382425945, Average MLM accuracy: 0.18384486181096352, Average SC accuracy: 1.0, Average S2S accuracy: 0.676460001799694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 51/51 [00:50<00:00,  1.01it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:30:40.188512] Epoch: 5 training ends. Status: Average loss: 18.99428311516257, Average MLM accuracy: 0.18461716057322844, Average SC accuracy: 0.9894217207334274, Average S2S accuracy: 0.6817781625912649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6/6 [00:02<00:00,  2.88it/s]\n",
      "  7%|         | 2/28 [00:00<00:01, 19.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:30:42.276758] Epoch: 5 Evalutation ends. Status: Average loss: 17.530580520629883, Average MLM accuracy: 0.19422812643151627, Average SC accuracy: 1.0, Average S2S accuracy: 0.6787996040673085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:01<00:00, 19.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 202207151000 is cleared.\n",
      "[INFO] SimpleEncoderDecoder is saved, 614.9739637374878 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC is saved, 2.2815237045288086 MB\n",
      "[INFO] SimpleEncoderHead_MLM is saved, 91.68078327178955 MB\n",
      "[INFO] 202207221500 is saved\n",
      "[INFO] finish 202207221500\n",
      "[INFO] SimpleEncoderDecoder loaded for 202207221500.\n",
      "[INFO] SimpleDecoderHead_S2S loaded for 202207221500.\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC loaded for 202207221500.\n",
      "[INFO] SimpleEncoderHead_MLM loaded for 202207221500.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:38<00:00,  1.00it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:32:12.357249] Epoch: 0 training ends. Status: Average loss: 64.67467400966547, Average MLM accuracy: 0.17309891643567427, Average SC accuracy: 0.5425925925925926, Average S2S accuracy: 0.31271101430168236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.21it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:32:13.915415] Epoch: 0 Evalutation ends. Status: Average loss: 49.82836761474609, Average MLM accuracy: 0.18061113132048018, Average SC accuracy: 0.6916666666666667, Average S2S accuracy: 0.39149828030954426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:38<00:00,  1.01it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:32:52.543902] Epoch: 1 training ends. Status: Average loss: 40.66525601118039, Average MLM accuracy: 0.17967008470798038, Average SC accuracy: 0.7675925925925926, Average S2S accuracy: 0.471365108480697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.14it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:32:54.138479] Epoch: 1 Evalutation ends. Status: Average loss: 33.2971794128418, Average MLM accuracy: 0.1811567842851946, Average SC accuracy: 0.8916666666666667, Average S2S accuracy: 0.5303095442820293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:38<00:00,  1.01it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:33:32.804426] Epoch: 2 training ends. Status: Average loss: 30.424632047995544, Average MLM accuracy: 0.1782356703948516, Average SC accuracy: 0.9768518518518519, Average S2S accuracy: 0.5277234817001519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.14it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:33:34.400236] Epoch: 2 Evalutation ends. Status: Average loss: 28.228779602050782, Average MLM accuracy: 0.18006547835576572, Average SC accuracy: 1.0, Average S2S accuracy: 0.5303095442820293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:38<00:00,  1.01it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:34:12.971512] Epoch: 3 training ends. Status: Average loss: 27.20786251165928, Average MLM accuracy: 0.17658803233247397, Average SC accuracy: 1.0, Average S2S accuracy: 0.5403571121492649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.20it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:34:14.534085] Epoch: 3 Evalutation ends. Status: Average loss: 25.815877151489257, Average MLM accuracy: 0.1867951982539105, Average SC accuracy: 1.0, Average S2S accuracy: 0.5817927773000859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:38<00:00,  1.02it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:34:52.903381] Epoch: 4 training ends. Status: Average loss: 24.9584290431096, Average MLM accuracy: 0.1738355075694431, Average SC accuracy: 1.0, Average S2S accuracy: 0.6025164082428134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.12it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:34:54.507823] Epoch: 4 Evalutation ends. Status: Average loss: 23.571857452392578, Average MLM accuracy: 0.17624590760276465, Average SC accuracy: 1.0, Average S2S accuracy: 0.6157029234737748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 39/39 [00:38<00:00,  1.01it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:35:33.278610] Epoch: 5 training ends. Status: Average loss: 23.00507501455454, Average MLM accuracy: 0.17656864835526953, Average SC accuracy: 1.0, Average S2S accuracy: 0.633447020721676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  3.23it/s]\n",
      "100%|| 3/3 [00:00<00:00, 31.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:35:34.826207] Epoch: 5 Evalutation ends. Status: Average loss: 21.712709808349608, Average MLM accuracy: 0.1860676609676246, Average SC accuracy: 1.0, Average S2S accuracy: 0.6509565778159931\n",
      "[INFO] 202207221500 is cleared.\n",
      "[INFO] SimpleEncoderDecoder is saved, 614.9739637374878 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC is saved, 2.2815237045288086 MB\n",
      "[INFO] SimpleEncoderHead_MLM is saved, 91.68078327178955 MB\n",
      "[INFO] 202207260728 is saved\n",
      "[INFO] finish 202207260728\n",
      "[INFO] SimpleEncoderDecoder loaded for 202207260728.\n",
      "[INFO] SimpleDecoderHead_S2S loaded for 202207260728.\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC loaded for 202207260728.\n",
      "[INFO] SimpleEncoderHead_MLM loaded for 202207260728.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.37it/s]\n",
      "100%|| 1/1 [00:00<00:00, 15.62it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:36:02.593157] Epoch: 0 training ends. Status: Average loss: 81.87540435791016, Average MLM accuracy: 0.0007930214115781126, Average SC accuracy: 0.34146341463414637, Average S2S accuracy: 0.059670304156025075\n",
      "[2023-12-04 07:36:02.658895] Epoch: 0 Evalutation ends. Status: Average loss: 77.72419738769531, Average MLM accuracy: 0.20430107526881722, Average SC accuracy: 1.0, Average S2S accuracy: 0.23510971786833856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.39it/s]\n",
      "100%|| 1/1 [00:00<00:00, 13.84it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:36:04.100982] Epoch: 1 training ends. Status: Average loss: 76.69381713867188, Average MLM accuracy: 0.17049960348929422, Average SC accuracy: 1.0, Average S2S accuracy: 0.2563269096819132\n",
      "[2023-12-04 07:36:04.175032] Epoch: 1 Evalutation ends. Status: Average loss: 75.46866607666016, Average MLM accuracy: 0.15053763440860216, Average SC accuracy: 1.0, Average S2S accuracy: 0.26959247648902823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.36it/s]\n",
      "100%|| 1/1 [00:00<00:00, 13.46it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:36:05.651382] Epoch: 2 training ends. Status: Average loss: 74.47569274902344, Average MLM accuracy: 0.1641554321966693, Average SC accuracy: 1.0, Average S2S accuracy: 0.2749013234269793\n",
      "[2023-12-04 07:36:05.727314] Epoch: 2 Evalutation ends. Status: Average loss: 74.06683349609375, Average MLM accuracy: 0.10752688172043011, Average SC accuracy: 1.0, Average S2S accuracy: 0.2445141065830721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.36it/s]\n",
      "100%|| 1/1 [00:00<00:00, 13.90it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:36:07.200109] Epoch: 3 training ends. Status: Average loss: 72.86184692382812, Average MLM accuracy: 0.16653449643140364, Average SC accuracy: 1.0, Average S2S accuracy: 0.276062224286046\n",
      "[2023-12-04 07:36:07.273800] Epoch: 3 Evalutation ends. Status: Average loss: 72.54127502441406, Average MLM accuracy: 0.13978494623655913, Average SC accuracy: 1.0, Average S2S accuracy: 0.23510971786833856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.36it/s]\n",
      "100%|| 1/1 [00:00<00:00, 14.53it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:36:08.743701] Epoch: 4 training ends. Status: Average loss: 71.44586181640625, Average MLM accuracy: 0.16098334655035687, Average SC accuracy: 1.0, Average S2S accuracy: 0.2742047829115394\n",
      "[2023-12-04 07:36:08.814291] Epoch: 4 Evalutation ends. Status: Average loss: 71.83132934570312, Average MLM accuracy: 0.12903225806451613, Average SC accuracy: 1.0, Average S2S accuracy: 0.2445141065830721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.38it/s]\n",
      "100%|| 1/1 [00:00<00:00, 14.88it/s]\n",
      " 10%|         | 2/20 [00:00<00:01, 17.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:36:10.260581] Epoch: 5 training ends. Status: Average loss: 70.0330696105957, Average MLM accuracy: 0.16732751784298175, Average SC accuracy: 1.0, Average S2S accuracy: 0.2816345484095658\n",
      "[2023-12-04 07:36:10.329670] Epoch: 5 Evalutation ends. Status: Average loss: 70.12541198730469, Average MLM accuracy: 0.10752688172043011, Average SC accuracy: 1.0, Average S2S accuracy: 0.26332288401253917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [00:01<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 202207260728 is cleared.\n",
      "[INFO] SimpleEncoderDecoder is saved, 614.9739637374878 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC is saved, 2.2815237045288086 MB\n",
      "[INFO] SimpleEncoderHead_MLM is saved, 91.68078327178955 MB\n",
      "[INFO] 202208031141 is saved\n",
      "[INFO] finish 202208031141\n",
      "[INFO] SimpleEncoderDecoder loaded for 202208031141.\n",
      "[INFO] SimpleDecoderHead_S2S loaded for 202208031141.\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC loaded for 202208031141.\n",
      "[INFO] SimpleEncoderHead_MLM loaded for 202208031141.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 27/27 [00:26<00:00,  1.00it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:37:00.644949] Epoch: 0 training ends. Status: Average loss: 68.90669674343533, Average MLM accuracy: 0.16189125522811404, Average SC accuracy: 0.7517146776406035, Average S2S accuracy: 0.21584031366957762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:01<00:00,  2.88it/s]\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:37:01.689276] Epoch: 0 Evalutation ends. Status: Average loss: 57.503098805745445, Average MLM accuracy: 0.17303128371089535, Average SC accuracy: 0.9259259259259259, Average S2S accuracy: 0.26476422035958797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 27/27 [00:26<00:00,  1.02it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:37:28.223709] Epoch: 1 training ends. Status: Average loss: 48.737848635073064, Average MLM accuracy: 0.17395798278928898, Average SC accuracy: 0.9917695473251029, Average S2S accuracy: 0.35949385136339335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:01<00:00,  2.76it/s]\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:37:29.312599] Epoch: 1 Evalutation ends. Status: Average loss: 39.725056966145836, Average MLM accuracy: 0.17281553398058253, Average SC accuracy: 0.9876543209876543, Average S2S accuracy: 0.5131486339497089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 27/27 [00:26<00:00,  1.03it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:37:55.587351] Epoch: 2 training ends. Status: Average loss: 34.898294024997284, Average MLM accuracy: 0.17290034132974377, Average SC accuracy: 1.0, Average S2S accuracy: 0.5091855284263055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:01<00:00,  2.88it/s]\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:37:56.630147] Epoch: 2 Evalutation ends. Status: Average loss: 30.54215431213379, Average MLM accuracy: 0.1745415318230852, Average SC accuracy: 0.9876543209876543, Average S2S accuracy: 0.5210186192334763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 27/27 [00:26<00:00,  1.03it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:38:22.762936] Epoch: 3 training ends. Status: Average loss: 29.068974177042644, Average MLM accuracy: 0.17496754963703667, Average SC accuracy: 1.0, Average S2S accuracy: 0.5163856710033862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:01<00:00,  2.75it/s]\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:38:23.856285] Epoch: 3 Evalutation ends. Status: Average loss: 27.359283447265625, Average MLM accuracy: 0.17411003236245956, Average SC accuracy: 0.9876543209876543, Average S2S accuracy: 0.5210186192334763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 27/27 [00:26<00:00,  1.03it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:38:50.183184] Epoch: 4 training ends. Status: Average loss: 26.5838948002568, Average MLM accuracy: 0.17306860247103506, Average SC accuracy: 1.0, Average S2S accuracy: 0.5163714132953128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:01<00:00,  2.71it/s]\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:38:51.291152] Epoch: 4 Evalutation ends. Status: Average loss: 25.231102625528973, Average MLM accuracy: 0.17432578209277239, Average SC accuracy: 0.9876543209876543, Average S2S accuracy: 0.5244737347239107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 27/27 [00:26<00:00,  1.03it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:39:17.582930] Epoch: 5 training ends. Status: Average loss: 24.558164455272532, Average MLM accuracy: 0.17323686361232632, Average SC accuracy: 1.0, Average S2S accuracy: 0.560698627695598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:01<00:00,  2.89it/s]\n",
      " 10%|         | 3/29 [00:00<00:00, 28.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:39:18.622996] Epoch: 5 Evalutation ends. Status: Average loss: 23.31017303466797, Average MLM accuracy: 0.17411003236245956, Average SC accuracy: 0.9876543209876543, Average S2S accuracy: 0.6326700364706636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 29/29 [00:01<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 202208031141 is cleared.\n",
      "[INFO] SimpleEncoderDecoder is saved, 614.9739637374878 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC is saved, 2.2815237045288086 MB\n",
      "[INFO] SimpleEncoderHead_MLM is saved, 91.68078327178955 MB\n",
      "[INFO] 202208172100 is saved\n",
      "[INFO] finish 202208172100\n",
      "[INFO] SimpleEncoderDecoder loaded for 202208172100.\n",
      "[INFO] SimpleDecoderHead_S2S loaded for 202208172100.\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC loaded for 202208172100.\n",
      "[INFO] SimpleEncoderHead_MLM loaded for 202208172100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 35/35 [00:34<00:00,  1.00it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:40:20.566442] Epoch: 0 training ends. Status: Average loss: 66.36116583687918, Average MLM accuracy: 0.1692752835171664, Average SC accuracy: 0.31275720164609055, Average S2S accuracy: 0.27443818770598827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:01<00:00,  2.78it/s]\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:40:22.005923] Epoch: 0 Evalutation ends. Status: Average loss: 52.66818618774414, Average MLM accuracy: 0.1806730250134965, Average SC accuracy: 0.4351851851851852, Average S2S accuracy: 0.34251171708564127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 35/35 [00:34<00:00,  1.01it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:40:56.810223] Epoch: 1 training ends. Status: Average loss: 43.40543071201869, Average MLM accuracy: 0.16993552897312414, Average SC accuracy: 0.4063786008230453, Average S2S accuracy: 0.4188672299648562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:01<00:00,  2.81it/s]\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:40:58.234575] Epoch: 1 Evalutation ends. Status: Average loss: 35.62223720550537, Average MLM accuracy: 0.16627676804030952, Average SC accuracy: 0.4722222222222222, Average S2S accuracy: 0.5128887942053686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 35/35 [00:35<00:00,  1.00s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:41:33.336368] Epoch: 2 training ends. Status: Average loss: 32.42517340523856, Average MLM accuracy: 0.1747514370048159, Average SC accuracy: 0.6018518518518519, Average S2S accuracy: 0.5176783218392127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:01<00:00,  2.87it/s]\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:41:34.730665] Epoch: 2 Evalutation ends. Status: Average loss: 30.20267343521118, Average MLM accuracy: 0.16609681482814467, Average SC accuracy: 0.7592592592592593, Average S2S accuracy: 0.513048572645931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 35/35 [00:35<00:00,  1.00s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:42:09.821457] Epoch: 3 training ends. Status: Average loss: 28.477603149414062, Average MLM accuracy: 0.17329501320490912, Average SC accuracy: 0.9053497942386831, Average S2S accuracy: 0.5190357592734258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:01<00:00,  2.76it/s]\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:42:11.271084] Epoch: 3 Evalutation ends. Status: Average loss: 27.064322471618652, Average MLM accuracy: 0.181212884649991, Average SC accuracy: 1.0, Average S2S accuracy: 0.5430336599914785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 35/35 [00:35<00:00,  1.00s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:42:46.324353] Epoch: 4 training ends. Status: Average loss: 25.991373552594865, Average MLM accuracy: 0.1752174926207861, Average SC accuracy: 0.992798353909465, Average S2S accuracy: 0.5744721236418436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:01<00:00,  2.74it/s]\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:42:47.788207] Epoch: 4 Evalutation ends. Status: Average loss: 24.924739837646484, Average MLM accuracy: 0.17167536440525463, Average SC accuracy: 1.0, Average S2S accuracy: 0.6136557307200682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 35/35 [00:35<00:00,  1.01s/it]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:43:23.044224] Epoch: 5 training ends. Status: Average loss: 24.083067866734098, Average MLM accuracy: 0.17411061053285692, Average SC accuracy: 1.0, Average S2S accuracy: 0.6220974709100008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:01<00:00,  2.73it/s]\n",
      " 67%|   | 2/3 [00:00<00:00, 14.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:43:24.508636] Epoch: 5 Evalutation ends. Status: Average loss: 23.294678688049316, Average MLM accuracy: 0.17617419470937556, Average SC accuracy: 1.0, Average S2S accuracy: 0.6349595227950575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:00<00:00, 12.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 202208172100 is cleared.\n",
      "[INFO] SimpleEncoderDecoder is saved, 614.9739637374878 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC is saved, 2.2815237045288086 MB\n",
      "[INFO] SimpleEncoderHead_MLM is saved, 91.68078327178955 MB\n",
      "[INFO] 202208190706 is saved\n",
      "[INFO] finish 202208190706\n",
      "[INFO] SimpleEncoderDecoder loaded for 202208190706.\n",
      "[INFO] SimpleDecoderHead_S2S loaded for 202208190706.\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC loaded for 202208190706.\n",
      "[INFO] SimpleEncoderHead_MLM loaded for 202208190706.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.32it/s]\n",
      "100%|| 1/1 [00:00<00:00, 14.37it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:43:49.181613] Epoch: 0 training ends. Status: Average loss: 82.63142395019531, Average MLM accuracy: 0.0014619883040935672, Average SC accuracy: 0.3170731707317073, Average S2S accuracy: 0.043246360098025086\n",
      "[2023-12-04 07:43:49.253050] Epoch: 0 Evalutation ends. Status: Average loss: 77.33135986328125, Average MLM accuracy: 0.16587677725118483, Average SC accuracy: 1.0, Average S2S accuracy: 0.1701534170153417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.35it/s]\n",
      "100%|| 1/1 [00:00<00:00, 11.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:43:50.732599] Epoch: 1 training ends. Status: Average loss: 77.38998413085938, Average MLM accuracy: 0.14571150097465888, Average SC accuracy: 1.0, Average S2S accuracy: 0.17269713132478015\n",
      "[2023-12-04 07:43:50.819194] Epoch: 1 Evalutation ends. Status: Average loss: 74.71690368652344, Average MLM accuracy: 0.18009478672985782, Average SC accuracy: 1.0, Average S2S accuracy: 0.2203626220362622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.32it/s]\n",
      "100%|| 1/1 [00:00<00:00, 12.08it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:43:52.336671] Epoch: 2 training ends. Status: Average loss: 75.08311080932617, Average MLM accuracy: 0.15838206627680312, Average SC accuracy: 1.0, Average S2S accuracy: 0.2029695833933977\n",
      "[2023-12-04 07:43:52.421154] Epoch: 2 Evalutation ends. Status: Average loss: 72.84557342529297, Average MLM accuracy: 0.1895734597156398, Average SC accuracy: 1.0, Average S2S accuracy: 0.2105997210599721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.32it/s]\n",
      "100%|| 1/1 [00:00<00:00, 15.22it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:43:53.935908] Epoch: 3 training ends. Status: Average loss: 73.35622787475586, Average MLM accuracy: 0.15838206627680312, Average SC accuracy: 1.0, Average S2S accuracy: 0.20700591033588006\n",
      "[2023-12-04 07:43:54.003213] Epoch: 3 Evalutation ends. Status: Average loss: 71.2105712890625, Average MLM accuracy: 0.17061611374407584, Average SC accuracy: 1.0, Average S2S accuracy: 0.19665271966527198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.36it/s]\n",
      "100%|| 1/1 [00:00<00:00, 15.56it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:43:55.480264] Epoch: 4 training ends. Status: Average loss: 71.90263748168945, Average MLM accuracy: 0.15935672514619884, Average SC accuracy: 1.0, Average S2S accuracy: 0.20988900100908173\n",
      "[2023-12-04 07:43:55.546202] Epoch: 4 Evalutation ends. Status: Average loss: 69.94770812988281, Average MLM accuracy: 0.1895734597156398, Average SC accuracy: 1.0, Average S2S accuracy: 0.1701534170153417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:01<00:00,  1.33it/s]\n",
      "100%|| 1/1 [00:00<00:00, 11.65it/s]\n",
      "100%|| 2/2 [00:00<00:00, 19.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:43:57.050653] Epoch: 5 training ends. Status: Average loss: 70.44095993041992, Average MLM accuracy: 0.15253411306042886, Average SC accuracy: 1.0, Average S2S accuracy: 0.1910047570996108\n",
      "[2023-12-04 07:43:57.138090] Epoch: 5 Evalutation ends. Status: Average loss: 68.4708480834961, Average MLM accuracy: 0.1943127962085308, Average SC accuracy: 1.0, Average S2S accuracy: 0.1701534170153417\n",
      "[INFO] 202208190706 is cleared.\n",
      "[INFO] SimpleEncoderDecoder is saved, 614.9739637374878 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC is saved, 2.2815237045288086 MB\n",
      "[INFO] SimpleEncoderHead_MLM is saved, 91.68078327178955 MB\n",
      "[INFO] 202208240500 is saved\n",
      "[INFO] finish 202208240500\n",
      "[INFO] SimpleEncoderDecoder loaded for 202208240500.\n",
      "[INFO] SimpleDecoderHead_S2S loaded for 202208240500.\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC loaded for 202208240500.\n",
      "[INFO] SimpleEncoderHead_MLM loaded for 202208240500.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00,  1.07it/s]\n",
      "100%|| 1/1 [00:00<00:00, 21.01it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:44:24.351899] Epoch: 0 training ends. Status: Average loss: 86.33868408203125, Average MLM accuracy: 0.0, Average SC accuracy: 0.0, Average S2S accuracy: 0.0\n",
      "[2023-12-04 07:44:24.401704] Epoch: 0 Evalutation ends. Status: Average loss: 79.4050521850586, Average MLM accuracy: 0.0, Average SC accuracy: 1.0, Average S2S accuracy: 0.27514792899408286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|| 1/1 [00:00<00:00, 19.84it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:44:25.329862] Epoch: 1 training ends. Status: Average loss: 79.775146484375, Average MLM accuracy: 0.0, Average SC accuracy: 1.0, Average S2S accuracy: 0.1988674217188541\n",
      "[2023-12-04 07:44:25.381814] Epoch: 1 Evalutation ends. Status: Average loss: 77.24745178222656, Average MLM accuracy: 0.18181818181818182, Average SC accuracy: 1.0, Average S2S accuracy: 0.1893491124260355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|| 1/1 [00:00<00:00, 19.80it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:44:26.308062] Epoch: 2 training ends. Status: Average loss: 77.72224426269531, Average MLM accuracy: 0.1431818181818182, Average SC accuracy: 1.0, Average S2S accuracy: 0.20919387075283144\n",
      "[2023-12-04 07:44:26.360267] Epoch: 2 Evalutation ends. Status: Average loss: 75.71564483642578, Average MLM accuracy: 0.1919191919191919, Average SC accuracy: 1.0, Average S2S accuracy: 0.22189349112426035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|| 1/1 [00:00<00:00, 17.79it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:44:27.283406] Epoch: 3 training ends. Status: Average loss: 76.25047302246094, Average MLM accuracy: 0.15568181818181817, Average SC accuracy: 1.0, Average S2S accuracy: 0.22051965356429049\n",
      "[2023-12-04 07:44:27.341348] Epoch: 3 Evalutation ends. Status: Average loss: 74.60443115234375, Average MLM accuracy: 0.15151515151515152, Average SC accuracy: 1.0, Average S2S accuracy: 0.2958579881656805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|| 1/1 [00:00<00:00, 15.73it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:44:28.268502] Epoch: 4 training ends. Status: Average loss: 74.97944641113281, Average MLM accuracy: 0.17613636363636365, Average SC accuracy: 1.0, Average S2S accuracy: 0.2401732178547635\n",
      "[2023-12-04 07:44:28.333607] Epoch: 4 Evalutation ends. Status: Average loss: 73.35182189941406, Average MLM accuracy: 0.15151515151515152, Average SC accuracy: 1.0, Average S2S accuracy: 0.33136094674556216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|| 1/1 [00:00<00:00, 20.88it/s]\n",
      " 11%|         | 4/37 [00:00<00:01, 31.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-04 07:44:29.249446] Epoch: 5 training ends. Status: Average loss: 73.98074340820312, Average MLM accuracy: 0.18409090909090908, Average SC accuracy: 1.0, Average S2S accuracy: 0.26015989340439705\n",
      "[2023-12-04 07:44:29.299009] Epoch: 5 Evalutation ends. Status: Average loss: 72.36466979980469, Average MLM accuracy: 0.2727272727272727, Average SC accuracy: 1.0, Average S2S accuracy: 0.4319526627218935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 37/37 [00:01<00:00, 30.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 202208240500 is cleared.\n",
      "[INFO] SimpleEncoderDecoder is saved, 614.9739637374878 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] SimpleEncoderHead_AveragePooling_SC is saved, 2.2815237045288086 MB\n",
      "[INFO] SimpleEncoderHead_MLM is saved, 91.68078327178955 MB\n",
      "[INFO] 202209081034 is saved\n",
      "[INFO] finish 202209081034\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1f18e8a73956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mversion_data_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mversion_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mversions_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     main(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mfolder_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_data_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostfix_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostfix_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_label_2_labelid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_labelid_2_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-716931a5ef28>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(folder_data, folder_output, version_data, version_data_last, postfix_train, postfix_test, tokenizer, collator, index_label_2_labelid, index_labelid_2_label, epochs, seq_max, batch_size, dim_hidden, dim_feedforward, n_head, n_layer, lr_base_optimizer, betas_optimizer, eps_optimizer, warmup)\u001b[0m\n\u001b[1;32m      6\u001b[0m ):\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_with_mlm_sc_s2s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_feedforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion_data_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3d2eec874d35>\u001b[0m in \u001b[0;36mload_model_with_mlm_sc_s2s\u001b[0;34m(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer, num_labels, name_checkpoint)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_item_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_s2s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_item_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_s2s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_item_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d3725c2fee2e>\u001b[0m in \u001b[0;36mload_item_state\u001b[0;34m(self, name_checkpoint, instance_item, name_item)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0minstance_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_checkpoint_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0minstance_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_checkpoint_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'data/{key}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntyped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m         \u001b[0;31m# TODO: Once we decide to break serialization FC, we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "gpu = 0\n",
    "torch.cuda.set_device(gpu)\n",
    "\n",
    "epochs=6\n",
    "\n",
    "# source\n",
    "seq_max = 256\n",
    "batch_size = 28\n",
    "\n",
    "\n",
    "# model & head\n",
    "dim_hidden = 768\n",
    "# dim_feedforward = 768\n",
    "dim_feedforward = 3072\n",
    "n_head = 12\n",
    "n_layer = 12\n",
    "\n",
    "# optimizer\n",
    "lr_base_optimizer = 1e-4\n",
    "betas_optimizer = (0.9, 0.999)\n",
    "eps_optimizer = 1e-9\n",
    "\n",
    "# scheduler\n",
    "warmup = 200\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "collator = Collator_SC(tokenizer, seq_max)  # TODO: here\n",
    "\n",
    "# labels\n",
    "index_label_2_labelid = {label:id_label for id_label, label in enumerate(sorted([\"product\",\"testcase\",\"testbed\",\"usererror\",\"targetvm\",\"nimbus\",\"infra\"]))}\n",
    "index_labelid_2_label = {id_label: label for label, id_label in index_label_2_labelid.items()}\n",
    "num_labels=len(index_label_2_labelid)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder_data = 'data'\n",
    "folder_output = 'outputs_gosv'\n",
    "postfix_train = '_train_0.35_15.csv'\n",
    "postfix_test = '_test.csv'\n",
    "\n",
    "versions_data = sorted([int(filename.split('_test.csv')[0]) for filename in os.listdir(folder_data) if '_test.csv' in filename and filename[0] != '.'])\n",
    "version_data_last = None\n",
    "for version_data in versions_data:\n",
    "    main(\n",
    "        folder_data, folder_output, version_data, version_data_last, postfix_train, postfix_test,\n",
    "        tokenizer, collator, index_label_2_labelid, index_labelid_2_label,\n",
    "        epochs, seq_max, batch_size, dim_hidden, dim_feedforward, n_head, n_layer,\n",
    "        lr_base_optimizer, betas_optimizer, eps_optimizer, warmup\n",
    "    )\n",
    "    \n",
    "    print('[INFO] finish {}'.format(version_data))\n",
    "    version_data_last = version_data\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856f3e6-33c4-4ce3-9837-28cde9fda578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
