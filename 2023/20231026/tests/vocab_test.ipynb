{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0c2627-bbd1-400e-8f44-7833c962939b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes:\n",
      "6191\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from spacy.lang.en.examples import sentences\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# import torchtext.datasets as datasets\n",
    "# from multi30k import Multi30k\n",
    "\n",
    "\n",
    "def Multi30k(language_pair=None):\n",
    "    corpus_lines_train = []\n",
    "\n",
    "    for lan in language_pair:\n",
    "        with open('text/train.{}'.format(lan), 'r') as file:\n",
    "            corpus_lines_train.append(file.read().splitlines())\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    corpus_train = list(zip(*corpus_lines_train))\n",
    "\n",
    "    corpus_lines_eval = []\n",
    "\n",
    "    for lan in language_pair:\n",
    "        with open('text/val.{}'.format(lan), 'r') as file:\n",
    "            corpus_lines_eval.append(file.read().splitlines())\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    corpus_lines_eval = list(zip(*corpus_lines_train))\n",
    "\n",
    "    return corpus_lines_train, corpus_lines_eval, None\n",
    "# end\n",
    "def load_tokenizers():\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_en\n",
    "# end\n",
    "\n",
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "# end\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, index):\n",
    "    for from_to_tuple in data_iter:\n",
    "        yield tokenizer(from_to_tuple[index])\n",
    "    # end\n",
    "# end\n",
    "\n",
    "def build_vocabulary(spacy_en):\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    print(\"Building English Vocabulary ...\")\n",
    "    train, val, test = Multi30k(language_pair=(\"de\", \"en\"))\n",
    "\n",
    "    vocab_tgt = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val, tokenize_en, index=1),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n",
    "\n",
    "    return vocab_tgt\n",
    "# end\n",
    "\n",
    "\n",
    "def load_vocab(spacy_en):\n",
    "    if not os.path.exists(\"vocab.pt\"):\n",
    "        vocab_tgt = build_vocabulary(spacy_en)\n",
    "        torch.save(vocab_tgt, \"vocab.pt\")\n",
    "    else:\n",
    "        vocab_tgt = torch.load(\"vocab.pt\")\n",
    "    print(\"Finished.\\nVocabulary sizes:\")\n",
    "    print(len(vocab_tgt))\n",
    "    return vocab_tgt\n",
    "# end\n",
    "\n",
    "vocab = load_vocab(load_tokenizers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe56dd55-764a-4161-8545-7253b880a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(\n",
    "    batch,\n",
    "    src_pipeline,\n",
    "    tgt_pipeline,\n",
    "    src_vocab,\n",
    "    tgt_vocab,\n",
    "    device,\n",
    "    max_padding=128,\n",
    "    pad_id=2,\n",
    "):\n",
    "    bs_id = torch.tensor([0], device=device)  # <s> token id\n",
    "    eos_id = torch.tensor([1], device=device)  # </s> token id\n",
    "    src_list, tgt_list = [], []\n",
    "    for (_src, _tgt) in batch:\n",
    "        processed_src = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    src_vocab(src_pipeline(_src)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        processed_tgt = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    tgt_vocab(tgt_pipeline(_tgt)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        src_list.append(\n",
    "            # warning - overwrites values for negative values of padding - len\n",
    "            pad(\n",
    "                processed_src,\n",
    "                (\n",
    "                    0,\n",
    "                    max_padding - len(processed_src),\n",
    "                ),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "        tgt_list.append(\n",
    "            pad(\n",
    "                processed_tgt,\n",
    "                (0, max_padding - len(processed_tgt)),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    src = torch.stack(src_list)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "    return (src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b43f18-fb0d-4f3a-b0f7-e0aea52121d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    device,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    batch_size=12000,\n",
    "    max_padding=128,\n",
    "    is_distributed=True,\n",
    "):\n",
    "    # def create_dataloaders(batch_size=12000):\n",
    "    def tokenize_de(text):\n",
    "        return tokenize(text, spacy_de)\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return tokenize(text, spacy_en)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(\n",
    "            batch,\n",
    "            tokenize_de,\n",
    "            tokenize_en,\n",
    "            vocab_src,\n",
    "            vocab_tgt,\n",
    "            device,\n",
    "            max_padding=max_padding,\n",
    "            pad_id=vocab_src.get_stoi()[\"<blank>\"],\n",
    "        )\n",
    "\n",
    "    train_iter, valid_iter, test_iter = Multi30k(\n",
    "        language_pair=(\"de\", \"en\")\n",
    "    )\n",
    "\n",
    "    train_iter_map = to_map_style_dataset(\n",
    "        train_iter\n",
    "    )  # DistributedSampler needs a dataset len()\n",
    "    train_sampler = (\n",
    "        DistributedSampler(train_iter_map) if is_distributed else None\n",
    "    )\n",
    "    valid_iter_map = to_map_style_dataset(valid_iter)\n",
    "    valid_sampler = (\n",
    "        DistributedSampler(valid_iter_map) if is_distributed else None\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(train_sampler is None),\n",
    "        sampler=train_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(valid_sampler is None),\n",
    "        sampler=valid_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return train_dataloader, valid_dataloader\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a54738e-c846-4080-b58c-6acc500825d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
