{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a082d2eb-6ca2-4b7e-92dc-386d7621124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"\n",
    " PyTorch DistilBERT model adapted in part from Facebook, Inc XLM model (https://github.com/facebookresearch/XLM) and in\n",
    " part from HuggingFace PyTorch version of Google AI Bert model (https://github.com/google-research/bert)\n",
    "\"\"\"\n",
    "\n",
    "def is_deepspeed_zero3_enabled():\n",
    "    return False\n",
    "\n",
    "def get_activation(klass_activation):\n",
    "    return klass_activation()\n",
    "# end\n",
    "\n",
    "class PretrainedConfig(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "# end\n",
    "\n",
    "\n",
    "class BaseModelOutput:\n",
    "    def __init__(self, last_hidden_state=None, hidden_states=None, attentions=None):\n",
    "        self.last_hidden_state = hidden_state\n",
    "        self.hidden_states = all_hidden_states\n",
    "        self.attentions = all_attentions\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class MaskedLMOutput:\n",
    "    def __init__(self, mlm_loss=None, prediction_logits=None, dlbrt_output=None):\n",
    "        self.loss=mlm_loss,\n",
    "        self.logits=prediction_logits,\n",
    "        self.hidden_states=dlbrt_output.hidden_states,\n",
    "        self.attentions=dlbrt_output.attentions,\n",
    "# end\n",
    "\n",
    "import math\n",
    "from typing import Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
    "\n",
    "\n",
    "\n",
    "def create_sinusoidal_embeddings(n_pos: int, dim: int, out: torch.Tensor):\n",
    "    if is_deepspeed_zero3_enabled():\n",
    "        import deepspeed\n",
    "\n",
    "        with deepspeed.zero.GatheredParameters(out, modifier_rank=0):\n",
    "            if torch.distributed.get_rank() == 0:\n",
    "                _create_sinusoidal_embeddings(n_pos=n_pos, dim=dim, out=out)\n",
    "    else:\n",
    "        _create_sinusoidal_embeddings(n_pos=n_pos, dim=dim, out=out)\n",
    "\n",
    "\n",
    "def _create_sinusoidal_embeddings(n_pos: int, dim: int, out: torch.Tensor):\n",
    "    position_enc = np.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])\n",
    "    out.requires_grad = False\n",
    "    out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "    out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "    out.detach_()\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.dim, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.dim)\n",
    "        if config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                n_pos=config.max_position_embeddings, dim=config.dim, out=self.position_embeddings.weight\n",
    "            )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, input_embeds: Optional[torch.Tensor] = None):\n",
    "\n",
    "        if input_ids is not None:\n",
    "            input_embeds = self.word_embeddings(input_ids)  # (bs, max_seq_length, dim)\n",
    "\n",
    "        seq_length = input_embeds.size(1)\n",
    "\n",
    "        if hasattr(self, \"position_ids\"):\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "        else:\n",
    "            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)  # (max_seq_length)\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)  # (bs, max_seq_length)\n",
    "\n",
    "        position_embeddings = self.position_embeddings(position_ids)  # (bs, max_seq_length, dim)\n",
    "\n",
    "        embeddings = input_embeds + position_embeddings  # (bs, max_seq_length, dim)\n",
    "        embeddings = self.LayerNorm(embeddings)  # (bs, max_seq_length, dim)\n",
    "        embeddings = self.dropout(embeddings)  # (bs, max_seq_length, dim)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.dim = config.dim\n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "        self.is_causal = False\n",
    "\n",
    "        # Have an even number of multi heads that divide the dimensions\n",
    "        if self.dim % self.n_heads != 0:\n",
    "            # Raise value errors for even multi-head attention nodes\n",
    "            raise ValueError(f\"self.n_heads: {self.n_heads} must divide self.dim: {self.dim} evenly\")\n",
    "\n",
    "        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "\n",
    "        self.pruned_heads: Set[int] = set()\n",
    "        self.attention_head_size = self.dim // self.n_heads\n",
    "\n",
    "    def prune_heads(self, heads: List[int]):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.n_heads, self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "        # Prune linear layers\n",
    "        self.q_lin = prune_linear_layer(self.q_lin, index)\n",
    "        self.k_lin = prune_linear_layer(self.k_lin, index)\n",
    "        self.v_lin = prune_linear_layer(self.v_lin, index)\n",
    "        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n",
    "        # Update hyper params\n",
    "        self.n_heads = self.n_heads - len(heads)\n",
    "        self.dim = self.attention_head_size * self.n_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ):\n",
    "\n",
    "        bs, q_length, dim = query.size()\n",
    "        k_length = key.size(1)\n",
    "\n",
    "        dim_per_head = self.dim // self.n_heads\n",
    "\n",
    "        mask_reshp = (bs, 1, 1, k_length)\n",
    "\n",
    "        def shape(x: torch.Tensor) -> torch.Tensor:\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "\n",
    "        def unshape(x: torch.Tensor) -> torch.Tensor:\n",
    "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "\n",
    "        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n",
    "        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n",
    "        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n",
    "\n",
    "        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\n",
    "        mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)\n",
    "        scores = scores.masked_fill(\n",
    "            mask, torch.tensor(torch.finfo(scores.dtype).min)\n",
    "        )  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "        weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)\n",
    "        weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            weights = weights * head_mask\n",
    "\n",
    "        context = torch.matmul(weights, v)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        context = unshape(context)  # (bs, q_length, dim)\n",
    "        context = self.out_lin(context)  # (bs, q_length, dim)\n",
    "\n",
    "        if output_attentions:\n",
    "            return (context, weights)\n",
    "        else:\n",
    "            return (context,)\n",
    "\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.lin1 = nn.Linear(in_features=config.dim, out_features=config.hidden_dim)\n",
    "        self.lin2 = nn.Linear(in_features=config.hidden_dim, out_features=config.dim)\n",
    "        self.activation = get_activation(config.activation)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return apply_chunking_to_forward(self.ff_chunk, self.chunk_size_feed_forward, self.seq_len_dim, input)\n",
    "\n",
    "    def ff_chunk(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.lin1(input)\n",
    "        x = self.activation(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        # Have an even number of Configure multi-heads\n",
    "        if config.dim % config.n_heads != 0:\n",
    "            raise ValueError(f\"config.n_heads {config.n_heads} must divide config.dim {config.dim} evenly\")\n",
    "\n",
    "        self.attention = (\n",
    "            MultiHeadSelfAttention(config)\n",
    "            if not getattr(config, \"_flash_attn_2_enabled\", False)\n",
    "            else DistilBertFlashAttention2(config)\n",
    "        )\n",
    "        self.sa_layer_norm = nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)\n",
    "\n",
    "        self.ffn = FFN(config)\n",
    "        self.output_layer_norm = nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attn_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "    ):\n",
    "        # Self-Attention\n",
    "        sa_output = self.attention(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            mask=attn_mask,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        if output_attentions:\n",
    "            sa_output, sa_weights = sa_output  # (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\n",
    "        else:  # To handle these `output_attentions` or `output_hidden_states` cases returning tuples\n",
    "            if type(sa_output) != tuple:\n",
    "                raise TypeError(f\"sa_output must be a tuple but it is {type(sa_output)} type\")\n",
    "\n",
    "            sa_output = sa_output[0]\n",
    "        sa_output = self.sa_layer_norm(sa_output + x)  # (bs, seq_length, dim)\n",
    "\n",
    "        # Feed Forward Network\n",
    "        ffn_output = self.ffn(sa_output)  # (bs, seq_length, dim)\n",
    "        ffn_output: torch.Tensor = self.output_layer_norm(ffn_output + sa_output)  # (bs, seq_length, dim)\n",
    "\n",
    "        output = (ffn_output,)\n",
    "        if output_attentions:\n",
    "            output = (sa_weights,) + output\n",
    "        return output\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.n_layers = config.n_layers\n",
    "        self.layer = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attn_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        hidden_state = x\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_state,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__,\n",
    "                    hidden_state,\n",
    "                    attn_mask,\n",
    "                    head_mask[i],\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_state,\n",
    "                    attn_mask,\n",
    "                    head_mask[i],\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_state = layer_outputs[-1]\n",
    "\n",
    "            if output_attentions:\n",
    "                if len(layer_outputs) != 2:\n",
    "                    raise ValueError(f\"The length of the layer_outputs should be 2, but it is {len(layer_outputs)}\")\n",
    "\n",
    "                attentions = layer_outputs[0]\n",
    "                all_attentions = all_attentions + (attentions,)\n",
    "            else:\n",
    "                if len(layer_outputs) != 1:\n",
    "                    raise ValueError(f\"The length of the layer_outputs should be 1, but it is {len(layer_outputs)}\")\n",
    "\n",
    "        # Add last layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_state,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_state, all_hidden_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_state, hidden_states=all_hidden_states, attentions=all_attentions\n",
    "        )\n",
    "\n",
    "\n",
    "# INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL #\n",
    "class DistilBertPreTrainedModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "    # end\n",
    "\n",
    "    config_class = PretrainedConfig\n",
    "    load_tf_weights = None\n",
    "    base_model_prefix = \"distilbert\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _supports_flash_attn_2 = False\n",
    "\n",
    "    def _init_weights(self, module: nn.Module):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "\n",
    "class DistilBertModel(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.embeddings = Embeddings(config)  # Embeddings\n",
    "        self.transformer = Transformer(config)  # Encoder\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "    # end\n",
    "    \n",
    "    def post_init():\n",
    "        pass\n",
    "    # end\n",
    "\n",
    "    def get_position_embeddings(self) -> nn.Embedding:\n",
    "        \"\"\"\n",
    "        Returns the position embeddings\n",
    "        \"\"\"\n",
    "        return self.embeddings.position_embeddings\n",
    "\n",
    "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
    "\n",
    "        num_position_embeds_diff = new_num_position_embeddings - self.config.max_position_embeddings\n",
    "\n",
    "        # no resizing needs to be done if the length stays the same\n",
    "        if num_position_embeds_diff == 0:\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Setting `config.max_position_embeddings={new_num_position_embeddings}`...\")\n",
    "        self.config.max_position_embeddings = new_num_position_embeddings\n",
    "\n",
    "        old_position_embeddings_weight = self.embeddings.position_embeddings.weight.clone()\n",
    "\n",
    "        self.embeddings.position_embeddings = nn.Embedding(self.config.max_position_embeddings, self.config.dim)\n",
    "\n",
    "        if self.config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                n_pos=self.config.max_position_embeddings, dim=self.config.dim, out=self.position_embeddings.weight\n",
    "            )\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if num_position_embeds_diff > 0:\n",
    "                    self.embeddings.position_embeddings.weight[:-num_position_embeds_diff] = nn.Parameter(\n",
    "                        old_position_embeddings_weight\n",
    "                    )\n",
    "                else:\n",
    "                    self.embeddings.position_embeddings.weight = nn.Parameter(\n",
    "                        old_position_embeddings_weight[:num_position_embeds_diff]\n",
    "                    )\n",
    "        # move position_embeddings to correct device\n",
    "        self.embeddings.position_embeddings.to(self.device)\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Embedding:\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, new_embeddings: nn.Embedding):\n",
    "        self.embeddings.word_embeddings = new_embeddings\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune: Dict[int, List[List[int]]]):\n",
    "\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.transformer.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "\n",
    "        embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n",
    "\n",
    "        if getattr(self.config, \"_flash_attn_2_enabled\", False):\n",
    "            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n",
    "        else:\n",
    "            if attention_mask is None:\n",
    "                attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n",
    "\n",
    "        return self.transformer(\n",
    "            x=embeddings,\n",
    "            attn_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class DistilBertForMaskedLM(DistilBertPreTrainedModel):\n",
    "    _tied_weights_keys = [\"vocab_projector.weight\"]\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.activation = get_activation(config.activation)\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.vocab_transform = nn.Linear(config.dim, config.dim)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(config.dim, config.vocab_size)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "        self.mlm_loss_fct = nn.CrossEntropyLoss()\n",
    "    # end\n",
    "    \n",
    "    def post_init(self):\n",
    "        pass\n",
    "    # end\n",
    "\n",
    "    def get_position_embeddings(self) -> nn.Embedding:\n",
    "        return self.distilbert.get_position_embeddings()\n",
    "\n",
    "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
    "        self.distilbert.resize_position_embeddings(new_num_position_embeddings)\n",
    "\n",
    "    def get_output_embeddings(self) -> nn.Module:\n",
    "        return self.vocab_projector\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings: nn.Module):\n",
    "        self.vocab_projector = new_embeddings\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        dlbrt_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = dlbrt_output[0]  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.vocab_transform(hidden_states)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.activation(prediction_logits)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "\n",
    "        mlm_loss = None\n",
    "        if labels is not None:\n",
    "            mlm_loss = self.mlm_loss_fct(prediction_logits.view(-1, prediction_logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_logits,) + dlbrt_output[1:]\n",
    "            return ((mlm_loss,) + output) if mlm_loss is not None else output\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=mlm_loss,\n",
    "            logits=prediction_logits,\n",
    "            hidden_states=dlbrt_output.hidden_states,\n",
    "            attentions=dlbrt_output.attentions,\n",
    "        )\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33e9212-8768-4974-bb31-0e338bcbc177",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertModel' object has no attribute 'post_init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b12c1868d57c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertForMaskedLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-47847a4cfcc9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistilbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_layer_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-47847a4cfcc9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;31m# Initialize weights and apply final processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_position_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1270\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DistilBertModel' object has no attribute 'post_init'"
     ]
    }
   ],
   "source": [
    "#TODO: weight tied not enabled\n",
    "\n",
    "\n",
    "config = PretrainedConfig()\n",
    "config._flash_attn_2_enabled=False\n",
    "config.activation=torch.nn.ReLU\n",
    "config.dim = 512\n",
    "config.hidden_dim=512\n",
    "config.vocab_size = 6195\n",
    "config.pad_token_id = 6194\n",
    "config.max_position_embeddings = 512\n",
    "config.sinusoidal_pos_embds = True\n",
    "config.n_layers = 4\n",
    "config.n_heads = 8\n",
    "config.attention_dropout = 0.1\n",
    "config.dropout=0.1\n",
    "\n",
    "model = DistilBertForMaskedLM(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
