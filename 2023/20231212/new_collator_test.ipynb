{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db07e30f-393e-4322-ad4f-2d14cbe65464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "class Batch:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if v is not None and type(v) is not bool:\n",
    "                self.kwargs[k] = v.cuda()\n",
    "            # end\n",
    "        # end\n",
    "        \n",
    "    # end\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.kwargs\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class Collator_Base:\n",
    "\n",
    "    def __init__(self, tokenizer, size_seq_max, need_masked=0.3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size_seq_max = size_seq_max\n",
    "        self.need_masked = need_masked\n",
    "\n",
    "        index_special_token_2_id = {k: v for k, v in zip(tokenizer.all_special_tokens, tokenizer.all_special_ids)}\n",
    "\n",
    "        self.id_pad = index_special_token_2_id['[PAD]']\n",
    "        self.id_mask = index_special_token_2_id['[MASK]']\n",
    "        self.id_cls = index_special_token_2_id['[CLS]']\n",
    "        self.id_sep = index_special_token_2_id['[SEP]']\n",
    "        self.id_unk = index_special_token_2_id['[UNK]']\n",
    "        \n",
    "        self.regex_special_token = re.compile(r'\\[(PAD|MASK|CLS|SEP|EOL|UNK)\\]')\n",
    "        \n",
    "        self.index_randtoken_start = 999\n",
    "        self.index_randtoken_end = 30521\n",
    "    # end\n",
    "\n",
    "    def _preprocess(self, line):\n",
    "        line = re.sub(self.regex_special_token, r'<\\1>', line)\n",
    "        line = re.sub(r'''('|\"|`){2}''', '', line)\n",
    "        line = re.sub(r'\\.{2,3}', '', line)\n",
    "        line = re.sub(r' {2,}', ' ', line)\n",
    "        line = line.lstrip().rstrip()\n",
    "        return line\n",
    "    # end\n",
    "    \n",
    "    def _get_random_tokens(self):\n",
    "        return random.randint(self.index_randtoken_start, self.index_randtoken_end)\n",
    "    # end\n",
    "\n",
    "    \n",
    "    def pad_sequences(self, sequences, size_seq_max, need_diagonal=False,\n",
    "                      need_masked=0):  # need_diagonal and need_masked cannot both set, one for bert seq one for s2s seq\n",
    "        \n",
    "        sequences = copy.deepcopy(sequences)\n",
    "        \n",
    "        id_pad = self.id_pad\n",
    "        id_mask = self.id_mask\n",
    "\n",
    "        sequences_masked_padded = []\n",
    "        labels_padded = []\n",
    "\n",
    "        for sequence in sequences:\n",
    "\n",
    "            len_seq = len(sequence)\n",
    "            label = copy.deepcopy(sequence)\n",
    "\n",
    "            if need_masked:\n",
    "                indexs_masked = list(range(1, len_seq - 1))  # 0 = cls, -1 = sep\n",
    "                random.shuffle(indexs_masked)\n",
    "                anchor_mask_all = round(need_masked * (len_seq - 2)) or 1\n",
    "                anchor_mask_replace = int(anchor_mask_all / 2)\n",
    "\n",
    "                if anchor_mask_replace:  # not 0\n",
    "                    indexs_replaced = indexs_masked[:anchor_mask_replace]\n",
    "                    for index_replaced in indexs_replaced:\n",
    "                        sequence[index_replaced] = self._get_random_tokens()\n",
    "                    # end\n",
    "                # end\n",
    "\n",
    "                indexs_masked = indexs_masked[anchor_mask_replace:anchor_mask_all]\n",
    "            # end\n",
    "\n",
    "\n",
    "            count_pad = size_seq_max - len_seq\n",
    "            \n",
    "            label = torch.LongTensor(label)\n",
    "            label_padded = torch.cat((label, torch.LongTensor([id_pad] * count_pad)))\n",
    "            labels_padded.append(label_padded)\n",
    "\n",
    "            if need_masked:\n",
    "\n",
    "                sequence_masked = torch.LongTensor(sequence)\n",
    "                sequence_masked.index_fill_(0, torch.LongTensor(indexs_masked), id_mask)\n",
    "                sequence_masked_padded = torch.cat((sequence_masked, torch.LongTensor([id_pad] * count_pad)))\n",
    "\n",
    "                sequences_masked_padded.append(sequence_masked_padded)\n",
    "            # end\n",
    "        #   # end for\n",
    "\n",
    "        inputs = torch.stack(labels_padded)  # (batch, size_seq_max)\n",
    "        if need_masked:\n",
    "            inputs_masked_padded = torch.stack(sequences_masked_padded)\n",
    "        # end\n",
    "\n",
    "        masks_segment = (inputs != self.id_pad).unsqueeze(-2)  # (nbatch, 1, seq)\n",
    "        masks_attention = self.make_std_mask(inputs, self.id_pad) if need_diagonal else masks_segment\n",
    "\n",
    "        if need_masked:\n",
    "            masks_masked = (inputs_masked_padded != id_mask).unsqueeze(-2)\n",
    "            masks_attention = masks_attention & masks_masked\n",
    "            return inputs_masked_padded, masks_attention, masks_segment, inputs  # (inputs, masks_attention, masks_segment, labels)\n",
    "        else:\n",
    "            return inputs, masks_attention, masks_segment, None\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "\n",
    "    def subsequent_mask(self, size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "            torch.uint8\n",
    "        )\n",
    "        return subsequent_mask == 0\n",
    "\n",
    "    def make_std_mask(self, tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & self.subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Collator_BERT_Encoded_254(Collator_Base):\n",
    "\n",
    "    def __call__(self, list_tokenized_merged):\n",
    "        \n",
    "        len_tokenized_accumulated = 2  # add cls and sep\n",
    "        list_tokenized_merged = [tokenized_merged[:self.size_seq_max - len_tokenized_accumulated] for tokenized_merged in list_tokenized_merged]\n",
    "\n",
    "        # Process III. Add begin and stop special token, same as jinyuj_transformers_quora.ipynb\n",
    "        tokens_input_encoder = []\n",
    "        tokens_input_decoder = []\n",
    "        tokens_label_decoder = []\n",
    "\n",
    "        for tokenized_merged in list_tokenized_merged:\n",
    "            tokens_input_encoder.append([self.id_cls] + tokenized_merged + [self.id_sep])\n",
    "            tokens_input_decoder.append([self.id_cls] + tokenized_merged)\n",
    "            tokens_label_decoder.append(tokenized_merged + [self.id_sep])\n",
    "        # end\n",
    "\n",
    "        inputs_encoder, masks_encoder, segments_encoder, labels_encoder = self.pad_sequences(tokens_input_encoder,\n",
    "                                                                                             self.size_seq_max,\n",
    "                                                                                             need_masked=self.need_masked)\n",
    "        inputs_decoder, masks_decoder, segments_decoder, _ = self.pad_sequences(tokens_input_decoder, self.size_seq_max,\n",
    "                                                                                need_diagonal=True)\n",
    "        labels_decoder, masks_label, segments_label, _ = self.pad_sequences(tokens_label_decoder, self.size_seq_max)\n",
    "\n",
    "        return Batch(\n",
    "            ids_encoder=inputs_encoder,  # contains [mask]s\n",
    "            masks_encoder=masks_encoder,\n",
    "            labels_encoder=labels_encoder,  # doesn't contain [mask]\n",
    "            segments_encoder=segments_encoder,\n",
    "            ids_decoder=inputs_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            labels_decoder=labels_decoder,\n",
    "            segments_label=segments_label\n",
    "        )\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00dee085-8668-4c4c-9383-46207531ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncodedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # info_file_rows = {'path_file': 1,000,000,...}\n",
    "    def __init__(self, folder_dataset_base, info_file_rows, split=0.001):\n",
    "        self.folder_dataset_base = folder_dataset_base\n",
    "        self.list_tokenized_eval = []\n",
    "        self.dict_filename_loaded = {filename: False for filename, num_rows in info_file_rows.items()}\n",
    "        self.list_corpus_idx_filename_train = []\n",
    "\n",
    "        for filename, num_lines in info_file_rows.items():\n",
    "            idxs_eval = list(range(num_lines))\n",
    "            random.shuffle(idxs_eval)\n",
    "            idxs_eval = idxs_eval[:round(len(idxs_eval) * split)]\n",
    "\n",
    "            for idx_eval in idxs_eval:\n",
    "                self.list_tokenized_eval.append((idx_eval, filename))\n",
    "            # end\n",
    "\n",
    "            set_idxs_eval = set(idxs_eval)\n",
    "            for idx_train in range(num_lines):\n",
    "                if idx_train in set_idxs_eval:\n",
    "                    continue\n",
    "                # end\n",
    "\n",
    "                self.list_corpus_idx_filename_train.append((idx_train, filename))\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        self.is_train = True\n",
    "        self.rows_cached = []\n",
    "        self.filename_cached = None\n",
    "    # end\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):  # should not have problem now\n",
    "        # if eval, use all cached eval tokenized\n",
    "        if not self.is_train:\n",
    "            return self.list_tokenized_eval[idx]\n",
    "        # end\n",
    "\n",
    "        # if train\n",
    "        idxs_in_file, filename_current = self.list_corpus_idx_filename_train[idx]\n",
    "\n",
    "        # if file not fully used\n",
    "        if filename_current != self.filename_cached:\n",
    "\n",
    "            # load new file\n",
    "            print('switch from {} to {}'.format(self.filename_cached, filename_current))\n",
    "            path_file = os.path.join(self.folder_dataset_base, filename_current)\n",
    "            with open(path_file, 'r') as file:  # update rows_cached\n",
    "                self.rows_cached = file.read().splitlines()\n",
    "            # end\n",
    "\n",
    "            self.filename_cached = filename_current\n",
    "\n",
    "            if not self.dict_filename_loaded[filename_current]:\n",
    "                for id_list_eval, tokenized_eval in enumerate(self.list_tokenized_eval):\n",
    "                    if type(tokenized_eval) is tuple:\n",
    "                        if tokenized_eval[1] == filename_current:\n",
    "                            self.list_tokenized_eval[id_list_eval] = self._fransfer_one_line_to_tokenized(self.rows_cached[tokenized_eval[0]])\n",
    "                        # end\n",
    "                    # end\n",
    "                # end\n",
    "                self.dict_filename_loaded[filename_current] = True\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        return self._fransfer_one_line_to_tokenized(self.rows_cached[idxs_in_file])\n",
    "    # end\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.list_corpus_idx_filename_train)\n",
    "        else:\n",
    "            return len(self.list_tokenized_eval)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def _fransfer_one_line_to_tokenized(self, str_line):\n",
    "        tokenized = [int(t) for t in str_line.split(', ')]\n",
    "        return tokenized\n",
    "    # end\n",
    "\n",
    "    def train(self):\n",
    "        self.is_train = True\n",
    "    # end\n",
    "\n",
    "    def eval(self):\n",
    "        self.is_train = False\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d048eb-4751-4c7f-9bcb-f6e51c92de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPUS = [1]\n",
    "torch.cuda.set_device(GPUS[0])\n",
    "\n",
    "# source\n",
    "seq_max = 256\n",
    "batch_size = 2\n",
    "len_dataset = 22345\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "collator = Collator_BERT_Encoded_254(tokenizer, seq_max)\n",
    "\n",
    "\n",
    "folder_dataset = 'bookcorpus_merged_254_20'\n",
    "filenames_dataset = sorted([f for f in os.listdir(folder_dataset) if f[0] != '.'], key=lambda name: int(name.split('.')[0]))\n",
    "# list_size_per_file = [10000, 10000, 2345]\n",
    "list_size_per_file = [20, 20, 13]\n",
    "\n",
    "info_filename_rows = {k:v for k,v in zip(filenames_dataset, list_size_per_file)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e53753a7-0ba7-40be-b4cc-97209dd21376",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = SimpleEncodedDataset(folder_dataset, info_filename_rows)\n",
    "dataloader_train = DataLoader(source, batch_size*len(GPUS), shuffle=False, collate_fn=collator)\n",
    "dataloader_eval = DataLoader(source, 1, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46701785-8840-41ab-8d07-a9fc77b56986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a8a72854c64e5faca0e5305cddbcb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switch from 2.encode to 0.encode\n",
      "switch from 0.encode to 1.encode\n",
      "switch from 1.encode to 2.encode\n"
     ]
    }
   ],
   "source": [
    "source.train()\n",
    "for batch in tqdm(dataloader_train):\n",
    "    info_batch = batch()\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e07378-2868-4572-a540-0a19725ef268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2002,   103, 17733,  1012,  1045,  2074,  2729,  2055, 16214,\n",
       "          1012,  1045,  4737,  2055,  2017,  1012,  2823, 22225, 14901,  2007,\n",
       "          2129,  2000, 19155,  2009, 14061, 23979,  2032,  1012,  2823,  1045,\n",
       "          2514, 26440,  2017,  2079,  1050,   103,  1056, 21793,   103,   103,\n",
       "          2115,  2925,  1012,   103,  4426,  1012,  2823,  1045,  2079,  1050,\n",
       "          1005, 12593,  1012,  1045,  3112,  2222,  2196,  3815,  2000,  2505,\n",
       "           103, 10272,   103,  2002,  5015,  2061,  3043, 24454,  2755,  1010,\n",
       "           103,  3480,  1012,  2079,  1050,  1005,   103, 27521,  1210,  1010,\n",
       "          3841,   103,  2009,   103,  1055,  2025,  2995,  6227,  2054,   103,\n",
       "          1045,  2204,   103,  1010,   103, 18890,  2054, 24908,  1045,   103,\n",
       "          2008,  1005,  1055,  6783, 19927, 16652,  1045,   103,  1050,  1005,\n",
       "          1056,   103,  2019,   103,  1012,  2002,  2106,  1050,  1005, 17595,\n",
       "          2428,  8225,   103,  7570, 27982,  2008,   103,  2354,  1997,   103,\n",
       "          2017,  1005,  2128,  1037,  2204,  2711,  1010,  3841,   103, 17165,\n",
       "         19656, 11725,  1012,  3071,  2515,  1012,   103,  2074,  2031,  2000,\n",
       "          2424,  6737,   103,  2017,  2614,  2717,  1037, 16477,  8606, 17220,\n",
       "          1012,  1045,  2079,  1050,  1005,   103, 19566,   103, 19373, 22140,\n",
       "          1012,  1045,  9623,  1049, 21898, 28228, 23936,   103,   103, 10925,\n",
       "           103,   103,  2204,  2012,  4855,  2009, 30351,  1045, 16261,  1049,\n",
       "          2204,  2012,  2108,  1037, 29398,   103,  1887,  1008,  2332,  7435,\n",
       "          1010,  2008,  3315,   103,  2054,  1045,  1005,  1049,  2204,  7467,\n",
       "          1012,  2002,   103,  1996,  2327,  6462,  2000,  2404,  1996,  3042,\n",
       "          2000,  3637,  1998,  7468,  2009, 14136,  1999,  2010,   103,   103,\n",
       "          1045,  5489,   103,  1045,  1005,  1049,  3374,  1010,  3841,  1010,\n",
       "          1045,  2106,   103,  1005, 26738, 22392,   103,  6314,  2017,  1012,\n",
       "           102,     0,     0,     0,     0,     0]], device='cuda:1')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_batch['ids_encoder']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
