{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3af7cf4-c53b-4eae-9637-481ae6537ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from os.path import exists\n",
    "import torch.nn as nn\n",
    "# from torch.nn.functional import log_softmax, pad, one_hot\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import re\n",
    "import threading\n",
    "\n",
    "### utils.py ###\n",
    "\n",
    "class Dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        for k, v in self.items():\n",
    "            if k in other and other[k]:\n",
    "                self[k] += other[k]\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        return self\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "# Takes the file paths as arguments\n",
    "def parse_csv_file_to_json(path_file_csv):\n",
    "    # create a dictionary\n",
    "    elements = []\n",
    "\n",
    "    # Open a csv reader called DictReader\n",
    "    with open(path_file_csv, encoding='utf-8') as file_csv:\n",
    "    #with open(path_file_csv) as file_csv:\n",
    "        reader_csv = csv.DictReader(file_csv, delimiter=\"\\t\")\n",
    "\n",
    "        # Convert each row into a dictionary\n",
    "        # and add it to data\n",
    "        for dict_head_value in reader_csv:\n",
    "            element = {}\n",
    "\n",
    "            for head, value in dict_head_value.items():\n",
    "                if value and (value[0] in [\"[\", \"{\"]):\n",
    "                    element[head] = value\n",
    "                else:\n",
    "                    element[head] = value\n",
    "\n",
    "            elements.append(element)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    return elements\n",
    "# end\n",
    "\n",
    "### utils.py ###\n",
    "\n",
    "\n",
    "\n",
    "### core.py ###\n",
    "\n",
    "\"Produce N identical layers.\"\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "# end\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    \"Take in model size and number of heads.\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    # end\n",
    "\n",
    "\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # print('jinyuj: scores: {}, mask: {}'.format(scores.shape, mask.shape))\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # end\n",
    "        p_attn = scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        # end\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "    # end\n",
    "\n",
    "\n",
    "    \"Implements Figure 2\"\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        # print('jinyuj: self.h: {}, self.d_k: {}'.format(self.h, self.d_k))\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = self.attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)\n",
    "    # end\n",
    "# end class\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A residual connection followed by a layer norm.\n",
    "Note for code simplicity the norm is first as opposed to last.\n",
    "\"\"\"\n",
    "class ResidualLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, size, dropout=0.1, eps=1e-6):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.norm = torch.nn.LayerNorm(size, eps)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    # end\n",
    "\n",
    "    \"Apply residual connection to any sublayer with the same size.\"\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    # end\n",
    "# end class\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleIDEmbeddings(nn.Module):\n",
    "    def __init__(self, size_vocab, dim_hidden, id_pad):\n",
    "        super(SimpleIDEmbeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(size_vocab, dim_hidden, padding_idx=id_pad)\n",
    "        self.dim_hidden = dim_hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.lut(x)\n",
    "        return result * math.sqrt(self.dim_hidden)\n",
    "    # end\n",
    "\n",
    "    def get_shape(self):\n",
    "        return (self.lut.num_embeddings, self.lut.embedding_dim)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\"Implement the PE function.\"\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_positional, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.dim_positional = dim_positional\n",
    "        pe = torch.zeros(max_len, dim_positional)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, dim_positional, 2) * -(math.log(10000.0) / dim_positional)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).to('cuda')\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return x\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleEmbedder(nn.Module):    # no segment embedder as we do not need that\n",
    "    def __init__(self, size_vocab=None, dim_hidden=128, dropout=0.1, id_pad=0):\n",
    "        super(SimpleEmbedder, self).__init__()\n",
    "        self.size_vocab = size_vocab\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.id_pad = id_pad\n",
    "\n",
    "        self.embedder = nn.Sequential(\n",
    "            SimpleIDEmbeddings(size_vocab, dim_hidden, id_pad),\n",
    "            PositionalEncoding(dim_hidden),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "    # end\n",
    "\n",
    "    def forward(self, ids_input):   # (batch, seqs_with_padding)\n",
    "        return self.embedder(ids_input)\n",
    "    # end\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.size_vocab\n",
    "    # end\n",
    "# end\n",
    "\n",
    "### core.py ###\n",
    "\n",
    "\n",
    "\n",
    "class SimpleEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, dim_feedforward, n_head, dropout=0.1):\n",
    "        super(SimpleEncoderLayer, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        self.layer_attention = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_feedforward = PositionwiseFeedForward(dim_hidden, dim_feedforward, dropout)\n",
    "        self.layers_residual = clones(ResidualLayer(dim_hidden, dropout), 2)\n",
    "    # end\n",
    "\n",
    "    def forward(self, embeddings, masks, *args):\n",
    "        embeddings = self.layers_residual[0](embeddings, lambda embeddings: self.layer_attention(embeddings, embeddings, embeddings, masks))\n",
    "        return self.layers_residual[1](embeddings, self.layer_feedforward)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class SimpleDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, dim_feedforward, n_head, dropout=0.1):\n",
    "        super(SimpleDecoderLayer, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        self.layer_attention_decoder = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_attention_encoder = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_feedforward = PositionwiseFeedForward(dim_hidden, dim_feedforward, dropout)\n",
    "        self.layers_residual = clones(ResidualLayer(dim_hidden, dropout), 3)\n",
    "\n",
    "    def forward(self, embeddings, masks_encoder, output_encoder, masks_decoder, *args):\n",
    "        embeddings = self.layers_residual[0](embeddings, lambda embeddings: self.layer_attention_decoder(embeddings, embeddings, embeddings, masks_decoder))\n",
    "        embeddings = self.layers_residual[1](embeddings, lambda embeddings: self.layer_attention_encoder(embeddings, output_encoder, output_encoder, masks_encoder))\n",
    "        return self.layers_residual[2](embeddings, self.layer_feedforward)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleTransformerStack(nn.Module):\n",
    "\n",
    "    def __init__(self, obj_layer, n_layers):\n",
    "        super(SimpleTransformerStack, self).__init__()\n",
    "        self.layers = clones(obj_layer, n_layers)\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(obj_layer.dim_hidden)\n",
    "    # end\n",
    "\n",
    "    def forward(self, embedding_encoder=None, masks_encoder=None, output_encoder=None, embedding_decoder=None, masks_decoder=None ,noncache=False, **kwargs):  # input -> (batch, len_seq, vocab)\n",
    "\n",
    "        if output_encoder is not None and embedding_decoder is not None and masks_decoder is not None:\n",
    "            embeddings = embedding_decoder\n",
    "        else:\n",
    "            embeddings = embedding_encoder\n",
    "        # end\n",
    "\n",
    "        for layer in self.layers:\n",
    "            embeddings = layer(embeddings, masks_encoder, output_encoder, masks_decoder)\n",
    "        # end\n",
    "\n",
    "        output = self.norm(embeddings)\n",
    "        return output\n",
    "    # end\n",
    "\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleEncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder_encoder, embedder_decoder, pooling=True):\n",
    "        super(SimpleEncoderDecoder, self).__init__()\n",
    "\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        self.embedder_encoder = embedder_encoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.embedder_decoder = embedder_decoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, ids_encoder=None, masks_encoder=None, ids_decoder=None, masks_decoder=None, **kwargs):\n",
    "        output_encoder = None\n",
    "        output_encoder_pooled = None\n",
    "        output_decoder = None\n",
    "        \n",
    "        # output_encoder = self.embed_and_encode(input_ids=ids_encoder, attention_mask=masks_encoder)\n",
    "        output_encoder = self.embed_and_encode(ids_encoder=ids_encoder, masks_encoder=masks_encoder)\n",
    "        output = output_encoder\n",
    "        \n",
    "        if self.pooling:\n",
    "            output_encoder_refilled = output_encoder.masked_fill(masks_encoder.transpose(-1,-2)==False, 0)\n",
    "            output_encoder_pooled = torch.mean(output_encoder_refilled, dim=-2)\n",
    "            \n",
    "            output_encoder_pooled_expanded = output_encoder_pooled.unsqueeze(-2).expand(output_encoder.shape)\n",
    "            output = output_encoder_pooled_expanded\n",
    "        # end\n",
    "        \n",
    "        if self.embedder_decoder and self.decoder:\n",
    "            output_decoder = self.embed_and_decode(ids_decoder=ids_decoder, masks_encoder=masks_encoder, output_encoder=output, masks_decoder=masks_decoder)\n",
    "        # end if\n",
    "        \n",
    "        return {'output_encoder': output_encoder, 'output_encoder_pooled': output_encoder_pooled, 'output_decoder': output_decoder}\n",
    "    # end\n",
    "    \n",
    "    def embed_and_encode(self, ids_encoder=None, masks_encoder=None, **kwargs):\n",
    "        \n",
    "        inputs_embeds = self.embedder_encoder(input_ids=ids_encoder)\n",
    "        output_encoder = self.encoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=masks_encoder,\n",
    "        )\n",
    "        \n",
    "        return output_encoder.last_hidden_state\n",
    "    # end\n",
    "\n",
    "    \n",
    "    def embed_and_decode(self, ids_decoder=None, masks_encoder=None, output_encoder=None, masks_decoder=None, **kwargs):\n",
    "        \n",
    "        embedding_decoder = self.embedder_decoder(ids_decoder)\n",
    "        output_decoder = self.decoder(\n",
    "            masks_encoder=masks_encoder,\n",
    "            output_encoder=output_encoder,    #(len_seq, dim_hidden) -> (1, dim_hidden)\n",
    "            embedding_decoder=embedding_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "        )\n",
    "\n",
    "        return output_decoder\n",
    "    # end\n",
    "    \n",
    "\n",
    "    def get_vocab_size(self, name_embedder):\n",
    "        embedder = getattr(self, f'embedder_{name_embedder}')\n",
    "        return embedder.get_vocab_size()\n",
    "    # end\n",
    "\n",
    "# end\n",
    "\n",
    "class LinearAndNorm(nn.Module):\n",
    "    def __init__(self, dim_in = None, dim_out = None, dropout=0.1, eps_norm=1e-12):\n",
    "        super(LinearAndNorm, self).__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(dim_in, dim_out)\n",
    "        self.norm = torch.nn.LayerNorm(dim_out, eps_norm)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    # end\n",
    "\n",
    "    def forward(self, seqs_in):\n",
    "        return self.dropout(self.norm(self.linear(seqs_in).relu()))\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Batch:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if v is not None and type(v) is not bool:\n",
    "                self.kwargs[k] = v.cuda()\n",
    "            # end\n",
    "        # end\n",
    "        \n",
    "    # end\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.kwargs\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class Collator_Base:\n",
    "\n",
    "    def __init__(self, tokenizer, size_seq_max, need_masked=0.3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size_seq_max = size_seq_max\n",
    "        self.need_masked = need_masked\n",
    "\n",
    "        index_special_token_2_id = {k: v for k, v in zip(tokenizer.all_special_tokens, tokenizer.all_special_ids)}\n",
    "\n",
    "        self.id_pad = index_special_token_2_id['[PAD]']\n",
    "        self.id_mask = index_special_token_2_id['[MASK]']\n",
    "        self.id_cls = index_special_token_2_id['[CLS]']\n",
    "        self.id_sep = index_special_token_2_id['[SEP]']\n",
    "        self.id_unk = index_special_token_2_id['[UNK]']\n",
    "        \n",
    "        self.regex_special_token = re.compile(r'\\[(PAD|MASK|CLS|SEP|EOL|UNK)\\]')\n",
    "        \n",
    "        self.index_randtoken_start = 999\n",
    "        self.index_randtoken_end = 30521\n",
    "    # end\n",
    "\n",
    "    def _preprocess(self, line):\n",
    "        line = re.sub(self.regex_special_token, r'<\\1>', line)\n",
    "        line = re.sub(r'''('|\"|`){2}''', '', line)\n",
    "        line = re.sub(r'\\.{2,3}', '', line)\n",
    "        line = re.sub(r' {2,}', ' ', line)\n",
    "        line = line.lstrip().rstrip()\n",
    "        return line\n",
    "    # end\n",
    "    \n",
    "    def _get_random_tokens(self):\n",
    "        return random.randint(self.index_randtoken_start, self.index_randtoken_end)\n",
    "    # end\n",
    "\n",
    "    \n",
    "    def pad_sequences(self, sequences, size_seq_max, need_diagonal=False,\n",
    "                      need_masked=0):  # need_diagonal and need_masked cannot both set, one for bert seq one for s2s seq\n",
    "        \n",
    "        sequences = copy.deepcopy(sequences)\n",
    "        \n",
    "        id_pad = self.id_pad\n",
    "        id_mask = self.id_mask\n",
    "\n",
    "        sequences_masked_padded = []\n",
    "        labels_padded = []\n",
    "\n",
    "        for sequence in sequences:\n",
    "\n",
    "            len_seq = len(sequence)\n",
    "            label = copy.deepcopy(sequence)\n",
    "\n",
    "            if need_masked:\n",
    "                indexs_masked = list(range(1, len_seq - 1))  # 0 = cls, -1 = sep\n",
    "                random.shuffle(indexs_masked)\n",
    "                anchor_mask_all = round(need_masked * (len_seq - 2)) or 1\n",
    "                anchor_mask_replace = int(anchor_mask_all / 2)\n",
    "\n",
    "                if anchor_mask_replace:  # not 0\n",
    "                    indexs_replaced = indexs_masked[:anchor_mask_replace]\n",
    "                    for index_replaced in indexs_replaced:\n",
    "                        sequence[index_replaced] = self._get_random_tokens()\n",
    "                    # end\n",
    "                # end\n",
    "\n",
    "                indexs_masked = indexs_masked[anchor_mask_replace:anchor_mask_all]\n",
    "            # end\n",
    "\n",
    "\n",
    "            count_pad = size_seq_max - len_seq\n",
    "            \n",
    "            label = torch.LongTensor(label)\n",
    "            label_padded = torch.cat((label, torch.LongTensor([id_pad] * count_pad)))\n",
    "            labels_padded.append(label_padded)\n",
    "\n",
    "            if need_masked:\n",
    "\n",
    "                sequence_masked = torch.LongTensor(sequence)\n",
    "                sequence_masked.index_fill_(0, torch.LongTensor(indexs_masked), id_mask)\n",
    "                sequence_masked_padded = torch.cat((sequence_masked, torch.LongTensor([id_pad] * count_pad)))\n",
    "\n",
    "                sequences_masked_padded.append(sequence_masked_padded)\n",
    "            # end\n",
    "        #   # end for\n",
    "\n",
    "        inputs = torch.stack(labels_padded)  # (batch, size_seq_max)\n",
    "        if need_masked:\n",
    "            inputs_masked_padded = torch.stack(sequences_masked_padded)\n",
    "        # end\n",
    "\n",
    "        masks_segment = (inputs != self.id_pad).unsqueeze(-2)  # (nbatch, 1, seq)\n",
    "        masks_attention = self.make_std_mask(inputs, self.id_pad) if need_diagonal else masks_segment\n",
    "\n",
    "        if need_masked:\n",
    "            masks_masked = (inputs_masked_padded != id_mask).unsqueeze(-2)\n",
    "            masks_attention = masks_attention & masks_masked\n",
    "            return inputs_masked_padded, masks_attention, masks_segment, inputs  # (inputs, masks_attention, masks_segment, labels)\n",
    "        else:\n",
    "            return inputs, masks_attention, masks_segment, None\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "\n",
    "    def subsequent_mask(self, size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "            torch.uint8\n",
    "        )\n",
    "        return subsequent_mask == 0\n",
    "\n",
    "    def make_std_mask(self, tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & self.subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class Collator_BERT_Encoded_254(Collator_Base):\n",
    "\n",
    "    def __call__(self, list_tokenized_merged):\n",
    "\n",
    "        if type(list_tokenized_merged[0]) == tuple:\n",
    "            return None\n",
    "        # end\n",
    "        \n",
    "        len_tokenized_accumulated = 2  # add cls and sep\n",
    "        list_tokenized_merged = [tokenized_merged[:self.size_seq_max - len_tokenized_accumulated] for tokenized_merged in list_tokenized_merged]\n",
    "\n",
    "        # Process III. Add begin and stop special token, same as jinyuj_transformers_quora.ipynb\n",
    "        tokens_input_encoder = []\n",
    "        tokens_input_decoder = []\n",
    "        tokens_label_decoder = []\n",
    "\n",
    "        for tokenized_merged in list_tokenized_merged:\n",
    "            tokens_input_encoder.append([self.id_cls] + tokenized_merged + [self.id_sep])\n",
    "            tokens_input_decoder.append([self.id_cls] + tokenized_merged)\n",
    "            tokens_label_decoder.append(tokenized_merged + [self.id_sep])\n",
    "        # end\n",
    "\n",
    "        inputs_encoder, masks_encoder, segments_encoder, labels_encoder = self.pad_sequences(tokens_input_encoder,\n",
    "                                                                                             self.size_seq_max,\n",
    "                                                                                             need_masked=self.need_masked)\n",
    "        inputs_decoder, masks_decoder, segments_decoder, _ = self.pad_sequences(tokens_input_decoder, self.size_seq_max,\n",
    "                                                                                need_diagonal=True)\n",
    "        labels_decoder, masks_label, segments_label, _ = self.pad_sequences(tokens_label_decoder, self.size_seq_max)\n",
    "\n",
    "        return Batch(\n",
    "            ids_encoder=inputs_encoder,  # contains [mask]s\n",
    "            masks_encoder=masks_encoder,\n",
    "            # input_ids=inputs_encoder,\n",
    "            # attention_mask=masks_encoder,\n",
    "            labels_encoder=labels_encoder,  # doesn't contain [mask]\n",
    "            segments_encoder=segments_encoder,\n",
    "            ids_decoder=inputs_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            labels_decoder=labels_decoder,\n",
    "            segments_label=segments_label\n",
    "        )\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class SimpleEncodedStepedFastFowardDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # info_file_rows = {'path_file': 1,000,000,...}\n",
    "    def __init__(self, folder_dataset_base, info_file_rows, split=0.001):\n",
    "        self.folder_dataset_base = folder_dataset_base\n",
    "        self.list_tokenized_eval = []\n",
    "        self.dict_filename_loaded = {filename: False for filename, num_rows in info_file_rows.items()}\n",
    "        self.list_corpus_idx_filename_train = []\n",
    "\n",
    "        for filename, num_lines in info_file_rows.items():\n",
    "            idxs_eval = list(range(num_lines))\n",
    "            random.shuffle(idxs_eval)\n",
    "            idxs_eval = idxs_eval[:round(len(idxs_eval) * split)]\n",
    "\n",
    "            for idx_eval in idxs_eval:\n",
    "                self.list_tokenized_eval.append((idx_eval, filename))\n",
    "            # end\n",
    "\n",
    "            set_idxs_eval = set(idxs_eval)\n",
    "            for idx_train in range(num_lines):\n",
    "                if idx_train in set_idxs_eval:\n",
    "                    continue\n",
    "                # end\n",
    "\n",
    "                self.list_corpus_idx_filename_train.append((idx_train, filename))\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        self.is_train = True\n",
    "        self.rows_cached = []\n",
    "        self.filename_cached = None\n",
    "        self.idx_restored = -1\n",
    "        self.idx_current = -1\n",
    "    # end\n",
    "\n",
    "    def restore(self, idx_restored=-1):\n",
    "        self.idx_restored = idx_restored\n",
    "    # end\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):  # should not have problem now\n",
    "\n",
    "        # if eval, use all cached eval tokenized\n",
    "        if not self.is_train:\n",
    "            return self.list_tokenized_eval[idx]\n",
    "        # end\n",
    "\n",
    "        if idx < self.idx_restored:\n",
    "            return (None, None) # same to eval for collator to skip\n",
    "        # end\n",
    "\n",
    "        self.idx_current = idx\n",
    "\n",
    "        # if train\n",
    "        idx_in_file, filename_current = self.list_corpus_idx_filename_train[idx]\n",
    "\n",
    "        # if file not fully used\n",
    "        if filename_current != self.filename_cached:\n",
    "\n",
    "            # load new file\n",
    "            print('switch from {} to {}'.format(self.filename_cached, filename_current))\n",
    "            path_file = os.path.join(self.folder_dataset_base, filename_current)\n",
    "            with open(path_file, 'r') as file:  # update rows_cached\n",
    "                self.rows_cached = file.read().splitlines()\n",
    "            # end\n",
    "\n",
    "            self.filename_cached = filename_current\n",
    "\n",
    "            if not self.dict_filename_loaded[filename_current]:\n",
    "                for id_list_eval, tokenized_eval in enumerate(self.list_tokenized_eval):\n",
    "                    if type(tokenized_eval) is tuple:\n",
    "                        if tokenized_eval[1] == filename_current:\n",
    "                            self.list_tokenized_eval[id_list_eval] = self._fransfer_one_line_to_tokenized(self.rows_cached[tokenized_eval[0]])\n",
    "                        # end\n",
    "                    # end\n",
    "                # end\n",
    "                self.dict_filename_loaded[filename_current] = True\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        return self._fransfer_one_line_to_tokenized(self.rows_cached[idx_in_file])\n",
    "    # end\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.list_corpus_idx_filename_train)\n",
    "        else:\n",
    "            return len(self.list_tokenized_eval)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def _fransfer_one_line_to_tokenized(self, str_line):\n",
    "        tokenized = [int(t) for t in str_line.split(', ') if t]\n",
    "        return tokenized\n",
    "    # end\n",
    "\n",
    "    def train(self):\n",
    "        self.is_train = True\n",
    "    # end\n",
    "\n",
    "    def eval(self):\n",
    "        self.is_train = False\n",
    "        self.idx_restored = -1\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "def BookCorpus2000(split=0.1):\n",
    "    filename = 'bookcorpus_2000.json'\n",
    "    \n",
    "    with open(filename, 'r') as file:\n",
    "        list_corpus = json.load(file)\n",
    "    # end\n",
    "    \n",
    "    indexs_all = list(range(len(list_corpus)))\n",
    "    random.shuffle(indexs_all)\n",
    "    \n",
    "    index_split = int(split * len(list_corpus))\n",
    "    \n",
    "    indexs_eval = indexs_all[:index_split]\n",
    "    indexs_train = indexs_all[index_split:]\n",
    "    \n",
    "    list_corpus_eval = [list_corpus[i_e] for i_e in indexs_eval]\n",
    "    list_corpus_train = [list_corpus[i_t] for i_t in indexs_train]\n",
    "    \n",
    "    return list_corpus_train, list_corpus_eval, None\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "def BookCorpus(split=0.0001, used=-1):\n",
    "    import datasets\n",
    "    \n",
    "    list_corpus = datasets.load_dataset('bookcorpus')['train']['text'][:used]   # 70,000,000, 70 Million\n",
    "    \n",
    "    indexs_all = list(range(len(list_corpus)))\n",
    "    random.shuffle(indexs_all)\n",
    "    \n",
    "    index_split = int(split * len(list_corpus))\n",
    "    \n",
    "    indexs_eval = indexs_all[:index_split]\n",
    "    indexs_train = indexs_all[index_split:]\n",
    "    \n",
    "    list_corpus_eval = [list_corpus[i_e] for i_e in indexs_eval]\n",
    "    list_corpus_train = [list_corpus[i_t] for i_t in indexs_train]\n",
    "    \n",
    "    return list_corpus_train, list_corpus_eval, None\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a0ffcd-779a-4086-b4d7-83c839c1a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertEncoderHead_MLM(nn.Module):\n",
    "\n",
    "    @classmethod\n",
    "    def get_info_accuracy_template(cls):\n",
    "        return Dotdict({\n",
    "            'corrects_segmented': 0,\n",
    "            'corrects_masked': 0,\n",
    "            'num_segmented': 0,\n",
    "            'num_masked': 0 \n",
    "        })\n",
    "    # end\n",
    "    \n",
    "    def __init__(self, model, size_vocab, dim_hidden=128, dropout=0.1):\n",
    "        super(DistilBertEncoderHead_MLM, self).__init__()\n",
    "        \n",
    "        self.ffn = LinearAndNorm(dim_in=dim_hidden, dim_out=dim_hidden, dropout=dropout)\n",
    "        self.extractor = torch.nn.Linear(dim_hidden, size_vocab, bias=False)\n",
    "        # self.extractor.weight = nn.Parameter(model.embedder_encoder.embedder[0].lut.weight)\n",
    "        # self.extractor.weight = nn.Parameter(model.embeddings.word_embeddings.lut.weight)\n",
    "        self.extractor.weight = nn.Parameter(model.embedder_encoder.word_embeddings.weight)\n",
    "\n",
    "        self.func_loss = torch.nn.CrossEntropyLoss().cuda()\n",
    "    # end\n",
    "\n",
    "\n",
    "    def forward(self, output_encoder=None, labels_encoder=None, segments_encoder=None, masks_encoder=None, **kwargs):   # labels_input -> (batch, seq, labels)\n",
    "        output_ffn = self.ffn(output_encoder)\n",
    "        output_mlm = self.extractor(output_ffn) # output_mlm = prediction_logits\n",
    "        \n",
    "        return {'output': output_mlm, 'labels_encoder': labels_encoder, 'segments_encoder': segments_encoder, 'masks_encoder': masks_encoder}\n",
    "\n",
    "\n",
    "    \n",
    "    def compute_loss(self, output=None, labels_encoder=None, segments_encoder=None, masks_encoder=None):\n",
    "        \n",
    "        output_mlm = output\n",
    "        labels_mlm = labels_encoder\n",
    "        \n",
    "        info_acc = DistilBertEncoderHead_MLM.get_info_accuracy_template()\n",
    "        \n",
    "        segments_encoder_2d = segments_encoder.transpose(-1,-2)[:,:,0]\n",
    "        hidden_mlm_segmented = output_mlm.masked_select(segments_encoder_2d.unsqueeze(-1)).reshape(-1, output_mlm.shape[-1]) # should be (segmented_all_batchs, size_vocab)\n",
    "        \n",
    "        loss_segments = self.func_loss(hidden_mlm_segmented, labels_mlm.masked_select(segments_encoder_2d))\n",
    "        info_acc.corrects_segmented = torch.sum(hidden_mlm_segmented.argmax(-1) == labels_mlm.masked_select(segments_encoder_2d)).cpu().item()\n",
    "        info_acc.num_segmented = hidden_mlm_segmented.shape[0]\n",
    "        \n",
    "        masks_masked = torch.logical_xor(masks_encoder, segments_encoder) & segments_encoder # True is masked\n",
    "        masks_masked_perbatch = masks_masked[:,0,:]\n",
    "        hidden_mlm_masked = output_mlm.masked_select(masks_masked_perbatch.unsqueeze(-1)).reshape(-1, output_mlm.shape[-1])\n",
    "\n",
    "        if hidden_mlm_masked.shape[0] != 0:\n",
    "            loss_masked = self.func_loss(hidden_mlm_masked, labels_mlm.masked_select(masks_masked_perbatch))       \n",
    "            info_acc.corrects_masked = torch.sum(hidden_mlm_masked.argmax(-1) == labels_mlm.masked_select(masks_masked_perbatch)).cpu().item()\n",
    "            info_acc.num_masked = hidden_mlm_masked.shape[0]\n",
    "        else:\n",
    "            loss_masked = 0\n",
    "            info_acc.corrects_masked = 0\n",
    "            info_acc.num_masked = 1\n",
    "        # end\n",
    "        \n",
    "        loss_mlm = loss_segments + loss_masked * 3\n",
    "        \n",
    "        return loss_mlm, info_acc\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleDecoderHead_S2S(nn.Module):\n",
    "\n",
    "    @classmethod\n",
    "    def get_info_accuracy_template(cls):\n",
    "        return Dotdict({\n",
    "            'corrects_segmented': 0,\n",
    "            'num_segmented': 0 \n",
    "        })\n",
    "    # end\n",
    "\n",
    "\n",
    "    def __init__(self, model, size_vocab, dim_hidden=128, dropout=0.1):\n",
    "        super(SimpleDecoderHead_S2S, self).__init__()\n",
    "        \n",
    "        self.ffn = LinearAndNorm(dim_in=dim_hidden, dim_out=dim_hidden, dropout=dropout)\n",
    "        self.extractor = torch.nn.Linear(dim_hidden, size_vocab, bias=False)\n",
    "        self.extractor.weight = nn.Parameter(model.embedder_decoder.embedder[0].lut.weight)\n",
    "\n",
    "        self.func_loss = torch.nn.CrossEntropyLoss().cuda()\n",
    "    # end\n",
    "\n",
    "\n",
    "    def forward(self, output_decoder=None, labels_decoder=None, segments_label=None, **kwargs):   # labels_input -> (batch, seq, labels)\n",
    "        \n",
    "        output_ffn = self.ffn(output_decoder)\n",
    "        output_s2s = self.extractor(output_ffn)   # output_mlm = prediction_logits\n",
    "        \n",
    "        return {'output': output_s2s, 'labels_decoder': labels_decoder, 'segments_label': segments_label}\n",
    "    # end\n",
    "\n",
    "\n",
    "    def compute_loss(self, output=None, labels_decoder=None, segments_label=None):\n",
    "        output_s2s = output\n",
    "        labels_s2s = labels_decoder\n",
    "        \n",
    "        info_acc = SimpleDecoderHead_S2S.get_info_accuracy_template()\n",
    "        \n",
    "        segments_label_2d = segments_label.transpose(-1,-2)[:,:,0]\n",
    "        hidden_s2s_segmented = output_s2s.masked_select(segments_label_2d.unsqueeze(-1)).reshape(-1, output_s2s.shape[-1])\n",
    "\n",
    "        loss_segments = self.func_loss(hidden_s2s_segmented, labels_s2s.masked_select(segments_label_2d))\n",
    "        info_acc.corrects_segmented = torch.sum(hidden_s2s_segmented.argmax(-1) == labels_s2s.masked_select(segments_label_2d)).cpu().item()\n",
    "        info_acc.num_segmented = hidden_s2s_segmented.shape[0]\n",
    "        \n",
    "        return loss_segments * 4, info_acc\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef9983b-6c31-40ca-b89e-13221ca3966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.index_name_head = set()\n",
    "        self.model = model\n",
    "    # end\n",
    "\n",
    "    def register(self, head):\n",
    "        name_head = head.__class__.__name__\n",
    "        setattr(self, name_head, head)\n",
    "        self.index_name_head.add(name_head)\n",
    "        return self\n",
    "    # end\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output_model = self.model(**kwargs)\n",
    "        dict_head_output = {}\n",
    "        \n",
    "        for name in self.index_name_head:\n",
    "            head = getattr(self, name)\n",
    "            dict_head_output[name] = head.forward(**{**output_model, **kwargs})\n",
    "        # end\n",
    "        \n",
    "        return dict_head_output\n",
    "    # end\n",
    "\n",
    "    def get_head(self, name_klass):\n",
    "        if type(name_klass) is type:\n",
    "            name_klass = klass.__name__\n",
    "        # end\n",
    "        \n",
    "        return getattr(self, name_klass)\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c70ebb0-fb6d-44fb-bedd-d7f6f1d21eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaverAndLoader:\n",
    "    def __init__(self, path_checkpoints='./checkpoints'):\n",
    "        self.dict_name_item = {}\n",
    "        self.path_checkpoints = path_checkpoints\n",
    "        self.metadata = None\n",
    "    # end\n",
    "    \n",
    "    def add_item(self, item, name=None):\n",
    "        if not name:\n",
    "            name = item.__class__.__name__\n",
    "        # end\n",
    "        \n",
    "        self.dict_name_item[name] = item\n",
    "        return self\n",
    "    # end\n",
    "    \n",
    "    \n",
    "    def update_checkpoint(self, name_checkpoint, name_checkpoint_previous=None, metadata=None):  # epoch_n\n",
    "        if not self.dict_name_item:\n",
    "            print(f'[ALERT] no item added, skip saving checkpoint.')\n",
    "            return\n",
    "        # end\n",
    "        \n",
    "        if name_checkpoint_previous:\n",
    "            result = self._delete_checkpoint_folder(name_checkpoint_previous)\n",
    "            if result:\n",
    "                print(f'[INFO] {name_checkpoint_previous} is cleared.')\n",
    "            else:\n",
    "                print(f'[ALERT] {name_checkpoint_previous} fail to be cleared.')\n",
    "            # end\n",
    "        # end\n",
    "        \n",
    "        folder_checkpoint = self._create_checkpoint_folder(name_checkpoint)\n",
    "        for name_item, item in self.dict_name_item.items():\n",
    "            path_checkpoint_item = os.path.join(folder_checkpoint, f'{name_item}.pt')\n",
    "            \n",
    "            if hasattr(item, 'save_pretrained'):\n",
    "                item.save_pretrained(path_checkpoint_item)\n",
    "            elif hasattr(item, 'save'):\n",
    "                item.save(path_checkpoint_item)\n",
    "            else:\n",
    "                torch.save(item.state_dict(), path_checkpoint_item)\n",
    "            # end\n",
    "            \n",
    "            size_file_saved_MB = os.path.getsize(path_checkpoint_item) / 1024 / 1024\n",
    "            print(f'[INFO] {name_item} is saved, {size_file_saved_MB} MB')\n",
    "        # end\n",
    "        \n",
    "        if metadata:\n",
    "            path_file_metadata = os.path.join(folder_checkpoint, 'metadata.json')\n",
    "            with open(path_file_metadata,'w+') as file:\n",
    "                file.write(json.dumps(metadata, indent=4))\n",
    "            # end\n",
    "            print(f'[INFO] metadata updated at {path_file_metadata}, : {metadata}')\n",
    "            self.metadata = metadata\n",
    "        # end\n",
    "        \n",
    "        print(f'[INFO] {name_checkpoint} is saved')\n",
    "    # end\n",
    "    \n",
    "    \n",
    "    def load_item_special(self, name_checkpoint, klass_item, name_item_default):\n",
    "        name_item = klass_item.__name__\n",
    "        path_checkpoint_item = os.path.join(self.path_checkpoints, name_checkpoint, f'{name_item}.pt')\n",
    "        \n",
    "        if os.path.exists(path_checkpoint_item):\n",
    "            target = path_checkpoint_item\n",
    "        else:\n",
    "            target = name_item_default\n",
    "        # end\n",
    "            \n",
    "        if hasattr(klass_item, 'from_pretrained'):\n",
    "            instance_item = klass_item.from_pretrained(target)\n",
    "        else:\n",
    "            instance_item = klass_item(target)\n",
    "        # end\n",
    "\n",
    "        print(f'[INFO] {klass_item} loaded for {target}')\n",
    "        \n",
    "        return instance_item\n",
    "    # end\n",
    "\n",
    "    \n",
    "    def load_item_state(self, name_checkpoint, instance_item, name_item=None):\n",
    "        if not name_item:\n",
    "            name_item = instance_item.__class__.__name__\n",
    "        # end\n",
    "        \n",
    "        path_checkpoint_item = os.path.join(self.path_checkpoints, name_checkpoint, f'{name_item}.pt')\n",
    "        if not os.path.exists(path_checkpoint_item):\n",
    "            print(f'[ERROR] {path_checkpoint_item} not exists')\n",
    "            return None\n",
    "        # end\n",
    "        \n",
    "        if issubclass(instance_item.__class__, torch.nn.Module):\n",
    "            instance_item.load_state_dict(torch.load(path_checkpoint_item), strict=False)\n",
    "        else:\n",
    "            instance_item.load_state_dict(torch.load(path_checkpoint_item))\n",
    "        # end\n",
    "        \n",
    "        print(f'[INFO] {name_item} loaded for {name_checkpoint}.')\n",
    "        return instance_item\n",
    "    # end\n",
    "    \n",
    "    def load_metadata(self, name_checkpoint):\n",
    "        path_folder_checkpoint = os.path.join(self.path_checkpoints, name_checkpoint)\n",
    "        path_metadata = os.path.join(path_folder_checkpoint, 'metadata.json')\n",
    "        \n",
    "        if os.path.exists(path_metadata):\n",
    "            with open(path_metadata, 'r') as file:\n",
    "                self.metadata = json.load(file)\n",
    "            # end\n",
    "            print(f'[INFO] {path_metadata} loaded: {self.metadata}')\n",
    "        else:\n",
    "            print(f'[WARN] no metadata found.')\n",
    "        # end\n",
    "    # end\n",
    "    \n",
    "    \n",
    "    def list_items(self):\n",
    "        return list(self.dict_name_item.keys())\n",
    "    # end\n",
    "    \n",
    "    def _create_checkpoint_folder(self, name_checkpoint):\n",
    "        path_folder_target = os.path.join(self.path_checkpoints, name_checkpoint)\n",
    "        Path(path_folder_target).mkdir(parents=True, exist_ok=True)\n",
    "        return path_folder_target\n",
    "    # end\n",
    "    \n",
    "    def _delete_checkpoint_folder(self, name_checkpoint_previous):\n",
    "        path_folder_target = os.path.join(self.path_checkpoints, name_checkpoint_previous)\n",
    "        if os.path.exists(path_folder_target):\n",
    "            shutil.rmtree(path_folder_target, ignore_errors=True)\n",
    "        # end\n",
    "        return (not os.path.exists(path_folder_target))\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Builder:\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def build_model_with_2heads(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "        \n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder, pooling=True)\n",
    "        head_s2s = SimpleDecoderHead_S2S(model, size_vocab, dim_hidden)\n",
    "        head_mlm = SimpleEncoderHead_MLM(model, size_vocab, dim_hidden)\n",
    "\n",
    "        trainer = Trainer(model).register(head_mlm).register(head_s2s)\n",
    "        return trainer\n",
    "    # end\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def build_model_with_s2s_from_pretrained_encoder(cls, klass_encoder, name_encoder):\n",
    "        \n",
    "        encoder_base = klass_encoder.from_pretrained(name_encoder)\n",
    "        \n",
    "        size_vocab = encoder_base.embeddings.word_embeddings.num_embeddings\n",
    "        dim_hidden = encoder_base.embeddings.word_embeddings.embedding_dim\n",
    "        n_head = int(dim_hidden / 64)\n",
    "        dim_feedforward = encoder_base.transformer.layer[0].ffn.lin1.out_features\n",
    "        n_layer = len(encoder_base.transformer.layer)\n",
    "        \n",
    "        embedder_encoder = encoder_base.embeddings\n",
    "        encoderstack = encoder_base\n",
    "        \n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder, pooling=True)\n",
    "        head_s2s = SimpleDecoderHead_S2S(model, size_vocab, dim_hidden)\n",
    "        trainer = Trainer(model).register(head_s2s)\n",
    "        return trainer\n",
    "    # end\n",
    "    \n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def load_model_with_s2s_from_distilbert(cls, klass_encoder, name_encoder, loader, name_checkpoint):\n",
    "        \n",
    "        encoder_base = loader.load_item_special(name_checkpoint, klass_encoder, name_encoder)\n",
    "        \n",
    "        size_vocab = encoder_base.embeddings.word_embeddings.num_embeddings\n",
    "        dim_hidden = encoder_base.embeddings.word_embeddings.embedding_dim\n",
    "        n_head = int(dim_hidden / 64)\n",
    "        dim_feedforward = encoder_base.transformer.layer[0].ffn.lin1.out_features\n",
    "        n_layer = len(encoder_base.transformer.layer)\n",
    "        \n",
    "        embedder_encoder = encoder_base.embeddings\n",
    "        encoderstack = encoder_base\n",
    "        \n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder, pooling=True)\n",
    "        head_s2s = SimpleDecoderHead_S2S(model, size_vocab, dim_hidden)\n",
    "        \n",
    "        for p in list(embedder_decoder.parameters()) + list(decoderstack.parameters()) + list(head_s2s.parameters()):\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            # end\n",
    "        # end\n",
    "        \n",
    "        loader.load_item_state(name_checkpoint, embedder_decoder)\n",
    "        loader.load_item_state(name_checkpoint, decoderstack)\n",
    "        loader.load_item_state(name_checkpoint, head_s2s)\n",
    "        loader.load_metadata(name_checkpoint)        \n",
    "\n",
    "        \n",
    "        loader.add_item(encoder_base)\n",
    "        loader.add_item(embedder_decoder)\n",
    "        loader.add_item(decoderstack)\n",
    "        loader.add_item(head_s2s)\n",
    "\n",
    "        trainer = Trainer(model).register(head_s2s)\n",
    "        return trainer\n",
    "    # end\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model_with_s2s(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer, loader, name_checkpoint):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "        \n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder, pooling=True)\n",
    "        head_s2s = SimpleDecoderHead_S2S(model, size_vocab, dim_hidden)\n",
    "        \n",
    "        loader.add_item(model)\n",
    "        loader.add_item(head_s2s)\n",
    "        \n",
    "        trainer = Trainer(model).register(head_s2s)\n",
    "        \n",
    "        loader.load_item_state(name_checkpoint, model)\n",
    "        loader.load_item_state(name_checkpoint, head_s2s)\n",
    "        loader.load_item_state(name_checkpoint, head_mlm)\n",
    "        loader.load_metadata(name_checkpoint)\n",
    "\n",
    "        return trainer\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e4f08e-6011-4e8a-bb6f-1c3f017cdd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] <class 'transformers.models.distilbert.modeling_distilbert.DistilBertModel'> loaded for checkpoints_distilbert_wikieng_0/epoch_4/DistilBertModel.pt\n",
      "[INFO] SimpleEmbedder loaded for epoch_4.\n",
      "[INFO] SimpleTransformerStack loaded for epoch_4.\n",
      "[INFO] SimpleDecoderHead_S2S loaded for epoch_4.\n",
      "[INFO] checkpoints_distilbert_wikieng_0/epoch_4/metadata.json loaded: {'idx_restored': 17280011}\n",
      "[INFO] AdamW loaded for epoch_4.\n",
      "[INFO] LambdaLR loaded for epoch_4.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "GPUS = [0]\n",
    "torch.cuda.set_device(GPUS[0])\n",
    "\n",
    "seq_max = 256\n",
    "batch_size = 12\n",
    "\n",
    "steps_per_save = 80000\n",
    "epoch_last = 4\n",
    "epochs = 3\n",
    "name_epoch_last = f'epoch_{epoch_last}'\n",
    "\n",
    "# model\n",
    "loader = SaverAndLoader('checkpoints_distilbert_wikieng_0')\n",
    "\n",
    "trainer = Builder.load_model_with_s2s_from_distilbert(DistilBertModel, 'distilbert-base-uncased', loader, name_epoch_last)\n",
    "trainer = trainer.to('cuda')\n",
    "trainer = torch.nn.DataParallel(trainer, device_ids=GPUS)\n",
    "\n",
    "# dataset\n",
    "# folder_dataset = 'bookcorpus_merged_254_10k'\n",
    "# folder_dataset = 'bookcorpus_encoded'\n",
    "# folder_dataset = 'bookcorpus_merged_254_20'\n",
    "# folder_dataset = 'wikieng_merged_254_1k'\n",
    "folder_dataset = 'wikipassage_merged_254_1m'\n",
    "filenames_dataset = sorted([f for f in os.listdir(folder_dataset) if f[0] != '.'], key=lambda name: int(name.split('.')[0]))\n",
    "# list_size_per_file = [1000000, 1000000, 1000000, 1000000, 237940]\n",
    "list_size_per_file = [\n",
    "    1000000,1000000,1000000,1000000,1000000,\n",
    "    1000000,1000000,1000000,1000000,1000000,\n",
    "    1000000,1000000,1000000,1000000,1000000,\n",
    "    1000000,1000000,1000000,398130\n",
    "]\n",
    "# list_size_per_file = [1000, 1000, 234]\n",
    "# list_size_per_file = [20, 20, 13]\n",
    "info_filename_rows = {k:v for k,v in zip(filenames_dataset, list_size_per_file)}\n",
    "\n",
    "source = SimpleEncodedStepedFastFowardDataset(folder_dataset, info_filename_rows)\n",
    "if loader.metadata:\n",
    "    source.restore(**loader.metadata)\n",
    "# end\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\") \n",
    "collator = Collator_BERT_Encoded_254(tokenizer, seq_max)\n",
    "\n",
    "dataloader_train = DataLoader(source, batch_size*len(GPUS), shuffle=False, collate_fn=collator)\n",
    "dataloader_eval = DataLoader(source, batch_size*len(GPUS), shuffle=False, collate_fn=collator)\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainer.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "lr_scheduler = transformers.get_scheduler(\n",
    "    name=\"cosine_with_restarts\", optimizer=optimizer, num_warmup_steps=100000, num_training_steps=len(dataloader_train) * 10\n",
    ")\n",
    "\n",
    "loader.load_item_state(name_epoch_last, optimizer)\n",
    "loader.load_item_state(name_epoch_last, lr_scheduler)\n",
    "\n",
    "loader.add_item(optimizer)\n",
    "loader.add_item(lr_scheduler)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9bfd2e3-736f-4b44-8948-69e9317b80ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 1437609/1531645 [00:21<00:01, 50598.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switch from None to 17.encode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 1498500/1531645 [5:10:49<2:52:21,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switch from 17.encode to 18.encode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 1520000/1531645 [7:05:14<1:00:51,  3.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] start to save model at epoch: epoch_5, step: 1520000, idx: 18240011\n",
      "[INFO] epoch_4 is cleared.\n",
      "[INFO] DistilBertModel is saved, 6.103515625e-05 MB\n",
      "[INFO] SimpleEmbedder is saved, 90.92114543914795 MB\n",
      "[INFO] SimpleTransformerStack is saved, 216.4018907546997 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] AdamW is saved, 1301.3601961135864 MB\n",
      "[INFO] LambdaLR is saved, 0.0005350112915039062 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 1520001/1531645 [7:05:56<42:00:20, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] metadata updated at checkpoints_distilbert_wikieng_0/epoch_5/metadata.json, : {'idx_restored': 18240011}\n",
      "[INFO] epoch_5 is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1531645/1531645 [8:07:53<00:00, 52.32it/s]   \n",
      " 92%|| 1418/1534 [00:00<00:00, 9863.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-14 15:27:23.850576] Epoch: 5 training ends. Status: Average loss: 7.734639041798505, Average s2s accuracy: 0.6213061702890835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1534/1534 [00:14<00:00, 105.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-14 15:27:38.396766] Epoch: 5 Evalutation ends. Status: Average loss: 6.708889162438547, Average S2S accuracy: 0.6697367398603744\n",
      "[INFO] epoch_5 is cleared.\n",
      "[INFO] DistilBertModel is saved, 6.103515625e-05 MB\n",
      "[INFO] SimpleEmbedder is saved, 90.92114543914795 MB\n",
      "[INFO] SimpleTransformerStack is saved, 216.4018907546997 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1531645 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] AdamW is saved, 1301.3601961135864 MB\n",
      "[INFO] LambdaLR is saved, 0.0005350112915039062 MB\n",
      "[INFO] epoch_5 is saved\n",
      "switch from 18.encode to 0.encode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 80000/1531645 [7:13:19<131:51:52,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] start to save model at epoch: epoch_6, step: 80000, idx: 960011\n",
      "[INFO] epoch_5 is cleared.\n",
      "[INFO] DistilBertModel is saved, 6.103515625e-05 MB\n",
      "[INFO] SimpleEmbedder is saved, 90.92114543914795 MB\n",
      "[INFO] SimpleTransformerStack is saved, 216.4018907546997 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] AdamW is saved, 1301.3601961135864 MB\n",
      "[INFO] LambdaLR is saved, 0.0005350112915039062 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 80001/1531645 [7:14:12<6493:24:21, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] metadata updated at checkpoints_distilbert_wikieng_0/epoch_6/metadata.json, : {'idx_restored': 960011}\n",
      "[INFO] epoch_6 is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 83250/1531645 [7:31:46<126:12:19,  3.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switch from 0.encode to 1.encode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 160000/1531645 [14:27:53<124:28:50,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] start to save model at epoch: epoch_6, step: 160000, idx: 1920011\n",
      "[INFO] epoch_6 is cleared.\n",
      "[INFO] DistilBertModel is saved, 6.103515625e-05 MB\n",
      "[INFO] SimpleEmbedder is saved, 90.92114543914795 MB\n",
      "[INFO] SimpleTransformerStack is saved, 216.4018907546997 MB\n",
      "[INFO] SimpleDecoderHead_S2S is saved, 91.68078327178955 MB\n",
      "[INFO] AdamW is saved, 1301.3601961135864 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 160001/1531645 [14:28:48<6363:09:23, 16.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] LambdaLR is saved, 0.0005350112915039062 MB\n",
      "[INFO] metadata updated at checkpoints_distilbert_wikieng_0/epoch_6/metadata.json, : {'idx_restored': 1920011}\n",
      "[INFO] epoch_6 is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 166500/1531645 [15:03:55<123:51:07,  3.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "switch from 1.encode to 2.encode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 214214/1531645 [19:21:51<119:27:20,  3.06it/s]"
     ]
    }
   ],
   "source": [
    "def train_a_batch(batch, trainer, optimizer=None, scheduler=None):\n",
    "    if batch is None:\n",
    "        return None, None\n",
    "    # end\n",
    "    \n",
    "    dict_head_output = trainer.forward(**batch())\n",
    "    \n",
    "    loss_s2s, info_acc_s2s = trainer.module.get_head('SimpleDecoderHead_S2S').compute_loss(**dict_head_output['SimpleDecoderHead_S2S'])\n",
    "    \n",
    "    # crossentropy loss\n",
    "    loss_all = loss_s2s\n",
    "    loss_all_value = loss_all.item()\n",
    "    \n",
    "    \n",
    "    loss_all.backward()\n",
    "    \n",
    "    if optimizer:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    # end\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    # end\n",
    "    \n",
    "    return loss_all_value, Dotdict({'s2s': info_acc_s2s})\n",
    "# end\n",
    "\n",
    "\n",
    "def evaluate_a_batch(batch, trainer, *args, **kwargs):\n",
    "    if batch is None:\n",
    "        return None, None\n",
    "    # end\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dict_head_output = trainer.forward(**batch())\n",
    "    # end\n",
    "    \n",
    "    loss_s2s, info_acc_s2s = trainer.module.get_head('SimpleDecoderHead_S2S').compute_loss(**dict_head_output['SimpleDecoderHead_S2S'])\n",
    "    \n",
    "\n",
    "    # crossentropy loss\n",
    "    loss_all = loss_s2s\n",
    "    loss_all_value = loss_all.item()\n",
    "    \n",
    "    return loss_all_value, Dotdict({'s2s': info_acc_s2s})\n",
    "# end\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "name_checkpoint_current = None\n",
    "name_checkpoint_last = name_epoch_last\n",
    "\n",
    "for e in range(epoch_last+1, epoch_last+1+epochs):\n",
    "\n",
    "    name_checkpoint_current = f'epoch_{e}'\n",
    "    \n",
    "    info_acc_heads_train = Dotdict({\n",
    "        's2s': SimpleDecoderHead_S2S.get_info_accuracy_template()\n",
    "    })\n",
    "\n",
    "    # train phase\n",
    "    trainer.train()\n",
    "    source.train()\n",
    "    \n",
    "    losss_per_e = []\n",
    "    for i, batch in enumerate(tqdm(dataloader_train)):\n",
    "        loss_current, info_acc_heads_batch = train_a_batch(batch, trainer, optimizer, lr_scheduler)\n",
    "        if loss_current is None:\n",
    "            continue\n",
    "        # end\n",
    "        info_acc_heads_train += info_acc_heads_batch\n",
    "        losss_per_e.append(loss_current)\n",
    "        \n",
    "        if i >= steps_per_save and i % steps_per_save == 0:\n",
    "            print(f'[INFO] start to save model at epoch: epoch_{e}, step: {i}, idx: {source.idx_current}')\n",
    "            loader.update_checkpoint(name_checkpoint_current, name_checkpoint_last, metadata={'idx_restored': source.idx_current})\n",
    "            name_checkpoint_last = name_checkpoint_current\n",
    "        # end\n",
    "    # end\n",
    "    \n",
    "    loss_average_per_e = sum(losss_per_e) / len(losss_per_e)\n",
    "    print('[{}] Epoch: {} training ends. Status: Average loss: {}, Average s2s accuracy: {}'.format(\n",
    "        datetime.utcnow(), e, loss_average_per_e,\n",
    "        info_acc_heads_train.s2s.corrects_segmented / info_acc_heads_train.s2s.num_segmented,\n",
    "    ))\n",
    "    \n",
    "    # eval phase\n",
    "    info_acc_heads_eval = Dotdict({\n",
    "        's2s': SimpleDecoderHead_S2S.get_info_accuracy_template()\n",
    "    })\n",
    "    \n",
    "    trainer.eval()\n",
    "    source.eval()\n",
    "    losss_per_e = []\n",
    "    for i, batch in enumerate(tqdm(dataloader_eval)):\n",
    "        loss_current, info_acc_heads_batch = evaluate_a_batch(batch, trainer)\n",
    "        if loss_current is None:\n",
    "            continue\n",
    "        # end\n",
    "        info_acc_heads_eval += info_acc_heads_batch\n",
    "        \n",
    "        losss_per_e.append(loss_current)\n",
    "    # end\n",
    "    \n",
    "    if losss_per_e:\n",
    "        loss_average_per_e = sum(losss_per_e) / len(losss_per_e)\n",
    "        print('[{}] Epoch: {} Evalutation ends. Status: Average loss: {}, Average S2S accuracy: {}'.format(        \n",
    "            datetime.utcnow(), e, loss_average_per_e,\n",
    "            info_acc_heads_eval.s2s.corrects_segmented / info_acc_heads_eval.s2s.num_segmented,\n",
    "        ))\n",
    "    \n",
    "    loader.update_checkpoint(name_checkpoint_current, name_checkpoint_last)\n",
    "    name_checkpoint_last = name_checkpoint_current\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c4c3a-756b-48c2-9eb3-9f8e40952de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0525a-d0ab-4222-8b56-75f6fa830794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
