{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4a56808-18df-4f29-825c-6eff2f0d561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import BoolTensor\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, EarlyStoppingCallback\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "FILENAME_TEST = 'test.csv'\n",
    "DIR_OUTPUT = 'results'\n",
    "\n",
    "DEVICE_DEFAULT = 'cuda'\n",
    "\n",
    "def get_ts():\n",
    "    return datetime.utcnow().replace(microsecond=0).isoformat()\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "def read_passages(path_data, path_label, test_size=0):\n",
    "    df = pd.read_csv(path_data)\n",
    "\n",
    "    documents = df['processed'].to_list()\n",
    "    labels_str = df['target'].to_list()\n",
    "\n",
    "    samples = documents\n",
    "\n",
    "    with open(path_label, 'r') as file:\n",
    "        labels_list = sorted(json.load(file))\n",
    "    # end\n",
    "\n",
    "    labels_all = {l: idx for idx, l in enumerate(labels_list)}\n",
    "\n",
    "    labels = [labels_all[label_str] for label_str in labels_str]\n",
    "\n",
    "    if test_size > 0:\n",
    "        return train_test_split(samples, labels, test_size=test_size, stratify=labels, random_state=234), labels_list\n",
    "    else:\n",
    "        return (samples, samples, labels, labels), labels_list\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids.reshape(-1)\n",
    "    preds = pred.predictions.argmax(-1).reshape(-1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=preds)\n",
    "    precision = precision_score(y_true=labels, y_pred=preds, zero_division=1, average='macro')\n",
    "    recall = recall_score(y_true=labels, y_pred=preds, zero_division=1, average='macro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, zero_division=1, average='macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "# end\n",
    "\n",
    "def predict_plus(input_tokenized, model):\n",
    "    masks_sample = input_tokenized.attention_mask\n",
    "\n",
    "    indicates_sample = BoolTensor(masks_sample == 1)\n",
    "    indicates_sample = indicates_sample.to(DEVICE_DEFAULT)\n",
    "\n",
    "    out = model(**input_tokenized.to(DEVICE_DEFAULT), output_attentions=True)\n",
    "\n",
    "    logits = out.logits.cpu()\n",
    "    attentions = out.attentions[-1].cpu()\n",
    "\n",
    "    attentions_sum = torch.masked_select((torch.sum(attentions[:, :, 0, :], 1) / attentions.shape[1])[0],\n",
    "                        indicates_sample).tolist()[1:-1]\n",
    "\n",
    "    return logits, attentions_sum\n",
    "# end\n",
    "\n",
    "def main_train_and_evaluate(name_train, path_train, path_label, path_test, path_output):\n",
    "    print('[{}] start main_train_and_evaluate with {} {}'.format(get_ts(), path_train, path_test))\n",
    "\n",
    "    model_name = MODEL_NAME\n",
    "    max_length = MAX_LENGTH\n",
    "    output_dir = DIR_OUTPUT\n",
    "\n",
    "    (train_samples, valid_samples, train_labels, valid_labels), target_names = read_passages(path_train, path_label,\n",
    "                                                                                             0.1)\n",
    "\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "    train_encodings = tokenizer.batch_encode_plus(train_samples, truncation=True, padding=True, max_length=max_length,\n",
    "                                                  return_tensors='pt')\n",
    "    valid_encodings = tokenizer.batch_encode_plus(valid_samples, truncation=True, padding=True, max_length=max_length,\n",
    "                                                  return_tensors='pt')\n",
    "\n",
    "    train_dataset = SimpleDataset(train_encodings, train_labels)\n",
    "    valid_dataset = SimpleDataset(valid_encodings, valid_labels)\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=len(target_names))\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,  # output directory\n",
    "        num_train_epochs=12,  # total number of training epochs\n",
    "        per_device_train_batch_size=8,  # batch size per device during training\n",
    "        per_device_eval_batch_size=8,  # batch size for evaluation\n",
    "        warmup_steps=0,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,  # strength of weight decay\n",
    "        logging_dir='./logs',  # directory for storing logs\n",
    "        load_best_model_at_end=True,\n",
    "        # load the best model when finished training (default metric is loss)    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "        logging_steps=1,  # log & save weights each logging_steps\n",
    "        evaluation_strategy=\"epoch\",  # evaluate each `logging_steps`\n",
    "        learning_rate=2e-5,\n",
    "        save_strategy='epoch',\n",
    "        save_total_limit=6,\n",
    "        metric_for_best_model='f1'\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,  # the instantiated Transformers model to be trained\n",
    "        args=training_args,  # training arguments, defined above\n",
    "        train_dataset=train_dataset,  # training dataset\n",
    "        eval_dataset=valid_dataset,  # evaluation dataset\n",
    "        compute_metrics=compute_metrics,  # the callback that computes metrics of interest\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=6)]\n",
    "    )\n",
    "\n",
    "    # trainer = Trainer(\n",
    "    #     model=model,  # the instantiated Transformers model to be trained\n",
    "    #     args=training_args,  # training arguments, defined above\n",
    "    #     train_dataset=train_dataset,  # training dataset\n",
    "    #     eval_dataset=valid_dataset,  # evaluation dataset\n",
    "    #     compute_metrics=compute_metrics\n",
    "    # )\n",
    "\n",
    "    print('[{}] start training...'.format(get_ts()))\n",
    "    trainer.train()\n",
    "\n",
    "    info_state_model = trainer.evaluate()\n",
    "    print('[{}] finish training.'.format(get_ts()))\n",
    "\n",
    "    ################## start to do eval ##################\n",
    "\n",
    "    \n",
    "    return model\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cac7e9-ef49-40d0-8b57-4b2cf389cc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-26T05:53:55] start main_train_and_evaluate with data/training/202205240000.csv data/test_from_train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-87d7a8807c3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mname_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_train_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-97a1a371d38a>\u001b[0m in \u001b[0;36mmain_train_and_evaluate\u001b[0;34m(name_train, path_train, path_label, path_test, path_output)\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     trainer = Trainer(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# the instantiated Transformers model to be trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# training arguments, defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_model_on_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_folder_train = 'data/training'\n",
    "path_test = 'data/test_from_train.csv'\n",
    "path_label = 'data/labels.json'\n",
    "path_output = 'data/output_noseed_train'\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "seed_val = 234\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "filenames = [filename for filename in os.listdir(path_folder_train) if filename[0] != '.']\n",
    "filename = filenames[0]\n",
    "path_train = os.path.join(path_folder_train, filename)\n",
    "name_train = filename.split('.')[0]\n",
    "\n",
    "model = main_train_and_evaluate(name_train, path_train, path_label, path_test, path_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272aebda-d47d-44bd-a8ca-23fea971b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = MODEL_NAME\n",
    "# max_length = MAX_LENGTH\n",
    "\n",
    "# (samples_test, _, indexs_label_test, _), target_names = read_passages(path_test, path_label, 0)\n",
    "# labels_test = [target_names[index_label_test] for index_label_test in indexs_label_test]\n",
    "# list_corpus_test = list(zip(samples_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04e266-d4e1-46d1-a580-83960d6ad452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_corpus_test_with_index = [[index, corpus] for index, corpus in enumerate(list_corpus_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9e60f-af6e-4066-a2b9-5138c81a8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def insert_token_to_sentence(token, sentence):\n",
    "    sentences_new = [sentence]\n",
    "\n",
    "    words_sentence = sentence.split()\n",
    "    for i in range(len(words_sentence)+1):\n",
    "        words_sentence_new = copy.copy(words_sentence)\n",
    "        words_sentence_new.insert(i, token)\n",
    "        sentences_new.append(' '.join(words_sentence_new))\n",
    "    # end\n",
    "\n",
    "    return sentences_new\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e8191-61d9-4fb1-bed9-920e637def73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MODEL_NAME\n",
    "max_length = MAX_LENGTH\n",
    "sample_test = 'timestamp failed at play mouse driver vm tools timestamp task wait for getting vm test vm ip address on esxi ip address fatal localhost un reachable failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo vmfs volumes data store number mkdir echo vmfs volumes data store number ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo vmfs volumes data store number ansible tmp hex id number timestamp exited with result number'\n",
    "\n",
    "# sample_test = 'permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask'\n",
    "# sample_test = 'permissions on the target directory consider changing the remote tmp path in ansible configuration to a rooted in tmp for more error information use v v v failed command was u mask' # nopath, all testcase\n",
    "token_target = 'number'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "\n",
    "# sample_test = 'permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask'\n",
    "# sample_test = 'permissions target directory consider changing remote tmp path  ansible configuration a path rooted in tmp more information use v failed command was mask' # no second path\n",
    "# sample_test = 'permissions target directory consider changing remote ansible configuration a path rooted in tmp more information use v failed mask' # no second path -> infra\n",
    "# sample_test = 'permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask'\n",
    "# sample_test = 'v v v failed command was u'\n",
    "\n",
    "\n",
    "samples_test = insert_token_to_sentence(token_target, sample_test)\n",
    "samples_test = [samples_test[0]]\n",
    "\n",
    "list_out_pred = []\n",
    "for sample_test in samples_test:\n",
    "    input_tokenized = tokenizer.encode_plus(sample_test, padding=True, truncation=True, max_length=max_length,\n",
    "                                        return_tensors='pt')\n",
    "\n",
    "    masks_sample = input_tokenized.attention_mask\n",
    "    indicates_sample = BoolTensor(masks_sample == 1)\n",
    "    indicates_sample = indicates_sample.to(DEVICE_DEFAULT)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(**input_tokenized.to(DEVICE_DEFAULT), output_attentions=True)\n",
    "    # end\n",
    "\n",
    "    logits = out.logits.cpu()\n",
    "    # attentions = out.attentions[-1].cpu()\n",
    "    attentions = out.attentions[0].cpu()\n",
    "    probas_evaluate = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "\n",
    "    answer_evaluate = int(probas_evaluate.argmax())\n",
    "    label_evaluate = target_names[answer_evaluate]\n",
    "    probas_max = probas_evaluate[answer_evaluate]\n",
    "    \n",
    "    tokens = [tokenizer.decode(id) for id in input_tokenized[0].ids][1:-1]\n",
    "    \n",
    "    attentions_sum = torch.masked_select((torch.sum(attentions[:, :, 0, :], 1) / attentions.shape[1])[0], indicates_sample).tolist()[1:-1]\n",
    "    # attentions_sum = torch.masked_select(attentions[0, 0, 0, :], indicates_sample).tolist()[1:-1] # 1batch(0), 12 heads(n), 1cls(0)\n",
    "    \n",
    "    pairs_token_attention = list(zip(list(range(1, len(tokens)+1)),tokens, attentions_sum))\n",
    "    \n",
    "    list_out_pred.append((label_evaluate, float(probas_max), sample_test, pairs_token_attention))\n",
    "# end\n",
    "\n",
    "list_out_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e6b3b-3f63-42d0-b8e8-b4b00f738dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine old version, which is not correct\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "# sample_test = 'permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask'\n",
    "# sample_test = 'permissions target directory consider changing remote ansible configuration a path rooted in tmp more information use v failed mask' # no second path\n",
    "\n",
    "input_tokenized = tokenizer.encode_plus(sample_test, padding=True, truncation=True, max_length=max_length,\n",
    "                                    return_tensors='pt')\n",
    "\n",
    "masks_sample = input_tokenized.attention_mask\n",
    "indicates_sample = BoolTensor(masks_sample == 1)\n",
    "indicates_sample = indicates_sample.to(DEVICE_DEFAULT)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(**input_tokenized.to(DEVICE_DEFAULT), output_attentions=True)\n",
    "# end\n",
    "\n",
    "logits = out.logits.cpu()\n",
    "# attentions = out.attentions[-1].cpu()\n",
    "attentions = out.attentions[0].cpu()\n",
    "probas_evaluate = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "\n",
    "answer_evaluate = int(probas_evaluate.argmax())\n",
    "label_evaluate = target_names[answer_evaluate]\n",
    "probas_max = probas_evaluate[answer_evaluate]\n",
    "\n",
    "# calculate attention\n",
    "tokens = [tokenizer.decode(id) for id in input_tokenized[0].ids][1:-1]\n",
    "attentions_sum = torch.masked_select((torch.sum(attentions[:, :, 0, :], 1) / attentions.shape[1])[0], indicates_sample).tolist()[1:-1]\n",
    "\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "\n",
    "list_cos = []\n",
    "for i in range(attentions.shape[-1]):\n",
    "    embedding_cls_a = torch.sum(attentions[0, :, :, 0], 1) / attentions.shape[1]\n",
    "    embedding_cls_b = torch.sum(attentions[0, :, :, i], 1) / attentions.shape[1]\n",
    "    sim_cos = float(cos(embedding_cls_a, embedding_cls_b))\n",
    "    list_cos.append(sim_cos)\n",
    "# end\n",
    "\n",
    "list_cos = list_cos[1:-1]\n",
    "\n",
    "list_factor = np.array(attentions_sum) * np.array(list_cos)\n",
    "list_factor_normed = list_factor / np.linalg.norm(list_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284d1192-3c4b-43fc-8db6-3f847a8a90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine new version\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "sample_test = 'timestamp failed at play mouse driver vm tools timestamp task wait for getting vm test vm ip address on esxi ip address fatal localhost un reachable failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo vmfs volumes data store number mkdir echo vmfs volumes data store number ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo vmfs volumes data store number ansible tmp hex id number timestamp exited with result number'\n",
    "# sample_test = 'permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask'\n",
    "# sample_test = 'v v v failed command was u'\n",
    "\n",
    "\n",
    "input_tokenized = tokenizer.encode_plus(sample_test, padding=True, truncation=True, max_length=max_length,\n",
    "                                    return_tensors='pt')\n",
    "\n",
    "masks_sample = input_tokenized.attention_mask\n",
    "indicates_sample = BoolTensor(masks_sample == 1)\n",
    "indicates_sample = indicates_sample.to(DEVICE_DEFAULT)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(**input_tokenized.to(DEVICE_DEFAULT), output_attentions=True, output_hidden_states=True)\n",
    "# end\n",
    "\n",
    "logits = out.logits.cpu()\n",
    "attentions = out.attentions[-1].cpu()\n",
    "# attentions = out.attentions[0].cpu()\n",
    "probas_evaluate = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
    "\n",
    "answer_evaluate = int(probas_evaluate.argmax())\n",
    "label_evaluate = target_names[answer_evaluate]\n",
    "probas_max = probas_evaluate[answer_evaluate]\n",
    "\n",
    "# calculate attention\n",
    "tokens = [tokenizer.decode(id) for id in input_tokenized[0].ids][1:-1]\n",
    "attentions_sum = torch.masked_select((torch.sum(attentions[:, :, 0, :], 1) / attentions.shape[1])[0], indicates_sample)\n",
    "attentions_sum_torch = torch.Tensor(attentions_sum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12208402-7bbc-464c-abe1-ffbcfa63e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "\n",
    "hidden_states = out.hidden_states\n",
    "h2 = hidden_states[-1].squeeze(0) # [24, 762]\n",
    "h2 = h2.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd99918-5867-4bae-ac50-16da6cc0d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cos = []\n",
    "for i in range(h2.shape[0]):\n",
    "\n",
    "    # embedding_cls_a = h2[0,:]\n",
    "    embedding_cls_a = torch.matmul(attentions_sum, h2)\n",
    "    embedding_cls_b = h2[i,:]\n",
    "    sim_cos = float(cos(embedding_cls_a, embedding_cls_b))\n",
    "    list_cos.append(sim_cos)\n",
    "# end\n",
    "\n",
    "# (list_cos[1:-1] / np.linalg.norm(list_cos[1:-1])).tolist()\n",
    "list_cos[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf06bce-3639-4025-a84b-3bedb58796c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy, sklearn\n",
    "# scipy.special.softmax(list_cos[1:-1])\n",
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "min_max_scaler.fit_transform(list_cos[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db6ce4b-faf2-4ee7-ad83-10e3c74a1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import BoolTensor\n",
    "import csv\n",
    "\n",
    "model_name = MODEL_NAME\n",
    "max_length = MAX_LENGTH\n",
    "\n",
    "\n",
    "(samples_test, _, indexs_label_test, _), target_names = read_passages(path_test, path_label, 0)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4685573-d078-4a37-aa8c-12bc8508e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_n(sentence, n):\n",
    "    words_sentence = sentence.split()\n",
    "    list_corpus_current = [' '.join(words_sentence[i:i + n]) for i in range(len(words_sentence)+1 - n)]\n",
    "    return list_corpus_current\n",
    "# end\n",
    "\n",
    "def ngram_investigation(sentence, tokenizer, model, target_names, n_gram_end=1, n_gram_start=0):\n",
    "    list_list_corpus = [split_by_n(sentence, n+1) for n in range(n_gram_start, n_gram_end)]\n",
    "    list_batch_encoded = [tokenizer.batch_encode_plus(list_corpus, padding=True, truncation=True, max_length=512, return_tensors='pt') for list_corpus in list_list_corpus]\n",
    "    # print(list_batch_encoded[0].input_ids[0])\n",
    "    # print(tokenizer.batch_decode(list_batch_encoded[0].input_ids[0]))\n",
    "    \n",
    "    # list_batch_encoded = [tokenizer.batch_encode_plus(list_corpus, return_tensors='pt') for list_corpus in list_list_corpus]\n",
    "    list_out = []\n",
    "    for batch_encoded in list_batch_encoded:\n",
    "        masks_attention = batch_encoded.attention_mask\n",
    "        indices_attention = BoolTensor(masks_attention == 1)\n",
    "        batch_ids_input = batch_encoded.input_ids.to('cpu')\n",
    "        # batch_decode = [tokenizer.batch_decode(batch_ids_input[i]) for i in range(batch_ids_input.shape[0])]\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(**batch_encoded.to(DEVICE_DEFAULT), output_attentions=True)\n",
    "        # end\n",
    "        \n",
    "        # scores_attention = (torch.sum(out.attentions[-1][:,:,0,:], 1)[:,1:-1] / out.attentions[-1].shape[1]).to('cpu').tolist()\n",
    "        \n",
    "        scores_attention_all = (torch.sum(out.attentions[-1][:,:,0,:], 1) / out.attentions[-1].shape[1]).to('cpu')\n",
    "        \n",
    "        scores_attention_flat = torch.masked_select(scores_attention_all, indices_attention).tolist()\n",
    "        ids_input_flat = torch.masked_select(batch_ids_input, indices_attention).tolist()\n",
    "        list_num_tokens = torch.sum(masks_attention, axis=1).tolist()\n",
    "        \n",
    "        list_ids_input = []\n",
    "        list_scores_attention = []\n",
    "        \n",
    "        for num_token in list_num_tokens:\n",
    "            list_ids_input.append(tokenizer.batch_decode(ids_input_flat[:num_token][1:-1]))\n",
    "            list_scores_attention.append(scores_attention_flat[:num_token][1:-1])\n",
    "            \n",
    "            ids_input_flat = ids_input_flat[num_token:]\n",
    "            scores_attention_flat = scores_attention_flat[num_token:]\n",
    "        # end\n",
    "        \n",
    "        list_corpuss_input_attention = []\n",
    "        for ids_input, scores_attention in zip(list_ids_input, list_scores_attention):\n",
    "            list_corpuss_input_attention.append([(id_input, score_attention) for id_input, score_attention in zip(ids_input, scores_attention)])\n",
    "        # end\n",
    "        \n",
    "        probas_evaluate = torch.nn.functional.softmax(out.logits, dim=-1).to('cpu')\n",
    "        answers_evaluate = probas_evaluate.argmax(axis=1).to('cpu')\n",
    "        confidences_evaluate = probas_evaluate.gather(1, answers_evaluate.reshape(-1, 1)).tolist()\n",
    "        labels_evaluate = [target_names[answer_current] for answer_current in answers_evaluate.tolist()]\n",
    "        \n",
    "        list_corpuss_label_confidence = []\n",
    "        for label_evaluate, confidence_evaluate in zip(labels_evaluate, confidences_evaluate):\n",
    "            list_corpuss_label_confidence.append((label_evaluate, confidence_evaluate[0]))\n",
    "        # end\n",
    "        \n",
    "        # print()\n",
    "        output_final = [{'input': corpuss_input_attention,'result': corpuss_label_confidence} for corpuss_input_attention, corpuss_label_confidence in zip(list_corpuss_input_attention, list_corpuss_label_confidence)]\n",
    "\n",
    "        # list_out.append({'inputs_with_attention': list_corpuss_input_attention, 'labels_with_confidence': list_corpuss_label_confidence})\n",
    "        list_out.append(output_final)\n",
    "    # end\n",
    "    \n",
    "    return list_out\n",
    "# end\n",
    "\n",
    "\n",
    "def transform_ngram_report(content_origin, n_gram_start):\n",
    "    list_all = []\n",
    "    for ngram, content_ngram in enumerate(content_origin):\n",
    "        \n",
    "        for i_sentence, dict_info_sentence in enumerate(content_ngram):\n",
    "            y_pred = dict_info_sentence['result'][0]\n",
    "            \n",
    "            conf_pred = dict_info_sentence['result'][1]\n",
    "\n",
    "            for corpus_token_attention in dict_info_sentence['input']:\n",
    "                token = corpus_token_attention[0]\n",
    "                attention = corpus_token_attention[1]\n",
    "\n",
    "                list_all.append({'ngram': ngram+n_gram_start+1, 'partition': i_sentence, 'token': token, 'attention': attention, 'predict': y_pred, 'confidence': conf_pred})\n",
    "                y_pred = ''\n",
    "                conf_pred = ''\n",
    "                i_sentence = ''\n",
    "            # end\n",
    "        # end\n",
    "    # end\n",
    "    return list_all\n",
    "# end\n",
    "\n",
    "def write_csv(data, filename):\n",
    "    with open(filename, 'w+') as outf:\n",
    "        writer = csv.DictWriter(outf, data[0].keys())\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "        # end\n",
    "    # end\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c4913-6716-46f8-962a-d8495b83b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'timestamp failed at play mouse driver vm tools timestamp task wait for getting vm test vm ip address on esxi ip address fatal localhost un reachable failed to create temporary directory in some cases you may have been able to authenticate and did not have permissions on the target directory consider changing the remote tmp path in ansible configuration to a path rooted in tmp for more error information use v v v failed command was u mask number mkdir p echo vmfs volumes data store number mkdir echo vmfs volumes data store number ansible tmp hex id number timestamp echo ansible tmp hex id number timestamp echo vmfs volumes data store number ansible tmp hex id number timestamp exited with result number'\n",
    "# sentence = 'timestamp failed at play mouse driver vm tools timestamp'\n",
    "content_ngram = ngram_investigation(sentence, tokenizer, model, target_names, 35, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67179381-db79-4ccd-8d0d-8b1fface7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_output = transform_ngram_report(content_ngram, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425bfc15-ae71-455c-b465-e2335a5428f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in content_output if i['predict'] == 'infra']\n",
    "# write_csv(content_output, 'hello.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90dc1c-187d-492b-aa1d-55e409362948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8f891-97fe-4b43-b5fc-add58564d415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
