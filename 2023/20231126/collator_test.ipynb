{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62aeab94-71d4-4e05-acbe-60100a1584a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b602819c-39af-4a57-b422-4212fde1c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bookcorpus = datasets.load_dataset(\"bookcorpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469929de-800a-44be-9659-c49a9bdc7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bookcorpus_2000 = bookcorpus['train']['text'][:2000]\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b4e2e18-7a7a-4eb6-85e6-7d3ba986489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('bookcorpus_2000.json', 'w+') as file:\n",
    "#     file.write(json.dumps(bookcorpus_2000))\n",
    "# # end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba7ec2b-0fdf-48b3-afe2-493cf96b2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "with oepn('bookcorpus_2000.json', 'r') as file:\n",
    "    bookcorpus_2000 = json.load(file)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e65ac57e-b223-4e1f-abd7-4aaab5339351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    DEVICE = 'cuda'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if v is not None and type(v) is not bool:\n",
    "                self.kwargs[k] = v.to(Batch.DEVICE)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.kwargs\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15335ee9-969c-4941-bd66-5d575b34ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator_BERT:\n",
    "    def __init__(self, tokenizer, size_seq_max, need_masked):\n",
    "        self.tokenizer = tokenizer  # \n",
    "        self.size_seq_max = size_seq_max\n",
    "        self.need_masked = need_masked\n",
    "        \n",
    "        index_special_token_2_id = {k:v for k,v in zip(tokenizer.all_special_tokens,tokenizer.all_special_ids)}\n",
    "        \n",
    "        self.id_pad = index_special_token_2_id['[PAD]']\n",
    "        self.id_mask = index_special_token_2_id['[MASK]']\n",
    "        self.id_cls = index_special_token_2_id['[CLS]']\n",
    "        self.id_sep = index_special_token_2_id['[SEP]']\n",
    "        self.id_eol = index_special_token_2_id['[EOL]']\n",
    "        self.id_unk = index_special_token_2_id['[UNK]']\n",
    "        \n",
    "        self.regex_special_token = re.compile(r'([PAD]|[MASK]|[CLS]|[SEP]|[EOL]|[UNK])')\n",
    "    # end\n",
    "    \n",
    "\n",
    "    def __call__(self, list_sequence_batch):\n",
    "        list_sequence_tokenized = self.tokenizer.batch_encode_plus(list_sequence_batch, add_special_tokens=False)['input_ids']\n",
    "        \n",
    "        # Process I. \n",
    "        list_list_tokenized = []\n",
    "        \n",
    "        # batch initialized condition\n",
    "        list_tokenized_cache = []\n",
    "        len_tokenized_accumulated = 1 # add cls, no sep as sep will be treated as eol\n",
    "        \n",
    "        while list_sequence_tokenized:\n",
    "            tokenized_poped = list_sequence_tokenized.pop(0)\n",
    "            len_tokenized_current = len(tokenized_poped) + 1\n",
    "            \n",
    "            if len_tokenized_accumulated + len_tokenized_current > self.size_seq_max:\n",
    "                list_list_tokenized.append(list_tokenized_cache)\n",
    "                \n",
    "                # clear\n",
    "                list_tokenized_cache = []\n",
    "                len_tokenized_accumulated = 1\n",
    "            # end\n",
    "            \n",
    "            len_tokenized_accumulated += len_tokenized_current\n",
    "            list_tokenized_cache.append(tokenized_poped)\n",
    "        # end\n",
    "        \n",
    "        list_list_tokenized.append(list_tokenized_cache)\n",
    "        \n",
    "        \n",
    "        # Process II. Merge list_tokenized\n",
    "        list_tokenized_merged = []\n",
    "        \n",
    "        for list_tokenized in list_list_tokenized:\n",
    "            tokenized_merged = [token for tokenized_padded in [tokenized + [self.id_eol] for tokenized in list_tokenized] for token in tokenized_padded]\n",
    "            tokenized_merged = tokenized_merged[:-1]    # remove last eol token\n",
    "            list_tokenized_merged.append(tokenized_merged)\n",
    "        # end\n",
    "        \n",
    "        \n",
    "        # Process III. Add begin and stop special token, same as jinyuj_transformers_quora.ipynb\n",
    "        tokens_input_encoder = []\n",
    "        tokens_input_decoder = []\n",
    "        tokens_label_decoder = []\n",
    "        \n",
    "        for tokenized_merged in list_tokenized_merged:\n",
    "            tokens_input_encoder.append([self.id_cls] + tokenized_merged + [self.id_sep])\n",
    "            tokens_input_decoder.append([self.id_cls] + tokenized_merged)\n",
    "            tokens_label_decoder.append(tokenized_merged + [self.id_sep])\n",
    "        # end\n",
    "        \n",
    "        \n",
    "        inputs_encoder, masks_encoder, segments_encoder, labels_encoder = self.pad_sequences(tokens_input_encoder, self.size_seq_max, need_masked=self.need_masked)\n",
    "        inputs_decoder, masks_decoder, segments_decoder, _ = self.pad_sequences(tokens_input_decoder, self.size_seq_max, need_diagonal=True)\n",
    "        labels_decoder, masks_label, segments_label, _ = self.pad_sequences(tokens_label_decoder, self.size_seq_max)\n",
    "        \n",
    "        return Batch(\n",
    "            ids_encoder=inputs_encoder,  # contains [mask]s\n",
    "            masks_encoder=masks_encoder,\n",
    "            labels_encoder=labels_encoder,  # doesn't contain [mask]\n",
    "            segments_encoder=segments_encoder,\n",
    "            ids_decoder=inputs_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            labels_decoder=labels_decoder,\n",
    "            segments_label=segments_label\n",
    "        )\n",
    "    # end\n",
    "\n",
    "\n",
    "    # return masks_attention?, return masks_segment?\n",
    "    def pad_sequences(self, sequences, size_seq_max, need_diagonal=False, need_masked=0): # need_diagonal and need_masked cannot both set, one for bert seq one for s2s seq\n",
    "        id_pad = self.id_pad\n",
    "        id_mask = self.id_mask\n",
    "\n",
    "        sequences_padded = []\n",
    "        sequences_masked_padded = []\n",
    "\n",
    "        for sequence in sequences:\n",
    "            len_seq = len(sequence)\n",
    "\n",
    "            count_pad = size_seq_max - len_seq\n",
    "\n",
    "            sequence = torch.LongTensor(sequence)\n",
    "            sequence_padded = torch.cat((sequence, torch.LongTensor([id_pad] * count_pad)))\n",
    "            sequences_padded.append(sequence_padded)\n",
    "\n",
    "            if need_masked:\n",
    "                index_masked = list(range(1, len_seq-1))\n",
    "                random.shuffle(index_masked)\n",
    "                index_masked = torch.LongTensor(index_masked[:int(need_masked * (len_seq-2))])\n",
    "\n",
    "                sequence_masked = sequence.detach().clone()\n",
    "                sequence_masked.index_fill_(0, index_masked, id_mask)\n",
    "                sequence_masked_padded = torch.cat((sequence_masked, torch.LongTensor([id_pad] * count_pad)))\n",
    "                \n",
    "                sequences_masked_padded.append(sequence_masked_padded)\n",
    "            # end\n",
    "    #   # end for\n",
    "\n",
    "        inputs = torch.stack(sequences_padded)  # (batch, size_seq_max)\n",
    "        if need_masked:\n",
    "            inputs_masked_padded = torch.stack(sequences_masked_padded)\n",
    "        # end\n",
    "\n",
    "        masks_segment = (inputs != self.id_pad).unsqueeze(-2)    #(nbatch, 1, seq)\n",
    "        masks_attention = self.make_std_mask(inputs, self.id_pad) if need_diagonal else masks_segment\n",
    "\n",
    "        if need_masked:\n",
    "            masks_masked = (inputs_masked_padded != id_mask).unsqueeze(-2)\n",
    "            masks_attention = masks_attention & masks_masked\n",
    "            return inputs_masked_padded, masks_attention, masks_segment, inputs # (inputs, masks_attention, masks_segment, labels)\n",
    "        else:\n",
    "            return inputs, masks_attention, masks_segment, None\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "\n",
    "    def subsequent_mask(self, size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "            torch.uint8\n",
    "        )\n",
    "        return subsequent_mask == 0\n",
    "    # end\n",
    "\n",
    "    \n",
    "    def make_std_mask(self, tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & self.subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89131d99-20c5-4574-935b-91204d9e7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "train_source = bookcorpus_2000\n",
    "batch_size = 32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['[EOL]']})\n",
    "\n",
    "collator = Collator_BERT(tokenizer, 128, need_masked=0)\n",
    "dataloader_train = DataLoader(train_source, batch_size, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b16cd12-d385-4394-9fe6-d9becbae1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "12789e5b-80ab-4ee5-b504-46c50e821f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:01<00:00, 38.84it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(dataloader_train):\n",
    "    pass\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e03b0a54-b0c3-4619-80fa-b24c0f767e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[CLS] usually, he would be tearing around the living room, playing with his toys. [EOL] but just one look at a minion sent him practically catatonic. [EOL] that had been megan's plan when she got him dressed earlier. [EOL] he'd seen the movie almost by mistake, considering he was a little young for the pg cartoon, but with older cousins, along with her brothers, mason was often exposed to things that were older. [EOL] she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
       " \"[CLS] ` ` aren't you being a good boy?'' [EOL] she said. [EOL] mason barely acknowledged her. [EOL] instead, his baby blues remained focused on the television. [EOL] since the movie was almost over, megan knew she better slip into the bedroom and finish getting ready. [EOL] each time she looked into mason's face, she was grateful that he looked nothing like his father. [EOL] his platinum blond hair and blue eyes were completely hers. [EOL] it was only his build that he was taking after his father. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
       " \"[CLS] where megan was a diminutive 5'3'', davis was 6'1'' and two hundred pounds. [EOL] mason was already registering off the charts in height and weight according to his pediatrician. [EOL] davis had seen mason only twice in his lifetime - the day he had been born and the day he came home from the hospital. [EOL] after that, he hadn't been interested in any of the pictures and emails megan sent. [EOL] with his professional football career on the rise, davis hadn't wanted to be shackled with the responsibilities of a baby. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
       " '[CLS] instead, he wanted to spend his time off the field partying until all hours of the night. [EOL] he only paid child support when megan threatened to have his wages garnished. [EOL] she dreaded the day when mason was old enough to ask about his father. [EOL] she never wanted anything in the world to hurt him, and she knew that being rejected by his father would. [EOL] with a sigh, she stepped into the dress and slid it over her hips. [EOL] wrestling around to get the zipper all the way up caused her to huff and puff. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " \"[CLS] standing back from the mirror, she turned to and fro to take in her appearance. [EOL] she'd always loved how the dress made her feel sexy, but at the same time was very respectable. [EOL] while it boasted a sweetheart neckline, the hemline fell just below her knees. [EOL] she put on her pearls - a high school graduation gift from her uncle aidan, or ` ` ankle'', as she often called him. [EOL] aidan was her mother's baby brother and only son of the family. [EOL] when she was born, he was only eight and a half. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
       " '[CLS] as the first grandchild, megan spent a lot of time with her grandparents, and that in turn, meant she spent a lot of time with aidan. [EOL] he had devoted hours to holding her and spoiling her rotten. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch()['ids_encoder'].cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a3d3b-a094-459f-87aa-a6bf01404928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
