{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef21632-273e-4f5e-859d-eb64dbcfcc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "[100, 102, 0, 101, 103]\n",
      "After\n",
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[EOT]']\n",
      "[100, 102, 0, 101, 103, 30522]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "print(\"Before\")\n",
    "print(tokenizer.all_special_tokens) # --> ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
    "print(tokenizer.all_special_ids)    # --> [100, 102, 0, 101, 103]\n",
    "\n",
    "\n",
    "special_tokens_dict = {'additional_special_tokens': ['[EOT]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# model.resize_token_embeddings(len(tokenizer))  # --> Embedding(30523, 768)\n",
    "\n",
    "tok_id = tokenizer.convert_tokens_to_ids('[EOT]')  # --> 30522\n",
    "\n",
    "print(\"After\")\n",
    "print(tokenizer.all_special_tokens) # --> ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
    "print(tokenizer.all_special_ids)    # --> [100, 102, 0, 101, 103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7711e08-d281-4fe1-984b-d8cf2c957660",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_encode = '''QUERY: I want to ask a question. [EOT]\n",
    "ANSWER: Sure, ask away. [EOT]\n",
    "QUERY: How is the weather today? [EOT]\n",
    "ANSWER: It is nice and sunny. [EOT]\n",
    "QUERY: Okay, nice to know. [EOT]\n",
    "ANSWER: Would you like to know anything else?'''\n",
    "\n",
    "enc = tokenizer.encode_plus(\n",
    "  text_to_encode,\n",
    "  max_length=128,\n",
    "truncation=True,\n",
    "  add_special_tokens=True,\n",
    "  return_token_type_ids=False,\n",
    "  return_attention_mask=False,\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf7fad9-b1c9-405d-a3b6-88d1eeb932e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'query', ':', 'i', 'want', 'to', 'ask', 'a', 'question', '.', '[EOT]', 'answer', ':', 'sure', ',', 'ask', 'away', '.', '[EOT]', 'query', ':', 'how', 'is', 'the', 'weather', 'today', '?', '[EOT]', 'answer', ':', 'it', 'is', 'nice', 'and', 'sunny', '.', '[EOT]', 'query', ':', 'okay', ',', 'nice', 'to', 'know', '.', '[EOT]', 'answer', ':', 'would', 'you', 'like', 'to', 'know', 'anything', 'else', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f46e55-8e7e-45bd-a8ea-01f515142dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[UNK]': 100,\n",
       " '[SEP]': 102,\n",
       " '[PAD]': 0,\n",
       " '[CLS]': 101,\n",
       " '[MASK]': 103,\n",
       " '[EOT]': 30522}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v for k,v in zip(tokenizer.all_special_tokens,tokenizer.all_special_ids)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
