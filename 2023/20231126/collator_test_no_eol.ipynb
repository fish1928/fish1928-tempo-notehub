{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62aeab94-71d4-4e05-acbe-60100a1584a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b602819c-39af-4a57-b422-4212fde1c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bookcorpus = datasets.load_dataset(\"bookcorpus\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "469929de-800a-44be-9659-c49a9bdc7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bookcorpus_2000 = bookcorpus['train']['text'][:2000]\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4e2e18-7a7a-4eb6-85e6-7d3ba986489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('bookcorpus_2000.json', 'w+') as file:\n",
    "#     file.write(json.dumps(bookcorpus_2000))\n",
    "# # end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba7ec2b-0fdf-48b3-afe2-493cf96b2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bookcorpus_2000.json', 'r') as file:\n",
    "    bookcorpus_2000 = json.load(file)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e65ac57e-b223-4e1f-abd7-4aaab5339351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    DEVICE = 'cuda'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if v is not None and type(v) is not bool:\n",
    "                self.kwargs[k] = v.to(Batch.DEVICE)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.kwargs\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15335ee9-969c-4941-bd66-5d575b34ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator_BERT:\n",
    "    def __init__(self, tokenizer, size_seq_max, need_masked=0.3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size_seq_max = size_seq_max\n",
    "        self.need_masked = need_masked\n",
    "        \n",
    "        index_special_token_2_id = {k:v for k,v in zip(tokenizer.all_special_tokens,tokenizer.all_special_ids)}\n",
    "        \n",
    "        self.id_pad = index_special_token_2_id['[PAD]']\n",
    "        self.id_mask = index_special_token_2_id['[MASK]']\n",
    "        self.id_cls = index_special_token_2_id['[CLS]']\n",
    "        self.id_sep = index_special_token_2_id['[SEP]']\n",
    "        self.id_unk = index_special_token_2_id['[UNK]']\n",
    "        \n",
    "        self.regex_special_token = re.compile(r'\\[(PAD|MASK|CLS|SEP|EOL|UNK)\\]')\n",
    "    # end\n",
    "    \n",
    "\n",
    "    def __call__(self, list_sequence_batch):\n",
    "        list_sequence_batch = [re.sub(self.regex_special_token, r'<\\1>', sequence) for sequence in list_sequence_batch]   # remove special tokens\n",
    "        \n",
    "        list_sequence_tokenized = self.tokenizer.batch_encode_plus(list_sequence_batch, add_special_tokens=False)['input_ids']\n",
    "        \n",
    "        # Process I. \n",
    "        list_list_tokenized = []\n",
    "        \n",
    "        # batch initialized condition\n",
    "        list_tokenized_cache = []\n",
    "        len_tokenized_accumulated = 2 # add cls and sep\n",
    "        \n",
    "        while list_sequence_tokenized:\n",
    "            tokenized_poped = list_sequence_tokenized.pop(0)\n",
    "            len_tokenized_current = len(tokenized_poped)\n",
    "            \n",
    "            if len_tokenized_accumulated + len_tokenized_current > self.size_seq_max:\n",
    "                if list_tokenized_cache:\n",
    "                    list_list_tokenized.append(list_tokenized_cache)\n",
    "                \n",
    "                    # clear\n",
    "                    list_tokenized_cache = []\n",
    "                    len_tokenized_accumulated = 2\n",
    "                # end\n",
    "            # end\n",
    "\n",
    "            list_tokenized_cache.append(tokenized_poped)\n",
    "            len_tokenized_accumulated += len_tokenized_current\n",
    "        # end\n",
    "        \n",
    "        list_list_tokenized.append(list_tokenized_cache)\n",
    "        \n",
    "        \n",
    "        # Process II. Merge list_tokenized\n",
    "        list_tokenized_merged = []\n",
    "        \n",
    "        for list_tokenized in list_list_tokenized:\n",
    "            # tokenized_merged = [token for tokenized_padded in [tokenized + [self.id_eol] for tokenized in list_tokenized] for token in tokenized_padded]\n",
    "            tokenized_merged = [token for tokenized in list_tokenized for token in tokenized][:self.size_seq_max-2]\n",
    "            list_tokenized_merged.append(tokenized_merged)\n",
    "        # end\n",
    "        \n",
    "        \n",
    "        # Process III. Add begin and stop special token, same as jinyuj_transformers_quora.ipynb\n",
    "        tokens_input_encoder = []\n",
    "        tokens_input_decoder = []\n",
    "        tokens_label_decoder = []\n",
    "        \n",
    "        for tokenized_merged in list_tokenized_merged:\n",
    "            tokens_input_encoder.append([self.id_cls] + tokenized_merged + [self.id_sep])\n",
    "            tokens_input_decoder.append([self.id_cls] + tokenized_merged)\n",
    "            tokens_label_decoder.append(tokenized_merged + [self.id_sep])\n",
    "        # end\n",
    "        \n",
    "        inputs_encoder, masks_encoder, segments_encoder, labels_encoder = self.pad_sequences(tokens_input_encoder, self.size_seq_max, need_masked=self.need_masked)\n",
    "        inputs_decoder, masks_decoder, segments_decoder, _ = self.pad_sequences(tokens_input_decoder, self.size_seq_max, need_diagonal=True)\n",
    "        labels_decoder, masks_label, segments_label, _ = self.pad_sequences(tokens_label_decoder, self.size_seq_max)\n",
    "        \n",
    "        return Batch(\n",
    "            ids_encoder=inputs_encoder,  # contains [mask]s\n",
    "            masks_encoder=masks_encoder,\n",
    "            labels_encoder=labels_encoder,  # doesn't contain [mask]\n",
    "            segments_encoder=segments_encoder,\n",
    "            ids_decoder=inputs_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            labels_decoder=labels_decoder,\n",
    "            segments_label=segments_label\n",
    "        )\n",
    "    # end\n",
    "\n",
    "\n",
    "    # return masks_attention?, return masks_segment?\n",
    "    def pad_sequences(self, sequences, size_seq_max, need_diagonal=False, need_masked=0): # need_diagonal and need_masked cannot both set, one for bert seq one for s2s seq\n",
    "        id_pad = self.id_pad\n",
    "        id_mask = self.id_mask\n",
    "\n",
    "        sequences_padded = []\n",
    "        sequences_masked_padded = []\n",
    "\n",
    "        for sequence in sequences:\n",
    "            len_seq = len(sequence)\n",
    "\n",
    "            count_pad = size_seq_max - len_seq\n",
    "\n",
    "            sequence = torch.LongTensor(sequence)\n",
    "            sequence_padded = torch.cat((sequence, torch.LongTensor([id_pad] * count_pad)))\n",
    "            # print(sequence_padded.shape)\n",
    "            sequences_padded.append(sequence_padded)\n",
    "\n",
    "            if need_masked:\n",
    "                index_masked = list(range(1, len_seq-1))\n",
    "                random.shuffle(index_masked)\n",
    "                index_masked = torch.LongTensor(index_masked[:int(need_masked * (len_seq-2))])\n",
    "\n",
    "                sequence_masked = sequence.detach().clone()\n",
    "                sequence_masked.index_fill_(0, index_masked, id_mask)\n",
    "                sequence_masked_padded = torch.cat((sequence_masked, torch.LongTensor([id_pad] * count_pad)))\n",
    "                \n",
    "                sequences_masked_padded.append(sequence_masked_padded)\n",
    "            # end\n",
    "    #   # end for\n",
    "\n",
    "        inputs = torch.stack(sequences_padded)  # (batch, size_seq_max)\n",
    "        if need_masked:\n",
    "            inputs_masked_padded = torch.stack(sequences_masked_padded)\n",
    "        # end\n",
    "\n",
    "        masks_segment = (inputs != self.id_pad).unsqueeze(-2)    #(nbatch, 1, seq)\n",
    "        masks_attention = self.make_std_mask(inputs, self.id_pad) if need_diagonal else masks_segment\n",
    "\n",
    "        if need_masked:\n",
    "            masks_masked = (inputs_masked_padded != id_mask).unsqueeze(-2)\n",
    "            masks_attention = masks_attention & masks_masked\n",
    "            return inputs_masked_padded, masks_attention, masks_segment, inputs # (inputs, masks_attention, masks_segment, labels)\n",
    "        else:\n",
    "            return inputs, masks_attention, masks_segment, None\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "\n",
    "    def subsequent_mask(self, size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "            torch.uint8\n",
    "        )\n",
    "        return subsequent_mask == 0\n",
    "    # end\n",
    "\n",
    "    \n",
    "    def make_std_mask(self, tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & self.subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89131d99-20c5-4574-935b-91204d9e7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "train_source = bookcorpus_2000\n",
    "batch_size = 32\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['[EOL]']})\n",
    "\n",
    "collator = Collator_BERT(tokenizer, 64, need_masked=0.2)\n",
    "dataloader_train = DataLoader(train_source, batch_size, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b16cd12-d385-4394-9fe6-d9becbae1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12789e5b-80ab-4ee5-b504-46c50e821f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 17.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(dataloader_train):\n",
    "    pass\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e03b0a54-b0c3-4619-80fa-b24c0f767e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[CLS] he glanced up and smiled at [MASK]sh. ` ` but thanks for the vote [MASK] confidence [MASK] me being [MASK] [MASK].'' ` ` i just [MASK] it as i see it.'[MASK] [MASK] ` i appreciate that.'' ` ` [MASK] about i check on your [MASK] [MASK]?'[MASK] [SEP] [PAD] [PAD]\",\n",
       " \"[CLS] aidan nodded. ` ` okay.'' pe [MASK] started [MASK] [MASK] unusual lumps and bumps on aidan's head. ` [MASK] [MASK], i [MASK]n't [MASK] anything abnormal. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
       " \"[CLS] but just to be [MASK] the [MASK] side, i [MASK] you to have a ct [MASK] to [MASK] out concussion or any brain bleed [MASK].'' [MASK] ` jesus, [MASK] could have all of that from just hitting my head [MASK] [MASK]'` ` you would be surprised. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
       " \"[CLS] let me go call in the [MASK], [MASK] they'll come [MASK] you back [MASK] hopefully [MASK] wo [MASK]'[MASK] have too long [MASK] wait. we're usually slower during the afternoons when the [MASK]'offices and imaging centers are open.'[MASK] as [MASK] started for [MASK] door, aidan stopped him. [SEP] [PAD]\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(batch()['ids_encoder'].cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c0a3d3b-a094-459f-87aa-a6bf01404928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch()['ids_encoder'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
