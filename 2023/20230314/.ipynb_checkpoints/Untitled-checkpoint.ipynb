{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2210defb-7028-464e-90ef-18b116a15da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1881c14-cf31-4dc6-a134-90d38bbeb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d6f5258d-6fd0-4688-aad2-c1863c6a2e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_first = 'hello my name is john nice to meet you today is a good day is not it'\n",
    "seq_second = 'hello i am marry first time to see you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e593b477-3eec-45e4-9205-dd33f767f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.encode_plus(seq_first, seq_second, add_special_tokens=True, max_length=64, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt', truncation=False)\n",
    "# a = tokenizer.encode_plus(seq_first, seq_second)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0b330edf-e062-4aca-a68d-84124b5e52d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 2026, 2171, 2003, 2198, 3835, 2000, 3113, 2017, 2651, 2003,\n",
       "         1037, 2204, 2154, 2003, 2025, 2009,  102, 7592, 1045, 2572, 5914, 2034,\n",
       "         2051, 2000, 2156, 2017,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "585cc455-4509-4b14-99ec-50657a967570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello my name is john nice to meet you today is a good day is not it [SEP] hello i am marry first time to see you [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(a['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b0c4a458-27b7-4c02-964e-3d459bfcc239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "class SimpleTokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # end\n",
    "\n",
    "    '''\n",
    "        {\n",
    "            \"input_ids\": [],\n",
    "            \"segment_masks\": [],\n",
    "            \"position_masks\": [],\n",
    "            \"attention_masks\": []\n",
    "        }\n",
    "    '''\n",
    "    #TODO: more than max_length?\n",
    "    def generate_training_embedding(self, seq_a, seq_b, probs_mask=0.15, max_length=64):\n",
    "        tokens_a = seq_a.split()\n",
    "        tokens_b = seq_b.split()\n",
    "\n",
    "\n",
    "\n",
    "        tokens_pair = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "        indexs_mask_all = [i + 1 for i in range(len(tokens_a))] + [i + 2 + len(tokens_a) for i in range(len(tokens_b))]\n",
    "        random.shuffle(indexs_mask_all)\n",
    "        indexs_masked = indexs_mask_all[:int(len(indexs_mask_all) * probs_mask)]\n",
    "\n",
    "        len_all = len(tokens_a) + len(tokens_b) + 3\n",
    "        tokens_pad = ['[PAD]' for i in range(max_length - len_all)]\n",
    "        tokens_all = tokens_pair + tokens_pad\n",
    "\n",
    "        t_segments_all = torch.IntTensor([0 for _ in range(len(tokens_a) + 2)] + [1 for _ in range(len(tokens_b) + 1)] + [0 for _ in range(len(tokens_pad))])\n",
    "        t_attentions_all = torch.IntTensor([1 for _ in range(len(tokens_pair))] + [0 for _ in range(len(tokens_pad))])\n",
    "        t_attentions_all[indexs_masked] = 0\n",
    "        t_masks = torch.zeros(len(tokens_all), dtype=torch.bool)\n",
    "        t_masks[indexs_masked] = True\n",
    "        t_position_all = torch.IntTensor([i for i in range(len(tokens_all))])\n",
    "        t_tokens_id = self.tokenizer.convert_tokens_to_ids(tokens_all)\n",
    "\n",
    "        return {\n",
    "            'tokens_id': t_tokens_id,\n",
    "            'masks': t_masks,\n",
    "            'segments': t_segments_all,\n",
    "            'attentions': t_attentions_all,\n",
    "            'positions': t_position_all\n",
    "        }\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "764d574f-d0a6-423d-9ff4-c9654c82fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_t = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4914b786-8f93-4bf4-8357-26ac40320ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens_id': [101,\n",
       "  7592,\n",
       "  2026,\n",
       "  2171,\n",
       "  2003,\n",
       "  2198,\n",
       "  3835,\n",
       "  2000,\n",
       "  3113,\n",
       "  2017,\n",
       "  2651,\n",
       "  2003,\n",
       "  1037,\n",
       "  2204,\n",
       "  2154,\n",
       "  2003,\n",
       "  2025,\n",
       "  2009,\n",
       "  102,\n",
       "  7592,\n",
       "  1045,\n",
       "  2572,\n",
       "  5914,\n",
       "  2034,\n",
       "  2051,\n",
       "  2000,\n",
       "  2156,\n",
       "  2017,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'masks': tensor([False,  True, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True, False, False, False, False,\n",
       "         False, False, False,  True, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False]),\n",
       " 'segments': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " 'attentions': tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " 'positions': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], dtype=torch.int32)}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_t.generate_training_embedding(seq_first, seq_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e0621d-4fc7-4486-bcef-14f366ca9624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
