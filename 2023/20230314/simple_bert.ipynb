{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03f15df2-c5c9-4f89-a696-930f72934fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Embedding\n",
    "\n",
    "import random\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Simple_BertEmbedder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, length_seq_max, length_vocab_max, length_segment_max, dim_hidden, prob_drop=0.15):\n",
    "        super().__init__()\n",
    "        self.length_seq_max = length_seq_max\n",
    "        self.length_vocab_max = length_vocab_max\n",
    "        self.length_segment_max = length_segment_max\n",
    "        self.dim_hidden = dim_hidden\n",
    "\n",
    "        self.embedding_token = Embedding(length_vocab_max, dim_hidden)  # num_embedding, embedding_dim\n",
    "        self.embedding_position = Embedding(length_seq_max, dim_hidden)\n",
    "        self.embedding_segment = Embedding(length_segment_max, dim_hidden)\n",
    "        self.norm = torch.nn.LayerNorm(dim_hidden)\n",
    "        self.dropout = torch.nn.Dropout(p=prob_drop)\n",
    "\n",
    "    # end\n",
    "\n",
    "    # TODO: segment embedding 0,0,0,1,1,1,0,0,0\n",
    "    def forward(self, token_ids=None, masked=None, segments=None, attentions=None, positions=None, is_next=None):\n",
    "        \n",
    "        # print(token_ids)\n",
    "        e_token = self.embedding_token(token_ids)\n",
    "        e_segment = self.embedding_segment(segments)\n",
    "        e_position = self.embedding_position(positions)\n",
    "        \n",
    "        sum_embedding = self.norm(e_token + e_segment + e_position)\n",
    "        return self.dropout(sum_embedding)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Simple_SelfAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, num_head=6):\n",
    "        super().__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.num_head = num_head\n",
    "        self.linear_Q = torch.nn.Linear(dim_hidden, dim_hidden * num_head)\n",
    "        self.linear_K = torch.nn.Linear(dim_hidden, dim_hidden * num_head)\n",
    "        self.linear_V = torch.nn.Linear(dim_hidden, dim_hidden * num_head)\n",
    "        self.linear_out = torch.nn.Linear(dim_hidden * num_head, dim_hidden)\n",
    "\n",
    "    # end\n",
    "\n",
    "    # attentions is masked already\n",
    "    def forward(self, seq_in, attentions):  # batch_size, len_seq, dim_in(dim_in = dim_embedding for first layer)\n",
    "        size_batch, len_seq, dim_in = seq_in.shape\n",
    "        dim_hidden = self.dim_hidden\n",
    "        w_q = self.linear_Q(seq_in)\n",
    "        w_k = self.linear_K(seq_in)\n",
    "        w_v = self.linear_V(seq_in)\n",
    "\n",
    "        q = w_q.view(size_batch, len_seq, -1, dim_hidden).transpose(1, 2)\n",
    "        k = w_k.view(size_batch, len_seq, -1, dim_hidden).transpose(1, 2)\n",
    "        v = w_v.view(size_batch, len_seq, -1, dim_hidden).transpose(1, 2)\n",
    "        \n",
    "        w_qk = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(dim_hidden)  # batch_size, num_head, len_seq, len_seq\n",
    "        \n",
    "        attentions_all = attentions[:, None, None, :].expand_as(w_qk)  # batch_size, num_head, len_seq, len_seq\n",
    "        \n",
    "        masks_all = (1 - attentions_all) * -1e10\n",
    "        scores_raw = w_qk + masks_all\n",
    "        scores = torch.nn.functional.softmax(scores_raw, dim=-1)  # batch_size, num_head, len_seq, len_seq\n",
    "        \n",
    "        z = torch.matmul(scores, v)  # batch_size, num_head, len_seq, dim_hidden\n",
    "        w_z = z.transpose(1, 2).contiguous().view(size_batch, len_seq, -1)  # batch_size, len_seq, num_head * dim_hidden\n",
    "\n",
    "        seq_out = self.linear_out(w_z)  # batch_size, len_seq, dim_hidden\n",
    "        return seq_out, scores\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Simple_Positionwise_FeedforwardNet(torch.nn.Module):\n",
    "    def __init__(self, dim_hidden, dim_network=None, proba_drop=0.15):\n",
    "        super().__init__()\n",
    "\n",
    "        if dim_network is None:\n",
    "            dim_network = dim_hidden\n",
    "        # end\n",
    "\n",
    "        self.linear_1 = torch.nn.Linear(dim_hidden, dim_network)\n",
    "        self.activation_1 = torch.nn.ReLU()\n",
    "        self.dropout_1 = torch.nn.Dropout(p=proba_drop)\n",
    "        self.linear_out = torch.nn.Linear(dim_network, dim_hidden)\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, seq_in):\n",
    "        return self.linear_out(self.dropout_1(self.activation_1(self.linear_1(seq_in))))\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class Simple_NormResidual(torch.nn.Module):\n",
    "    def __init__(self, dim_hidden):\n",
    "        super().__init__()\n",
    "        self.norm = torch.nn.LayerNorm(dim_hidden)\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, origin, target):\n",
    "        return self.norm(origin + target)\n",
    "    # end\n",
    "\n",
    "# end\n",
    "\n",
    "class Simple_TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, dim_hidden):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_selfattention = Simple_SelfAttention(dim_hidden)\n",
    "        self.layer_positionwise_feedforwardnet = Simple_Positionwise_FeedforwardNet(dim_hidden)\n",
    "        self.layer_norm1 = Simple_NormResidual(dim_hidden)\n",
    "        self.layer_norm2 = Simple_NormResidual(dim_hidden)\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, seq_in, attentions):\n",
    "        seq_attention, scores_attention = self.layer_selfattention(seq_in, attentions)\n",
    "        seq_norm1 = self.layer_norm1(seq_in, seq_attention)\n",
    "        seq_feedforwardnet = self.layer_positionwise_feedforwardnet(seq_norm1)\n",
    "        seq_norm2 = self.layer_norm2(seq_norm1, seq_feedforwardnet)\n",
    "        return seq_norm2, attentions\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Simple_Bert(torch.nn.Module):\n",
    "    def __init__(self, length_seq_max, length_vocab_max, length_segment_max, dim_hidden, num_encoder=6):\n",
    "        super().__init__()\n",
    "        self.embedder = Simple_BertEmbedder(length_seq_max, length_vocab_max, length_segment_max, dim_hidden)\n",
    "        self.layers_encoder = [Simple_TransformerEncoder(dim_hidden) for i in range(num_encoder)]\n",
    "        self.dim_hidden = dim_hidden\n",
    "    # end\n",
    "\n",
    "    def forward(self, token_ids=None, masked=None, segments=None, attentions=None, positions=None, is_next=None):\n",
    "        seq_in = self.embedder(token_ids, masked, segments, attentions, positions, is_next)\n",
    "        seq_out = seq_in\n",
    "        \n",
    "        for layer_encoder in self.layers_encoder:\n",
    "            seq_out, _ = layer_encoder(seq_out, attentions)  # seq_out, scores(batch_size, num_head, len_seq, len_seq)\n",
    "        # end\n",
    "\n",
    "        return seq_out\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "689b8ea8-7ec2-45ec-ac36-414c2304e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Masked Language Model\n",
    "class Simple_BertMLMTiedDecoder(torch.nn.Module):\n",
    "    '''\n",
    "        # Seems transpose is not required when tying Embedding -> Linear\n",
    "        a = torch.nn.Embedding(3,2)\n",
    "        b = torch.nn.Linear(2,3, bias=False)\n",
    "        b.weight = a.weight\n",
    "        seq_a = torch.ones([1,1,3], dtype=torch.int64)\n",
    "        seq_b = torch.ones([1,1,2])\n",
    "\n",
    "        a(seq_a)    # this works\n",
    "        b(seq_b)    # this also work\n",
    "    '''\n",
    "\n",
    "    def __init__(self, tied_embedder: torch.nn.Embedding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_in = tied_embedder.embedding_dim\n",
    "        self.dim_out = tied_embedder.num_embeddings\n",
    "\n",
    "        self.linear_1 = torch.nn.Linear(self.dim_in, self.dim_in)\n",
    "        self.activation_1 = torch.nn.GELU()\n",
    "        self.norm_1 = torch.nn.LayerNorm(self.dim_in)\n",
    "\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(self.dim_out))\n",
    "        self.linear_decoder = torch.nn.Linear(self.dim_in, self.dim_out, bias=False)\n",
    "        self.linear_decoder.weight = tied_embedder.weight\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, seq_in, masks):  # seq_in: batch_size, length_seq, dim_hidden?\n",
    "\n",
    "        shape_seq = seq_in.shape  # batch_size, length_seq, dim_hidden\n",
    "        masks_one = masks[:, :, None].expand_as(seq_in)\n",
    "\n",
    "        seq_masked = torch.masked_select(seq_in, masks_one).view(1, -1, shape_seq[-1])  # TODO: 1.using gather? 2.remove batch concept?\n",
    "        h_masked = self.norm_1(self.activation_1(self.linear_1(seq_masked)))\n",
    "\n",
    "        return self.linear_decoder(h_masked) + self.bias  # batch_size, length_masked, dim_vocab\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for NextSentencePrediction\n",
    "class Simple_BertCLSDecoder(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out=2):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(dim_in, dim_in)\n",
    "        self.activation_1 = torch.nn.Tanh()\n",
    "        self.linear_out = torch.nn.Linear(dim_in, dim_out)\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, seq_in):\n",
    "        seq_cls = seq_in[:, 0, :]\n",
    "        h_1 = self.activation_1(self.linear_1(seq_cls))\n",
    "        return self.linear_out(h_1)\n",
    "    # end\n",
    "# end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9956903-6908-435a-8aae-2dd0af14586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.length_seq_max = 64\n",
    "        self.length_vocab_max = self.tokenizer.vocab_size\n",
    "        self.length_segment_max = 2\n",
    "\n",
    "    # end\n",
    "\n",
    "    '''\n",
    "    {\n",
    "        \"tokens_id\":[\n",
    "            101,7592,2026,2171,2003,2198,3835,2000,3113,2017,2651,2003,1037,2204,2154,2003,2025,2009,102,7592,1045,2572,5914,2034,2051,2000,2156,2017,102,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "        \"masks\":[\n",
    "            false,false,false,false,false,false,false,true,false,false,false,false,false,false,false,false,true,false,false,false,false,false,false,false,false,false,false,true,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false],\n",
    "        \"segments\":[\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "        \"attentions\":[\n",
    "            1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "        \"positions\":[\n",
    "            0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63],\n",
    "        \"is_next\": true    \n",
    "    }\n",
    "    '''\n",
    "    #TODO: more than max_length?\n",
    "    def generate_training_embedding(self, seq_a, seq_b, probs_mask=0.15, max_length=64, is_next=True):\n",
    "        tokens_a = seq_a.split()\n",
    "        tokens_b = seq_b.split()\n",
    "\n",
    "\n",
    "\n",
    "        tokens_pair = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "        indexs_mask_all = [i + 1 for i in range(len(tokens_a))] + [i + 2 + len(tokens_a) for i in range(len(tokens_b))]\n",
    "        random.shuffle(indexs_mask_all)\n",
    "        indexs_masked = indexs_mask_all[:int(len(indexs_mask_all) * probs_mask)]\n",
    "\n",
    "        len_all = len(tokens_a) + len(tokens_b) + 3\n",
    "        tokens_pad = ['[PAD]' for i in range(max_length - len_all)]\n",
    "        tokens_all = tokens_pair + tokens_pad\n",
    "\n",
    "        t_segments_all = torch.LongTensor([0 for _ in range(len(tokens_a) + 2)] + [1 for _ in range(len(tokens_b) + 1)] + [0 for _ in range(len(tokens_pad))])\n",
    "        t_attentions_all = torch.LongTensor([1 for _ in range(len(tokens_pair))] + [0 for _ in range(len(tokens_pad))])\n",
    "        t_attentions_all[indexs_masked] = 0\n",
    "        t_masks = torch.zeros(len(tokens_all), dtype=torch.bool)\n",
    "        t_masks[indexs_masked] = True\n",
    "        t_position_all = torch.LongTensor([i for i in range(len(tokens_all))])\n",
    "        t_tokens_id = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(tokens_all))\n",
    "\n",
    "        t_isnext = torch.LongTensor([is_next])\n",
    "\n",
    "        return {\n",
    "            'token_ids': t_tokens_id,\n",
    "            'masked': t_masks,\n",
    "            'segments': t_segments_all,\n",
    "            'attentions': t_attentions_all,\n",
    "            'positions': t_position_all,\n",
    "            'is_next': t_isnext\n",
    "        }\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebb67528-ae85-4dd6-8447-31424f088dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBatchMaker:\n",
    "    @classmethod\n",
    "    def make_batch(cls, list_dict_info):\n",
    "        keys_dict = list_dict_info[0].keys()\n",
    "        \n",
    "        dict_merged = {}\n",
    "        for key_dict in keys_dict:\n",
    "            target_items = [dict_info[key_dict] for dict_info in list_dict_info]\n",
    "            target_items_new = [item[None, :] for item in target_items]\n",
    "            dict_merged[key_dict] = torch.cat(target_items_new, dim=0)\n",
    "        # end\n",
    "        \n",
    "        return dict_merged\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40fd6478-da9d-4352-94f0-e00f810146d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_hidden=128\n",
    "\n",
    "# sample_1\n",
    "seq_first_1 = 'hello my name is john nice to meet you today is a good day is not it'\n",
    "seq_second_1 = 'hello i am marry first time to see you'\n",
    "is_next_1 = True\n",
    "\n",
    "# sample_2\n",
    "seq_first_2 = 'hello my name is hello kitty'\n",
    "seq_second_2 = 'today is a good day for work and i go to the office'\n",
    "is_next_2 = False\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "length_seq_max = tokenizer.length_seq_max\n",
    "length_vocab_max = tokenizer.length_vocab_max\n",
    "length_segment_max = tokenizer.length_segment_max\n",
    "\n",
    "\n",
    "sample_1 = tokenizer.generate_training_embedding(seq_first_1, seq_second_1, is_next=is_next_1)\n",
    "sample_2 = tokenizer.generate_training_embedding(seq_first_2, seq_second_2, is_next=is_next_2)\n",
    "\n",
    "samples = SimpleBatchMaker.make_batch([sample_1, sample_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ae4945f-3b8c-49cf-9e81-7c85f3b653ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_1 = Simple_Bert(length_seq_max, length_vocab_max, length_segment_max, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b8c94f7-014b-4a30-b6e0-077051ad9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBertPretrainer(torch.nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "\n",
    "        self.dim_hidden = bert.dim_hidden\n",
    "        self.head_nsp = Simple_BertCLSDecoder(self.dim_hidden)\n",
    "        self.head_mlm = Simple_BertMLMTiedDecoder(self.bert.embedder.embedding_token)\n",
    "\n",
    "        self.fn_loss_nsp = torch.nn.CrossEntropyLoss()\n",
    "        self.fn_loss_mlm = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, token_ids=None, masked=None, segments=None, attentions=None, positions=None, is_next=None):\n",
    "        seq_bert = self.bert(token_ids=token_ids, masked=masked, segments=segments, attentions=attentions, positions=positions, is_next=is_next)\n",
    "        h_mlm = self.head_mlm(seq_bert, masked).squeeze(0)   # merge batch and seq as batch = 1\n",
    "        \n",
    "        masked_ids = token_ids.masked_select(masked).to(torch.int64)\n",
    "        loss_mlm = self.fn_loss_mlm(h_mlm, masked_ids).float().mean()\n",
    "        \n",
    "        h_nsp = self.head_nsp(seq_bert)\n",
    "        loss_nsp = self.fn_loss_nsp(h_nsp, is_next.squeeze(1)).float().mean()\n",
    "        \n",
    "        loss_all = loss_nsp + loss_mlm\n",
    "        return loss_all\n",
    "    # end\n",
    "\n",
    "# end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9867fed6-c20c-4771-add3-f3c41a592e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pretrainer = SimpleBertPretrainer(bert_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d839040-9172-45e0-bddf-8ce0d44f855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(bert_pretrainer.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "optimizer.zero_grad()\n",
    "loss_all = bert_pretrainer(**samples)\n",
    "loss_all.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7349ff9e-5e32-49c7-8497-41a9c9227ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_1.state_dict(), 'bert_1.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
