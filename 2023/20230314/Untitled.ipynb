{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2210defb-7028-464e-90ef-18b116a15da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1881c14-cf31-4dc6-a134-90d38bbeb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f5258d-6fd0-4688-aad2-c1863c6a2e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_first = 'hello my name is john nice to meet you today is a good day is not it'\n",
    "seq_second = 'hello i am marry first time to see you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e593b477-3eec-45e4-9205-dd33f767f495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "a = tokenizer.encode_plus(seq_first, seq_second, add_special_tokens=True, max_length=64, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt', truncation=False)\n",
    "# a = tokenizer.encode_plus(seq_first, seq_second)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b330edf-e062-4aca-a68d-84124b5e52d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 2026, 2171, 2003, 2198, 3835, 2000, 3113, 2017, 2651, 2003,\n",
       "         1037, 2204, 2154, 2003, 2025, 2009,  102, 7592, 1045, 2572, 5914, 2034,\n",
       "         2051, 2000, 2156, 2017,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585cc455-4509-4b14-99ec-50657a967570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello my name is john nice to meet you today is a good day is not it [SEP] hello i am marry first time to see you [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(a['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c4a458-27b7-4c02-964e-3d459bfcc239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "class SimpleTokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    # end\n",
    "\n",
    "    '''\n",
    "        {\n",
    "            \"input_ids\": [],\n",
    "            \"segment_masks\": [],\n",
    "            \"position_masks\": [],\n",
    "            \"attention_masks\": []\n",
    "        }\n",
    "    '''\n",
    "    #TODO: more than max_length?\n",
    "    def generate_training_embedding(self, seq_a, seq_b, probs_mask=0.15, max_length=64):\n",
    "        tokens_a = seq_a.split()\n",
    "        tokens_b = seq_b.split()\n",
    "\n",
    "\n",
    "\n",
    "        tokens_pair = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "        indexs_mask_all = [i + 1 for i in range(len(tokens_a))] + [i + 2 + len(tokens_a) for i in range(len(tokens_b))]\n",
    "        random.shuffle(indexs_mask_all)\n",
    "        indexs_masked = indexs_mask_all[:int(len(indexs_mask_all) * probs_mask)]\n",
    "\n",
    "        len_all = len(tokens_a) + len(tokens_b) + 3\n",
    "        tokens_pad = ['[PAD]' for i in range(max_length - len_all)]\n",
    "        tokens_all = tokens_pair + tokens_pad\n",
    "\n",
    "        t_segments_all = torch.IntTensor([0 for _ in range(len(tokens_a) + 2)] + [1 for _ in range(len(tokens_b) + 1)] + [0 for _ in range(len(tokens_pad))])\n",
    "        t_attentions_all = torch.IntTensor([1 for _ in range(len(tokens_pair))] + [0 for _ in range(len(tokens_pad))])\n",
    "        t_attentions_all[indexs_masked] = 0\n",
    "        t_masks = torch.zeros(len(tokens_all), dtype=torch.bool)\n",
    "        t_masks[indexs_masked] = True\n",
    "        t_position_all = torch.IntTensor([i for i in range(len(tokens_all))])\n",
    "        t_tokens_id = self.tokenizer.convert_tokens_to_ids(tokens_all)\n",
    "\n",
    "        return {\n",
    "            'tokens_id': t_tokens_id,\n",
    "            'masks': t_masks,\n",
    "            'segments': t_segments_all,\n",
    "            'attentions': t_attentions_all,\n",
    "            'positions': t_position_all\n",
    "        }\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "764d574f-d0a6-423d-9ff4-c9654c82fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_t = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4914b786-8f93-4bf4-8357-26ac40320ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens_id': [101,\n",
       "  7592,\n",
       "  2026,\n",
       "  2171,\n",
       "  2003,\n",
       "  2198,\n",
       "  3835,\n",
       "  2000,\n",
       "  3113,\n",
       "  2017,\n",
       "  2651,\n",
       "  2003,\n",
       "  1037,\n",
       "  2204,\n",
       "  2154,\n",
       "  2003,\n",
       "  2025,\n",
       "  2009,\n",
       "  102,\n",
       "  7592,\n",
       "  1045,\n",
       "  2572,\n",
       "  5914,\n",
       "  2034,\n",
       "  2051,\n",
       "  2000,\n",
       "  2156,\n",
       "  2017,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'masks': tensor([False, False, False, False, False, False, False,  True, False, False,\n",
       "         False, False, False, False, False, False,  True, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False]),\n",
       " 'segments': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " 'attentions': tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       " 'positions': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
       "         54, 55, 56, 57, 58, 59, 60, 61, 62, 63], dtype=torch.int32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_t.generate_training_embedding(seq_first, seq_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "497fa071-7d09-4d46-aaab-88526c6871d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = torch.nn.Embedding(100,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6cd1dad-f11d-4805-aad8-e9a0f9c19843",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = torch.nn.Linear(512, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee05efdb-e906-403f-ae04-3589b5e4fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.weight = torch.nn.Parameter(embedder.weight.t())\n",
    "# decoder.weight[:] = embedder.weight.T[:]\n",
    "# decoder.weight = embedder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "507c92bc-50fb-4673-bea3-fb10c174e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.BoolTensor([True, False, True])\n",
    "b= torch.arange(24).view(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa4f1738-5a32-43f1-91d7-f4da7766152c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2d4f1f7-0d92-4dfb-b095-20bbf6aa166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_new = a[None,:,None].expand_as(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0b95314-d2e4-4dcc-b6de-41fd7d492b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.masked_select(b, a[None,:,None]).view(b.shape[0], -1, b.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f2a9d107-eaf4-4443-a00a-cda06af169a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9112fc35-3cc7-4ad0-9c5d-965d1ffa1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gather(b, 1, torch.tensor([[0],[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752c7fe8-c6d8-4e21-8077-110998cc298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf3ba77-8665-4b8f-bfdd-3e0aff726b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9730114-2808-4fd2-bbec-3b1ee0ae56a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
