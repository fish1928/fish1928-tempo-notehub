{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b45a7f3-3f4f-4c32-a3c3-9ab33382ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (/home/jovyan/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1cfbe4a-6a2b-4073-b1de-452e02eb265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "def fn_remove_special_tokens(seq_origin):\n",
    "    seq_target = re.sub(r'[^a-zA-Z0-9 ]', ' ', seq_origin)\n",
    "    seq_target = re.sub(r'\\d+', 'number', seq_target)\n",
    "    seq_target = re.sub(r' +', ' ', seq_target)\n",
    "    return seq_target.lstrip().rstrip()\n",
    "# end\n",
    "\n",
    "\n",
    "def train_test_split(index_all, rate=0.15):\n",
    "    index_all = deepcopy(index_all)\n",
    "    random.shuffle(index_all)\n",
    "    index_test, index_train = index_all[:int(len(index_all) * rate)], index_all[int(len(index_all) * rate):]\n",
    "    return index_train, index_test\n",
    "# end\n",
    "\n",
    "\n",
    "def create_random_index_isnext(len_all, rate_selected=0.5):\n",
    "    list_index = [i for i in range(len_all-1)]\n",
    "    random.shuffle(list_index)\n",
    "    list_index_isnext = list_index[:int(len(list_index) * rate_selected)]\n",
    "    pairs_target = [(i, i+1, 1) for i in list_index_isnext]\n",
    "    return pairs_target\n",
    "# end\n",
    "\n",
    "def create_random_index_notnext(len_all, rate_selected=0.5):\n",
    "\n",
    "    list_index_a = [i for i in range(len_all)][:int(len_all * rate_selected)]\n",
    "    list_index_b = [i for i in range(len_all)][:int(len_all * rate_selected)]\n",
    "\n",
    "    random.shuffle(list_index_a)\n",
    "    random.shuffle(list_index_b)\n",
    "\n",
    "    pairs_target = [(a, b, 0) for a, b in zip(list_index_a, list_index_b) if abs(a - b) > 1]\n",
    "\n",
    "    return pairs_target\n",
    "# end\n",
    "\n",
    "#   pairs_index_mixed = random.shuffle(deepcopy(pairs_index_mixed)) should be shuffled before for train/eval\n",
    "def select_pair_from_origin(dataset_train, pairs_index_mixed, size_batch=2):\n",
    "\n",
    "    index_end_mixed = len(pairs_index_mixed) - len(pairs_index_mixed) % size_batch\n",
    "    pairs_index_target = pairs_index_mixed[:index_end_mixed]\n",
    "    n_batches = int(len(pairs_index_target) / size_batch)\n",
    "    print('size_batch: {}, len_origin: {}, index_end_mixed: {}, n_batches: {}'.format(size_batch, len(pairs_index_mixed), index_end_mixed, n_batches))\n",
    "\n",
    "    for i_batches in range(n_batches):\n",
    "        index_batch_start = i_batches\n",
    "        index_batch_end = i_batches + size_batch\n",
    "        pairs_batch_current = pairs_index_target[index_batch_start:index_batch_end]\n",
    "\n",
    "        pairs_sentences = [(\n",
    "            fn_remove_special_tokens(dataset_train[pair_batch_current[0]]),\n",
    "            fn_remove_special_tokens(dataset_train[pair_batch_current[1]]),\n",
    "            pair_batch_current[-1])\n",
    "                for pair_batch_current in pairs_batch_current\n",
    "        ]\n",
    "\n",
    "        yield pairs_sentences\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9177bf49-72d6-42c8-acd7-5dff5cd1f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class SimpleBatchMaker:\n",
    "    @classmethod\n",
    "    def make_batch(cls, list_dict_info):\n",
    "        keys_dict = list_dict_info[0].keys()\n",
    "\n",
    "        dict_merged = {}\n",
    "        for key_dict in keys_dict:\n",
    "            target_items = [dict_info[key_dict] for dict_info in list_dict_info]\n",
    "            target_items_new = [item[None, :] for item in target_items]\n",
    "            dict_merged[key_dict] = torch.cat(target_items_new, dim=0)\n",
    "        # end\n",
    "\n",
    "        return dict_merged\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleTokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.length_seq_max = 64\n",
    "        self.length_vocab_max = self.tokenizer.vocab_size\n",
    "        self.length_segment_max = 2\n",
    "\n",
    "    # end\n",
    "\n",
    "    '''\n",
    "    {\n",
    "        \"tokens_id\":[\n",
    "            101,7592,2026,2171,2003,2198,3835,2000,3113,2017,2651,2003,1037,2204,2154,2003,2025,2009,102,7592,1045,2572,5914,2034,2051,2000,2156,2017,102,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "        \"masks\":[\n",
    "            false,false,false,false,false,false,false,true,false,false,false,false,false,false,false,false,true,false,false,false,false,false,false,false,false,false,false,true,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false,false],\n",
    "        \"segments\":[\n",
    "            0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "        \"attentions\":[\n",
    "            1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "        \"positions\":[\n",
    "            0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63],\n",
    "        \"is_next\": true    \n",
    "    }\n",
    "    '''\n",
    "\n",
    "    # TODO: more than max_length?\n",
    "    def generate_training_embedding(self, seq_a, seq_b, probs_mask=0.15, max_length=64, is_next=True):\n",
    "        tokens_a = seq_a.split()\n",
    "        tokens_b = seq_b.split()\n",
    "\n",
    "        tokens_pair = ['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]']\n",
    "        indexs_mask_all = [i + 1 for i in range(len(tokens_a))] + [i + 2 + len(tokens_a) for i in range(len(tokens_b))]\n",
    "        random.shuffle(indexs_mask_all)\n",
    "        indexs_masked = indexs_mask_all[:int(len(indexs_mask_all) * probs_mask)]\n",
    "\n",
    "        len_all = len(tokens_a) + len(tokens_b) + 3\n",
    "        tokens_pad = ['[PAD]' for i in range(max_length - len_all)]\n",
    "        tokens_all = tokens_pair + tokens_pad\n",
    "\n",
    "        t_segments_all = torch.IntTensor(\n",
    "            [0 for _ in range(len(tokens_a) + 2)] + [1 for _ in range(len(tokens_b) + 1)] + [0 for _ in\n",
    "                                                                                             range(len(tokens_pad))])\n",
    "        t_attentions_all = torch.IntTensor([1 for _ in range(len(tokens_pair))] + [0 for _ in range(len(tokens_pad))])\n",
    "        t_attentions_all[indexs_masked] = 0\n",
    "        t_masks = torch.zeros(len(tokens_all), dtype=torch.bool)\n",
    "        t_masks[indexs_masked] = True\n",
    "        t_position_all = torch.IntTensor([i for i in range(len(tokens_all))])\n",
    "        t_tokens_id = torch.IntTensor(self.tokenizer.convert_tokens_to_ids(tokens_all))\n",
    "\n",
    "        t_isnext = torch.LongTensor([is_next])\n",
    "\n",
    "        return {\n",
    "            'token_ids': t_tokens_id,\n",
    "            'masked': t_masks,\n",
    "            'segments': t_segments_all,\n",
    "            'attentions': t_attentions_all,\n",
    "            'positions': t_position_all,\n",
    "            'is_next': t_isnext\n",
    "        }\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2395ca13-dfb7-41c9-807b-d24abd093b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import Embedding\n",
    "\n",
    "\n",
    "class Simple_BertEmbedder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, length_seq_max, length_vocab_max, length_segment_max, dim_hidden, prob_drop=0.15):\n",
    "        super().__init__()\n",
    "        self.length_seq_max = length_seq_max\n",
    "        self.length_vocab_max = length_vocab_max\n",
    "        self.length_segment_max = length_segment_max\n",
    "        self.dim_hidden = dim_hidden\n",
    "\n",
    "        self.embedding_token = Embedding(length_vocab_max, dim_hidden)  # num_embedding, embedding_dim\n",
    "        self.embedding_position = Embedding(length_seq_max, dim_hidden)\n",
    "        self.embedding_segment = Embedding(length_segment_max, dim_hidden)\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(dim_hidden)\n",
    "        self.dropout = torch.nn.Dropout(p=prob_drop)\n",
    "\n",
    "    # end\n",
    "\n",
    "    # TODO: segment embedding 0,0,0,1,1,1,0,0,0\n",
    "    def forward(self, token_ids=None, masked=None, segments=None, attentions=None, positions=None, is_next=None):\n",
    "\n",
    "        # print(token_ids)\n",
    "        e_token = self.embedding_token(token_ids)\n",
    "        e_segment = self.embedding_segment(segments)\n",
    "        e_position = self.embedding_position(positions)\n",
    "\n",
    "        sum_embedding = self.norm(e_token + e_segment + e_position)\n",
    "        return self.dropout(sum_embedding)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class Simple_Positionwise_FeedforwardNet(torch.nn.Module):\n",
    "    def __init__(self, dim_hidden, dim_network=None, proba_drop=0.15):\n",
    "        super().__init__()\n",
    "\n",
    "        if dim_network is None:\n",
    "            dim_network = dim_hidden\n",
    "        # end\n",
    "\n",
    "        self.linear_1 = torch.nn.Linear(dim_hidden, dim_network)\n",
    "        self.activation_1 = torch.nn.ReLU()\n",
    "        self.dropout_1 = torch.nn.Dropout(p=proba_drop)\n",
    "        self.linear_out = torch.nn.Linear(dim_network, dim_hidden)\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, seq_in):\n",
    "        return self.linear_out(self.dropout_1(self.activation_1(self.linear_1(seq_in))))\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Simple_SelfAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, num_head=6):\n",
    "        super().__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.num_head = num_head\n",
    "        self.linear_Q = torch.nn.Linear(dim_hidden, dim_hidden * num_head)\n",
    "        self.linear_K = torch.nn.Linear(dim_hidden, dim_hidden * num_head)\n",
    "        self.linear_V = torch.nn.Linear(dim_hidden, dim_hidden * num_head)\n",
    "        self.linear_out = torch.nn.Linear(dim_hidden * num_head, dim_hidden)\n",
    "\n",
    "    # end\n",
    "\n",
    "    # attentions is masked already\n",
    "    def forward(self, seq_in, attentions):  # batch_size, len_seq, dim_in(dim_in = dim_embedding for first layer)\n",
    "        size_batch, len_seq, dim_in = seq_in.shape\n",
    "        dim_hidden = self.dim_hidden\n",
    "\n",
    "        w_q = self.linear_Q(seq_in)\n",
    "        w_k = self.linear_K(seq_in)\n",
    "        w_v = self.linear_V(seq_in)\n",
    "\n",
    "        q = w_q.view(size_batch, len_seq, -1, dim_hidden).transpose(1, 2)\n",
    "        k = w_k.view(size_batch, len_seq, -1, dim_hidden).transpose(1, 2)\n",
    "        v = w_v.view(size_batch, len_seq, -1, dim_hidden).transpose(1, 2)\n",
    "\n",
    "        w_qk = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(dim_hidden)  # batch_size, num_head, len_seq, len_seq\n",
    "\n",
    "        # from neptune.ai: attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        # test code:\n",
    "        # attentions = torch.from_numpy(np.array([[1,0,1,0],[0,0,1,1]]))\n",
    "        # w_all = torch.ones([2,3,4,4])\n",
    "        # attentions_all = attentions[:,None,None,:].expand_as(w_all) -> repeat automatically\n",
    "        attentions_all = attentions[:, None, None, :].expand_as(w_qk)\n",
    "\n",
    "\n",
    "        masks_all = (1 - attentions_all) * -1e10\n",
    "        scores_raw = w_qk + masks_all\n",
    "        scores = torch.nn.functional.softmax(scores_raw, dim=-1)  # batch_size, num_head, len_seq, len_seq\n",
    "\n",
    "        z = torch.matmul(scores, v)  # batch_size, num_head, len_seq, dim_hidden\n",
    "        w_z = z.transpose(1, 2).contiguous().view(size_batch, len_seq, -1)  # batch_size, len_seq, num_head * dim_hidden\n",
    "\n",
    "        seq_out = self.linear_out(w_z)  # batch_size, len_seq, dim_hidden\n",
    "        return seq_out, scores\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class Simple_NormResidual(torch.nn.Module):\n",
    "    def __init__(self, dim_hidden):\n",
    "        super().__init__()\n",
    "        self.norm = torch.nn.LayerNorm(dim_hidden)\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, origin, target):\n",
    "        return self.norm(origin + target)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class Simple_TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, dim_hidden, num_head=6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_selfattention = Simple_SelfAttention(dim_hidden, num_head)\n",
    "        self.layer_positionwise_feedforwardnet = Simple_Positionwise_FeedforwardNet(dim_hidden, dim_hidden * num_head)\n",
    "        self.layer_norm1 = Simple_NormResidual(dim_hidden)\n",
    "        self.layer_norm2 = Simple_NormResidual(dim_hidden)\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, seq_in, attentions):\n",
    "        seq_attention, scores_attention = self.layer_selfattention(seq_in, attentions)\n",
    "        seq_norm1 = self.layer_norm1(seq_in, seq_attention)\n",
    "        seq_feedforwardnet = self.layer_positionwise_feedforwardnet(seq_norm1)\n",
    "        seq_norm2 = self.layer_norm2(seq_norm1, seq_feedforwardnet)\n",
    "        return seq_norm2, attentions\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class Simple_Bert(torch.nn.Module):\n",
    "    def __init__(self, length_seq_max, length_vocab_max, length_segment_max, dim_hidden, num_encoder=6, num_head=6):\n",
    "        super().__init__()\n",
    "        self.embedder = Simple_BertEmbedder(length_seq_max, length_vocab_max, length_segment_max, dim_hidden)\n",
    "        self.layers_encoder = [Simple_TransformerEncoder(dim_hidden, num_head) for i in range(num_encoder)]\n",
    "        self.dim_hidden = dim_hidden\n",
    "    # end\n",
    "\n",
    "    def forward(self, token_ids=None, masked=None, segments=None, attentions=None, positions=None, is_next=None):\n",
    "        seq_in = self.embedder(token_ids, masked, segments, attentions, positions, is_next)\n",
    "        seq_out = seq_in\n",
    "\n",
    "        for layer_encoder in self.layers_encoder:\n",
    "            seq_out, _ = layer_encoder(seq_out, attentions)  # seq_out, scores(batch_size, num_head, len_seq, len_seq)\n",
    "        # end\n",
    "\n",
    "        return seq_out\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5cbed40-67c8-4216-8f83-e7af491ee272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_BertCLSDecoder(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out=2):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(dim_in, dim_in)\n",
    "        self.activation_1 = torch.nn.Tanh()\n",
    "        self.linear_out = torch.nn.Linear(dim_in, dim_out)\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, seq_in):\n",
    "        seq_cls = seq_in[:, 0, :]\n",
    "        h_1 = self.activation_1(self.linear_1(seq_cls))\n",
    "        return self.linear_out(h_1)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class Simple_BertMLMTiedDecoder(torch.nn.Module):\n",
    "    '''\n",
    "        # Seems transpose is not required when tying Embedding -> Linear\n",
    "        a = torch.nn.Embedding(3,2)\n",
    "        b = torch.nn.Linear(2,3, bias=False)\n",
    "        b.weight = a.weight\n",
    "        seq_a = torch.ones([1,1,3], dtype=torch.int64)\n",
    "        seq_b = torch.ones([1,1,2])\n",
    "\n",
    "        a(seq_a)    # this works\n",
    "        b(seq_b)    # this also work\n",
    "    '''\n",
    "\n",
    "    def __init__(self, tied_embedder: torch.nn.Embedding):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_in = tied_embedder.embedding_dim\n",
    "        self.dim_out = tied_embedder.num_embeddings\n",
    "\n",
    "        self.linear_1 = torch.nn.Linear(self.dim_in, self.dim_in)\n",
    "        self.activation_1 = torch.nn.GELU()\n",
    "        self.norm_1 = torch.nn.LayerNorm(self.dim_in)\n",
    "\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(self.dim_out))\n",
    "        self.linear_decoder = torch.nn.Linear(self.dim_in, self.dim_out, bias=False)\n",
    "        self.linear_decoder.weight = tied_embedder.weight\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, seq_in, masks):  # seq_in: batch_size, length_seq, dim_hidden?\n",
    "\n",
    "        shape_seq = seq_in.shape  # batch_size, length_seq, dim_hidden\n",
    "        masks_one = masks[:, :, None].expand_as(seq_in)\n",
    "\n",
    "        seq_masked = torch.masked_select(seq_in, masks_one).view(1, -1, shape_seq[-1])  # TODO: 1.using gather? 2.remove batch concept?\n",
    "        h_masked = self.norm_1(self.activation_1(self.linear_1(seq_masked)))\n",
    "\n",
    "        return self.linear_decoder(h_masked) + self.bias  # batch_size, length_masked, dim_vocab\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class Simple_BertPretrainer(torch.nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "\n",
    "        self.dim_hidden = bert.dim_hidden\n",
    "        self.head_nsp = Simple_BertCLSDecoder(self.dim_hidden)\n",
    "        self.head_mlm = Simple_BertMLMTiedDecoder(self.bert.embedder.embedding_token)\n",
    "\n",
    "        self.fn_loss_nsp = torch.nn.CrossEntropyLoss()\n",
    "        self.fn_loss_mlm = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    #        self.optimizer = torch.optim.Adam(lr=1e-4, betas=(0.9, 0.999), weight_decay=0.01, warmup_steps=10000)\n",
    "    #        self.optimizer = torch.optim.Adam(lr=1e-4, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "    # end\n",
    "\n",
    "    def forward(self, token_ids=None, masked=None, segments=None, attentions=None, positions=None, is_next=None):\n",
    "        seq_bert = self.bert(token_ids=token_ids, masked=masked, segments=segments, attentions=attentions,\n",
    "                             positions=positions, is_next=is_next)\n",
    "        h_mlm = self.head_mlm(seq_bert, masked).squeeze(0)  # merge batch and seq as batch = 1\n",
    "\n",
    "        masked_ids = token_ids.masked_select(masked).to(torch.int64)\n",
    "        loss_mlm = self.fn_loss_mlm(h_mlm, masked_ids).float().mean()\n",
    "\n",
    "        h_nsp = self.head_nsp(seq_bert)\n",
    "        loss_nsp = self.fn_loss_nsp(h_nsp, is_next.squeeze(1)).float().mean()\n",
    "\n",
    "        loss_all = loss_nsp + loss_mlm\n",
    "        return loss_all\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28fead73-3bb4-4fa4-8bb5-303306da633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_OptimizerHelper:\n",
    "\n",
    "    def __init__(self, dim_hidden, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self._rate = 0\n",
    "    # end\n",
    "\n",
    "    @classmethod\n",
    "    def rate(cls, factor, dim_hidden, warmup, step):\n",
    "        # https://nlp.seas.harvard.edu/2018/04/03/attention.html#batches-and-masking\n",
    "        return factor * (dim_hidden ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5)))\n",
    "    # end\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = Simple_OptimizerHelper.rate(self.factor, self.dim_hidden, self.warmup, self._step)\n",
    "        for param_group in self.optimizer.param_groups: # for all layers\n",
    "            param_group['lr'] = rate\n",
    "        # end\n",
    "\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "    # end\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0534d69-24c3-4ef0-b76b-d6f8ab1828de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (/home/jovyan/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    }
   ],
   "source": [
    "# dataset initialize\n",
    "probs_mask = 0.15\n",
    "max_length = 64\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a6d096a-9ae2-482c-94fe-674d4be4376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_all = bookcorpus[:200]['text']\n",
    "index_all_mixed = create_random_index_isnext(len(corpus_train_all)) + create_random_index_notnext(len(corpus_train_all))\n",
    "random.shuffle(index_all_mixed)\n",
    "\n",
    "index_train, index_test = train_test_split(index_all_mixed)\n",
    "tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe02984c-6fce-4a32-a8f6-5c29aca5d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model initialize\n",
    "length_seq_max = tokenizer.length_seq_max\n",
    "length_vocab_max = tokenizer.length_vocab_max\n",
    "length_segment_max = tokenizer.length_segment_max\n",
    "dim_hidden = 256\n",
    "num_encoder = 6\n",
    "num_head = 6\n",
    "\n",
    "lr_default = 1e-4\n",
    "betas_default = (0.9, 0.999)\n",
    "warmup_steps_default = 1000\n",
    "weight_decay_default = 0.01\n",
    "\n",
    "factor_default = 1\n",
    "\n",
    "bert_test = Simple_Bert(length_seq_max, length_vocab_max, length_segment_max, dim_hidden, num_encoder, num_head)\n",
    "pretrain_test = Simple_BertPretrainer(bert_test)\n",
    "optimizer_test = torch.optim.Adam(pretrain_test.parameters(), lr=lr_default, betas=betas_default, weight_decay=weight_decay_default)\n",
    "helper_optimizer_test = Simple_OptimizerHelper(factor_default, dim_hidden, warmup_steps_default, optimizer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9ec4967-a92d-4e84-9726-f07e6366b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(bert_pretrainer.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "# optimizer.zero_grad()\n",
    "# loss_all = bert_pretrainer(**samples)\n",
    "# loss_all.backward()\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff922953-155b-4f10-838b-af5cba08a1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_batch: 5, len_origin: 169, index_end_mixed: 165, n_batches: 33\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n",
      "one round finish\n"
     ]
    }
   ],
   "source": [
    "# train phase\n",
    "for corpus_train_batch in select_pair_from_origin(corpus_train_all, deepcopy(index_train), 5):\n",
    "    list_info_embedding_batch = []\n",
    "\n",
    "    for seq_a, seq_b, is_next in corpus_train_batch:\n",
    "        info_embedding_one = tokenizer.generate_training_embedding(seq_a, seq_b, probs_mask=probs_mask, max_length=max_length)\n",
    "        list_info_embedding_batch.append(info_embedding_one)\n",
    "    # end\n",
    "\n",
    "    info_embedding_batch = SimpleBatchMaker.make_batch(list_info_embedding_batch)\n",
    "    helper_optimizer_test.zero_grad()\n",
    "    loss_current = pretrain_test(**info_embedding_batch)\n",
    "    loss_current.backward()\n",
    "    helper_optimizer_test.step()\n",
    "    print('one round finish')\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669e784-91c8-4728-b6ea-d9efacd376bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
