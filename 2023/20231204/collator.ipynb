{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312cc29-34d0-46c1-9810-0591f4a3ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    DEVICE = 'cuda'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if v is not None and type(v) is not bool:\n",
    "                self.kwargs[k] = v.to(Batch.DEVICE)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.kwargs\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class Collator_Base:\n",
    "\n",
    "    def __init__(self, tokenizer, size_seq_max, need_masked=0.3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size_seq_max = size_seq_max\n",
    "        self.need_masked = need_masked\n",
    "\n",
    "        index_special_token_2_id = {k: v for k, v in zip(tokenizer.all_special_tokens, tokenizer.all_special_ids)}\n",
    "\n",
    "        self.id_pad = index_special_token_2_id['[PAD]']\n",
    "        self.id_mask = index_special_token_2_id['[MASK]']\n",
    "        self.id_cls = index_special_token_2_id['[CLS]']\n",
    "        self.id_sep = index_special_token_2_id['[SEP]']\n",
    "        self.id_unk = index_special_token_2_id['[UNK]']\n",
    "\n",
    "        self.regex_special_token = re.compile(r'\\[(PAD|MASK|CLS|SEP|EOL|UNK)\\]')\n",
    "    # end\n",
    "\n",
    "    def _preprocess(self, line):\n",
    "        line = re.sub(self.regex_special_token, r'<\\1>', line)\n",
    "        line = re.sub(r'''('|\"|`){2}''', '', line)\n",
    "        line = re.sub(r'\\.{2,3}', '', line)\n",
    "        line = re.sub(r' {2,}', ' ', line)\n",
    "        line = line.lstrip().rstrip()\n",
    "        return line\n",
    "    # end\n",
    "\n",
    "    # return masks_attention?, return masks_segment?\n",
    "    def pad_sequences(self, sequences, size_seq_max, need_diagonal=False,\n",
    "                      need_masked=0):  # need_diagonal and need_masked cannot both set, one for bert seq one for s2s seq\n",
    "        id_pad = self.id_pad\n",
    "        id_mask = self.id_mask\n",
    "\n",
    "        sequences_padded = []\n",
    "        sequences_masked_padded = []\n",
    "\n",
    "        for sequence in sequences:\n",
    "            len_seq = len(sequence)\n",
    "\n",
    "            count_pad = size_seq_max - len_seq\n",
    "\n",
    "            sequence = torch.LongTensor(sequence)\n",
    "            sequence_padded = torch.cat((sequence, torch.LongTensor([id_pad] * count_pad)))\n",
    "            sequences_padded.append(sequence_padded)\n",
    "\n",
    "            if need_masked:\n",
    "                index_masked = list(range(1, len_seq - 1))\n",
    "                random.shuffle(index_masked)\n",
    "                anchor_mask = int(need_masked * (len_seq - 2)) or 1\n",
    "                index_masked = torch.LongTensor(index_masked[:anchor_mask])\n",
    "                # index_masked = torch.LongTensor(index_masked[:int(need_masked * (len_seq-2))])\n",
    "\n",
    "                sequence_masked = sequence.detach().clone()\n",
    "                sequence_masked.index_fill_(0, index_masked, id_mask)\n",
    "                sequence_masked_padded = torch.cat((sequence_masked, torch.LongTensor([id_pad] * count_pad)))\n",
    "\n",
    "                sequences_masked_padded.append(sequence_masked_padded)\n",
    "            # end\n",
    "        #   # end for\n",
    "\n",
    "        inputs = torch.stack(sequences_padded)  # (batch, size_seq_max)\n",
    "        if need_masked:\n",
    "            inputs_masked_padded = torch.stack(sequences_masked_padded)\n",
    "        # end\n",
    "\n",
    "        masks_segment = (inputs != self.id_pad).unsqueeze(-2)  # (nbatch, 1, seq)\n",
    "        masks_attention = self.make_std_mask(inputs, self.id_pad) if need_diagonal else masks_segment\n",
    "\n",
    "        if need_masked:\n",
    "            masks_masked = (inputs_masked_padded != id_mask).unsqueeze(-2)\n",
    "            masks_attention = masks_attention & masks_masked\n",
    "            return inputs_masked_padded, masks_attention, masks_segment, inputs  # (inputs, masks_attention, masks_segment, labels)\n",
    "        else:\n",
    "            return inputs, masks_attention, masks_segment, None\n",
    "        # end\n",
    "\n",
    "    # end\n",
    "\n",
    "    def subsequent_mask(self, size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "            torch.uint8\n",
    "        )\n",
    "        return subsequent_mask == 0\n",
    "\n",
    "    def make_std_mask(self, tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & self.subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Collator_SC(Collator_Base):\n",
    "\n",
    "    def __call__(self, list_corpus_source):\n",
    "\n",
    "        tokens_input_encoder = []\n",
    "        tokens_input_decoder = []\n",
    "        tokens_label_decoder = []\n",
    "        labels_similarity = []\n",
    "        labels_sc = []\n",
    "\n",
    "        for corpus_source in list_corpus_source:  # (line0, line1, sim), output of zip remove single case\n",
    "            if len(corpus_source) == 3:  # (line0, line1, sim)\n",
    "                corpus_line = [corpus_source[0], corpus_source[1]]\n",
    "                labels_similarity.append(corpus_source[2])\n",
    "            elif len(corpus_source) == 2:  # (line, label_sc)\n",
    "                corpus_line = [corpus_source[0]]\n",
    "                labels_sc.append(corpus_source[1])\n",
    "            else:\n",
    "                corpus_line = [corpus_source[0]]\n",
    "            # end\n",
    "\n",
    "            for line in corpus_line:\n",
    "                tokens = self.tokenizer.encode(self._preprocess(line), add_special_tokens=False)\n",
    "\n",
    "                # TODO: check edge\n",
    "                if len(tokens) > self.size_seq_max - 2:\n",
    "                    tokens = tokens[:self.size_seq_max - 2]\n",
    "                # end\n",
    "\n",
    "                tokens_input_encoder.append([self.id_cls] + tokens + [self.id_sep])\n",
    "                tokens_input_decoder.append([self.id_cls] + tokens)\n",
    "                tokens_label_decoder.append(tokens + [self.id_sep])\n",
    "            # end\n",
    "\n",
    "        # end\n",
    "\n",
    "        inputs_encoder, masks_encoder, segments_encoder, labels_encoder = self.pad_sequences(tokens_input_encoder,\n",
    "                                                                                             self.size_seq_max,\n",
    "                                                                                             need_masked=self.need_masked)\n",
    "        inputs_decoder, masks_decoder, segments_decoder, _ = self.pad_sequences(tokens_input_decoder, self.size_seq_max,\n",
    "                                                                                need_diagonal=True)\n",
    "        labels_decoder, masks_label, segments_label, _ = self.pad_sequences(tokens_label_decoder, self.size_seq_max)\n",
    "        # labels_similarity = torch.Tensor(labels_similarity).unsqueeze(0).transpose(0,1)\n",
    "        labels_similarity = torch.Tensor(labels_similarity)\n",
    "        labels_sc = torch.LongTensor(labels_sc)\n",
    "\n",
    "        return Batch(\n",
    "            ids_encoder=inputs_encoder,  # contains [mask]s\n",
    "            masks_encoder=masks_encoder,\n",
    "            labels_encoder=labels_encoder,  # doesn't contain [mask]\n",
    "            segments_encoder=segments_encoder,\n",
    "            ids_decoder=inputs_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            labels_decoder=labels_decoder,\n",
    "            segments_label=segments_label,\n",
    "            labels_similarity=labels_similarity,\n",
    "            labels_sc=labels_sc\n",
    "        )\n",
    "\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Collator_BERT:\n",
    "\n",
    "    def __call__(self, list_sequence_batch):\n",
    "        list_sequence_batch = [self._preprocess(sequence) for sequence in list_sequence_batch]  # remove special tokens\n",
    "\n",
    "        list_sequence_tokenized = self.tokenizer.batch_encode_plus(list_sequence_batch, add_special_tokens=False)[\n",
    "            'input_ids']\n",
    "\n",
    "        # Process I.\n",
    "        list_list_tokenized = []\n",
    "\n",
    "        # batch initialized condition\n",
    "        list_tokenized_cache = []\n",
    "        len_tokenized_accumulated = 2  # add cls and sep\n",
    "\n",
    "        while list_sequence_tokenized:\n",
    "            tokenized_poped = list_sequence_tokenized.pop(0)\n",
    "            len_tokenized_current = len(tokenized_poped)\n",
    "\n",
    "            if len_tokenized_accumulated + len_tokenized_current > self.size_seq_max:\n",
    "                if list_tokenized_cache:\n",
    "                    list_list_tokenized.append(list_tokenized_cache)\n",
    "\n",
    "                    # clear\n",
    "                    list_tokenized_cache = []\n",
    "                    len_tokenized_accumulated = 2\n",
    "                # end\n",
    "            # end\n",
    "\n",
    "            list_tokenized_cache.append(tokenized_poped)\n",
    "            len_tokenized_accumulated += len_tokenized_current\n",
    "        # end\n",
    "\n",
    "        list_list_tokenized.append(list_tokenized_cache)\n",
    "\n",
    "        # Process II. Merge list_tokenized\n",
    "        list_tokenized_merged = []\n",
    "\n",
    "        for list_tokenized in list_list_tokenized:\n",
    "            # tokenized_merged = [token for tokenized_padded in [tokenized + [self.id_eol] for tokenized in list_tokenized] for token in tokenized_padded]\n",
    "            tokenized_merged = [token for tokenized in list_tokenized for token in tokenized][:self.size_seq_max - 2]\n",
    "            list_tokenized_merged.append(tokenized_merged)\n",
    "        # end\n",
    "\n",
    "        # Process III. Add begin and stop special token, same as jinyuj_transformers_quora.ipynb\n",
    "        tokens_input_encoder = []\n",
    "        tokens_input_decoder = []\n",
    "        tokens_label_decoder = []\n",
    "\n",
    "        for tokenized_merged in list_tokenized_merged:\n",
    "            tokens_input_encoder.append([self.id_cls] + tokenized_merged + [self.id_sep])\n",
    "            tokens_input_decoder.append([self.id_cls] + tokenized_merged)\n",
    "            tokens_label_decoder.append(tokenized_merged + [self.id_sep])\n",
    "        # end\n",
    "\n",
    "        inputs_encoder, masks_encoder, segments_encoder, labels_encoder = self.pad_sequences(tokens_input_encoder,\n",
    "                                                                                             self.size_seq_max,\n",
    "                                                                                             need_masked=self.need_masked)\n",
    "        inputs_decoder, masks_decoder, segments_decoder, _ = self.pad_sequences(tokens_input_decoder, self.size_seq_max,\n",
    "                                                                                need_diagonal=True)\n",
    "        labels_decoder, masks_label, segments_label, _ = self.pad_sequences(tokens_label_decoder, self.size_seq_max)\n",
    "\n",
    "        return Batch(\n",
    "            ids_encoder=inputs_encoder,  # contains [mask]s\n",
    "            masks_encoder=masks_encoder,\n",
    "            labels_encoder=labels_encoder,  # doesn't contain [mask]\n",
    "            segments_encoder=segments_encoder,\n",
    "            ids_decoder=inputs_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            labels_decoder=labels_decoder,\n",
    "            segments_label=segments_label\n",
    "        )\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Collator_BERT_MIXED(Collator_Base):\n",
    "\n",
    "    def __call__(self, list_sequence_batch):\n",
    "        list_sequence_batch = [self._preprocess(sequence) for sequence in list_sequence_batch]  # remove special tokens\n",
    "\n",
    "        list_sequence_tokenized = self.tokenizer.batch_encode_plus(list_sequence_batch, add_special_tokens=False)[\n",
    "            'input_ids']\n",
    "\n",
    "        # Process I.\n",
    "        list_list_tokenized = []\n",
    "\n",
    "        # batch initialized condition\n",
    "        list_tokenized_cache = []\n",
    "        len_tokenized_accumulated = 2  # add cls and sep\n",
    "\n",
    "        # while list_sequence_tokenized:\n",
    "        for tokenized_top in list_sequence_tokenized:\n",
    "            len_tokenized_current = len(tokenized_top)\n",
    "\n",
    "            if len_tokenized_accumulated + len_tokenized_current > self.size_seq_max:\n",
    "                if list_tokenized_cache:\n",
    "                    list_list_tokenized.append(list_tokenized_cache)\n",
    "\n",
    "                    # clear\n",
    "                    list_tokenized_cache = []\n",
    "                    len_tokenized_accumulated = 2\n",
    "                # end\n",
    "            # end\n",
    "\n",
    "            list_tokenized_cache.append(tokenized_top)\n",
    "            len_tokenized_accumulated += len_tokenized_current\n",
    "        # end\n",
    "\n",
    "        list_list_tokenized.append(list_tokenized_cache)\n",
    "\n",
    "        # Process II. Merge list_tokenized\n",
    "        list_tokenized_merged = []\n",
    "\n",
    "        for list_tokenized in list_list_tokenized:\n",
    "            # tokenized_merged = [token for tokenized_padded in [tokenized + [self.id_eol] for tokenized in list_tokenized] for token in tokenized_padded]\n",
    "            tokenized_merged = [token for tokenized in list_tokenized for token in tokenized][:self.size_seq_max - 2]\n",
    "            list_tokenized_merged.append(tokenized_merged)\n",
    "        # end\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Process III. Mix origin sentence if merged\n",
    "        len_mixed = len(list_tokenized_merged)\n",
    "        if len_mixed >= 1 and len(list_tokenized_merged[0]) > len(list_sequence_tokenized[0]):\n",
    "            indexs_origin = list(range(len(list_sequence_tokenized)))\n",
    "            random.shuffle(indexs_origin)\n",
    "            indexs_mixed = indexs_origin[:len_mixed]\n",
    "            list_tokenized_merged += [list_sequence_tokenized[index_mixed] for index_mixed in indexs_mixed]\n",
    "        # end\n",
    "        \n",
    "\n",
    "        # Process IV. Add begin and stop special token, same as jinyuj_transformers_quora.ipynb\n",
    "        tokens_input_encoder = []\n",
    "        tokens_input_decoder = []\n",
    "        tokens_label_decoder = []\n",
    "\n",
    "        for tokenized_merged in list_tokenized_merged:\n",
    "            tokens_input_encoder.append([self.id_cls] + tokenized_merged + [self.id_sep])\n",
    "            tokens_input_decoder.append([self.id_cls] + tokenized_merged)\n",
    "            tokens_label_decoder.append(tokenized_merged + [self.id_sep])\n",
    "        # end\n",
    "\n",
    "        inputs_encoder, masks_encoder, segments_encoder, labels_encoder = self.pad_sequences(tokens_input_encoder,\n",
    "                                                                                             self.size_seq_max,\n",
    "                                                                                             need_masked=self.need_masked)\n",
    "        inputs_decoder, masks_decoder, segments_decoder, _ = self.pad_sequences(tokens_input_decoder, self.size_seq_max,\n",
    "                                                                                need_diagonal=True)\n",
    "        labels_decoder, masks_label, segments_label, _ = self.pad_sequences(tokens_label_decoder, self.size_seq_max)\n",
    "\n",
    "        return Batch(\n",
    "            ids_encoder=inputs_encoder,  # contains [mask]s\n",
    "            masks_encoder=masks_encoder,\n",
    "            labels_encoder=labels_encoder,  # doesn't contain [mask]\n",
    "            segments_encoder=segments_encoder,\n",
    "            ids_decoder=inputs_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            labels_decoder=labels_decoder,\n",
    "            segments_label=segments_label\n",
    "        )\n",
    "    # end\n",
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
