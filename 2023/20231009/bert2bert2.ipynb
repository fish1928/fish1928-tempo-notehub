{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02768afb-2cee-429f-a555-f0bb64ed694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://discuss.huggingface.co/t/issues-with-fine-tuning-an-encoder-decoder-model/48880\n",
    "\n",
    "from transformers import EncoderDecoderConfig, EncoderDecoderModel, BertTokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c377bad6-d9d6-4961-8694-0c8371f383a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "encoder_max_length=128\n",
    "decoder_max_length=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bea6423-6df0-4658-b329-1a7d48c7e4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16779a9-b03c-4106-88d0-09133fb33634",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"bookcorpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1cda57d-5a59-4f48-8568-d0593f474488",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.decoder.decoder_start_token_id = tokenizer.cls_token_id\n",
    "# model.config.bos_token_id = tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2742c3c4-1117-4899-a5ab-474d4fffcf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.vocab_size = tokenizer.vocab_size\n",
    "model.config.max_length = 16\n",
    "model.config.min_length = 4\n",
    "model.config.no_repeat_ngram_size = 1\n",
    "model.config.early_stopping = True\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee083dba-1b8e-45d2-9921-8c7c804f5b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595bcefd-3458-462b-b956-b882ebc8f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_data_to_model_inputs(batch):\n",
    "#     inputs = tokenizer(batch[\"title_s_article_s\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "#     outputs = tokenizer(batch[\"highlight\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "#     batch[\"input_ids\"] = inputs.input_ids\n",
    "#     batch[\"attention_mask\"] = inputs.attention_mask\n",
    "#     batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "#     batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "#     batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "#     # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`.\n",
    "#     # We have to make sure that the PAD token is ignored\n",
    "#     batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "#     return batch\n",
    "# # end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0793a1-b526-4188-ba19-16a6518e4ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    inputs = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "    # outputs = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = inputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"labels\"] = inputs.input_ids.copy()\n",
    "\n",
    "    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`.\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "    return batch\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32187ae2-0dd1-418e-b3e3-f6b053e9d38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a95b26a4df4f10903a5b878ecce538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_data = dataset[\"train\"].select(range(int(len(dataset['train'])/2)))\n",
    "\n",
    "train_data = dataset[\"train\"].select(range(72000))\n",
    "# train_data = dataset[\"train\"]\n",
    "\n",
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ef5d5ec-0d0b-4421-88d4-462adf5e7d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2737901f914082813b791a764592f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_data = dataset[\"train\"].select(range(72000, 75000))\n",
    "# val_data = dataset[\"train\"].select(range(int(len(dataset['train'])/10*9)),len(dataset['train']))\n",
    "\n",
    "val_data = val_data.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=['text']\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "099b1b6e-60bb-418b-a2ea-6bc8caa08ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    eval_steps=100,\n",
    "    warmup_steps=0,\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs = 12,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    lr_scheduler_type = \"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d36b298e-1c83-4f1e-8281-ddf06e86f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    # print('START')\n",
    "    # for x,y in zip(pred_str[:10],label_str[:10]):\n",
    "    #     print('PRED: ',x,\"LABEL: \",y)\n",
    "    # print('END')\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred_str,\n",
    "        references=label_str\n",
    "    )\n",
    "    return rouge_output\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25aa5b8d-da11-4e70-bcbf-a015a4ccc73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "482d0dcd-5531-457c-b813-256c1a963d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 72000\n",
      "  Num Epochs = 12\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 108000\n",
      "Trainer is attempting to log a value of \"{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'architectures': ['BertForMaskedLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': 0, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'bert-base-uncased', 'transformers_version': '4.12.0', 'gradient_checkpointing': False, 'vocab_size': 30522, 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'hidden_act': 'gelu', 'intermediate_size': 3072, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'type_vocab_size': 2, 'initializer_range': 0.02, 'layer_norm_eps': 1e-12, 'position_embedding_type': 'absolute', 'use_cache': True, 'classifier_dropout': None, 'model_type': 'bert'}\" for key \"encoder\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': True, 'cross_attention_hidden_size': None, 'add_cross_attention': True, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'architectures': ['BertForMaskedLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': 0, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': 101, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'bert-base-uncased', 'transformers_version': '4.12.0', 'gradient_checkpointing': False, 'vocab_size': 30522, 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'hidden_act': 'gelu', 'intermediate_size': 3072, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'type_vocab_size': 2, 'initializer_range': 0.02, 'layer_norm_eps': 1e-12, 'position_embedding_type': 'absolute', 'use_cache': True, 'classifier_dropout': None, 'model_type': 'bert'}\" for key \"decoder\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108000' max='108000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108000/108000 6:30:00, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010402</td>\n",
       "      <td>0.030519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028822</td>\n",
       "      <td>0.028861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>0.020017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019950</td>\n",
       "      <td>0.019981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.023511</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.023412</td>\n",
       "      <td>0.023363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>0.029937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028753</td>\n",
       "      <td>0.028709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008620</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015109</td>\n",
       "      <td>0.054362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052278</td>\n",
       "      <td>0.052245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009316</td>\n",
       "      <td>0.067935</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.060481</td>\n",
       "      <td>0.060513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007644</td>\n",
       "      <td>0.078172</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.067848</td>\n",
       "      <td>0.067829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>0.049220</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.043761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>0.055745</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.048472</td>\n",
       "      <td>0.048426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/checkpoint-1000\n",
      "Configuration saved in outputs/checkpoint-1000/config.json\n",
      "Model weights saved in outputs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-2000\n",
      "Configuration saved in outputs/checkpoint-2000/config.json\n",
      "Model weights saved in outputs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-1000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-3000\n",
      "Configuration saved in outputs/checkpoint-3000/config.json\n",
      "Model weights saved in outputs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-2000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-4000\n",
      "Configuration saved in outputs/checkpoint-4000/config.json\n",
      "Model weights saved in outputs/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-3000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-5000\n",
      "Configuration saved in outputs/checkpoint-5000/config.json\n",
      "Model weights saved in outputs/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-4000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-6000\n",
      "Configuration saved in outputs/checkpoint-6000/config.json\n",
      "Model weights saved in outputs/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-5000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-7000\n",
      "Configuration saved in outputs/checkpoint-7000/config.json\n",
      "Model weights saved in outputs/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-6000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-8000\n",
      "Configuration saved in outputs/checkpoint-8000/config.json\n",
      "Model weights saved in outputs/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-7000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-9000\n",
      "Configuration saved in outputs/checkpoint-9000/config.json\n",
      "Model weights saved in outputs/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-8000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "Saving model checkpoint to outputs/checkpoint-10000\n",
      "Configuration saved in outputs/checkpoint-10000/config.json\n",
      "Model weights saved in outputs/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-9000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-11000\n",
      "Configuration saved in outputs/checkpoint-11000/config.json\n",
      "Model weights saved in outputs/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-10000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-12000\n",
      "Configuration saved in outputs/checkpoint-12000/config.json\n",
      "Model weights saved in outputs/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-11000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-13000\n",
      "Configuration saved in outputs/checkpoint-13000/config.json\n",
      "Model weights saved in outputs/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-12000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-14000\n",
      "Configuration saved in outputs/checkpoint-14000/config.json\n",
      "Model weights saved in outputs/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-13000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-15000\n",
      "Configuration saved in outputs/checkpoint-15000/config.json\n",
      "Model weights saved in outputs/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-14000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-16000\n",
      "Configuration saved in outputs/checkpoint-16000/config.json\n",
      "Model weights saved in outputs/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-15000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-17000\n",
      "Configuration saved in outputs/checkpoint-17000/config.json\n",
      "Model weights saved in outputs/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-16000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-18000\n",
      "Configuration saved in outputs/checkpoint-18000/config.json\n",
      "Model weights saved in outputs/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-17000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "Saving model checkpoint to outputs/checkpoint-19000\n",
      "Configuration saved in outputs/checkpoint-19000/config.json\n",
      "Model weights saved in outputs/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-18000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-20000\n",
      "Configuration saved in outputs/checkpoint-20000/config.json\n",
      "Model weights saved in outputs/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-19000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-21000\n",
      "Configuration saved in outputs/checkpoint-21000/config.json\n",
      "Model weights saved in outputs/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-20000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-22000\n",
      "Configuration saved in outputs/checkpoint-22000/config.json\n",
      "Model weights saved in outputs/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-21000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-23000\n",
      "Configuration saved in outputs/checkpoint-23000/config.json\n",
      "Model weights saved in outputs/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-22000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-24000\n",
      "Configuration saved in outputs/checkpoint-24000/config.json\n",
      "Model weights saved in outputs/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-23000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-25000\n",
      "Configuration saved in outputs/checkpoint-25000/config.json\n",
      "Model weights saved in outputs/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-24000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-26000\n",
      "Configuration saved in outputs/checkpoint-26000/config.json\n",
      "Model weights saved in outputs/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-25000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-27000\n",
      "Configuration saved in outputs/checkpoint-27000/config.json\n",
      "Model weights saved in outputs/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-26000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "Saving model checkpoint to outputs/checkpoint-28000\n",
      "Configuration saved in outputs/checkpoint-28000/config.json\n",
      "Model weights saved in outputs/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-27000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-29000\n",
      "Configuration saved in outputs/checkpoint-29000/config.json\n",
      "Model weights saved in outputs/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-28000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-30000\n",
      "Configuration saved in outputs/checkpoint-30000/config.json\n",
      "Model weights saved in outputs/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-29000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-31000\n",
      "Configuration saved in outputs/checkpoint-31000/config.json\n",
      "Model weights saved in outputs/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-30000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-32000\n",
      "Configuration saved in outputs/checkpoint-32000/config.json\n",
      "Model weights saved in outputs/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-31000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-33000\n",
      "Configuration saved in outputs/checkpoint-33000/config.json\n",
      "Model weights saved in outputs/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-32000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-34000\n",
      "Configuration saved in outputs/checkpoint-34000/config.json\n",
      "Model weights saved in outputs/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-34000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-33000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-35000\n",
      "Configuration saved in outputs/checkpoint-35000/config.json\n",
      "Model weights saved in outputs/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-35000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-34000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-36000\n",
      "Configuration saved in outputs/checkpoint-36000/config.json\n",
      "Model weights saved in outputs/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-36000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "Saving model checkpoint to outputs/checkpoint-37000\n",
      "Configuration saved in outputs/checkpoint-37000/config.json\n",
      "Model weights saved in outputs/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-37000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-36000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-38000\n",
      "Configuration saved in outputs/checkpoint-38000/config.json\n",
      "Model weights saved in outputs/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-38000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-37000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-39000\n",
      "Configuration saved in outputs/checkpoint-39000/config.json\n",
      "Model weights saved in outputs/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-39000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-38000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-40000\n",
      "Configuration saved in outputs/checkpoint-40000/config.json\n",
      "Model weights saved in outputs/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-40000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-39000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-41000\n",
      "Configuration saved in outputs/checkpoint-41000/config.json\n",
      "Model weights saved in outputs/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-41000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-40000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-42000\n",
      "Configuration saved in outputs/checkpoint-42000/config.json\n",
      "Model weights saved in outputs/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-42000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-41000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-43000\n",
      "Configuration saved in outputs/checkpoint-43000/config.json\n",
      "Model weights saved in outputs/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-43000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-42000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-44000\n",
      "Configuration saved in outputs/checkpoint-44000/config.json\n",
      "Model weights saved in outputs/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-44000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-43000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-45000\n",
      "Configuration saved in outputs/checkpoint-45000/config.json\n",
      "Model weights saved in outputs/checkpoint-45000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-45000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-45000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-44000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "Saving model checkpoint to outputs/checkpoint-46000\n",
      "Configuration saved in outputs/checkpoint-46000/config.json\n",
      "Model weights saved in outputs/checkpoint-46000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-46000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-46000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-45000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-47000\n",
      "Configuration saved in outputs/checkpoint-47000/config.json\n",
      "Model weights saved in outputs/checkpoint-47000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-47000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-47000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-46000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-48000\n",
      "Configuration saved in outputs/checkpoint-48000/config.json\n",
      "Model weights saved in outputs/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-48000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-47000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-49000\n",
      "Configuration saved in outputs/checkpoint-49000/config.json\n",
      "Model weights saved in outputs/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-49000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-48000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-50000\n",
      "Configuration saved in outputs/checkpoint-50000/config.json\n",
      "Model weights saved in outputs/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-50000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-49000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-51000\n",
      "Configuration saved in outputs/checkpoint-51000/config.json\n",
      "Model weights saved in outputs/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-51000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-50000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-52000\n",
      "Configuration saved in outputs/checkpoint-52000/config.json\n",
      "Model weights saved in outputs/checkpoint-52000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-52000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-52000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-51000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-53000\n",
      "Configuration saved in outputs/checkpoint-53000/config.json\n",
      "Model weights saved in outputs/checkpoint-53000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-53000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-53000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-52000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-54000\n",
      "Configuration saved in outputs/checkpoint-54000/config.json\n",
      "Model weights saved in outputs/checkpoint-54000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-54000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-54000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-53000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "Saving model checkpoint to outputs/checkpoint-55000\n",
      "Configuration saved in outputs/checkpoint-55000/config.json\n",
      "Model weights saved in outputs/checkpoint-55000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-55000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-55000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-54000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-56000\n",
      "Configuration saved in outputs/checkpoint-56000/config.json\n",
      "Model weights saved in outputs/checkpoint-56000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-56000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-56000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-55000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-57000\n",
      "Configuration saved in outputs/checkpoint-57000/config.json\n",
      "Model weights saved in outputs/checkpoint-57000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-57000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-57000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-56000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-58000\n",
      "Configuration saved in outputs/checkpoint-58000/config.json\n",
      "Model weights saved in outputs/checkpoint-58000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-58000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-58000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-57000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-59000\n",
      "Configuration saved in outputs/checkpoint-59000/config.json\n",
      "Model weights saved in outputs/checkpoint-59000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-59000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-59000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-58000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-60000\n",
      "Configuration saved in outputs/checkpoint-60000/config.json\n",
      "Model weights saved in outputs/checkpoint-60000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-60000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-60000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-59000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-61000\n",
      "Configuration saved in outputs/checkpoint-61000/config.json\n",
      "Model weights saved in outputs/checkpoint-61000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-61000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-61000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-60000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-62000\n",
      "Configuration saved in outputs/checkpoint-62000/config.json\n",
      "Model weights saved in outputs/checkpoint-62000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-62000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-62000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-61000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-63000\n",
      "Configuration saved in outputs/checkpoint-63000/config.json\n",
      "Model weights saved in outputs/checkpoint-63000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-63000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-63000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-62000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "Saving model checkpoint to outputs/checkpoint-64000\n",
      "Configuration saved in outputs/checkpoint-64000/config.json\n",
      "Model weights saved in outputs/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-64000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-63000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-65000\n",
      "Configuration saved in outputs/checkpoint-65000/config.json\n",
      "Model weights saved in outputs/checkpoint-65000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-65000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-65000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-64000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-66000\n",
      "Configuration saved in outputs/checkpoint-66000/config.json\n",
      "Model weights saved in outputs/checkpoint-66000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-66000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-66000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-65000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-67000\n",
      "Configuration saved in outputs/checkpoint-67000/config.json\n",
      "Model weights saved in outputs/checkpoint-67000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-67000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-67000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-66000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-68000\n",
      "Configuration saved in outputs/checkpoint-68000/config.json\n",
      "Model weights saved in outputs/checkpoint-68000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-68000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-68000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-67000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-69000\n",
      "Configuration saved in outputs/checkpoint-69000/config.json\n",
      "Model weights saved in outputs/checkpoint-69000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-69000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-69000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-68000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-70000\n",
      "Configuration saved in outputs/checkpoint-70000/config.json\n",
      "Model weights saved in outputs/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-70000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-69000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-71000\n",
      "Configuration saved in outputs/checkpoint-71000/config.json\n",
      "Model weights saved in outputs/checkpoint-71000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-71000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-71000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-70000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-72000\n",
      "Configuration saved in outputs/checkpoint-72000/config.json\n",
      "Model weights saved in outputs/checkpoint-72000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-72000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-72000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-71000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "Saving model checkpoint to outputs/checkpoint-73000\n",
      "Configuration saved in outputs/checkpoint-73000/config.json\n",
      "Model weights saved in outputs/checkpoint-73000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-73000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-73000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-72000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-74000\n",
      "Configuration saved in outputs/checkpoint-74000/config.json\n",
      "Model weights saved in outputs/checkpoint-74000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-74000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-74000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-73000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-75000\n",
      "Configuration saved in outputs/checkpoint-75000/config.json\n",
      "Model weights saved in outputs/checkpoint-75000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-75000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-75000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-74000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-76000\n",
      "Configuration saved in outputs/checkpoint-76000/config.json\n",
      "Model weights saved in outputs/checkpoint-76000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-76000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-76000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-75000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-77000\n",
      "Configuration saved in outputs/checkpoint-77000/config.json\n",
      "Model weights saved in outputs/checkpoint-77000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-77000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-77000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-76000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-78000\n",
      "Configuration saved in outputs/checkpoint-78000/config.json\n",
      "Model weights saved in outputs/checkpoint-78000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-78000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-78000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-77000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-79000\n",
      "Configuration saved in outputs/checkpoint-79000/config.json\n",
      "Model weights saved in outputs/checkpoint-79000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-79000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-79000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-78000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-80000\n",
      "Configuration saved in outputs/checkpoint-80000/config.json\n",
      "Model weights saved in outputs/checkpoint-80000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-80000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-80000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-79000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-81000\n",
      "Configuration saved in outputs/checkpoint-81000/config.json\n",
      "Model weights saved in outputs/checkpoint-81000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-81000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-81000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-80000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "Saving model checkpoint to outputs/checkpoint-82000\n",
      "Configuration saved in outputs/checkpoint-82000/config.json\n",
      "Model weights saved in outputs/checkpoint-82000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-82000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-82000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-81000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-83000\n",
      "Configuration saved in outputs/checkpoint-83000/config.json\n",
      "Model weights saved in outputs/checkpoint-83000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-83000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-83000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-82000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-84000\n",
      "Configuration saved in outputs/checkpoint-84000/config.json\n",
      "Model weights saved in outputs/checkpoint-84000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-84000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-84000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-83000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-85000\n",
      "Configuration saved in outputs/checkpoint-85000/config.json\n",
      "Model weights saved in outputs/checkpoint-85000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-85000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-85000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-84000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-86000\n",
      "Configuration saved in outputs/checkpoint-86000/config.json\n",
      "Model weights saved in outputs/checkpoint-86000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-86000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-86000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-85000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-87000\n",
      "Configuration saved in outputs/checkpoint-87000/config.json\n",
      "Model weights saved in outputs/checkpoint-87000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-87000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-87000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-86000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-88000\n",
      "Configuration saved in outputs/checkpoint-88000/config.json\n",
      "Model weights saved in outputs/checkpoint-88000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-88000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-88000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-87000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-89000\n",
      "Configuration saved in outputs/checkpoint-89000/config.json\n",
      "Model weights saved in outputs/checkpoint-89000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-89000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-89000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-88000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-90000\n",
      "Configuration saved in outputs/checkpoint-90000/config.json\n",
      "Model weights saved in outputs/checkpoint-90000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-90000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-90000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-89000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "Saving model checkpoint to outputs/checkpoint-91000\n",
      "Configuration saved in outputs/checkpoint-91000/config.json\n",
      "Model weights saved in outputs/checkpoint-91000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-91000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-91000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-90000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-92000\n",
      "Configuration saved in outputs/checkpoint-92000/config.json\n",
      "Model weights saved in outputs/checkpoint-92000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-92000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-92000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-91000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-93000\n",
      "Configuration saved in outputs/checkpoint-93000/config.json\n",
      "Model weights saved in outputs/checkpoint-93000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-93000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-93000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-92000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-94000\n",
      "Configuration saved in outputs/checkpoint-94000/config.json\n",
      "Model weights saved in outputs/checkpoint-94000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-94000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-94000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-93000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-95000\n",
      "Configuration saved in outputs/checkpoint-95000/config.json\n",
      "Model weights saved in outputs/checkpoint-95000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-95000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-95000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-94000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-96000\n",
      "Configuration saved in outputs/checkpoint-96000/config.json\n",
      "Model weights saved in outputs/checkpoint-96000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-96000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-96000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-95000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-97000\n",
      "Configuration saved in outputs/checkpoint-97000/config.json\n",
      "Model weights saved in outputs/checkpoint-97000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-97000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-97000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-96000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-98000\n",
      "Configuration saved in outputs/checkpoint-98000/config.json\n",
      "Model weights saved in outputs/checkpoint-98000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-98000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-98000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-97000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-99000\n",
      "Configuration saved in outputs/checkpoint-99000/config.json\n",
      "Model weights saved in outputs/checkpoint-99000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-99000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-99000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-98000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "Saving model checkpoint to outputs/checkpoint-100000\n",
      "Configuration saved in outputs/checkpoint-100000/config.json\n",
      "Model weights saved in outputs/checkpoint-100000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-100000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-100000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-99000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-101000\n",
      "Configuration saved in outputs/checkpoint-101000/config.json\n",
      "Model weights saved in outputs/checkpoint-101000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-101000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-101000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-100000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-102000\n",
      "Configuration saved in outputs/checkpoint-102000/config.json\n",
      "Model weights saved in outputs/checkpoint-102000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-102000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-102000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-101000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-103000\n",
      "Configuration saved in outputs/checkpoint-103000/config.json\n",
      "Model weights saved in outputs/checkpoint-103000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-103000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-103000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-102000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-104000\n",
      "Configuration saved in outputs/checkpoint-104000/config.json\n",
      "Model weights saved in outputs/checkpoint-104000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-104000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-104000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-103000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-105000\n",
      "Configuration saved in outputs/checkpoint-105000/config.json\n",
      "Model weights saved in outputs/checkpoint-105000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-105000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-105000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-104000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-106000\n",
      "Configuration saved in outputs/checkpoint-106000/config.json\n",
      "Model weights saved in outputs/checkpoint-106000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-106000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-106000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-105000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-107000\n",
      "Configuration saved in outputs/checkpoint-107000/config.json\n",
      "Model weights saved in outputs/checkpoint-107000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-107000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-107000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-106000] due to args.save_total_limit\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Saving model checkpoint to outputs/checkpoint-108000\n",
      "Configuration saved in outputs/checkpoint-108000/config.json\n",
      "Model weights saved in outputs/checkpoint-108000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-108000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-108000/special_tokens_map.json\n",
      "Deleting older checkpoint [outputs/checkpoint-107000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:490: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "INFO:absl:Using default tokenizer.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=108000, training_loss=0.0036797359274930936, metrics={'train_runtime': 23403.1064, 'train_samples_per_second': 36.918, 'train_steps_per_second': 4.615, 'total_flos': 2.6501217140736e+17, 'train_loss': 0.0036797359274930936, 'epoch': 12.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63ace2e6-8dca-4be8-8b33-3d850d792c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'usually , he would be tearing around the living room , playing with his toys .'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c60fa798-3855-4e46-a1cb-a93aa151e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = tokenizer.encode_plus('good morning', return_tensors='pt')\n",
    "test_input = test_input.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8ecb81b-287b-401a-8104-d4ee820a6f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2788,  1010,  2002,  2052,  2022, 13311,  2105,  1996,  2542,\n",
       "          2282,  1010,  2652,  2007,  2010, 10899,   102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c77fe6d-a162-47f4-b845-a813cc764a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[ C L S ]',\n",
       " '[ S E P ]',\n",
       " ',',\n",
       " 'a n d',\n",
       " 'w a s',\n",
       " 'b e',\n",
       " 'c o',\n",
       " 'n',\n",
       " 'd',\n",
       " 'h a d',\n",
       " 'h a s',\n",
       " 'i s',\n",
       " 'd o e s',\n",
       " 'd o',\n",
       " 'h a v e',\n",
       " 'o f']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [tokenizer.decode(i) for i in model.generate(test_input['input_ids']).cpu().tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5ec6e6e-22c6-44f4-8ae8-a4581d158aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101,  102, 1010, 1998, 2001, 2022, 2522, 1050, 1040, 2018, 2038, 2003,\n",
       "         2515, 2079, 2031, 1997]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(test_input['input_ids'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
