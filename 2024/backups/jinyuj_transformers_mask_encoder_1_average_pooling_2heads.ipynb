{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a36c05-aa20-4909-a5e9-a172529e22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from os.path import exists\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad, one_hot\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "### utils.py ###\n",
    "\n",
    "class Dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "# end\n",
    "\n",
    "### utils.py ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### core.py ###\n",
    "\n",
    "\"Produce N identical layers.\"\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    \"Take in model size and number of heads.\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    # end\n",
    "\n",
    "\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # print('jinyuj: scores: {}, mask: {}'.format(scores.shape, mask.shape))\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # end\n",
    "        p_attn = scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        # end\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "    # end\n",
    "\n",
    "\n",
    "    \"Implements Figure 2\"\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = self.attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)\n",
    "    # end\n",
    "# end class\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A residual connection followed by a layer norm.\n",
    "Note for code simplicity the norm is first as opposed to last.\n",
    "\"\"\"\n",
    "class ResidualLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, size, dropout=0.1, eps=1e-6):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.norm = torch.nn.LayerNorm(size, eps)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    # end\n",
    "\n",
    "    \"Apply residual connection to any sublayer with the same size.\"\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    # end\n",
    "# end class\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleIDEmbeddings(nn.Module):\n",
    "    def __init__(self, size_vocab, dim_hidden, id_pad):\n",
    "        super(SimpleIDEmbeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(size_vocab, dim_hidden, padding_idx=id_pad)\n",
    "        self.dim_hidden = dim_hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.lut(x)\n",
    "        return result * math.sqrt(self.dim_hidden)\n",
    "    # end\n",
    "\n",
    "    def get_shape(self):\n",
    "        return (self.lut.num_embeddings, self.lut.embedding_dim)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\"Implement the PE function.\"\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_positional, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.dim_positional = dim_positional\n",
    "        pe = torch.zeros(max_len, dim_positional)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, dim_positional, 2) * -(math.log(10000.0) / dim_positional)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).to('cuda')\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return x\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleEmbedder(nn.Module):    # no segment embedder as we do not need that\n",
    "    def __init__(self, size_vocab=None, dim_hidden=128, dropout=0.1, id_pad=0):\n",
    "        super(SimpleEmbedder, self).__init__()\n",
    "        self.size_vocab = size_vocab\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.id_pad = id_pad\n",
    "\n",
    "        self.embedder = nn.Sequential(\n",
    "            SimpleIDEmbeddings(size_vocab, dim_hidden, id_pad),\n",
    "            PositionalEncoding(dim_hidden),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    # end\n",
    "\n",
    "    def forward(self, ids_input):   # (batch, seqs_with_padding)\n",
    "        return self.embedder(ids_input)\n",
    "    # end\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.size_vocab\n",
    "    # end\n",
    "# end\n",
    "\n",
    "### core.py ###\n",
    "\n",
    "\n",
    "\n",
    "class SimpleEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, dim_feedforward, n_head, dropout=0.1):\n",
    "        super(SimpleEncoderLayer, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        self.layer_attention = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_feedforward = PositionwiseFeedForward(dim_hidden, dim_feedforward, dropout)\n",
    "        self.layers_residual = clones(ResidualLayer(dim_hidden, dropout), 2)\n",
    "    # end\n",
    "\n",
    "    def forward(self, embeddings, masks, *args):\n",
    "        embeddings = self.layers_residual[0](embeddings, lambda embeddings: self.layer_attention(embeddings, embeddings, embeddings, masks))\n",
    "        return self.layers_residual[1](embeddings, self.layer_feedforward)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class SimpleDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, dim_feedforward, n_head, dropout=0.1):\n",
    "        super(SimpleDecoderLayer, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        self.layer_attention_decoder = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_attention_encoder = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_feedforward = PositionwiseFeedForward(dim_hidden, dim_feedforward, dropout)\n",
    "        self.layers_residual = clones(ResidualLayer(dim_hidden, dropout), 3)\n",
    "\n",
    "    def forward(self, embeddings, masks_encoder, output_encoder, masks_decoder, *args):\n",
    "        embeddings = self.layers_residual[0](embeddings, lambda embeddings: self.layer_attention_decoder(embeddings, embeddings, embeddings, masks_decoder))\n",
    "        # print('jinyuj: embeddings.shape {}, masks_encoder.shape {}'.format(embeddings.shape, masks_encoder.shape))\n",
    "        # if embeddings.shape[1] != masks_encoder.shape[1]:\n",
    "        #     masks_encoder = masks_encoder[:,:embeddings.shape[1],:]\n",
    "        # # end\n",
    "        embeddings = self.layers_residual[1](embeddings, lambda embeddings: self.layer_attention_encoder(embeddings, output_encoder, output_encoder, masks_encoder))\n",
    "        return self.layers_residual[2](embeddings, self.layer_feedforward)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleTransformerStack(nn.Module):\n",
    "\n",
    "    def __init__(self, obj_layer, n_layers):\n",
    "        super(SimpleTransformerStack, self).__init__()\n",
    "        self.layers = clones(obj_layer, n_layers)\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(obj_layer.dim_hidden)\n",
    "        self.keys_cache = ['output']\n",
    "        self.cache = Dotdict({\n",
    "            'outputs': None\n",
    "        })\n",
    "    # end\n",
    "\n",
    "    def forward(self, embedding_encoder=None, masks_encoder=None, output_encoder=None, embedding_decoder=None, masks_decoder=None ,noncache=False, **kwargs):  # input -> (batch, len_seq, vocab)\n",
    "\n",
    "        if output_encoder is not None and embedding_decoder is not None and masks_decoder is not None:\n",
    "            embeddings = embedding_decoder\n",
    "        else:\n",
    "            embeddings = embedding_encoder\n",
    "        # end\n",
    "\n",
    "        for layer in self.layers:\n",
    "            embeddings = layer(embeddings, masks_encoder, output_encoder, masks_decoder)\n",
    "        # end\n",
    "\n",
    "        outputs = self.norm(embeddings)\n",
    "\n",
    "        if not noncache:\n",
    "            self.cache.outputs = outputs\n",
    "        # end\n",
    "\n",
    "        return outputs\n",
    "    # end\n",
    "\n",
    "    # def get_vocab_size(self):\n",
    "    #     return self.embedder.embedder_token.shape[-1]\n",
    "    # # end\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleEncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder_encoder, embedder_decoder, pooling=False):\n",
    "        super(SimpleEncoderDecoder, self).__init__()\n",
    "\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        self.embedder_encoder = embedder_encoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.embedder_decoder = embedder_decoder\n",
    "        self.decoder = decoder\n",
    "    # end\n",
    "\n",
    "    def forward(self, ids_encoder=None, masks_encoder=None, ids_decoder=None, masks_decoder=None, nocache=False, **kwargs):\n",
    "        \n",
    "        output_encoder = self.embed_and_encode(ids_encoder=ids_encoder, masks_encoder=masks_encoder, nocache=nocache)\n",
    "        output = output_encoder\n",
    "        \n",
    "        if self.pooling:\n",
    "            output_encoder_refilled = output_encoder.masked_fill(masks_encoder.transpose(-1,-2)==False, 0)\n",
    "            output_encoder_pooled = torch.mean(output_encoder_refilled, dim=-2)\n",
    "            output_encoder_pooled_expanded = output_encoder_pooled.unsqueeze(-2).expand(output_encoder.shape)\n",
    "            output = output_encoder_pooled_expanded\n",
    "        # end\n",
    "        \n",
    "        if self.embedder_decoder and self.decoder:\n",
    "            output_decoder = self.embed_and_decode(ids_decoder=ids_decoder, masks_encoder=masks_encoder, output_encoder=output, masks_decoder=masks_decoder, nocache=nocache)\n",
    "            output = output_decoder\n",
    "        # end if\n",
    "        \n",
    "        return output\n",
    "    # end\n",
    "    \n",
    "    def embed_and_encode(self, ids_encoder=None, masks_encoder=None, nocache=False, **kwargs):\n",
    "        self.encoder.clear_cache()\n",
    "        \n",
    "        embedding_encoder = self.embedder_encoder(ids_encoder)\n",
    "        output_encoder = self.encoder(\n",
    "            embedding_encoder=embedding_encoder,\n",
    "            masks_encoder=masks_encoder,\n",
    "            nocache=nocache\n",
    "        )\n",
    "        \n",
    "        return output_encoder\n",
    "    # end\n",
    "\n",
    "    \n",
    "    def embed_and_decode(self, ids_decoder=None, masks_encoder=None, output_encoder=None, masks_decoder=None, nocache=False, **kwargs):\n",
    "        self.decoder.clear_cache()\n",
    "        \n",
    "        embedding_decoder = self.embedder_decoder(ids_decoder)\n",
    "        output_decoder = self.decoder(\n",
    "            masks_encoder=masks_encoder,\n",
    "            output_encoder=output_encoder,    #(len_seq, dim_hidden) -> (1, dim_hidden)\n",
    "            embedding_decoder=embedding_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            nocache=nocache\n",
    "        )\n",
    "\n",
    "        return output_decoder\n",
    "    # end\n",
    "    \n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.encoder.clear_cache()\n",
    "        if self.decoder:\n",
    "            self.decoder.clear_cache()\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def get_vocab_size(self, name_embedder):\n",
    "        embedder = getattr(self, f'embedder_{name_embedder}')\n",
    "        return embedder.get_vocab_size()\n",
    "    # end\n",
    "\n",
    "# end\n",
    "\n",
    "class LinearAndNorm(nn.Module):\n",
    "    def __init__(self, dim_in = None, dim_out = None, eps_norm=1e-12):\n",
    "        super(LinearAndNorm, self).__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(dim_in, dim_out)\n",
    "        self.norm = torch.nn.LayerNorm(dim_out, eps_norm)\n",
    "    # end\n",
    "\n",
    "    def forward(self, seqs_in):\n",
    "        return self.norm(self.linear(seqs_in).relu())\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TokenizerWrapper:\n",
    "    def __init__(self, vocab, splitter):\n",
    "        self.splitter = splitter\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.id_pad = len(vocab)\n",
    "        self.id_cls = len(vocab) + 1\n",
    "        self.id_sep = len(vocab) + 2\n",
    "        self.id_mask = len(vocab) + 3\n",
    "        \n",
    "        self.size_vocab = len(vocab) + 4\n",
    "\n",
    "        self.token_pad = '[PAD]'\n",
    "        self.token_cls = '[CLS]'\n",
    "        self.token_sep = '[SEP]'\n",
    "        self.token_mask = '[MASK]'\n",
    "           \n",
    "        self.index_id_token_special = {\n",
    "            self.id_pad: self.token_pad,\n",
    "            self.id_cls: self.token_cls,\n",
    "            self.id_sep: self.token_sep,\n",
    "            self.id_mask: self.token_mask\n",
    "        }\n",
    "        \n",
    "    # end\n",
    "\n",
    "    def encode(self, line):\n",
    "        return self.vocab([doc.text for doc in self.splitter(line)])\n",
    "    # end\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            token = int(token)\n",
    "            \n",
    "            if token in self.index_id_token_special:\n",
    "                word_target = self.index_id_token_special[token]\n",
    "            else:\n",
    "                try:\n",
    "                    word_target = vocab.lookup_token(token)\n",
    "                except:\n",
    "                    word_target = '[ERROR_LOOKUP_{}]'.format(token)\n",
    "                # end\n",
    "            # end\n",
    "            \n",
    "            words.append(word_target)\n",
    "        # end\n",
    "        \n",
    "        return ' '.join(words)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    DEVICE = 'cuda'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            if v is not None and type(v) is not bool:\n",
    "                self.kwargs[k] = v.to(Batch.DEVICE)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.kwargs\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class Collator_S2S:\n",
    "\n",
    "    def __init__(self, tokenizer, size_seq_max, need_masked=0.3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size_seq_max = size_seq_max\n",
    "        self.need_masked = need_masked\n",
    "    # end\n",
    "    \n",
    "\n",
    "    def __call__(self, list_corpus_source):\n",
    "\n",
    "        tokens_input_encoder = []\n",
    "        tokens_input_decoder = []\n",
    "        tokens_label_decoder = []\n",
    "        labels_similarity = []\n",
    "\n",
    "        for corpus_source in list_corpus_source: # (line0, line1, sim), output of zip remove single case\n",
    "            if len(corpus_source) == 3:\n",
    "                corpus_line = [courpus_source[0], corpus_source[1]]\n",
    "                labels_similarity.append(corpus_line[2])\n",
    "            else:\n",
    "                corpus_line = [corpus_source[1]]\n",
    "            # end\n",
    "            \n",
    "            for line in corpus_line:\n",
    "                tokens = self.tokenizer.encode(line)\n",
    "\n",
    "                # TODO: check edge\n",
    "                if len(tokens) > self.size_seq_max - 2:\n",
    "                    tokens = tokens[:self.size_seq_max-2]\n",
    "                # end\n",
    "\n",
    "                tokens_input_encoder.append([self.tokenizer.id_cls] + tokens + [self.tokenizer.id_sep])\n",
    "                tokens_input_decoder.append([self.tokenizer.id_cls] + tokens)\n",
    "                tokens_label_decoder.append(tokens + [self.tokenizer.id_sep])\n",
    "            # end\n",
    "            \n",
    "\n",
    "        # end\n",
    "\n",
    "        inputs_encoder, masks_encoder, segments_encoder, labels_encoder = self.pad_sequences(tokens_input_encoder, self.size_seq_max, need_masked=self.need_masked)\n",
    "        inputs_decoder, masks_decoder, segments_decoder, _ = self.pad_sequences(tokens_input_decoder, self.size_seq_max, need_diagonal=True)\n",
    "        labels_decoder, masks_label, segments_label, _ = self.pad_sequences(tokens_label_decoder, self.size_seq_max)\n",
    "        # labels_similarity = torch.Tensor(labels_similarity).unsqueeze(0).transpose(0,1)\n",
    "        labels_similarity = torch.Tensor(labels_similarity)\n",
    "\n",
    "        return Batch(\n",
    "            ids_encoder=inputs_encoder,  # contains [mask]s\n",
    "            masks_encoder=masks_encoder,\n",
    "            labels_encoder=labels_encoder,  # doesn't contain [mask]\n",
    "            segments_encoder=segments_encoder,\n",
    "            ids_decoder=inputs_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            labels_decoder=labels_decoder,\n",
    "            segments_label=segments_label,\n",
    "            labels_similarity=labels_similarity\n",
    "        )\n",
    "    # end\n",
    "\n",
    "    \n",
    "    # return masks_attention?, return masks_segment?\n",
    "    def pad_sequences(self, sequences, size_seq_max, need_diagonal=False, need_masked=0): # need_diagonal and need_masked cannot both set, one for bert seq one for s2s seq\n",
    "        id_pad = self.tokenizer.id_pad\n",
    "        id_mask = self.tokenizer.id_mask\n",
    "\n",
    "        sequences_padded = []\n",
    "        sequences_masked_padded = []\n",
    "\n",
    "        for sequence in sequences:\n",
    "            len_seq = len(sequence)\n",
    "\n",
    "            count_pad = size_seq_max - len_seq\n",
    "\n",
    "            sequence = torch.LongTensor(sequence)\n",
    "            sequence_padded = torch.cat((sequence, torch.LongTensor([id_pad] * count_pad)))\n",
    "            sequences_padded.append(sequence_padded)\n",
    "\n",
    "            if need_masked:\n",
    "                index_masked = list(range(1, len_seq-1))\n",
    "                random.shuffle(index_masked)\n",
    "                index_masked = torch.LongTensor(index_masked[:int(need_masked * (len_seq-2))])\n",
    "\n",
    "                sequence_masked = sequence.detach().clone()\n",
    "                sequence_masked.index_fill_(0, index_masked, id_mask)\n",
    "                sequence_masked_padded = torch.cat((sequence_masked, torch.LongTensor([id_pad] * count_pad)))\n",
    "                \n",
    "                sequences_masked_padded.append(sequence_masked_padded)\n",
    "            # end\n",
    "    #   # end for\n",
    "\n",
    "        inputs = torch.stack(sequences_padded)  # (batch, size_seq_max)\n",
    "        if need_masked:\n",
    "            inputs_masked_padded = torch.stack(sequences_masked_padded)\n",
    "        # end\n",
    "\n",
    "        masks_segment = (inputs != self.tokenizer.id_pad).unsqueeze(-2)    #(nbatch, 1, seq)\n",
    "        masks_attention = self.make_std_mask(inputs, self.tokenizer.id_pad) if need_diagonal else masks_segment\n",
    "\n",
    "        if need_masked:\n",
    "            masks_masked = (inputs_masked_padded != id_mask).unsqueeze(-2)\n",
    "            masks_attention = masks_attention & masks_masked\n",
    "            return inputs_masked_padded, masks_attention, masks_segment, inputs # (inputs, masks_attention, masks_segment, labels)\n",
    "        else:\n",
    "            return inputs, masks_attention, masks_segment, None\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "\n",
    "    def subsequent_mask(self, size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "            torch.uint8\n",
    "        )\n",
    "        return subsequent_mask == 0\n",
    "\n",
    "    def make_std_mask(self, tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & self.subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51933c37-2474-4ea8-8819-d67e8061c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def Multi30k(language_pair=None):\n",
    "    corpus_lines_train = []\n",
    "\n",
    "    for lan in language_pair:\n",
    "        with open('text/train.{}'.format(lan), 'r') as file:\n",
    "            corpus_lines_train.append(file.read().splitlines())\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    corpus_train = list(zip(*corpus_lines_train))\n",
    "\n",
    "    corpus_lines_eval = []\n",
    "\n",
    "    for lan in language_pair:\n",
    "        with open('text/val.{}'.format(lan), 'r') as file:\n",
    "            corpus_lines_eval.append(file.read().splitlines())\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    corpus_eval = list(zip(*corpus_lines_eval))\n",
    "\n",
    "    return corpus_train, corpus_eval, None\n",
    "# end\n",
    "\n",
    "\n",
    "def load_vocab(spacy_en):\n",
    "    if not os.path.exists(\"vocab.pt\"):\n",
    "        vocab_tgt = build_vocabulary(spacy_en)\n",
    "        torch.save(vocab_tgt, \"vocab.pt\")\n",
    "    else:\n",
    "        vocab_tgt = torch.load(\"vocab.pt\")\n",
    "    print(\"Finished.\\nVocabulary sizes: {}\".format(len(vocab_tgt)))\n",
    "    return vocab_tgt\n",
    "# end\n",
    "\n",
    "def load_spacy():\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_en\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c03c357-ab7a-4589-a7a0-b87c2a3572b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoderHead_MLM(nn.Module):\n",
    "\n",
    "    def __init__(self, model, size_vocab, dim_hidden=128):\n",
    "        super(SimpleEncoderHead_MLM, self).__init__()\n",
    "        \n",
    "        self.ffn = LinearAndNorm(dim_in=dim_hidden, dim_out=dim_hidden)\n",
    "        self.extractor = torch.nn.Linear(dim_hidden, size_vocab, bias=False)\n",
    "        self.extractor.weight = nn.Parameter(model.embedder_encoder.embedder[0].lut.weight)\n",
    "        \n",
    "        self.keys_cache = ['labels_mlm', 'masks_encoder', 'segments_encoder', 'outputs']\n",
    "        self.cache = Dotdict({\n",
    "            'labels_mlm': None,\n",
    "            'masks_encoder': None,\n",
    "            'segments_encoder': None,\n",
    "            'outputs': None\n",
    "        })\n",
    "        \n",
    "        self.func_loss = torch.nn.CrossEntropyLoss()\n",
    "    # end\n",
    "\n",
    "\n",
    "    def forward(self, model, labels_encoder=None, segments_encoder=None, masks_encoder=None, nocache=False, **kwargs):   # labels_input -> (batch, seq, labels)\n",
    "        outputs_encoder = model.encoder.cache.outputs\n",
    "        outputs_ffn = self.ffn(outputs_encoder)\n",
    "        outputs_mlm = self.extractor(outputs_ffn) # outputs_mlm = prediction_logits\n",
    "\n",
    "        if not nocache:\n",
    "            self.cache.labels_mlm = labels_encoder\n",
    "            self.cache.masks_encoder = masks_encoder\n",
    "            self.cache.segments_encoder = segments_encoder\n",
    "            self.cache.outputs = outputs_mlm\n",
    "        # end\n",
    "\n",
    "        return outputs_mlm\n",
    "    # end\n",
    "    \n",
    "    def get_loss(self):\n",
    "        \n",
    "        labels_mlm = self.cache.labels_mlm\n",
    "        masks_encoder = self.cache.masks_encoder\n",
    "        segments_encoder = self.cache.segments_encoder\n",
    "        outputs_mlm = self.cache.outputs\n",
    "        \n",
    "        segments_encoder_2d = segments_encoder.transpose(-1,-2)[:,:,0]\n",
    "        loss_segments = self.func_loss(outputs_mlm.masked_select(segments_encoder_2d.unsqueeze(-1)).reshape(-1, outputs_mlm.shape[-1]), labels_mlm.masked_select(segments_encoder_2d))\n",
    "        \n",
    "        masks_masked = torch.logical_xor(masks_encoder, segments_encoder) & segments_encoder # True is masked\n",
    "        masks_masked_perbatch = masks_masked[:,0,:]\n",
    "        loss_masked = self.func_loss(outputs_mlm.masked_select(masks_masked_perbatch.unsqueeze(-1)).reshape(-1, outputs_mlm.shape[-1]), labels_mlm.masked_select(masks_masked_perbatch))       \n",
    "        \n",
    "        loss_mlm = loss_segments + loss_masked * 3\n",
    "        return loss_mlm\n",
    "    # end\n",
    "\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b97fee-21de-4a79-a082-4eb70288e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoderHead_S2S(nn.Module):\n",
    "\n",
    "    def __init__(self, model, size_vocab, dim_hidden=128):\n",
    "        super(SimpleDecoderHead_S2S, self).__init__()\n",
    "        \n",
    "        self.ffn = LinearAndNorm(dim_in=dim_hidden, dim_out=dim_hidden)\n",
    "        self.extractor = torch.nn.Linear(dim_hidden, size_vocab, bias=False)\n",
    "        self.extractor.weight = nn.Parameter(model.embedder_decoder.embedder[0].lut.weight)\n",
    "\n",
    "        self.func_loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.keys_cache = ['outputs', 'labels_s2s', 'segments_decoder']\n",
    "        self.cache = Dotdict({\n",
    "            'outputs': None,\n",
    "            'labels_s2s': None,\n",
    "            'segments_decoder': None\n",
    "        })\n",
    "\n",
    "    # end\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, model, labels_decoder=None, segments_label=None, nocache=False, **kwargs):   # labels_input -> (batch, seq, labels)\n",
    "        outputs_decoder = model.decoder.cache.outputs\n",
    "        outputs_ffn = self.ffn(outputs_decoder)\n",
    "        outputs_s2s = self.extractor(outputs_ffn)   # outputs_mlm = prediction_logits\n",
    "        \n",
    "        if not nocache:\n",
    "            self.cache.segments_label = segments_label\n",
    "            self.cache.labels_s2s =  labels_decoder\n",
    "            self.cache.outputs = outputs_s2s\n",
    "        # end\n",
    "\n",
    "        return outputs_s2s\n",
    "    # end\n",
    "\n",
    "\n",
    "    def get_loss(self):\n",
    "        labels_s2s = self.cache.labels_s2s\n",
    "        outputs_s2s = self.cache.outputs\n",
    "        \n",
    "        segments_label = self.cache.segments_label\n",
    "        segments_label_2d = segments_label.transpose(-1,-2)[:,:,0]\n",
    "\n",
    "        loss_segments = self.func_loss(outputs_s2s.masked_select(segments_label_2d.unsqueeze(-1)).reshape(-1, outputs_s2s.shape[-1]), labels_s2s.masked_select(segments_label_2d))\n",
    "        return loss_segments * 4\n",
    "    # end\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        pass\n",
    "    # end\n",
    "\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe4c3e2f-17aa-44ad-aa98-43ed56627067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadManager(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HeadManager, self).__init__()\n",
    "        self.index_name_head = set()\n",
    "    # end\n",
    "\n",
    "    def register(self, head):\n",
    "        name_head = head.__class__.__name__\n",
    "        setattr(self, name_head, head)\n",
    "        self.index_name_head.add(name_head)\n",
    "        return self\n",
    "    # end\n",
    "\n",
    "    def forward(self, model, **kwargs):\n",
    "        for name in self.index_name_head:\n",
    "            head = getattr(self, name)\n",
    "            head.forward(model, **kwargs)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def get_head(self, klass):\n",
    "        return getattr(self, klass.__name__)\n",
    "    # end\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for name_head in self.index_name_head:\n",
    "            getattr(self, name_head).clear_cache()\n",
    "        # end\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Trainer(nn.Module):\n",
    "    def __init__(self, model=None, manager=None):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.manager = manager\n",
    "    # end\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        self.clear_cache()\n",
    "        \n",
    "        self.model.forward(**kwargs)\n",
    "        self.manager.forward(self.model, **kwargs)\n",
    "    # end\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        self.model.clear_cache() if self.model else None\n",
    "        self.manager.clear_cache() if self.manager else None\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "947695c3-1322-4748-8174-a1f1003cc8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Builder:\n",
    "\n",
    "    @classmethod\n",
    "    def build_model_with_mlm(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, None, embedder_encoder, None)\n",
    "        head_mlm = SimpleEncoderHead_MLM(model, size_vocab, dim_hidden)\n",
    "\n",
    "        return head_mlm\n",
    "    # end\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def build_model_with_mlm_v2(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, None, embedder_encoder, None)\n",
    "        head_mlm = SimpleEncoderHead_MLM(model, size_vocab, dim_hidden)\n",
    "\n",
    "        manager = HeadManager().register(head_mlm)\n",
    "        trainer = Trainer(model=model, manager=manager)\n",
    "\n",
    "        return trainer\n",
    "    # end\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def build_model_with_s2s(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "        \n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder, pooling=True)\n",
    "        head_s2s = SimpleDecoderHead_S2S(model, size_vocab, dim_hidden)\n",
    "\n",
    "        return head_s2s\n",
    "    # end\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def build_model_with_s2s_v2(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "        \n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder, pooling=True)\n",
    "        head_s2s = SimpleDecoderHead_S2S(model, size_vocab, dim_hidden)\n",
    "        \n",
    "        manager = HeadManager().register(head_s2s)\n",
    "        trainer = Trainer(model=model, manager=manager)\n",
    "\n",
    "        return trainer\n",
    "    # end\n",
    "\n",
    "    @classmethod\n",
    "    def build_model_with_2heads(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "        \n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder, pooling=True)\n",
    "        head_s2s = SimpleDecoderHead_S2S(model, size_vocab, dim_hidden)\n",
    "        head_mlm = SimpleEncoderHead_MLM(model, size_vocab, dim_hidden)\n",
    "        \n",
    "        manager = HeadManager().register(head_s2s).register(head_mlm)\n",
    "        trainer = Trainer(model=model, manager=manager)\n",
    "\n",
    "        return trainer\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60b018b-b983-4755-90ce-c07976ccde1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes: 6191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "\n",
    "gpu = 0\n",
    "torch.cuda.set_device(gpu)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# source\n",
    "seq_max = 16\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# model & head\n",
    "dim_hidden = 512\n",
    "dim_feedforward = 512\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "\n",
    "# optimizer\n",
    "lr_base_optimizer = 5e-4\n",
    "betas_optimizer = (0.9, 0.999)\n",
    "eps_optimizer = 1e-9\n",
    "\n",
    "# scheduler\n",
    "warmup = 200\n",
    "\n",
    "spacy_en = load_spacy()\n",
    "vocab = load_vocab(spacy_en)\n",
    "tokenizer = TokenizerWrapper(vocab, spacy_en)\n",
    "\n",
    "train_iter, valid_iter, _ = Multi30k(language_pair=(\"de\", \"en\"))\n",
    "train_source = to_map_style_dataset(train_iter)\n",
    "\n",
    "collator = Collator_S2S(tokenizer, seq_max)\n",
    "dataloader_train = DataLoader(train_source, batch_size, shuffle=False, collate_fn=collator)\n",
    "\n",
    "# trainer = Builder.build_model_with_mlm_v2(tokenizer.size_vocab, dim_hidden, dim_feedforward, n_head, n_layer)\n",
    "trainer = Builder.build_model_with_2heads(tokenizer.size_vocab, dim_hidden, dim_feedforward, n_head, n_layer)\n",
    "\n",
    "for p in trainer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "trainer = trainer.to('cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(trainer.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "decayRate = 0.96\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b499ec3-35e4-49c7-952d-5e58d0b899f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a_batch(batch, trainer, optimizer=None, scheduler=None):\n",
    "    trainer.train()\n",
    "    trainer.forward(**batch())\n",
    "    \n",
    "    loss_s2s = trainer.manager.get_head(SimpleDecoderHead_S2S).get_loss()\n",
    "    loss_mlm = trainer.manager.get_head(SimpleEncoderHead_MLM).get_loss()\n",
    "\n",
    "    # crossentropy loss\n",
    "    \n",
    "    # loss_all = loss_s2s * 5\n",
    "    # loss_all = loss_mlm\n",
    "    loss_all = (loss_s2s + loss_mlm) / 2\n",
    "    loss_all_value = loss_all.item()\n",
    "    \n",
    "    # print(loss_all)\n",
    "    loss_all.backward()\n",
    "\n",
    "    \n",
    "    if optimizer:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    # end\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    # end\n",
    "    \n",
    "    trainer.clear_cache()\n",
    "    return loss_all_value\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fcf19d1-b502-4a47-99dc-66bf2b071347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Batch: 0, loss: 35.384857177734375, rate: 0.0001\n",
      "Epoch: 0 Batch: 100, loss: 22.297853469848633, rate: 0.0001\n",
      "Epoch: 0 Batch: 200, loss: 21.158954620361328, rate: 0.0001\n",
      "Epoch: 0 Batch: 300, loss: 16.247922897338867, rate: 0.0001\n",
      "Epoch: 0 Batch: 400, loss: 17.586936950683594, rate: 0.0001\n",
      "Epoch: 0 Batch: 500, loss: 20.228824615478516, rate: 0.0001\n",
      "Epoch: 0 Batch: 600, loss: 15.174994468688965, rate: 0.0001\n",
      "Epoch: 0 Batch: 700, loss: 17.960613250732422, rate: 0.0001\n",
      "Epoch: 0 Batch: 800, loss: 15.767772674560547, rate: 0.0001\n",
      "Epoch: 0 Batch: 900, loss: 17.716533660888672, rate: 0.0001\n",
      "Epoch: 0 Batch: 1000, loss: 17.6490535736084, rate: 0.0001\n",
      "Epoch: 0 Batch: 1100, loss: 15.730562210083008, rate: 0.0001\n",
      "Epoch: 0 Batch: 1200, loss: 15.979668617248535, rate: 0.0001\n",
      "Epoch: 0 Batch: 1300, loss: 13.830490112304688, rate: 0.0001\n",
      "Epoch: 0 Batch: 1400, loss: 15.417766571044922, rate: 0.0001\n",
      "Epoch: 0 Batch: 1500, loss: 14.931241989135742, rate: 0.0001\n",
      "Epoch: 0 Batch: 1600, loss: 15.431269645690918, rate: 0.0001\n",
      "Epoch: 0 Batch: 1700, loss: 17.01565170288086, rate: 0.0001\n",
      "Epoch: 0 Batch: 1800, loss: 16.439697265625, rate: 0.0001\n",
      "[2023-11-19 11:23:40.899505] Epoch: 0 ends. Average loss: 17.399588281386343\n",
      "Epoch: 1 Batch: 0, loss: 15.051902770996094, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 100, loss: 15.64123821258545, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 200, loss: 15.845497131347656, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 300, loss: 11.960416793823242, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 400, loss: 13.844894409179688, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 500, loss: 15.645421981811523, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 600, loss: 13.773841857910156, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 700, loss: 16.051048278808594, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 800, loss: 13.633419036865234, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 900, loss: 15.884210586547852, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 1000, loss: 14.257070541381836, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 1100, loss: 14.61085033416748, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 1200, loss: 14.656597137451172, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 1300, loss: 12.851999282836914, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 1400, loss: 12.392921447753906, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 1500, loss: 14.83218002319336, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 1600, loss: 14.284154891967773, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 1700, loss: 16.149070739746094, rate: 9.6e-05\n",
      "Epoch: 1 Batch: 1800, loss: 15.077760696411133, rate: 9.6e-05\n",
      "[2023-11-19 11:30:23.838635] Epoch: 1 ends. Average loss: 14.482421789258735\n",
      "Epoch: 2 Batch: 0, loss: 14.719536781311035, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 100, loss: 15.086936950683594, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 200, loss: 14.982760429382324, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 300, loss: 11.47119140625, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 400, loss: 13.11819839477539, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 500, loss: 16.15729331970215, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 600, loss: 13.318385124206543, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 700, loss: 14.743728637695312, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 800, loss: 13.43021011352539, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 900, loss: 13.7762451171875, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 1000, loss: 13.700626373291016, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 1100, loss: 14.900354385375977, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 1200, loss: 14.206253051757812, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 1300, loss: 12.006954193115234, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 1400, loss: 13.985296249389648, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 1500, loss: 13.800480842590332, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 1600, loss: 13.656013488769531, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 1700, loss: 16.195104598999023, rate: 9.6e-05\n",
      "Epoch: 2 Batch: 1800, loss: 14.536674499511719, rate: 9.6e-05\n",
      "[2023-11-19 11:37:09.438547] Epoch: 2 ends. Average loss: 13.931762942541914\n",
      "Epoch: 3 Batch: 0, loss: 14.858607292175293, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 100, loss: 16.065034866333008, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 200, loss: 14.15780258178711, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 300, loss: 10.856510162353516, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 400, loss: 13.119136810302734, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 500, loss: 15.064289093017578, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 600, loss: 12.351985931396484, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 700, loss: 14.237420082092285, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 800, loss: 13.958842277526855, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 900, loss: 14.016419410705566, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 1000, loss: 13.839508056640625, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 1100, loss: 13.574764251708984, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 1200, loss: 14.256568908691406, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 1300, loss: 13.79486083984375, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 1400, loss: 13.121784210205078, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 1500, loss: 12.892891883850098, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 1600, loss: 14.168815612792969, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 1700, loss: 15.182762145996094, rate: 9.6e-05\n",
      "Epoch: 3 Batch: 1800, loss: 15.083293914794922, rate: 9.6e-05\n",
      "[2023-11-19 11:43:57.193346] Epoch: 3 ends. Average loss: 13.764787650279294\n",
      "Epoch: 4 Batch: 0, loss: 13.616737365722656, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 100, loss: 13.279817581176758, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 200, loss: 14.257174491882324, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 300, loss: 11.138296127319336, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 400, loss: 12.644795417785645, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 500, loss: 15.595085144042969, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 600, loss: 11.958953857421875, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 700, loss: 14.124651908874512, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 800, loss: 13.101822853088379, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 900, loss: 13.932754516601562, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 1000, loss: 13.405046463012695, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 1100, loss: 14.665225982666016, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 1200, loss: 15.26561164855957, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 1300, loss: 14.221611022949219, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 1400, loss: 11.681150436401367, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 1500, loss: 14.778499603271484, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 1600, loss: 12.555318832397461, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 1700, loss: 15.350303649902344, rate: 9.216e-05\n",
      "Epoch: 4 Batch: 1800, loss: 14.969717025756836, rate: 9.216e-05\n",
      "[2023-11-19 11:50:49.250587] Epoch: 4 ends. Average loss: 13.676279915411435\n",
      "Epoch: 5 Batch: 0, loss: 12.910420417785645, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 100, loss: 14.228837966918945, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 200, loss: 13.752612113952637, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 300, loss: 10.696512222290039, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 400, loss: 12.300020217895508, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 500, loss: 14.657001495361328, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 600, loss: 11.899182319641113, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 700, loss: 14.288090705871582, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 800, loss: 14.407698631286621, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 900, loss: 14.407600402832031, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 1000, loss: 14.334415435791016, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 1100, loss: 13.488945007324219, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 1200, loss: 12.949076652526855, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 1300, loss: 12.830611228942871, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 1400, loss: 11.802288055419922, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 1500, loss: 12.781351089477539, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 1600, loss: 13.36878776550293, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 1700, loss: 14.507747650146484, rate: 9.216e-05\n",
      "Epoch: 5 Batch: 1800, loss: 14.98013973236084, rate: 9.216e-05\n",
      "[2023-11-19 11:57:35.500960] Epoch: 5 ends. Average loss: 13.491424879841034\n",
      "Epoch: 6 Batch: 0, loss: 13.346458435058594, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 100, loss: 13.271209716796875, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 200, loss: 14.34136962890625, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 300, loss: 10.432418823242188, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 400, loss: 13.022062301635742, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 500, loss: 14.262784957885742, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 600, loss: 11.829713821411133, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 700, loss: 14.487327575683594, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 800, loss: 13.411218643188477, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 900, loss: 13.549732208251953, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 1000, loss: 14.299261093139648, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 1100, loss: 13.055071830749512, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 1200, loss: 13.353631973266602, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 1300, loss: 12.413368225097656, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 1400, loss: 11.180788040161133, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 1500, loss: 12.68450927734375, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 1600, loss: 13.227689743041992, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 1700, loss: 14.708877563476562, rate: 9.216e-05\n",
      "Epoch: 6 Batch: 1800, loss: 14.104982376098633, rate: 9.216e-05\n",
      "[2023-11-19 12:04:23.761081] Epoch: 6 ends. Average loss: 13.322583186974922\n",
      "Epoch: 7 Batch: 0, loss: 13.646871566772461, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 100, loss: 13.913926124572754, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 200, loss: 14.972929000854492, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 300, loss: 12.287701606750488, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 400, loss: 11.128700256347656, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 500, loss: 14.725122451782227, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 600, loss: 11.47659969329834, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 700, loss: 13.109302520751953, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 800, loss: 13.083196640014648, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 900, loss: 13.062446594238281, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 1000, loss: 12.923064231872559, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 1100, loss: 14.274280548095703, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 1200, loss: 13.173979759216309, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 1300, loss: 12.384514808654785, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 1400, loss: 11.693354606628418, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 1500, loss: 12.779489517211914, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 1600, loss: 13.664482116699219, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 1700, loss: 15.169490814208984, rate: 8.847359999999999e-05\n",
      "Epoch: 7 Batch: 1800, loss: 14.776533126831055, rate: 8.847359999999999e-05\n",
      "[2023-11-19 12:11:12.145081] Epoch: 7 ends. Average loss: 13.122847466592425\n",
      "Epoch: 8 Batch: 0, loss: 14.141366958618164, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 100, loss: 13.582990646362305, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 200, loss: 13.940906524658203, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 300, loss: 9.651716232299805, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 400, loss: 11.12980842590332, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 500, loss: 14.245260238647461, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 600, loss: 10.91043758392334, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 700, loss: 14.248441696166992, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 800, loss: 12.01528549194336, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 900, loss: 12.969775199890137, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 1000, loss: 14.4605712890625, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 1100, loss: 13.611358642578125, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 1200, loss: 12.885433197021484, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 1300, loss: 11.593761444091797, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 1400, loss: 11.651496887207031, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 1500, loss: 13.629161834716797, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 1600, loss: 13.603853225708008, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 1700, loss: 15.065801620483398, rate: 8.847359999999999e-05\n",
      "Epoch: 8 Batch: 1800, loss: 13.80786418914795, rate: 8.847359999999999e-05\n",
      "[2023-11-19 12:18:02.471819] Epoch: 8 ends. Average loss: 12.918288789003249\n",
      "Epoch: 9 Batch: 0, loss: 12.939029693603516, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 100, loss: 13.545780181884766, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 200, loss: 14.647623062133789, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 300, loss: 9.547211647033691, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 400, loss: 11.709969520568848, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 500, loss: 14.009305953979492, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 600, loss: 11.679079055786133, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 700, loss: 13.98372745513916, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 800, loss: 12.638362884521484, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 900, loss: 12.790651321411133, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 1000, loss: 13.942294120788574, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 1100, loss: 12.952960968017578, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 1200, loss: 12.691620826721191, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 1300, loss: 12.066900253295898, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 1400, loss: 11.389089584350586, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 1500, loss: 11.702885627746582, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 1600, loss: 12.405628204345703, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 1700, loss: 14.252764701843262, rate: 8.847359999999999e-05\n",
      "Epoch: 9 Batch: 1800, loss: 13.518548965454102, rate: 8.847359999999999e-05\n",
      "[2023-11-19 12:24:56.704052] Epoch: 9 ends. Average loss: 12.779065381474608\n",
      "Epoch: 10 Batch: 0, loss: 13.241545677185059, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 100, loss: 12.26982307434082, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 200, loss: 13.284071922302246, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 300, loss: 9.8671875, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 400, loss: 11.683027267456055, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 500, loss: 13.679571151733398, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 600, loss: 11.392195701599121, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 700, loss: 12.84326171875, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 800, loss: 13.959579467773438, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 900, loss: 13.042781829833984, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 1000, loss: 12.807927131652832, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 1100, loss: 12.694976806640625, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 1200, loss: 13.991930961608887, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 1300, loss: 12.509275436401367, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 1400, loss: 11.259303092956543, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 1500, loss: 12.599320411682129, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 1600, loss: 11.879657745361328, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 1700, loss: 13.883773803710938, rate: 8.493465599999999e-05\n",
      "Epoch: 10 Batch: 1800, loss: 13.284613609313965, rate: 8.493465599999999e-05\n",
      "[2023-11-19 12:31:47.637871] Epoch: 10 ends. Average loss: 12.608800373613933\n",
      "Epoch: 11 Batch: 0, loss: 12.732973098754883, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 100, loss: 13.18130874633789, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 200, loss: 12.625968933105469, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 300, loss: 9.99069595336914, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 400, loss: 10.690658569335938, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 500, loss: 14.025619506835938, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 600, loss: 11.64072036743164, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 700, loss: 12.911235809326172, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 800, loss: 12.23812198638916, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 900, loss: 12.731247901916504, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 1000, loss: 12.495401382446289, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 1100, loss: 13.345064163208008, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 1200, loss: 13.03635025024414, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 1300, loss: 12.091050148010254, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 1400, loss: 11.53127670288086, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 1500, loss: 11.786989212036133, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 1600, loss: 12.005128860473633, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 1700, loss: 13.643892288208008, rate: 8.493465599999999e-05\n",
      "Epoch: 11 Batch: 1800, loss: 13.246809005737305, rate: 8.493465599999999e-05\n",
      "[2023-11-19 12:38:42.564966] Epoch: 11 ends. Average loss: 12.517518495895294\n",
      "Epoch: 12 Batch: 0, loss: 12.658041000366211, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 100, loss: 13.543933868408203, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 200, loss: 14.499475479125977, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 300, loss: 10.413728713989258, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 400, loss: 11.211000442504883, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 500, loss: 13.274663925170898, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 600, loss: 11.682050704956055, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 700, loss: 12.966299057006836, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 800, loss: 12.663581848144531, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 900, loss: 12.211366653442383, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 1000, loss: 12.315714836120605, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 1100, loss: 13.321954727172852, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 1200, loss: 12.447882652282715, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 1300, loss: 11.064224243164062, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 1400, loss: 10.819243431091309, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 1500, loss: 11.941469192504883, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 1600, loss: 12.691000938415527, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 1700, loss: 14.086502075195312, rate: 8.493465599999999e-05\n",
      "Epoch: 12 Batch: 1800, loss: 13.416193008422852, rate: 8.493465599999999e-05\n",
      "[2023-11-19 12:45:31.438277] Epoch: 12 ends. Average loss: 12.371159544585492\n",
      "Epoch: 13 Batch: 0, loss: 13.75639820098877, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 100, loss: 13.901468276977539, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 200, loss: 12.99323844909668, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 300, loss: 9.544746398925781, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 400, loss: 11.12441635131836, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 500, loss: 14.007355690002441, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 600, loss: 9.958650588989258, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 700, loss: 12.36250114440918, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 800, loss: 12.90365982055664, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 900, loss: 11.930968284606934, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 1000, loss: 13.417421340942383, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 1100, loss: 12.247838973999023, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 1200, loss: 13.36952018737793, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 1300, loss: 11.095077514648438, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 1400, loss: 10.503015518188477, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 1500, loss: 12.111217498779297, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 1600, loss: 12.05929946899414, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 1700, loss: 14.171351432800293, rate: 8.153726975999998e-05\n",
      "Epoch: 13 Batch: 1800, loss: 13.144180297851562, rate: 8.153726975999998e-05\n",
      "[2023-11-19 12:52:19.266534] Epoch: 13 ends. Average loss: 12.270691717519197\n",
      "Epoch: 14 Batch: 0, loss: 13.263328552246094, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 100, loss: 12.455717086791992, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 200, loss: 12.904253005981445, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 300, loss: 10.700117111206055, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 400, loss: 10.637605667114258, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 500, loss: 12.90807056427002, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 600, loss: 11.09113883972168, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 700, loss: 12.794968605041504, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 800, loss: 12.530094146728516, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 900, loss: 12.537696838378906, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 1000, loss: 12.690521240234375, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 1100, loss: 11.826574325561523, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 1200, loss: 12.78278923034668, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 1300, loss: 10.680076599121094, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 1400, loss: 10.04588508605957, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 1500, loss: 12.037735939025879, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 1600, loss: 11.307092666625977, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 1700, loss: 14.852571487426758, rate: 8.153726975999998e-05\n",
      "Epoch: 14 Batch: 1800, loss: 13.058712005615234, rate: 8.153726975999998e-05\n",
      "[2023-11-19 12:59:14.056215] Epoch: 14 ends. Average loss: 12.180683575213054\n",
      "Epoch: 15 Batch: 0, loss: 12.336978912353516, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 100, loss: 12.259686470031738, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 200, loss: 13.690452575683594, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 300, loss: 9.299394607543945, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 400, loss: 10.944356918334961, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 500, loss: 13.80710220336914, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 600, loss: 11.041662216186523, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 700, loss: 12.323925018310547, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 800, loss: 11.614662170410156, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 900, loss: 12.063234329223633, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 1000, loss: 12.786005020141602, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 1100, loss: 12.827720642089844, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 1200, loss: 11.815650939941406, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 1300, loss: 11.734027862548828, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 1400, loss: 11.100860595703125, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 1500, loss: 12.871149063110352, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 1600, loss: 10.541036605834961, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 1700, loss: 14.153057098388672, rate: 8.153726975999998e-05\n",
      "Epoch: 15 Batch: 1800, loss: 13.330375671386719, rate: 8.153726975999998e-05\n",
      "[2023-11-19 13:06:08.478273] Epoch: 15 ends. Average loss: 12.091845707543568\n",
      "Epoch: 16 Batch: 0, loss: 11.948053359985352, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 100, loss: 12.681367874145508, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 200, loss: 13.554590225219727, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 300, loss: 8.892889022827148, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 400, loss: 11.233226776123047, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 500, loss: 13.484983444213867, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 600, loss: 11.075016975402832, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 700, loss: 13.196781158447266, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 800, loss: 11.247425079345703, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 900, loss: 11.622560501098633, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 1000, loss: 12.489788055419922, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 1100, loss: 12.656517028808594, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 1200, loss: 12.682195663452148, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 1300, loss: 11.113126754760742, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 1400, loss: 11.465948104858398, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 1500, loss: 12.29551887512207, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 1600, loss: 11.51305103302002, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 1700, loss: 13.524698257446289, rate: 7.827577896959998e-05\n",
      "Epoch: 16 Batch: 1800, loss: 13.914203643798828, rate: 7.827577896959998e-05\n",
      "[2023-11-19 13:13:04.192311] Epoch: 16 ends. Average loss: 12.02352304453358\n",
      "Epoch: 17 Batch: 0, loss: 11.944080352783203, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 100, loss: 12.64163589477539, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 200, loss: 11.92680549621582, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 300, loss: 9.772497177124023, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 400, loss: 10.628570556640625, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 500, loss: 12.617288589477539, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 600, loss: 10.451579093933105, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 700, loss: 12.455514907836914, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 800, loss: 12.46178150177002, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 900, loss: 11.543128967285156, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 1000, loss: 12.660194396972656, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 1100, loss: 11.798739433288574, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 1200, loss: 12.22806167602539, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 1300, loss: 12.081686019897461, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 1400, loss: 9.763407707214355, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 1500, loss: 12.53523063659668, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 1600, loss: 11.932353973388672, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 1700, loss: 14.243319511413574, rate: 7.827577896959998e-05\n",
      "Epoch: 17 Batch: 1800, loss: 13.363252639770508, rate: 7.827577896959998e-05\n",
      "[2023-11-19 13:19:59.889001] Epoch: 17 ends. Average loss: 11.973484281275855\n",
      "Epoch: 18 Batch: 0, loss: 11.761505126953125, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 100, loss: 12.656862258911133, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 200, loss: 11.804632186889648, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 300, loss: 9.116329193115234, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 400, loss: 11.84583568572998, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 500, loss: 13.38987922668457, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 600, loss: 10.644063949584961, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 700, loss: 12.56790828704834, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 800, loss: 11.590751647949219, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 900, loss: 11.703857421875, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 1000, loss: 11.586819648742676, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 1100, loss: 12.73002815246582, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 1200, loss: 12.204328536987305, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 1300, loss: 10.756929397583008, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 1400, loss: 10.224125862121582, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 1500, loss: 12.505929946899414, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 1600, loss: 11.20937728881836, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 1700, loss: 13.696678161621094, rate: 7.827577896959998e-05\n",
      "Epoch: 18 Batch: 1800, loss: 12.510040283203125, rate: 7.827577896959998e-05\n",
      "[2023-11-19 13:26:54.014826] Epoch: 18 ends. Average loss: 11.900694216934935\n",
      "Epoch: 19 Batch: 0, loss: 12.403390884399414, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 100, loss: 12.722574234008789, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 200, loss: 11.992961883544922, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 300, loss: 9.630117416381836, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 400, loss: 11.389873504638672, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 500, loss: 13.11039924621582, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 600, loss: 11.61485481262207, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 700, loss: 11.719304084777832, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 800, loss: 12.55925464630127, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 900, loss: 12.822429656982422, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 1000, loss: 12.742523193359375, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 1100, loss: 12.544113159179688, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 1200, loss: 12.29788589477539, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 1300, loss: 10.679431915283203, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 1400, loss: 9.51119327545166, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 1500, loss: 12.577545166015625, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 1600, loss: 10.890275001525879, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 1700, loss: 13.897088050842285, rate: 7.514474781081598e-05\n",
      "Epoch: 19 Batch: 1800, loss: 11.992088317871094, rate: 7.514474781081598e-05\n",
      "[2023-11-19 13:33:51.704551] Epoch: 19 ends. Average loss: 11.867320516263327\n",
      "Epoch: 20 Batch: 0, loss: 12.592781066894531, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 100, loss: 12.556182861328125, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 200, loss: 12.798099517822266, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 300, loss: 8.416871070861816, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 400, loss: 10.006675720214844, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 500, loss: 11.929819107055664, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 600, loss: 10.733080863952637, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 700, loss: 11.97984504699707, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 800, loss: 11.982017517089844, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 900, loss: 11.442597389221191, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 1000, loss: 11.535736083984375, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 1100, loss: 12.690766334533691, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 1200, loss: 13.20019245147705, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 1300, loss: 10.392837524414062, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 1400, loss: 11.12445068359375, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 1500, loss: 11.366058349609375, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 1600, loss: 11.030265808105469, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 1700, loss: 14.878765106201172, rate: 7.514474781081598e-05\n",
      "Epoch: 20 Batch: 1800, loss: 11.78337287902832, rate: 7.514474781081598e-05\n",
      "[2023-11-19 13:40:51.810181] Epoch: 20 ends. Average loss: 11.803075730438989\n",
      "Epoch: 21 Batch: 0, loss: 11.93272590637207, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 100, loss: 12.446393966674805, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 200, loss: 13.10500717163086, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 300, loss: 9.509855270385742, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 400, loss: 9.898736953735352, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 500, loss: 13.347969055175781, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 600, loss: 10.243395805358887, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 700, loss: 12.352161407470703, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 800, loss: 11.50085735321045, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 900, loss: 12.137033462524414, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 1000, loss: 12.391348838806152, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 1100, loss: 12.147127151489258, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 1200, loss: 12.118244171142578, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 1300, loss: 9.976597785949707, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 1400, loss: 11.266183853149414, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 1500, loss: 12.017356872558594, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 1600, loss: 12.202672958374023, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 1700, loss: 13.083393096923828, rate: 7.514474781081598e-05\n",
      "Epoch: 21 Batch: 1800, loss: 13.57469367980957, rate: 7.514474781081598e-05\n",
      "[2023-11-19 13:47:49.165891] Epoch: 21 ends. Average loss: 11.781500420778151\n",
      "Epoch: 22 Batch: 0, loss: 11.658864974975586, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 100, loss: 11.637096405029297, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 200, loss: 12.260361671447754, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 300, loss: 9.350446701049805, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 400, loss: 9.966801643371582, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 500, loss: 12.558860778808594, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 600, loss: 9.755037307739258, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 700, loss: 12.703514099121094, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 800, loss: 11.934206008911133, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 900, loss: 10.894853591918945, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 1000, loss: 11.4559326171875, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 1100, loss: 12.40228271484375, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 1200, loss: 12.133426666259766, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 1300, loss: 10.14338207244873, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 1400, loss: 10.557918548583984, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 1500, loss: 11.530550003051758, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 1600, loss: 10.463932037353516, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 1700, loss: 13.914146423339844, rate: 7.213895789838334e-05\n",
      "Epoch: 22 Batch: 1800, loss: 12.365413665771484, rate: 7.213895789838334e-05\n",
      "[2023-11-19 13:54:47.732946] Epoch: 22 ends. Average loss: 11.681235828915158\n",
      "Epoch: 23 Batch: 0, loss: 12.889974594116211, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 100, loss: 11.51868724822998, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 200, loss: 11.846551895141602, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 300, loss: 8.835400581359863, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 400, loss: 10.417705535888672, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 500, loss: 11.866630554199219, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 600, loss: 10.637818336486816, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 700, loss: 11.235111236572266, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 800, loss: 11.023165702819824, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 900, loss: 11.528215408325195, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 1000, loss: 12.487120628356934, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 1100, loss: 12.246877670288086, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 1200, loss: 13.082122802734375, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 1300, loss: 10.747889518737793, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 1400, loss: 10.446802139282227, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 1500, loss: 10.824848175048828, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 1600, loss: 12.404001235961914, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 1700, loss: 14.303354263305664, rate: 7.213895789838334e-05\n",
      "Epoch: 23 Batch: 1800, loss: 12.14520263671875, rate: 7.213895789838334e-05\n",
      "[2023-11-19 14:01:44.353956] Epoch: 23 ends. Average loss: 11.652014433581462\n",
      "Epoch: 24 Batch: 0, loss: 12.250955581665039, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 100, loss: 11.510642051696777, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 200, loss: 12.566669464111328, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 300, loss: 8.403913497924805, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 400, loss: 9.857494354248047, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 500, loss: 12.60887336730957, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 600, loss: 9.155637741088867, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 700, loss: 12.290706634521484, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 800, loss: 11.901110649108887, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 900, loss: 11.430530548095703, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 1000, loss: 12.21207046508789, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 1100, loss: 11.448628425598145, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 1200, loss: 11.497291564941406, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 1300, loss: 11.781478881835938, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 1400, loss: 10.498756408691406, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 1500, loss: 11.499526977539062, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 1600, loss: 11.309082984924316, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 1700, loss: 13.24601936340332, rate: 7.213895789838334e-05\n",
      "Epoch: 24 Batch: 1800, loss: 12.120635986328125, rate: 7.213895789838334e-05\n",
      "[2023-11-19 14:08:42.688889] Epoch: 24 ends. Average loss: 11.616001676737243\n",
      "Epoch: 25 Batch: 0, loss: 13.257843017578125, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 100, loss: 12.745845794677734, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 200, loss: 12.41246223449707, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 300, loss: 8.951456069946289, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 400, loss: 10.749370574951172, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 500, loss: 12.454028129577637, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 600, loss: 10.917182922363281, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 700, loss: 12.545862197875977, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 800, loss: 13.05573844909668, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 900, loss: 12.796598434448242, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 1000, loss: 11.13621997833252, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 1100, loss: 12.603349685668945, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 1200, loss: 12.864182472229004, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 1300, loss: 10.59487533569336, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 1400, loss: 11.18238639831543, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 1500, loss: 12.97770881652832, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 1600, loss: 10.753211975097656, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 1700, loss: 14.925605773925781, rate: 6.9253399582448e-05\n",
      "Epoch: 25 Batch: 1800, loss: 12.254753112792969, rate: 6.9253399582448e-05\n",
      "[2023-11-19 14:15:35.982554] Epoch: 25 ends. Average loss: 11.586731449912019\n",
      "Epoch: 26 Batch: 0, loss: 12.35586166381836, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 100, loss: 12.130415916442871, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 200, loss: 12.095779418945312, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 300, loss: 8.793659210205078, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 400, loss: 10.187268257141113, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 500, loss: 12.854301452636719, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 600, loss: 9.927353858947754, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 700, loss: 12.090499877929688, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 800, loss: 12.050090789794922, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 900, loss: 12.711904525756836, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 1000, loss: 11.736598014831543, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 1100, loss: 11.380941390991211, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 1200, loss: 11.973287582397461, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 1300, loss: 11.18910026550293, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 1400, loss: 10.00877857208252, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 1500, loss: 11.399792671203613, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 1600, loss: 10.953580856323242, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 1700, loss: 14.193973541259766, rate: 6.9253399582448e-05\n",
      "Epoch: 26 Batch: 1800, loss: 13.250283241271973, rate: 6.9253399582448e-05\n",
      "[2023-11-19 14:22:32.740331] Epoch: 26 ends. Average loss: 11.537754345518808\n",
      "Epoch: 27 Batch: 0, loss: 12.569232940673828, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 100, loss: 12.250101089477539, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 200, loss: 11.643072128295898, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 300, loss: 9.074974060058594, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 400, loss: 9.680986404418945, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 500, loss: 12.502569198608398, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 600, loss: 10.164433479309082, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 700, loss: 12.464067459106445, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 800, loss: 12.037612915039062, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 900, loss: 11.26473617553711, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 1000, loss: 13.262116432189941, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 1100, loss: 12.220476150512695, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 1200, loss: 12.261402130126953, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 1300, loss: 10.631237030029297, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 1400, loss: 10.351476669311523, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 1500, loss: 12.270827293395996, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 1600, loss: 10.458906173706055, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 1700, loss: 13.035136222839355, rate: 6.9253399582448e-05\n",
      "Epoch: 27 Batch: 1800, loss: 12.362763404846191, rate: 6.9253399582448e-05\n",
      "[2023-11-19 14:29:32.030609] Epoch: 27 ends. Average loss: 11.527479905287864\n",
      "Epoch: 28 Batch: 0, loss: 12.680257797241211, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 100, loss: 12.116413116455078, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 200, loss: 12.675987243652344, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 300, loss: 9.252044677734375, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 400, loss: 10.339391708374023, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 500, loss: 12.463027954101562, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 600, loss: 10.216257095336914, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 700, loss: 11.687080383300781, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 800, loss: 13.125782012939453, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 900, loss: 10.71806526184082, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 1000, loss: 12.061111450195312, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 1100, loss: 12.377616882324219, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 1200, loss: 12.098243713378906, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 1300, loss: 11.58298397064209, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 1400, loss: 10.485747337341309, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 1500, loss: 12.008007049560547, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 1600, loss: 11.275130271911621, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 1700, loss: 13.228254318237305, rate: 6.648326359915008e-05\n",
      "Epoch: 28 Batch: 1800, loss: 11.807722091674805, rate: 6.648326359915008e-05\n",
      "[2023-11-19 14:36:23.770959] Epoch: 28 ends. Average loss: 11.47668016805086\n",
      "Epoch: 29 Batch: 0, loss: 11.381723403930664, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 100, loss: 12.602699279785156, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 200, loss: 12.169390678405762, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 300, loss: 8.421377182006836, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 400, loss: 10.117446899414062, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 500, loss: 13.053583145141602, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 600, loss: 9.252171516418457, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 700, loss: 11.42245864868164, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 800, loss: 11.803491592407227, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 900, loss: 10.913257598876953, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 1000, loss: 11.812880516052246, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 1100, loss: 11.594322204589844, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 1200, loss: 12.298271179199219, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 1300, loss: 10.602518081665039, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 1400, loss: 10.465749740600586, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 1500, loss: 11.557550430297852, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 1600, loss: 11.300933837890625, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 1700, loss: 13.678893089294434, rate: 6.648326359915008e-05\n",
      "Epoch: 29 Batch: 1800, loss: 12.55202865600586, rate: 6.648326359915008e-05\n",
      "[2023-11-19 14:43:21.327243] Epoch: 29 ends. Average loss: 11.445114601532925\n",
      "Epoch: 30 Batch: 0, loss: 12.560003280639648, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 100, loss: 11.303955078125, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 200, loss: 12.223005294799805, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 300, loss: 9.292478561401367, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 400, loss: 10.198580741882324, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 500, loss: 11.114747047424316, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 600, loss: 9.738876342773438, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 700, loss: 11.440746307373047, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 800, loss: 12.64140510559082, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 900, loss: 11.449151992797852, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 1000, loss: 12.44826889038086, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 1100, loss: 12.201687812805176, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 1200, loss: 11.953886032104492, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 1300, loss: 10.364690780639648, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 1400, loss: 10.125589370727539, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 1500, loss: 10.428510665893555, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 1600, loss: 10.268668174743652, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 1700, loss: 13.523660659790039, rate: 6.648326359915008e-05\n",
      "Epoch: 30 Batch: 1800, loss: 12.40342903137207, rate: 6.648326359915008e-05\n",
      "[2023-11-19 14:50:18.974116] Epoch: 30 ends. Average loss: 11.416791567142274\n",
      "Epoch: 31 Batch: 0, loss: 12.362194061279297, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 100, loss: 12.101245880126953, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 200, loss: 12.680497169494629, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 300, loss: 8.225597381591797, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 400, loss: 9.672124862670898, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 500, loss: 12.25482177734375, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 600, loss: 9.766603469848633, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 700, loss: 11.315045356750488, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 800, loss: 11.69318675994873, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 900, loss: 10.700885772705078, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 1000, loss: 12.298666954040527, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 1100, loss: 12.414066314697266, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 1200, loss: 11.064188003540039, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 1300, loss: 11.103938102722168, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 1400, loss: 10.663877487182617, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 1500, loss: 11.080429077148438, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 1600, loss: 10.424112319946289, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 1700, loss: 13.298653602600098, rate: 6.382393305518408e-05\n",
      "Epoch: 31 Batch: 1800, loss: 12.307039260864258, rate: 6.382393305518408e-05\n",
      "[2023-11-19 14:57:15.045965] Epoch: 31 ends. Average loss: 11.398587055385146\n",
      "Epoch: 32 Batch: 0, loss: 10.725995063781738, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 100, loss: 12.983026504516602, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 200, loss: 11.478076934814453, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 300, loss: 9.01396656036377, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 400, loss: 9.781196594238281, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 500, loss: 12.188596725463867, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 600, loss: 10.11379623413086, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 700, loss: 11.996101379394531, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 800, loss: 11.973770141601562, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 900, loss: 11.614654541015625, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 1000, loss: 11.513755798339844, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 1100, loss: 11.979536056518555, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 1200, loss: 11.093324661254883, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 1300, loss: 9.942126274108887, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 1400, loss: 9.699899673461914, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 1500, loss: 11.719657897949219, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 1600, loss: 10.908937454223633, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 1700, loss: 13.08778190612793, rate: 6.382393305518408e-05\n",
      "Epoch: 32 Batch: 1800, loss: 11.29062271118164, rate: 6.382393305518408e-05\n",
      "[2023-11-19 15:04:08.900531] Epoch: 32 ends. Average loss: 11.347745231022444\n",
      "Epoch: 33 Batch: 0, loss: 11.307638168334961, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 100, loss: 12.621255874633789, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 200, loss: 11.727815628051758, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 300, loss: 8.906319618225098, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 400, loss: 9.557868003845215, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 500, loss: 13.108224868774414, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 600, loss: 9.475675582885742, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 700, loss: 11.930269241333008, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 800, loss: 10.965818405151367, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 900, loss: 10.326581001281738, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 1000, loss: 11.553500175476074, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 1100, loss: 12.207588195800781, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 1200, loss: 12.148393630981445, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 1300, loss: 10.733902931213379, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 1400, loss: 10.506624221801758, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 1500, loss: 11.02751350402832, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 1600, loss: 11.723730087280273, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 1700, loss: 12.45156478881836, rate: 6.382393305518408e-05\n",
      "Epoch: 33 Batch: 1800, loss: 12.335291862487793, rate: 6.382393305518408e-05\n",
      "[2023-11-19 15:11:02.059332] Epoch: 33 ends. Average loss: 11.311745791427352\n",
      "Epoch: 34 Batch: 0, loss: 11.579824447631836, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 100, loss: 11.630941390991211, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 200, loss: 11.444313049316406, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 300, loss: 9.902835845947266, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 400, loss: 9.730892181396484, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 500, loss: 13.09176254272461, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 600, loss: 9.360511779785156, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 700, loss: 10.314538955688477, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 800, loss: 12.049503326416016, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 900, loss: 12.414644241333008, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 1000, loss: 12.925162315368652, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 1100, loss: 11.923213958740234, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 1200, loss: 11.04139518737793, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 1300, loss: 10.57600212097168, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 1400, loss: 10.876850128173828, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 1500, loss: 11.712844848632812, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 1600, loss: 10.375814437866211, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 1700, loss: 13.628742218017578, rate: 6.127097573297672e-05\n",
      "Epoch: 34 Batch: 1800, loss: 12.623275756835938, rate: 6.127097573297672e-05\n",
      "[2023-11-19 15:17:47.579790] Epoch: 34 ends. Average loss: 11.313179820134899\n",
      "Epoch: 35 Batch: 0, loss: 12.79122543334961, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 100, loss: 12.2259521484375, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 200, loss: 12.270913124084473, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 300, loss: 8.415342330932617, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 400, loss: 9.727290153503418, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 500, loss: 12.361358642578125, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 600, loss: 9.956775665283203, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 700, loss: 11.277361869812012, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 800, loss: 11.705276489257812, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 900, loss: 10.859317779541016, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 1000, loss: 12.148491859436035, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 1100, loss: 11.24203109741211, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 1200, loss: 12.239566802978516, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 1300, loss: 10.354107856750488, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 1400, loss: 9.47097396850586, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 1500, loss: 10.797632217407227, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 1600, loss: 10.57025146484375, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 1700, loss: 13.396507263183594, rate: 6.127097573297672e-05\n",
      "Epoch: 35 Batch: 1800, loss: 11.565888404846191, rate: 6.127097573297672e-05\n",
      "[2023-11-19 15:24:19.861836] Epoch: 35 ends. Average loss: 11.292901731891916\n",
      "Epoch: 36 Batch: 0, loss: 11.468276977539062, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 100, loss: 11.313301086425781, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 200, loss: 13.652315139770508, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 300, loss: 8.62887191772461, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 400, loss: 10.22143840789795, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 500, loss: 12.546150207519531, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 600, loss: 11.26167106628418, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 700, loss: 12.262840270996094, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 800, loss: 11.402862548828125, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 900, loss: 11.37325668334961, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 1000, loss: 11.96327018737793, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 1100, loss: 11.332501411437988, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 1200, loss: 11.510255813598633, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 1300, loss: 10.53726577758789, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 1400, loss: 9.610329627990723, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 1500, loss: 11.673325538635254, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 1600, loss: 11.7540283203125, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 1700, loss: 12.535904884338379, rate: 6.127097573297672e-05\n",
      "Epoch: 36 Batch: 1800, loss: 12.125499725341797, rate: 6.127097573297672e-05\n",
      "[2023-11-19 15:30:50.548701] Epoch: 36 ends. Average loss: 11.27071880965488\n",
      "Epoch: 37 Batch: 0, loss: 12.833251953125, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 100, loss: 12.641301155090332, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 200, loss: 12.213350296020508, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 300, loss: 8.042378425598145, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 400, loss: 10.147945404052734, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 500, loss: 11.224230766296387, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 600, loss: 10.489870071411133, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 700, loss: 11.434980392456055, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 800, loss: 10.806142807006836, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 900, loss: 11.244396209716797, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 1000, loss: 11.790729522705078, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 1100, loss: 11.133346557617188, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 1200, loss: 11.544576644897461, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 1300, loss: 11.655903816223145, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 1400, loss: 9.966909408569336, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 1500, loss: 11.00841999053955, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 1600, loss: 9.970937728881836, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 1700, loss: 13.08279037475586, rate: 5.882013670365765e-05\n",
      "Epoch: 37 Batch: 1800, loss: 12.353677749633789, rate: 5.882013670365765e-05\n",
      "[2023-11-19 15:37:22.497588] Epoch: 37 ends. Average loss: 11.251826786034947\n",
      "Epoch: 38 Batch: 0, loss: 11.012947082519531, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 100, loss: 11.429506301879883, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 200, loss: 11.835474014282227, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 300, loss: 8.299882888793945, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 400, loss: 11.096248626708984, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 500, loss: 12.677362442016602, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 600, loss: 10.275867462158203, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 700, loss: 12.772268295288086, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 800, loss: 11.526934623718262, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 900, loss: 10.175875663757324, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 1000, loss: 11.63752555847168, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 1100, loss: 11.715394973754883, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 1200, loss: 12.38071060180664, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 1300, loss: 10.503694534301758, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 1400, loss: 10.399921417236328, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 1500, loss: 10.57362174987793, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 1600, loss: 10.86465072631836, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 1700, loss: 13.061241149902344, rate: 5.882013670365765e-05\n",
      "Epoch: 38 Batch: 1800, loss: 12.854455947875977, rate: 5.882013670365765e-05\n",
      "[2023-11-19 15:43:53.714260] Epoch: 38 ends. Average loss: 11.201580056812773\n",
      "Epoch: 39 Batch: 0, loss: 11.56319808959961, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 100, loss: 12.512866973876953, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 200, loss: 12.236865997314453, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 300, loss: 8.45608901977539, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 400, loss: 9.522499084472656, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 500, loss: 12.520870208740234, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 600, loss: 10.031553268432617, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 700, loss: 11.932540893554688, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 800, loss: 11.076162338256836, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 900, loss: 10.665116310119629, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 1000, loss: 11.876953125, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 1100, loss: 11.503207206726074, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 1200, loss: 11.744085311889648, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 1300, loss: 10.942066192626953, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 1400, loss: 10.962766647338867, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 1500, loss: 11.72203254699707, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 1600, loss: 10.896564483642578, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 1700, loss: 14.138603210449219, rate: 5.882013670365765e-05\n",
      "Epoch: 39 Batch: 1800, loss: 11.624267578125, rate: 5.882013670365765e-05\n",
      "[2023-11-19 15:50:25.226636] Epoch: 39 ends. Average loss: 11.195281817251585\n",
      "Epoch: 40 Batch: 0, loss: 12.602424621582031, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 100, loss: 11.792312622070312, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 200, loss: 12.250362396240234, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 300, loss: 8.228557586669922, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 400, loss: 9.728153228759766, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 500, loss: 11.827478408813477, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 600, loss: 9.678339958190918, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 700, loss: 11.57973861694336, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 800, loss: 12.132135391235352, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 900, loss: 10.337126731872559, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 1000, loss: 10.850488662719727, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 1100, loss: 12.244922637939453, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 1200, loss: 10.96554183959961, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 1300, loss: 9.867837905883789, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 1400, loss: 10.217394828796387, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 1500, loss: 11.077173233032227, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 1600, loss: 10.628426551818848, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 1700, loss: 13.463628768920898, rate: 5.6467331235511337e-05\n",
      "Epoch: 40 Batch: 1800, loss: 11.846013069152832, rate: 5.6467331235511337e-05\n",
      "[2023-11-19 15:56:56.105834] Epoch: 40 ends. Average loss: 11.1866559803453\n",
      "Epoch: 41 Batch: 0, loss: 12.718659400939941, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 100, loss: 11.780553817749023, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 200, loss: 13.153152465820312, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 300, loss: 9.005327224731445, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 400, loss: 9.918575286865234, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 500, loss: 12.084346771240234, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 600, loss: 9.61332893371582, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 700, loss: 11.16064453125, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 800, loss: 11.432389259338379, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 900, loss: 11.607099533081055, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 1000, loss: 11.323187828063965, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 1100, loss: 10.23952865600586, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 1200, loss: 11.956450462341309, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 1300, loss: 9.865198135375977, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 1400, loss: 10.309944152832031, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 1500, loss: 11.485147476196289, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 1600, loss: 10.186443328857422, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 1700, loss: 12.85275650024414, rate: 5.6467331235511337e-05\n",
      "Epoch: 41 Batch: 1800, loss: 11.96452522277832, rate: 5.6467331235511337e-05\n",
      "[2023-11-19 16:03:25.317837] Epoch: 41 ends. Average loss: 11.19050814812182\n",
      "Epoch: 42 Batch: 0, loss: 12.055253982543945, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 100, loss: 12.91013240814209, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 200, loss: 10.89982795715332, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 300, loss: 8.265336990356445, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 400, loss: 9.868875503540039, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 500, loss: 12.616260528564453, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 600, loss: 10.158188819885254, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 700, loss: 11.993545532226562, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 800, loss: 10.701943397521973, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 900, loss: 10.90727424621582, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 1000, loss: 12.266265869140625, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 1100, loss: 11.250019073486328, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 1200, loss: 11.776357650756836, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 1300, loss: 10.74812126159668, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 1400, loss: 10.131412506103516, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 1500, loss: 10.889762878417969, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 1600, loss: 11.252603530883789, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 1700, loss: 14.414680480957031, rate: 5.6467331235511337e-05\n",
      "Epoch: 42 Batch: 1800, loss: 13.29387092590332, rate: 5.6467331235511337e-05\n",
      "[2023-11-19 16:09:56.020256] Epoch: 42 ends. Average loss: 11.146421436177258\n",
      "Epoch: 43 Batch: 0, loss: 12.15196704864502, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 100, loss: 13.124341011047363, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 200, loss: 11.816969871520996, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 300, loss: 8.178112983703613, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 400, loss: 9.577808380126953, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 500, loss: 12.636557579040527, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 600, loss: 9.4737548828125, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 700, loss: 11.825675010681152, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 800, loss: 11.259700775146484, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 900, loss: 10.756402969360352, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 1000, loss: 11.331548690795898, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 1100, loss: 11.324365615844727, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 1200, loss: 11.10046100616455, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 1300, loss: 10.51716423034668, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 1400, loss: 10.509997367858887, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 1500, loss: 11.624080657958984, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 1600, loss: 9.997360229492188, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 1700, loss: 12.924959182739258, rate: 5.4208637986090884e-05\n",
      "Epoch: 43 Batch: 1800, loss: 12.913349151611328, rate: 5.4208637986090884e-05\n",
      "[2023-11-19 16:16:26.083039] Epoch: 43 ends. Average loss: 11.13889066119649\n",
      "Epoch: 44 Batch: 0, loss: 11.465920448303223, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 100, loss: 11.87817096710205, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 200, loss: 12.059319496154785, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 300, loss: 7.881809234619141, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 400, loss: 9.301698684692383, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 500, loss: 12.029243469238281, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 600, loss: 9.213203430175781, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 700, loss: 10.805459976196289, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 800, loss: 11.02315902709961, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 900, loss: 10.088764190673828, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 1000, loss: 11.085588455200195, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 1100, loss: 11.313405990600586, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 1200, loss: 11.918573379516602, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 1300, loss: 9.75955581665039, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 1400, loss: 9.88225269317627, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 1500, loss: 10.930913925170898, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 1600, loss: 9.721855163574219, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 1700, loss: 13.305757522583008, rate: 5.4208637986090884e-05\n",
      "Epoch: 44 Batch: 1800, loss: 13.346656799316406, rate: 5.4208637986090884e-05\n",
      "[2023-11-19 16:22:55.928529] Epoch: 44 ends. Average loss: 11.095231278719853\n",
      "Epoch: 45 Batch: 0, loss: 12.01708698272705, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 100, loss: 10.944994926452637, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 200, loss: 12.906624794006348, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 300, loss: 8.85764217376709, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 400, loss: 9.452573776245117, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 500, loss: 13.065245628356934, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 600, loss: 8.463218688964844, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 700, loss: 11.667396545410156, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 800, loss: 10.588754653930664, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 900, loss: 10.788249969482422, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 1000, loss: 11.646991729736328, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 1100, loss: 11.488170623779297, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 1200, loss: 12.317166328430176, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 1300, loss: 11.440988540649414, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 1400, loss: 9.864681243896484, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 1500, loss: 10.364740371704102, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 1600, loss: 11.260259628295898, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 1700, loss: 13.615493774414062, rate: 5.4208637986090884e-05\n",
      "Epoch: 45 Batch: 1800, loss: 12.97254467010498, rate: 5.4208637986090884e-05\n",
      "[2023-11-19 16:29:26.480220] Epoch: 45 ends. Average loss: 11.08065957360286\n",
      "Epoch: 46 Batch: 0, loss: 10.851702690124512, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 100, loss: 11.389601707458496, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 200, loss: 12.53189468383789, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 300, loss: 8.148250579833984, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 400, loss: 9.272783279418945, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 500, loss: 11.961418151855469, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 600, loss: 10.65230655670166, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 700, loss: 11.687393188476562, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 800, loss: 11.297835350036621, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 900, loss: 9.672880172729492, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 1000, loss: 12.157697677612305, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 1100, loss: 11.682202339172363, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 1200, loss: 12.135313034057617, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 1300, loss: 9.995044708251953, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 1400, loss: 10.28093147277832, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 1500, loss: 10.78664779663086, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 1600, loss: 9.104986190795898, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 1700, loss: 13.156662940979004, rate: 5.2040292466647244e-05\n",
      "Epoch: 46 Batch: 1800, loss: 11.67133903503418, rate: 5.2040292466647244e-05\n",
      "[2023-11-19 16:35:58.251503] Epoch: 46 ends. Average loss: 11.06453868242107\n",
      "Epoch: 47 Batch: 0, loss: 10.885719299316406, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 100, loss: 11.487366676330566, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 200, loss: 12.142101287841797, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 300, loss: 8.151193618774414, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 400, loss: 10.694921493530273, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 500, loss: 11.95202922821045, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 600, loss: 10.55785846710205, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 700, loss: 12.39058780670166, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 800, loss: 10.608537673950195, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 900, loss: 10.823980331420898, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 1000, loss: 11.138862609863281, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 1100, loss: 11.556267738342285, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 1200, loss: 11.649234771728516, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 1300, loss: 10.196792602539062, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 1400, loss: 9.638090133666992, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 1500, loss: 11.212839126586914, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 1600, loss: 10.727751731872559, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 1700, loss: 13.406415939331055, rate: 5.2040292466647244e-05\n",
      "Epoch: 47 Batch: 1800, loss: 11.767951965332031, rate: 5.2040292466647244e-05\n",
      "[2023-11-19 16:42:27.577122] Epoch: 47 ends. Average loss: 11.039187529964206\n",
      "Epoch: 48 Batch: 0, loss: 11.263744354248047, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 100, loss: 10.546676635742188, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 200, loss: 10.280794143676758, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 300, loss: 8.802213668823242, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 400, loss: 9.615304946899414, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 500, loss: 12.230039596557617, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 600, loss: 9.46728515625, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 700, loss: 12.463665008544922, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 800, loss: 11.8067626953125, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 900, loss: 10.019380569458008, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 1000, loss: 10.281322479248047, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 1100, loss: 11.42379379272461, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 1200, loss: 11.465007781982422, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 1300, loss: 10.113635063171387, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 1400, loss: 8.895273208618164, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 1500, loss: 10.68424129486084, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 1600, loss: 11.134956359863281, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 1700, loss: 11.65846061706543, rate: 5.2040292466647244e-05\n",
      "Epoch: 48 Batch: 1800, loss: 11.962882041931152, rate: 5.2040292466647244e-05\n",
      "[2023-11-19 16:48:54.552349] Epoch: 48 ends. Average loss: 11.036602015266482\n",
      "Epoch: 49 Batch: 0, loss: 11.209741592407227, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 100, loss: 10.9585542678833, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 200, loss: 11.259201049804688, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 300, loss: 8.535510063171387, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 400, loss: 9.066784858703613, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 500, loss: 12.674783706665039, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 600, loss: 9.944588661193848, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 700, loss: 11.890840530395508, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 800, loss: 10.906028747558594, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 900, loss: 11.397942543029785, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 1000, loss: 10.780123710632324, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 1100, loss: 11.813362121582031, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 1200, loss: 12.406482696533203, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 1300, loss: 11.008365631103516, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 1400, loss: 9.453737258911133, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 1500, loss: 10.895618438720703, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 1600, loss: 10.151124954223633, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 1700, loss: 13.681489944458008, rate: 4.995868076798135e-05\n",
      "Epoch: 49 Batch: 1800, loss: 11.643969535827637, rate: 4.995868076798135e-05\n",
      "[2023-11-19 16:55:12.113574] Epoch: 49 ends. Average loss: 10.997706747817782\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    losss_per_e = []\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        loss_current = train_a_batch(batch, trainer, optimizer, None)\n",
    "        losss_per_e.append(loss_current)\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {} Batch: {}, loss: {}, rate: {}'.format(e, i, loss_current, optimizer.param_groups[0]['lr']))\n",
    "            # break\n",
    "        # end\n",
    "    # end\n",
    "    \n",
    "    loss_average_per_e = sum(losss_per_e) / len(losss_per_e)\n",
    "    print('[{}] Epoch: {} ends. Average loss: {}'.format(datetime.utcnow(), e, loss_average_per_e))\n",
    "    \n",
    "    if e % 3 == 0:\n",
    "        lr_scheduler.step() # schedule per 3 epoch\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96eec954-3671-422e-82d2-5dad9edcb702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8df05c45-ed31-4d3f-ab04-ed182c182027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: Two young , White males are outside near many bushes . [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "target: [CLS] Two group of people are standing on them , with two men .\n",
      "\n",
      "\n",
      "source: Several men in hard hats are operating a giant pulley system . [SEP] [PAD] [PAD] [PAD]\n",
      "target: [CLS] Men wearing hard hats work work work work on a construction work work .\n",
      "\n",
      "\n",
      "source: A little girl climbing into a wooden playhouse . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "target: [CLS] A little little girl jumping over a swing over water .\n",
      "\n",
      "\n",
      "source: A man in a blue shirt is standing on a ladder cleaning a window [SEP] [PAD]\n",
      "target: [CLS] A man in a blue shirt is standing on a blue shirt on a\n",
      "\n",
      "\n",
      "source: Two men are at the stove preparing food . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "target: [CLS] Two men are at the food at the food at the .\n",
      "\n",
      "\n",
      "source: A man in green holds a guitar while the other man observes his shirt [SEP] [PAD]\n",
      "target: [CLS] A man wearing a man plays guitar while another man plays his guitar while\n",
      "\n",
      "\n",
      "source: A man is smiling at a stuffed lion [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "target: [CLS] A young baby looking at a table looking at the camera .\n",
      "\n",
      "\n",
      "source: A trendy girl talking on her cellphone while gliding slowly down the street . [SEP] [PAD]\n",
      "target: [CLS] A woman is talking on her cellphone while talking on her cellphone while her\n",
      "\n",
      "\n",
      "source: A woman with a large purse is walking by a gate . [SEP] [PAD] [PAD] [PAD]\n",
      "target: [CLS] A man with a black dog is walking a small child walking a small\n",
      "\n",
      "\n",
      "source: Boys dancing on poles in the middle of the night . [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "target: [CLS] People on the street in the street on the street in the street .\n",
      "\n",
      "\n",
      "source: A ballet class of five girls jumping in sequence . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "target: [CLS] A group of young girls race is jumping in a race .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For s2s head\n",
    "def greedy_generate(model, head, tokenizer, collator, **kwargs):\n",
    "    id_start = tokenizer.id_cls\n",
    "    id_end = tokenizer.id_sep\n",
    "    size_seq_max = collator.size_seq_max\n",
    "\n",
    "    ids_encoder = kwargs['ids_encoder']\n",
    "    masks_encoder = kwargs['masks_encoder']\n",
    "\n",
    "    outputs_encoder = model.embed_and_encode(ids_encoder=ids_encoder, masks_encoder=masks_encoder)\n",
    "    ids_decoder = torch.zeros(1, 1).fill_(id_start).type_as(ids_encoder.data)\n",
    "\n",
    "    for i in range(size_seq_max - 1):\n",
    "        masks_decoder = collator.subsequent_mask(ids_decoder.size(1)).type_as(ids_encoder.data)\n",
    "        outputs_decoder = model.embed_and_decode(ids_decoder=ids_decoder, masks_encoder=masks_encoder, output_encoder=outputs_encoder, masks_decoder=masks_decoder)\n",
    "        outputs_ffn = head.ffn(outputs_decoder)\n",
    "        outputs_s2s = head.extractor(outputs_ffn)   # outputs_mlm = prediction_logits\n",
    "\n",
    "        logits_nextword = torch.softmax(outputs_s2s[:, -1], dim=-1)  # mynote: select dim2=-1, remain=all; last is the next\n",
    "        \n",
    "        id_nextword = torch.argmax(logits_nextword, dim=-1)\n",
    "        id_nextword = id_nextword.data[0]\n",
    "\n",
    "        if id_nextword == id_end:\n",
    "            break\n",
    "        # end\n",
    "\n",
    "        ids_decoder = torch.cat([ids_decoder, torch.zeros(1, 1).type_as(ids_encoder.data).fill_(id_nextword)], dim=1)\n",
    "    # end\n",
    "\n",
    "    return ids_decoder\n",
    "# end\n",
    "\n",
    "eval_source = to_map_style_dataset(valid_iter)\n",
    "dataloader_eval = DataLoader(train_source, 1, shuffle=False, collate_fn=collator)\n",
    "\n",
    "for i, batch in enumerate(dataloader_eval):\n",
    "    info_batch = batch()\n",
    "    result = greedy_generate(trainer.model, trainer.manager.get_head(SimpleDecoderHead_S2S), tokenizer, collator, **info_batch)\n",
    "    sentence_predicted = tokenizer.decode(result.cpu().tolist()[0])\n",
    "    sentence_origin = tokenizer.decode(info_batch['labels_decoder'].cpu().tolist()[0])\n",
    "    print('source: {}\\ntarget: {}\\n\\n'.format(sentence_origin, sentence_predicted))\n",
    "    if i >= 10:\n",
    "        break\n",
    "# end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68df3a99-6f6a-4fee-8e7a-c9ffc168e9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.158493995666504, acc: 0.3333333432674408\n",
      "source: of men onto\n",
      "input: [MASK] [MASK] [MASK]\n",
      "predict: of people on\n",
      "\n",
      "\n",
      "loss: 5.933133602142334, acc: 0.6666666865348816\n",
      "source: A green .\n",
      "input: [MASK] [MASK] [MASK]\n",
      "predict: A large .\n",
      "\n",
      "\n",
      "loss: 5.286011219024658, acc: 0.6666666865348816\n",
      "source: on woman .\n",
      "input: [MASK] [MASK] [MASK]\n",
      "predict: on child .\n",
      "\n",
      "\n",
      "loss: 22.68341827392578, acc: 0.25\n",
      "source: men hut iced over\n",
      "input: [MASK] [MASK] [MASK] [MASK]\n",
      "predict: men up outdoor a\n",
      "\n",
      "\n",
      "loss: 8.720269203186035, acc: 0.5\n",
      "source: is sitting a small\n",
      "input: [MASK] [MASK] [MASK] [MASK]\n",
      "predict: is standing a a\n",
      "\n",
      "\n",
      "loss: 8.706644058227539, acc: 0.5\n",
      "source: lady in coat holding\n",
      "input: [MASK] [MASK] [MASK] [MASK]\n",
      "predict: man in shirt holding\n",
      "\n",
      "\n",
      "loss: 4.33071756362915, acc: 0.6666666865348816\n",
      "source: dog running black\n",
      "input: [MASK] [MASK] [MASK]\n",
      "predict: dog running brown\n",
      "\n",
      "\n",
      "loss: 13.611639022827148, acc: 0.25\n",
      "source: wearing jersey bat at\n",
      "input: [MASK] [MASK] [MASK] [MASK]\n",
      "predict: in , game at\n",
      "\n",
      "\n",
      "loss: 10.238090515136719, acc: 0.6666666865348816\n",
      "source: A office is\n",
      "input: [MASK] [MASK] [MASK]\n",
      "predict: A shirt is\n",
      "\n",
      "\n",
      "loss: 16.66707420349121, acc: 0.3333333432674408\n",
      "source: smiling in bike\n",
      "input: [MASK] [MASK] [MASK]\n",
      "predict: young in .\n",
      "\n",
      "\n",
      "loss: 7.971144676208496, acc: 0.6666666865348816\n",
      "source: A standing rocks\n",
      "input: [MASK] [MASK] [MASK]\n",
      "predict: A standing <unk>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_source = to_map_style_dataset(valid_iter)\n",
    "dataloader_eval = DataLoader(eval_source, 1, shuffle=False, collate_fn=collator)\n",
    "# dataloader_eval = DataLoader(train_source, 1, shuffle=False, collate_fn=collator)\n",
    "\n",
    "for i, batch in enumerate(dataloader_eval):\n",
    "    info_batch = batch()\n",
    "    trainer.forward(**info_batch)\n",
    "    \n",
    "    head = trainer.manager.get_head(SimpleEncoderHead_MLM)\n",
    "    out_mlm = head.cache.outputs\n",
    "    loss_mlm = head.get_loss()\n",
    "    \n",
    "    out_mlm = out_mlm.cpu().detach()\n",
    "    loss_mlm = loss_mlm.cpu().detach()\n",
    "    labels_mlm = info_batch['labels_encoder'].cpu().detach()\n",
    "    masks_encoder = info_batch['masks_encoder'].cpu().detach()\n",
    "    segments_encoder = info_batch['segments_encoder'].cpu().detach()\n",
    "    ids_encoder = info_batch['ids_encoder'].cpu().detach()\n",
    "    \n",
    "    masks_masked = torch.logical_xor(masks_encoder, segments_encoder) & segments_encoder # True is masked\n",
    "    masks_masked_perbatch = masks_masked[:,0,:]\n",
    "    \n",
    "    predicts = out_mlm.softmax(dim=-1).argmax(dim=-1)\n",
    "    # print segments\n",
    "    # sentence_predicts = tokenizer.decode(out_mlm.softmax(dim=-1).argmax(dim=-1).masked_select(segments_encoder[:, 0, :]).numpy().tolist())\n",
    "    # sentence_labels = tokenizer.decode(labels_mlm.masked_select(segments_encoder[:, 0, :]).numpy().tolist())\n",
    "    # sentence_inputs = tokenizer.decode(ids_encoder.masked_select(segments_encoder[:, 0, :]).numpy().tolist())\n",
    "    \n",
    "    # print masks\n",
    "    sentence_predicts = tokenizer.decode(out_mlm.softmax(dim=-1).argmax(dim=-1).masked_select(masks_masked_perbatch).numpy().tolist())\n",
    "    sentence_labels = tokenizer.decode(labels_mlm.masked_select(masks_masked_perbatch).numpy().tolist())\n",
    "    sentence_inputs = tokenizer.decode(ids_encoder.masked_select(masks_masked_perbatch).numpy().tolist())\n",
    "    \n",
    "    \n",
    "#     sentence_predicts = tokenizer.decode(out_mlm.softmax(dim=-1).argmax(dim=-1).numpy().tolist()[0])\n",
    "#     sentence_labels = tokenizer.decode(labels_mlm.numpy().tolist()[0])\n",
    "#     sentence_inputs = tokenizer.decode(ids_encoder.numpy().tolist()[0])\n",
    "    \n",
    "    predicts_masked = out_mlm.softmax(dim=-1).argmax(dim=-1).masked_select(masks_masked_perbatch)\n",
    "    labels_masked = labels_mlm.masked_select(masks_masked_perbatch)\n",
    "    \n",
    "    acc = torch.count_nonzero(predicts_masked == labels_masked) / labels_masked.shape[0]\n",
    "    \n",
    "    # acc = torch.count_nonzero(out_mlm.softmax(dim=-1).argmax(dim=-1).view(-1) == labels_mlm.view(-1)) / labels_mlm.view(-1).shape[0]\n",
    "    print('loss: {}, acc: {}\\nsource: {}\\ninput: {}\\npredict: {}\\n\\n'.format(loss_mlm.item(), acc, sentence_labels, sentence_inputs, sentence_predicts))\n",
    "    \n",
    "    if i >= 10:\n",
    "        break\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc3bec-6f6c-4dfc-bb43-78bf90d4ecf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b612b-5c04-4894-bac8-79e970a0abb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
