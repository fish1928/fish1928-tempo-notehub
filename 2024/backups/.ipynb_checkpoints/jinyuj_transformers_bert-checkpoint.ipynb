{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a36c05-aa20-4909-a5e9-a172529e22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from os.path import exists\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad, one_hot\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "### utils.py ###\n",
    "\n",
    "class Dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "# end\n",
    "\n",
    "### utils.py ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### core.py ###\n",
    "\n",
    "\"Produce N identical layers.\"\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "    \"Take in model size and number of heads.\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    # end\n",
    "\n",
    "\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # end\n",
    "        p_attn = scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        # end\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "    # end\n",
    "\n",
    "\n",
    "    \"Implements Figure 2\"\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = self.attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)\n",
    "    # end\n",
    "# end class\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A residual connection followed by a layer norm.\n",
    "Note for code simplicity the norm is first as opposed to last.\n",
    "\"\"\"\n",
    "class ResidualLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, size, dropout=0.1, eps=1e-6):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.norm = torch.nn.LayerNorm(size, eps)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    # end\n",
    "\n",
    "    \"Apply residual connection to any sublayer with the same size.\"\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    # end\n",
    "# end class\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleIDEmbeddings(nn.Module):\n",
    "    def __init__(self, size_vocab, dim_hidden, id_pad):\n",
    "        super(SimpleIDEmbeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(size_vocab, dim_hidden, padding_idx=id_pad)\n",
    "        self.dim_hidden = dim_hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.lut(x)\n",
    "        return result * math.sqrt(self.dim_hidden)\n",
    "    # end\n",
    "\n",
    "    def get_shape(self):\n",
    "        return (self.lut.num_embeddings, self.lut.embedding_dim)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\"Implement the PE function.\"\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_positional, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        self.dim_positional = dim_positional\n",
    "        pe = torch.zeros(max_len, dim_positional)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, dim_positional, 2) * -(math.log(10000.0) / dim_positional)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).to('cuda')\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return x\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleEmbedder(nn.Module):    # no segment embedder as we do not need that\n",
    "    def __init__(self, size_vocab=None, dim_hidden=128, dropout=0.1, id_pad=0):\n",
    "        super(SimpleEmbedder, self).__init__()\n",
    "        self.size_vocab = size_vocab\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.id_pad = id_pad\n",
    "\n",
    "        self.embedder = nn.Sequential(\n",
    "            SimpleIDEmbeddings(size_vocab, dim_hidden, id_pad),\n",
    "            PositionalEncoding(dim_hidden),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    # end\n",
    "\n",
    "    def forward(self, ids_input):   # (batch, seqs_with_padding)\n",
    "        return self.embedder(ids_input)\n",
    "    # end\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.size_vocab\n",
    "    # end\n",
    "# end\n",
    "\n",
    "### core.py ###\n",
    "\n",
    "\n",
    "\n",
    "class SimpleEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, dim_feedforward, n_head, dropout=0.1):\n",
    "        super(SimpleEncoderLayer, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        self.layer_attention = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_feedforward = PositionwiseFeedForward(dim_hidden, dim_feedforward, dropout)\n",
    "        self.layers_residual = clones(ResidualLayer(dim_hidden, dropout), 2)\n",
    "    # end\n",
    "\n",
    "    def forward(self, embeddings, masks, *args):\n",
    "        embeddings = self.layers_residual[0](embeddings, lambda embeddings: self.layer_attention(embeddings, embeddings, embeddings, masks))\n",
    "        return self.layers_residual[1](embeddings, self.layer_feedforward)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "class SimpleDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_hidden, dim_feedforward, n_head, dropout=0.1):\n",
    "        super(SimpleDecoderLayer, self).__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        self.layer_attention_decoder = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_attention_encoder = MultiHeadedAttention(n_head, dim_hidden)\n",
    "        self.layer_feedforward = PositionwiseFeedForward(dim_hidden, dim_feedforward, dropout)\n",
    "        self.layers_residual = clones(ResidualLayer(dim_hidden, dropout), 3)\n",
    "\n",
    "    def forward(self, embeddings, masks_decoder, output_encoder, masks_encoder, *args):\n",
    "        embeddings = self.layers_residual[0](embeddings, lambda embeddings: self.layer_attention_decoder(embeddings, embeddings, embeddings, masks_decoder))\n",
    "        embeddings = self.layers_residual[1](embeddings, lambda embeddings: self.layer_attention_encoder(embeddings, output_encoder, output_encoder, masks_encoder))\n",
    "        return self.layers_residual[2](embeddings, self.layer_feedforward)\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleTransformerStack(nn.Module):\n",
    "\n",
    "    def __init__(self, obj_layer, n_layers):\n",
    "        super(SimpleTransformerStack, self).__init__()\n",
    "        self.layers = clones(obj_layer, n_layers)\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(obj_layer.dim_hidden)\n",
    "        self.keys_cache = ['output']\n",
    "        self.cache = Dotdict({\n",
    "            'outputs': None\n",
    "        })\n",
    "    # end\n",
    "\n",
    "    def forward(self, embedding_encoder=None, masks_encoder=None, output_encoder=None, embedding_decoder=None, masks_decoder=None ,noncache=False, **kwargs):  # input -> (batch, len_seq, vocab)\n",
    "\n",
    "        if output_encoder is not None and embedding_decoder is not None and masks_decoder is not None:\n",
    "            embeddings = embedding_decoder\n",
    "        else:\n",
    "            embeddings = embedding_encoder\n",
    "        # end\n",
    "\n",
    "        for layer in self.layers:\n",
    "            embeddings = layer(embeddings, masks_encoder, output_encoder, masks_decoder)\n",
    "        # end\n",
    "\n",
    "        outputs = self.norm(embeddings)\n",
    "\n",
    "        if not noncache:\n",
    "            self.cache.outputs = outputs\n",
    "        # end\n",
    "\n",
    "        return outputs\n",
    "    # end\n",
    "\n",
    "    # def get_vocab_size(self):\n",
    "    #     return self.embedder.embedder_token.shape[-1]\n",
    "    # # end\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class SimpleEncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, embedder_encoder, embedder_decoder):\n",
    "        super(SimpleEncoderDecoder, self).__init__()\n",
    "\n",
    "        self.embedder_encoder = embedder_encoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.embedder_decoder = embedder_decoder\n",
    "        self.decoder = decoder\n",
    "    # end\n",
    "\n",
    "    def forward(self, ids_encoder=None, masks_encoder=None, ids_decoder=None, masks_decoder=None, nocache=False, **kwargs):\n",
    "        self.clear_cache()\n",
    "\n",
    "        embedding_encoder = self.embedder_encoder(ids_encoder)\n",
    "        output_encoder = self.encoder(\n",
    "            embedding_encoder=embedding_encoder,\n",
    "            masks_encoder=masks_encoder,\n",
    "            nocache=nocache\n",
    "        )\n",
    "        embedding_decoder = self.embedder_decoder(ids_decoder)\n",
    "        output_decoder = self.decoder(\n",
    "            masks_encoder=masks_encoder,\n",
    "            output_encoder=output_encoder,\n",
    "            embedding_decoder=embedding_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            nocache=nocache\n",
    "        )\n",
    "        return output_decoder\n",
    "    # end\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.encoder.clear_cache()\n",
    "        self.decoder.clear_cache()\n",
    "    # end\n",
    "\n",
    "    def get_vocab_size(self, name_embedder):\n",
    "        embedder = getattr(self, f'embedder_{name_embedder}')\n",
    "        return embedder.get_vocab_size()\n",
    "    # end\n",
    "\n",
    "# end\n",
    "\n",
    "class LinearAndNorm(nn.Module):\n",
    "    def __init__(self, dim_in = None, dim_out = None, eps_norm=1e-12):\n",
    "        super(LinearAndNorm, self).__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(dim_in, dim_out)\n",
    "        self.norm = torch.nn.LayerNorm(dim_out, eps_norm)\n",
    "    # end\n",
    "\n",
    "    def forward(self, seqs_in):\n",
    "        return self.norm(self.linear(seqs_in))\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class SimpleEncoderHead_MLM(nn.Module):\n",
    "\n",
    "    def __init__(self, size_vocab, dim_hidden=128):\n",
    "        super(SimpleEncoderHead_MLM, self).__init__()\n",
    "\n",
    "        self.ffn = LinearAndNorm(dim_in=dim_hidden, dim_out=dim_hidden)\n",
    "        self.extractor = torch.nn.Linear(dim_hidden, size_vocab)\n",
    "\n",
    "        self.func_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.keys_cache = ['loss', 'outputs', 'labels_mlm']\n",
    "        self.cache = Dotdict({\n",
    "            'loss': None,\n",
    "            'outputs': None,\n",
    "            'labels_mlm': None\n",
    "        })\n",
    "    # end\n",
    "\n",
    "    def forward(self, model, labels_encoder=None, nocache=False, **kwargs):   # labels_input -> (batch, seq, labels)\n",
    "        self.clear_cache()\n",
    "\n",
    "        outputs_encoder = model.encoder.cache.outputs    # -> (batch, seqs_input, dim_hidden)\n",
    "        outputs_mlm = self.extractor(self.ffn(outputs_encoder)) # outputs_mlm = prediction_logits\n",
    "\n",
    "        if not nocache:\n",
    "            self.cache.outputs = outputs_mlm\n",
    "            self.cache.labels_mlm = labels_encoder\n",
    "        # end\n",
    "\n",
    "        return outputs_mlm\n",
    "    # end\n",
    "\n",
    "    def get_loss(self):\n",
    "        outputs_mlm = self.cache.outputs\n",
    "        labels_mlm = self.cache.labels_mlm\n",
    "\n",
    "        loss_mlm = self.func_loss(outputs_mlm.view(-1, outputs_mlm.size(-1)), labels_mlm.view(-1))  # labels is 1-hot labels\n",
    "        self.cache.loss = loss_mlm\n",
    "        return loss_mlm\n",
    "    # end\n",
    "\n",
    "    def evaluate(self):\n",
    "        pass\n",
    "    # end\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class SimpleEncoderHead_Similarity(nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(SimpleEncoderHead_Similarity, self).__init__()\n",
    "        self.model = model\n",
    "        self.func_loss = torch.nn.MSELoss()\n",
    "        self.cos_score_transformation = torch.nn.Identity()\n",
    "        self.cache = Dotdict({\n",
    "            'loss': None,\n",
    "            'labels_sim': None\n",
    "        })\n",
    "    # end\n",
    "\n",
    "    def forward(self, labels_sim=None, nocache=False, **kwargs):  # labels_sim (batch/2, 1)   for every two sentences, we have a label\n",
    "        self.clear_cache()\n",
    "\n",
    "        outputs_encoder = self.model.encoder.outputs\n",
    "        size_batch, len_seq, dim_hidden = outputs_encoder.shape\n",
    "\n",
    "        if size_batch % 2 != 0:\n",
    "            raise Exception('sim calculation is not prepared as size_batch % 2 != 0')\n",
    "        # end\n",
    "\n",
    "        # pooling (batch, pair, dim_hidden)\n",
    "        outputs_pooling = outputs_encoder[:, 0, :].squeeze(1).view(-1, 2, dim_hidden)   # might cls + sep, but abandon now (as it's not easy to get sep for every batch, different location)\n",
    "        sims = self.cos_score_transformation(torch.cosine_similarity(outputs_pooling))  # -> (batch, scores)\n",
    "\n",
    "        if not nocache:\n",
    "            self.cache.sims = sims\n",
    "            self.cache.labels_sim = labels_sim\n",
    "        # end\n",
    "\n",
    "        return sims\n",
    "    # end\n",
    "\n",
    "    def get_loss(self):\n",
    "        sims = self.cache.sims\n",
    "        labels_sim = self.cache.labels_sim\n",
    "\n",
    "        loss_sim = self.func_loss(sims, labels_sim)\n",
    "        self.cache.loss = loss_sim\n",
    "        return loss_sim\n",
    "    # end\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def evaluate(self):\n",
    "        pass\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class SimpleDecoderHead_S2S(nn.Module):\n",
    "\n",
    "    def __init__(self, size_vocab, dim_hidden=128):\n",
    "        super(SimpleDecoderHead_S2S, self).__init__()\n",
    "        self.ffn = LinearAndNorm(dim_in=dim_hidden, dim_out=dim_hidden)\n",
    "        self.extractor = torch.nn.Linear(dim_hidden, size_vocab)\n",
    "\n",
    "        self.func_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.keys_cache = ['loss', 'outputs', 'labels_s2s']\n",
    "        self.cache = Dotdict({\n",
    "            'loss': None,\n",
    "            'outputs': None,\n",
    "            'labels_s2s': None\n",
    "        })\n",
    "    # end\n",
    "\n",
    "    def forward(self, model, labels_decoder=None, nocache=False, **kwargs):   # labels_input -> (batch, seq, labels)\n",
    "        self.clear_cache()\n",
    "\n",
    "        outputs_decoder = model.decoder.cache.outputs    # -> (batch, seqs_input, dim_hidden)\n",
    "        outputs_s2s = self.extractor(self.ffn(outputs_decoder))   # outputs_mlm = prediction_logits\n",
    "\n",
    "\n",
    "        if not nocache:\n",
    "            self.cache.outputs = outputs_s2s\n",
    "            self.cache.labels_s2s = labels_decoder\n",
    "        # end\n",
    "\n",
    "        return outputs_s2s\n",
    "    # end\n",
    "\n",
    "\n",
    "    def get_loss(self):\n",
    "        outputs_s2s = self.cache.outputs\n",
    "        labels_s2s = self.cache.labels_s2s\n",
    "\n",
    "        loss_s2s = self.func_loss(outputs_s2s.view(-1, outputs_s2s.size(-1)), labels_s2s.view(-1))  # labels is 1-hot labels\n",
    "        self.cache.loss = loss_s2s\n",
    "        return loss_s2s\n",
    "    # end\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        pass\n",
    "    # end\n",
    "\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for key_cache in self.keys_cache:\n",
    "            self.cache[key_cache] = None\n",
    "        # end\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class HeadManager:\n",
    "    def __init__(self):\n",
    "        self.index_name_head = {}\n",
    "    # end\n",
    "\n",
    "    def register(self, head):\n",
    "        name_head = head.__class__.__name__\n",
    "        self.index_name_head[name_head] = head\n",
    "    # end\n",
    "\n",
    "    def forward(self, model, **kwargs):\n",
    "        for name_head, head in self.index_name_head.items():\n",
    "            head.forward(model, **kwargs)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def get_head(self, klass):\n",
    "        return self.index_name_head.get(klass.__name__, None)\n",
    "    # end\n",
    "    \n",
    "    def to(self, device):\n",
    "        for name_head in self.index_name_head:\n",
    "            self.index_name_head[name_head] = self.index_name_head[name_head].to(device)\n",
    "        # end\n",
    "        \n",
    "        return self\n",
    "    # end\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        for _, head in self.index_name_head.items():\n",
    "            head.clear_cache()\n",
    "        # end\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Builder:\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def build_model_with_s2s(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder)\n",
    "\n",
    "        manager = HeadManager()\n",
    "        manager.register(SimpleDecoderHead_S2S(size_vocab, dim_hidden))\n",
    "        return model, manager\n",
    "    # end\n",
    "    \n",
    "    @classmethod\n",
    "    def build_model_with_mlm(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder)\n",
    "\n",
    "        manager = HeadManager()\n",
    "        manager.register(SimpleEncoderHead_MLM(size_vocab, dim_hidden))\n",
    "        return model, manager\n",
    "    # end\n",
    "\n",
    "    @classmethod\n",
    "    def build_model(cls, size_vocab, dim_hidden, dim_feedforward, n_head, n_layer):\n",
    "        embedder_encoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "        embedder_decoder = SimpleEmbedder(size_vocab=size_vocab, dim_hidden=dim_hidden)\n",
    "\n",
    "        sample_encoder = SimpleEncoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "        sample_decoder = SimpleDecoderLayer(dim_hidden, dim_feedforward, n_head)\n",
    "\n",
    "        encoderstack = SimpleTransformerStack(sample_encoder, n_layer)\n",
    "        decoderstack = SimpleTransformerStack(sample_decoder, n_layer)\n",
    "\n",
    "        model = SimpleEncoderDecoder(encoderstack, decoderstack, embedder_encoder, embedder_decoder)\n",
    "\n",
    "        manager = HeadManager()\n",
    "        manager.register(SimpleEncoderHead_MLM(size_vocab, dim_hidden))\n",
    "        manager.register(SimpleDecoderHead_S2S(size_vocab, dim_hidden))\n",
    "        return model, manager\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class TokenizerWrapper:\n",
    "    def __init__(self, vocab, splitter):\n",
    "        self.splitter = splitter\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.id_pad = len(vocab)\n",
    "        self.id_cls = len(vocab) + 1\n",
    "        self.id_sep = len(vocab) + 2\n",
    "        self.id_mask = len(vocab) + 3\n",
    "        \n",
    "        self.size_vocab = len(vocab) + 4\n",
    "\n",
    "        self.token_pad = '[PAD]'\n",
    "        self.token_cls = '[CLS]'\n",
    "        self.token_sep = '[SEP]'\n",
    "        self.token_mask = '[MASK]'\n",
    "        \n",
    "    # end\n",
    "\n",
    "    def encode(self, line):\n",
    "        return self.vocab([doc.text for doc in self.splitter(line)])\n",
    "    # end\n",
    "\n",
    "    def decode(self):\n",
    "        pass\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    DEVICE = 'cuda'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}\n",
    "        for k, v in kwargs.items():\n",
    "            self.kwargs[k] = v.to(Batch.DEVICE)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.kwargs\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "class Collator_S2S:\n",
    "\n",
    "    def __init__(self, tokenizer, size_seq_max):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size_seq_max = size_seq_max\n",
    "    # end\n",
    "\n",
    "    def __call__(self, list_corpus_line):\n",
    "\n",
    "        tokens_input_encoder = []\n",
    "        tokens_input_decoder = []\n",
    "        tokens_label_decoder = []\n",
    "\n",
    "        for corpus_line in list_corpus_line:\n",
    "            tokens = self.tokenizer.encode(corpus_line[1])\n",
    "            \n",
    "            # TODO: check edge\n",
    "            if len(tokens) > self.size_seq_max - 2:\n",
    "                tokens = tokens[:self.size_seq_max-2]\n",
    "            # end\n",
    "\n",
    "            tokens_input_encoder.append([self.tokenizer.id_cls] + tokens + [self.tokenizer.id_sep])\n",
    "            tokens_input_decoder.append([self.tokenizer.id_cls] + tokens)\n",
    "            tokens_label_decoder.append(tokens + [self.tokenizer.id_sep])\n",
    "        # end\n",
    "\n",
    "        inputs_encoder, masks_encoder, segments_encoder, labels_encoder = self.pad_sequences(tokens_input_encoder, self.size_seq_max, need_masked=0.1)\n",
    "        inputs_decoder, masks_decoder, segments_decoder = self.pad_sequences(tokens_input_decoder, self.size_seq_max, need_diagonal=True)\n",
    "        labels_decoder, masks_label, segments_label = self.pad_sequences(tokens_label_decoder, self.size_seq_max)\n",
    "\n",
    "        return Batch(\n",
    "            ids_encoder=inputs_encoder,  # contains [mask]s\n",
    "            masks_encoder=masks_encoder,\n",
    "            labels_encoder=labels_encoder,  # doesn't contain [mask]\n",
    "            ids_decoder=inputs_decoder,\n",
    "            masks_decoder=masks_decoder,\n",
    "            labels_decoder=labels_decoder,\n",
    "            segments_label=segments_label\n",
    "        )\n",
    "    # end\n",
    "\n",
    "    # return masks_attention?, return masks_segment?\n",
    "    def pad_sequences(self, sequences, size_seq_max, need_diagonal=False, need_masked=0): # need_diagonal and need_masked cannot both set, one for bert seq one for s2s seq\n",
    "        id_pad = self.tokenizer.id_pad\n",
    "        id_mask = self.tokenizer.id_mask\n",
    "\n",
    "        sequences_padded = []\n",
    "        sequences_masked_padded = []\n",
    "\n",
    "        for sequence in sequences:\n",
    "            len_seq = len(sequence)\n",
    "\n",
    "            count_pad = size_seq_max - len_seq\n",
    "\n",
    "            sequence = torch.LongTensor(sequence)\n",
    "            sequence_padded = torch.cat((sequence, torch.LongTensor([id_pad] * count_pad)))\n",
    "            sequences_padded.append(sequence_padded)\n",
    "\n",
    "            if need_masked:\n",
    "                index_masked = list(range(len_seq))\n",
    "                random.shuffle(index_masked)\n",
    "                index_masked = torch.LongTensor(index_masked[:int(need_masked * len_seq)])\n",
    "\n",
    "                sequence_masked = sequence.detach().clone()\n",
    "                sequence_masked.index_fill_(0, index_masked, id_mask)\n",
    "                sequence_masked_padded = torch.cat((sequence, torch.LongTensor([id_pad] * count_pad)))\n",
    "                sequences_masked_padded.append(sequence_masked_padded)\n",
    "            # end\n",
    "    #   # end for\n",
    "\n",
    "        inputs = torch.stack(sequences_padded)  # (batch, size_seq_max)\n",
    "        if need_masked:\n",
    "            inputs_masked_padded = torch.stack(sequences_masked_padded)\n",
    "\n",
    "        masks_segment = (inputs != self.tokenizer.id_pad).unsqueeze(-2).expand(inputs.shape[0], inputs.shape[-1], inputs.shape[-1]) #(nbatch, seq, seq)\n",
    "        masks_attention = self.make_std_mask(inputs, self.tokenizer.id_pad) if need_diagonal else masks_segment\n",
    "\n",
    "        if need_masked:\n",
    "            masks_masked = (inputs_masked_padded == id_mask).unsqueeze(-2).expand(inputs.shape[0], inputs.shape[-1], inputs.shape[-1])\n",
    "            masks_attention = masks_attention & masks_masked\n",
    "            return inputs_masked_padded, masks_attention, masks_segment, inputs # (inputs, masks_attention, masks_segment, labels)\n",
    "        else:\n",
    "            return inputs, masks_attention, masks_segment\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "\n",
    "    def subsequent_mask(self, size):\n",
    "        \"Mask out subsequent positions.\"\n",
    "        attn_shape = (1, size, size)\n",
    "        subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "            torch.uint8\n",
    "        )\n",
    "        return subsequent_mask == 0\n",
    "\n",
    "    def make_std_mask(self, tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & self.subsequent_mask(tgt.size(-1)).type_as(\n",
    "            tgt_mask.data\n",
    "        )\n",
    "        return tgt_mask\n",
    "    # end\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c31c3af2-4245-47ca-bdab-82812054d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def Multi30k(language_pair=None):\n",
    "    corpus_lines_train = []\n",
    "\n",
    "    for lan in language_pair:\n",
    "        with open('text/train.{}'.format(lan), 'r') as file:\n",
    "            corpus_lines_train.append(file.read().splitlines())\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    corpus_train = list(zip(*corpus_lines_train))\n",
    "\n",
    "    corpus_lines_eval = []\n",
    "\n",
    "    for lan in language_pair:\n",
    "        with open('text/val.{}'.format(lan), 'r') as file:\n",
    "            corpus_lines_eval.append(file.read().splitlines())\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    corpus_lines_eval = list(zip(*corpus_lines_train))\n",
    "\n",
    "    return corpus_lines_train, corpus_lines_eval, None\n",
    "# end\n",
    "\n",
    "\n",
    "def load_vocab(spacy_en):\n",
    "    if not os.path.exists(\"vocab.pt\"):\n",
    "        vocab_tgt = build_vocabulary(spacy_en)\n",
    "        torch.save(vocab_tgt, \"vocab.pt\")\n",
    "    else:\n",
    "        vocab_tgt = torch.load(\"vocab.pt\")\n",
    "    print(\"Finished.\\nVocabulary sizes: {}\".format(len(vocab_tgt)))\n",
    "    return vocab_tgt\n",
    "# end\n",
    "\n",
    "def load_spacy():\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_en\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055cc657-caf5-4495-ba1e-c40637a03901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes: 6191\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "\n",
    "gpu = 0\n",
    "torch.cuda.set_device(gpu)\n",
    "\n",
    "seq_max = 64\n",
    "batch_size = 4\n",
    "dim_hidden = 128\n",
    "dim_feedforward = 128\n",
    "n_head = 4\n",
    "n_layer = 2\n",
    "\n",
    "spacy_en = load_spacy()\n",
    "vocab = load_vocab(spacy_en)\n",
    "tokenizer = TokenizerWrapper(vocab, spacy_en)\n",
    "\n",
    "train_iter, valid_iter, _ = Multi30k(language_pair=(\"de\", \"en\"))\n",
    "train_source = to_map_style_dataset(valid_iter)\n",
    "\n",
    "\n",
    "collator = Collator_S2S(tokenizer, seq_max)\n",
    "dataloader_train = DataLoader(train_source, batch_size, shuffle=False, collate_fn=collator)\n",
    "\n",
    "model, manager = Builder.build_model(tokenizer.size_vocab, dim_hidden, dim_feedforward, n_head, n_layer)\n",
    "model = model.to('cuda')\n",
    "manager = manager.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b499ec3-35e4-49c7-952d-5e58d0b899f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a_batch(batch, model, manager, optimizer=None):\n",
    "    model.forward(**batch())    # save to cache\n",
    "    manager.forward(model, **batch())    # save to cache\n",
    "\n",
    "    loss_mlm = manager.get_head(SimpleEncoderHead_MLM).get_loss()\n",
    "    loss_s2s = manager.get_head(SimpleDecoderHead_S2S).get_loss()\n",
    "    # loss_sim = manager_head.get_head('sim_encoder').get_loss()\n",
    "\n",
    "    # cross entropy loss\n",
    "    loss_crossentropy = loss_mlm + loss_s2s\n",
    "    # loss_crossentropy = loss_s2s\n",
    "    # loss_crossentropy = loss_mlm\n",
    "    print(loss_crossentropy)\n",
    "    loss_crossentropy.backward()\n",
    "\n",
    "    # mean square loss\n",
    "    # loss_sim.backward()\n",
    "    if optimizer:\n",
    "        optimizer.step()\n",
    "    # end\n",
    "    \n",
    "    manager.clear_cache()\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42cdcefe-6f82-4ac8-bebf-fff5b298be99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.9083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9723, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9284, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9623, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9714, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9698, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0188, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9418, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9667, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0255, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0253, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9853, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9595, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0515, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1340, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1388, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1025, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0440, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9221, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1966, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8982, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9172, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0028, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9144, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9819, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.7886, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.2703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.2788, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8747, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.3106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8946, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9718, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9787, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9950, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9345, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0657, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9205, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9571, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1451, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9427, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9794, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1264, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9908, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8837, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9279, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0620, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0528, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1190, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0154, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9727, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0503, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9629, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0130, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9676, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0086, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.2364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9428, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9801, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9713, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0044, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0506, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1200, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0379, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1184, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1031, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8272, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0456, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9732, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9993, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.2591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.2305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8904, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1343, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9840, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1764, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9356, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1339, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9815, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9922, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8765, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9131, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0124, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1570, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1907, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9809, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9280, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.8500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9934, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.0229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9651, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(16.1084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9553, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9276, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(15.9541, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8e1843276605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_a_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-368502444f10>\u001b[0m in \u001b[0;36mtrain_a_batch\u001b[0;34m(batch, model, manager, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_a_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# save to cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# save to cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss_mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSimpleEncoderHead_MLM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9e4883be78ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids_encoder, masks_encoder, ids_decoder, masks_decoder, nocache, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m         )\n\u001b[1;32m    312\u001b[0m         \u001b[0membedding_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         output_decoder = self.decoder(\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0mmasks_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0moutput_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9e4883be78ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embedding_encoder, masks_encoder, output_encoder, embedding_decoder, masks_decoder, noncache, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;31m# end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9e4883be78ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, masks_decoder, output_encoder, masks_encoder, *args)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_residual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_attention_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_residual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_attention_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_residual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_feedforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9e4883be78ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;34m\"Apply residual connection to any sublayer with the same size.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;31m# end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# end class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9e4883be78ab>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(embeddings)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_residual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_attention_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_decoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_residual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_attention_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_residual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_feedforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-9e4883be78ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# 2) Apply attention on all the projected vectors in batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         x, self.attn = self.attention(\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         )\n",
      "\u001b[0;32m<ipython-input-1-9e4883be78ab>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(self, query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mp_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mp_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;31m# end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in dataloader_train:\n",
    "    train_a_batch(batch, model, manager)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0cfdb-8ffe-4ef4-92d9-1d0dbbac0465",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
