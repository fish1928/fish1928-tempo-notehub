{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06cc04e9-4b6a-4eff-89ba-27022f5f6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from numpy import argmax\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class FactoryEncoder:\n",
    "\n",
    "    def __init__(self, tokenizer, device, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "    # end\n",
    "\n",
    "    # Q: WHY NOT USING PARAMETERS?\n",
    "    # A: FOR FUN\n",
    "    def get_instance(self, sentences):\n",
    "        instance = SimpleEncoder(sentences)\n",
    "        instance.set_tokenizer(self.tokenizer)\n",
    "        instance.set_device(self.device)\n",
    "        instance.set_max_length(self.max_length)\n",
    "        return instance\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class SimpleEncoder(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "    # end\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index]\n",
    "        sentence = ' '.join(sentence.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence, None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        for key in inputs:\n",
    "            inputs[key].to(self.device)\n",
    "        # end\n",
    "\n",
    "        return inputs\n",
    "    # end\n",
    "\n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    # end\n",
    "\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "    # end\n",
    "\n",
    "    def set_max_length(self, max_length):\n",
    "        self.max_length = max_length\n",
    "    # end\n",
    "\n",
    "\n",
    "class FactoryDecoder:\n",
    "    def __init__(self, labels):\n",
    "        self.labels = labels\n",
    "    # end\n",
    "\n",
    "    # outputs_raw:\n",
    "    def get_instance(self, outputs_raw):\n",
    "        instance = SimpleDecoder(self.labels, outputs_raw)\n",
    "        instance.enable_label().enable_proba().enable_index()\n",
    "        return instance\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class SimpleDecoder:\n",
    "\n",
    "    def __init__(self, labels, outputs_raw):\n",
    "        self.outputs_raw = outputs_raw\n",
    "        self.labels = labels\n",
    "        self.decoders = {}\n",
    "    # end\n",
    "\n",
    "    def enable_label(self):\n",
    "        def decode_label(outputs_raw):\n",
    "            return [self.labels for i in range(len(outputs_raw))]\n",
    "        # end\n",
    "\n",
    "        self.decoders['label'] = decode_label\n",
    "        return self\n",
    "    # end\n",
    "\n",
    "    def enable_proba(self):\n",
    "        def decode_proba(outputs_raw):\n",
    "            # print('jinyuj: decode_proba:185 outputs_raw: {}'.format(outputs_raw))\n",
    "            probas = softmax(outputs_raw, axis=1).tolist()\n",
    "            return probas\n",
    "        # end\n",
    "\n",
    "        self.decoders['proba'] = decode_proba\n",
    "        return self\n",
    "    # end\n",
    "\n",
    "    def enable_index(self):\n",
    "        def decode_index(outputs_raw):\n",
    "            return [list(range(len(self.labels))) for i in range(len(outputs_raw))]\n",
    "        # end\n",
    "\n",
    "        self.decoders['index'] = decode_index\n",
    "        return self\n",
    "    # end\n",
    "\n",
    "    def decode(self, str_items_output):\n",
    "        items_output = str_items_output.split(',')\n",
    "        dict_output_decoded = {}\n",
    "\n",
    "        for item_output in items_output:\n",
    "            if item_output in self.decoders:\n",
    "                func_decode = self.decoders[item_output]\n",
    "                dict_output_decoded[item_output] = func_decode(self.outputs_raw)\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        # get inner quantity(quantity of samples)\n",
    "        num_samples = len(dict_output_decoded[list(dict_output_decoded.keys())[0]])\n",
    "        num_klasses = len(dict_output_decoded[list(dict_output_decoded.keys())[0]][0])\n",
    "\n",
    "        # infos_outputs = [[{}] * num_klasses] * num_samples\n",
    "        infos_outputs = [[{} for j in range(num_klasses)] for i in range(num_samples)]\n",
    "\n",
    "        # transform\n",
    "        #  {\n",
    "        #       'label': [['safe', 'comment'], ['safe', 'comment']],\n",
    "        #       'proba': [[0.02207409217953682, 0.9779258370399475], [0.7, 0.3]],\n",
    "        #       'index': [[0, 1], [0, 1]]\n",
    "        #  }\n",
    "        # to\n",
    "        #  [[{'label': 'safe', 'proba': 0.01701054722070694, 'index': 0}, {'label': 'comment', 'proba': 0.982989490032196, 'index': 1}]]\n",
    "        for item_output in dict_output_decoded:\n",
    "            outputs_decoded = dict_output_decoded[item_output]\n",
    "            for i, output_decoded in enumerate(outputs_decoded):        # i for num_samples\n",
    "                for j, item_decoded in enumerate(output_decoded):\n",
    "                    infos_outputs[i][j][item_output] = item_decoded           # output_decoded = ['safe', comment]\n",
    "                # end\n",
    "            # end\n",
    "        # end transformation\n",
    "\n",
    "        return infos_outputs\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class SimpleBertClassifier(torch.nn.Module):\n",
    "\n",
    "    DEFAULT_FILENAME_LABEL = 'labels.json'\n",
    "\n",
    "    def __init__(self, path_folder_model=None, name_model=None, max_length=512):\n",
    "        super(SimpleBertClassifier, self).__init__()\n",
    "\n",
    "        self.path_folder_model = path_folder_model\n",
    "        self.name_model = name_model\n",
    "        path_file_labels = os.path.join(self.path_folder_model, self.__class__.DEFAULT_FILENAME_LABEL)\n",
    "\n",
    "        with open(path_file_labels, 'r') as file:\n",
    "            self.labels_output_classifier = sorted(json.load(file))\n",
    "        # end\n",
    "\n",
    "        self.dict_label_index = {label: index for index, label in enumerate(self.labels_output_classifier)}\n",
    "        self.classifier_max_length = max_length\n",
    "        self.classifier = None\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.loaded = False\n",
    "    # end\n",
    "\n",
    "    def forward(self, *inputs, **params):\n",
    "        output = self.classifier(*inputs, **params)\n",
    "        return output\n",
    "    # end\n",
    "\n",
    "    def load(self):\n",
    "        if not self.loaded:\n",
    "            self.classifier = DistilBertForSequenceClassification.from_pretrained(self.path_folder_model)\n",
    "            self.tokenizer = DistilBertTokenizerFast.from_pretrained(self.name_model)\n",
    "\n",
    "            self.factory_encoder = FactoryEncoder(self.tokenizer, self.device, self.classifier_max_length)\n",
    "            self.factory_decoder = FactoryDecoder(self.labels_output_classifier)\n",
    "            self.loaded = True\n",
    "        # end\n",
    "\n",
    "        return self\n",
    "    # end\n",
    "\n",
    "    def predicts(self, samples_input, outputs='label'):\n",
    "        items_output = outputs  # transform name\n",
    "        encoder = self.factory_encoder.get_instance(samples_input)\n",
    "        outputs_raw = []\n",
    "        for sample_encoded in encoder:\n",
    "            # sample_encoded['output_hidden_states'] = True\n",
    "            # sample_encoded['output_attentions'] = True\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_raw = self(**sample_encoded).logits.cpu().numpy().flatten()\n",
    "                outputs_raw.append(output_raw)\n",
    "            # end\n",
    "        # end\n",
    "        outputs_raw = np.array(outputs_raw)\n",
    "\n",
    "        decoder = self.factory_decoder.get_instance(outputs_raw)\n",
    "        info_output = decoder.decode(items_output)\n",
    "        return info_output\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e366184b-d47f-4d62-9936-5c497a0b73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import copy\n",
    "\n",
    "\n",
    "class SimpleLimeExplainer:\n",
    "\n",
    "    TOKENIZER_DEFAULT = str.split\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            token_mask=None,\n",
    "            num_samples=None,\n",
    "            alpha=None,\n",
    "            name_distance_function=None,\n",
    "            scale_distance=None,\n",
    "            name_solver=None,\n",
    "            width_kernel=None\n",
    "        ):\n",
    "        # hard code configuration starts\n",
    "        self.tokenizer = self.__class__.TOKENIZER_DEFAULT\n",
    "        self.function_kernel = self._exponential_kernel\n",
    "        self.width_kernel = width_kernel\n",
    "\n",
    "        # parameter configuration starts\n",
    "        self.token_mask = token_mask\n",
    "        self.num_samples = num_samples\n",
    "        self.alpha = alpha\n",
    "        self.function_distance = functools.partial(metrics.pairwise.pairwise_distances, metric=name_distance_function)\n",
    "        self.scale_distance = scale_distance\n",
    "        self.name_solver = name_solver\n",
    "    # end\n",
    "\n",
    "\n",
    "    def explain(self, sentence, model_predict):\n",
    "        tokens = self.tokenizer(sentence)\n",
    "        num_features = len(tokens)\n",
    "\n",
    "        masks_input = self._prepare_masks(self.num_samples, num_features)\n",
    "        distances_input = self._calculate_distance(self.function_distance, self.scale_distance, self.function_kernel, self.width_kernel, masks_input)\n",
    "        samples_input = list(self._get_perturbations(tokens, masks_input, self.token_mask))\n",
    "\n",
    "        #\n",
    "        # [\n",
    "        #       [\n",
    "        #           {'label': 'safe', 'proba': 0.01701054722070694, 'index': 0},        # item 1\n",
    "        #           {'label': 'comment', 'proba': 0.982989490032196, 'index': 1},       # item 2\n",
    "        #           {...}                                                               # item 3\n",
    "        #       ],      # sample 1\n",
    "        #       [...],  # sample 2\n",
    "        #       ...     # sample N\n",
    "        # ]\n",
    "        list_items_sample = model_predict.predicts(samples_input, outputs='label,proba,index')    # numpy outputs\n",
    "        list_items_sample_sorted = [sorted(items_sample, key=lambda item: -item['proba']) for items_sample in list_items_sample]\n",
    "        indexes_base = np.array([items_sample_sorted[0]['index'] for items_sample_sorted in list_items_sample_sorted])\n",
    "        # print('indexes_base: {}'.format(indexes_base.tolist()))\n",
    "        info_final = {}\n",
    "        items_sample_sorted_origin = list_items_sample_sorted[0]\n",
    "        for item_sample_sorted_root in items_sample_sorted_origin:\n",
    "\n",
    "            index_item = item_sample_sorted_root['index']\n",
    "            label_item = item_sample_sorted_root['label']\n",
    "            proba_item = item_sample_sorted_root['proba']\n",
    "            # print('jinyuj: simple_lime_explainer.explain {} {} {}'.format(index_item, label_item, proba_item))\n",
    "\n",
    "            indexes = copy.deepcopy(indexes_base)\n",
    "            indexes[np.where(indexes != index_item)] = -1\n",
    "            indexes[np.where(indexes == index_item)] = 1\n",
    "\n",
    "            # print('indexes: {}'.format(indexes.tolist()))\n",
    "\n",
    "            model_explain = linear_model.Ridge(alpha=self.alpha, solver=self.name_solver)\n",
    "            model_explain.fit(masks_input, indexes, sample_weight=distances_input)  # one-time use\n",
    "            coefs_token = model_explain.coef_\n",
    "\n",
    "            info_final[label_item] = {\n",
    "                'confidence': proba_item,\n",
    "                'features': [(token, coefs_token) for token, coefs_token in zip(tokens, coefs_token)]\n",
    "            }\n",
    "\n",
    "        return info_final\n",
    "    # end\n",
    "\n",
    "    def _prepare_masks(self, num_samples, num_features):\n",
    "        masks = self._sample_masks(num_samples + 1, num_features)\n",
    "        # print('jinyuj: _prepare_masks:64: masks.shape: {}, num_samples: {}'.format(masks.shape, num_samples))\n",
    "        assert masks.shape[0] == num_samples + 1, 'Expected num_samples + 1 masks.'\n",
    "        all_true_mask = np.ones_like(masks[0], dtype=np.bool)\n",
    "        masks[0] = all_true_mask\n",
    "        return masks\n",
    "    # end\n",
    "\n",
    "    def _sample_masks(self, num_samples, num_features):\n",
    "        rng = np.random.RandomState()\n",
    "        positions = np.tile(np.arange(num_features), (num_samples, 1))\n",
    "        permutation_fn = np.vectorize(rng.permutation, signature='(n)->(n)')\n",
    "        permutations = permutation_fn(positions)  # A shuffled range of positions.\n",
    "        num_disabled_features = rng.randint(1, num_features + 1, (num_samples, 1))\n",
    "        return permutations >= num_disabled_features\n",
    "    # end\n",
    "\n",
    "    def _calculate_distance(self, function_distance, scale_distance, function_kernel, width_kernel, masks):\n",
    "        distances = function_distance(masks[0].reshape(1, -1), masks).flatten()\n",
    "        distances = scale_distance * distances\n",
    "        distances = function_kernel(distances, width_kernel)\n",
    "        return distances\n",
    "    # end\n",
    "\n",
    "    def _get_perturbations(self, tokens, masks, token_mask):\n",
    "        for mask in masks:\n",
    "            parts = [t if mask[i] else token_mask for i, t in enumerate(tokens)]\n",
    "            yield ' '.join(parts)\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    def _exponential_kernel(self, distance, width_kernel):\n",
    "        return np.sqrt(np.exp(-(distance ** 2) / width_kernel ** 2))\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70dcfbf6-6835-47b8-9a12-658510c9d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_explainer = {\n",
    "    \"token_mask\": \"[MASK]\",\n",
    "    \"alpha\": 1.0,\n",
    "    \"name_distance_function\": \"cosine\",\n",
    "    \"name_solver\": \"cholesky\",\n",
    "    \"num_samples\": 128,\n",
    "    \"width_kernel\": 25,\n",
    "    \"scale_distance\": 100.0\n",
    "}\n",
    "explainer = SimpleLimeExplainer(**config_explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689251bf-e153-4576-b2ad-c999fe928feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3599fed6-47e9-4f2c-932d-010fbb9b3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bert-base-uncased\"\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_dir = 'models_target'\n",
    "max_length = 512\n",
    "\n",
    "dir_data = 'data'\n",
    "name_data_file = 'log_content_3_20220209.csv'\n",
    "#name_data_file_target = 'log_content_3_20220209_target.csv'\n",
    "\n",
    "name_label_file = 'labels.json'\n",
    "path_data_relative = os.path.join(dir_data, name_data_file)\n",
    "path_label_relative = os.path.join(dir_data, name_label_file)\n",
    "#path_file_target_relative = os.path.join(dir_data, name_data_file_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869edaaf-4412-4af4-8d08-06b1bde46c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_passages(path_data, path_label):\n",
    "    df = pd.read_csv(path_data)\n",
    "    documents = df['processed'].to_list()\n",
    "    \n",
    "    with open(path_label, 'r') as file:\n",
    "        labels_list = sorted(json.load(file))\n",
    "    # end\n",
    "    \n",
    "    labels_all = {l:idx for idx, l in enumerate(labels_list)}\n",
    "\n",
    "    return documents, labels_list\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a70e6a9-1c2b-4e58-b984-6c72954b672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_predict, target_names = read_passages(path_data_relative, path_label_relative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e6ba056-ef8b-40da-a77a-e32d33029ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleBertClassifier(\n",
       "  (classifier): DistilBertForSequenceClassification(\n",
       "    (distilbert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SimpleBertClassifier('models_target', 'distilbert-base-uncased')\n",
    "classifier.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408d7134-a291-40bd-9445-8bc9922f2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "dict_text_predict = defaultdict(list)\n",
    "\n",
    "for id_text, text_predict in enumerate(texts_predict):\n",
    "    \n",
    "    if text_predict not in dict_text_predict:\n",
    "\n",
    "        print('[{}] start to handle {}: {}'.format(datetime.now().strftime(\"%H:%M:%S\"), id_text, text_predict))\n",
    "        info_explained = explainer.explain(text_predict, classifier)\n",
    "\n",
    "        with open('data/explains/{}.json'.format(id_text), 'w+') as file:\n",
    "            file.write(json.dumps({\n",
    "                    'processed': text_predict,\n",
    "                    'detail': info_explained\n",
    "                }, indent=4)\n",
    "            )\n",
    "        # end\n",
    "        print('[{}] done with {}'.format(datetime.now().strftime(\"%H:%M:%S\"), id_text))\n",
    "    else:\n",
    "        print('[{}] ignore duplicated case: {}'.format(datetime.now().strftime(\"%H:%M:%S\"), id_text))\n",
    "    # end\n",
    "    dict_text_predict[text_predict].append(id_text)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efeda879-bf2a-4ce1-beba-aa265e7151a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dict_text_predict.json', 'w+') as file:\n",
    "    file.write(json.dumps(dict_text_predict))\n",
    "# end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
