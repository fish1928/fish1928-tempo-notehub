{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d55c56-c3f7-4070-aa74-dc4ecaa7dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from numpy import argmax\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class FactoryEncoder:\n",
    "\n",
    "    def __init__(self, tokenizer, device, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "    # end\n",
    "\n",
    "    # Q: WHY NOT USING PARAMETERS?\n",
    "    # A: FOR FUN\n",
    "    def get_instance(self, sentences):\n",
    "        instance = SimpleEncoder(sentences)\n",
    "        instance.set_tokenizer(self.tokenizer)\n",
    "        instance.set_device(self.device)\n",
    "        instance.set_max_length(self.max_length)\n",
    "        return instance\n",
    "    # end\n",
    "# end\n",
    "\n",
    "class SimpleEncoder(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "    # end\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index]\n",
    "        sentence = ' '.join(sentence.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            sentence, None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(self.device)\n",
    "        # end\n",
    "\n",
    "        return inputs\n",
    "    # end\n",
    "\n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    # end\n",
    "\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "    # end\n",
    "\n",
    "    def set_max_length(self, max_length):\n",
    "        self.max_length = max_length\n",
    "    # end\n",
    "\n",
    "\n",
    "class SimpleBertClassifier(torch.nn.Module):\n",
    "\n",
    "    DEFAULT_FILENAME_CLASSIFIER = '.model.json'\n",
    "    DEFAULT_FILENAME_BERT = 'bert_config.json'\n",
    "    DEFAULT_FILENAME_MODEL = 'model.pt'\n",
    "    DEFAULT_KEYS_IGNORED_CLASSIFIER = ['metrics', 'allmetrics']\n",
    "\n",
    "    def __init__(self, path_folder_model=None):\n",
    "        super(SimpleBertClassifier, self).__init__()\n",
    "\n",
    "        filename_config_classifier = self.__class__.DEFAULT_FILENAME_CLASSIFIER\n",
    "        filename_config_bert = self.__class__.DEFAULT_FILENAME_BERT\n",
    "        filename_model = self.__class__.DEFAULT_FILENAME_MODEL\n",
    "        keys_ignored_classifier = self.__class__.DEFAULT_KEYS_IGNORED_CLASSIFIER\n",
    "\n",
    "\n",
    "        self.path_folder_model = path_folder_model\n",
    "        self.path_config_bert = os.path.join(path_folder_model, filename_config_bert)\n",
    "        self.path_config_classifier = os.path.join(path_folder_model, filename_config_classifier)\n",
    "        self.path_file_model = os.path.join(path_folder_model, filename_model)\n",
    "\n",
    "        with open(self.path_config_classifier, 'r') as file:\n",
    "            config_classifier = json.load(file)\n",
    "        # end\n",
    "\n",
    "        for key in keys_ignored_classifier:\n",
    "            del(config_classifier[key])\n",
    "        # end\n",
    "\n",
    "        # classfier parameters\n",
    "        self.classifier_input_size = config_classifier.get('bert').get('input_size')\n",
    "        self.classifier_max_length = config_classifier.get('bert').get('max_length')\n",
    "        self.classifier_output_size = config_classifier.get('bert').get('output_size')\n",
    "        #\n",
    "\n",
    "        self.labels_output_classifier = config_classifier.get('classes')\n",
    "        self.dict_label_index = {label: index for index, label in enumerate(self.labels_output_classifier)}\n",
    "\n",
    "        self.l1 = None\n",
    "        self.linear = None\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.loaded = False\n",
    "    # end\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
    "        output_bert = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_bert[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "    # end\n",
    "\n",
    "    def load(self):\n",
    "        if not self.loaded:\n",
    "            print('loading BERT from: {}'.format(self.path_config_bert))\n",
    "            self.l1 = BertModel(BertConfig.from_pretrained(self.path_config_bert))\n",
    "            print('loading {} x {} linear classfier layer'.format(self.classifier_input_size, self.classifier_output_size))\n",
    "            self.classifier = torch.nn.Linear(self.classifier_input_size, self.classifier_output_size)\n",
    "\n",
    "            self.to(self.device)\n",
    "            self.load_state_dict(torch.load(self.path_file_model, map_location=torch.device(self.device)))\n",
    "            self.l1.myname = 'jinyuj'\n",
    "\n",
    "            print('loading Tokenizer from: {}'.format(self.path_folder_model))\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.path_folder_model)\n",
    "            self.factory_encoder = FactoryEncoder(self.tokenizer, self.device, self.classifier_max_length)\n",
    "            self.loaded = True\n",
    "        # end\n",
    "\n",
    "        return self\n",
    "    # end\n",
    "\n",
    "    def predicts(self, samples_input):\n",
    "        encoder = self.factory_encoder.get_instance(samples_input)\n",
    "        outputs_raw = []\n",
    "        for sample_encoded in encoder:\n",
    "            with torch.no_grad():\n",
    "                output_raw = self(**sample_encoded).cpu().numpy().flatten().tolist()\n",
    "                # print('jinyuj: predicts:83 output_raw: {}'.format(output_raw))\n",
    "                outputs_raw.append(output_raw)\n",
    "            # end\n",
    "        # end\n",
    "\n",
    "        info_result = {\n",
    "            'outputs': outputs_raw,\n",
    "            'labels': self.labels_output_classifier\n",
    "        }\n",
    "\n",
    "        return info_result\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed85e3d6-78b3-4d95-b50d-e9c1c09d0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, errno\n",
    "import json\n",
    "import yaml\n",
    "import argparse\n",
    "import sys\n",
    "import traceback\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# default constants\n",
    "\n",
    "# The name of the bert model to load\n",
    "VBERT_MODEL = \"vBERT-2020-Base\"\n",
    "\n",
    "# vBERT-Base's hidden layer size is 768\n",
    "# vBERT-Large's hidden layer size is 1024\n",
    "CLASSIFICATION_LAYER_WIDTH = 768 if \"Base\" in VBERT_MODEL else 1024\n",
    "\n",
    "# Maximum string length\n",
    "# Anything longer will be truncated\n",
    "# Maximum length can not exceed 512, though generally can be shorter\n",
    "MAX_STRING_LEN = 128\n",
    "\n",
    "\n",
    "# ## Class for transforming text for input into BERT\n",
    "# \n",
    "# This class tokenizes the text and converts the tokens into tensors of token ids with an attention mask, along with the string's class label\n",
    "\n",
    "class Prepare(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data[1][index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        label = self.data[0][index]\n",
    "\n",
    "        target = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': label\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# ## vBERT Class for classification\n",
    "# \n",
    "# To use vBERT for classification, you have to add a classification layer on top of the language model. \n",
    "# vBERT encodes the data.  The classification layer receives the encoding from vBERT and claculates class probabilities.\n",
    "\n",
    "class BertClassifier(torch.nn.Module):\n",
    "    # Create a classification network that uses vBERT to encode the input and pass to a classification layer\n",
    "    #\n",
    "    # Input configurable parameters:\n",
    "    # root: full path name of the model directory, where the .pt, vocab and .model.json/bert_config.json file are found\n",
    "    # name: name of the model (for reporting/diagnostic)\n",
    "    # input_size: width of classification layer (e.g. 768)\n",
    "    # output_size: number of output classes (task dependent)\n",
    "    # max_length: maximum number of characters in input string for trainig/inference\n",
    "    # classes: array of output_size with class names\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.config = config\n",
    "        self.model_dir = config.get(\"root\", \".\")\n",
    "        self.model_name = config.get(\"name\", VBERT_MODEL)\n",
    "        self.classification_layer_width = config.get(\"input_size\", CLASSIFICATION_LAYER_WIDTH)\n",
    "        self.classes = config.get(\"classes\", [\"0\", \"1\"])\n",
    "        self.num_classes = config.get(\"output_size\", len(self.classes))\n",
    "        self.max_string_length = config.get(\"max_length\", MAX_STRING_LEN)\n",
    "        self.bert_config_file = self.model_dir + \"/bert_config.json\"\n",
    "        self.model_file_path = self.model_dir + \"/model.pt\"\n",
    "        self.device = config.get(\"device\", \"cpu\")\n",
    "        self.metrics = [ \"accuracy\", \"precision\", \"recall\", \"F-score\" ]\n",
    "\n",
    "        # Load the vBERT vocabulary into the tokenizer\n",
    "        print('Loading Tokenizer from: {}'.format(self.model_dir))\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_dir)\n",
    "\n",
    "    def load_bert_for_training(self):\n",
    "        print('Loading vBERT model: ' + self.model_name)\n",
    "        self.l1 = BertModel.from_pretrained(pretrained_model_name_or_path=self.config.bert_model_dir)\n",
    "        print(\"Adding {}x{} classification layer\".format(self.classification_layer_width, self.num_classes))\n",
    "        self.classifier = torch.nn.Linear(self.classification_layer_width, self.num_classes)\n",
    "\n",
    "    def load_bert_for_inference(self):\n",
    "        print('Loading vBERT config from: {}'.format(self.bert_config_file))\n",
    "        self.l1 = BertModel(BertConfig.from_pretrained(self.bert_config_file))\n",
    "        print(\"Adding {}x{} classification layer\".format(self.classification_layer_width, self.num_classes))\n",
    "        self.classifier = torch.nn.Linear(self.classification_layer_width, self.num_classes)\n",
    "\n",
    "    # Encode the input with vBERT, read output from the last layer of vBERT, and pass to the classification layer\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "    def do_inference(self, data_loader, test_output=None):\n",
    "        model = self.eval()\n",
    "        predictions = []\n",
    "        real_values = []\n",
    "        with torch.no_grad():\n",
    "            for data in data_loader:\n",
    "                input_ids = data['ids'].to(self.device)\n",
    "                attention_mask = data['mask'].to(self.device)\n",
    "                targets = data['targets'].to(self.device)\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                predictions.extend(preds)\n",
    "                real_values.extend(targets)\n",
    "                if test_output != None:\n",
    "                    test_output.extend(outputs)\n",
    "        predictions = torch.stack(predictions).cpu()\n",
    "        real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "        test_accu = 100 * accuracy_score(real_values, predictions)\n",
    "        test_precision, test_recall, test_fscore, ignore = precision_recall_fscore_support(real_values, predictions, average='macro')\n",
    "        test_precision *= 100\n",
    "        test_recall *= 100\n",
    "        test_fscore *= 100\n",
    "        metrics = [ test_accu, test_precision, test_recall, test_fscore ]\n",
    "        return predictions, real_values, metrics\n",
    "\n",
    "    def classify_text(self, txt: str, conf=None):\n",
    "        dataset = pd.DataFrame.from_dict({ 'row': [ 0, txt ]}, orient='index')\n",
    "        prepared_set = Prepare(dataset, self.tokenizer, self.max_string_length)\n",
    "        params = {'batch_size': 1,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': 0\n",
    "                }\n",
    "        loader = DataLoader(prepared_set, **params)\n",
    "        outputs = []\n",
    "        predictions, real_values, metrics = self.do_inference(loader, outputs)\n",
    "        classes = self.classes\n",
    "        result = { 'classes': [ ], 'text': txt, 'top_class': classes[int(predictions[0])], 'top_class_index': int(predictions[0]) }\n",
    "        xi = 0\n",
    "        for x in classes:\n",
    "            result['classes'].append({ 'class_name': x, 'confidence': float(outputs[0][xi]) })\n",
    "            xi += 1\n",
    "        return result\n",
    "\n",
    "class InstaMLConfig():\n",
    "    def __init__(self, config_json_file):\n",
    "        with open(config_json_file, \"r\") as cjf:\n",
    "            self.jconf = json.load(cjf)\n",
    "        pth, fname = os.path.split(config_json_file)\n",
    "        if len(pth) < 1:\n",
    "            pth = \".\"\n",
    "        self.name = self.jconf.get('name', '')\n",
    "        self.model_dir = pth\n",
    "\n",
    "    def load_instaML_model_file(self):\n",
    "        model_info_file_name = self.model_dir + \"/.model.json\"\n",
    "        with open(model_info_file_name, \"r\") as mif:\n",
    "            self.model_info = json.load(mif)\n",
    "        return self.model_info\n",
    "\n",
    "    def save_instaML_model_file(self):\n",
    "        model_info_file_name = self.model_dir + \"/.model.json\"\n",
    "        with open(model_info_file_name, \"w\") as mif:\n",
    "            json.dump(self.model_info, mif, indent=4)\n",
    "        # end\n",
    "    # end\n",
    "# end\n",
    "\n",
    "\n",
    "def get_config(path_config):\n",
    "\n",
    "    configPath = path_config\n",
    "    print(configPath)\n",
    "\n",
    "    with open(configPath, \"r\") as yf:\n",
    "        config = yaml.safe_load(yf)\n",
    "    return config\n",
    "# end\n",
    "\n",
    "\n",
    "class InstaMLConfig():\n",
    "    def __init__(self, config_json_file):\n",
    "        with open(config_json_file, \"r\") as cjf:\n",
    "            self.jconf = json.load(cjf)\n",
    "        pth, fname = os.path.split(config_json_file)\n",
    "        if len(pth) < 1:\n",
    "            pth = \".\"\n",
    "        self.name = self.jconf.get('name', '')\n",
    "        self.model_dir = pth\n",
    "\n",
    "    def load_instaML_model_file(self):\n",
    "        model_info_file_name = self.model_dir + \"/.model.json\"\n",
    "        with open(model_info_file_name, \"r\") as mif:\n",
    "            self.model_info = json.load(mif)\n",
    "        return self.model_info\n",
    "\n",
    "    def save_instaML_model_file(self):\n",
    "        model_info_file_name = self.model_dir + \"/.model.json\"\n",
    "        with open(model_info_file_name, \"w\") as mif:\n",
    "            json.dump(self.model_info, mif, indent=4)\n",
    "        # end\n",
    "    # end\n",
    "# end\n",
    "\n",
    "def load_model_readymlbert(conf, device):\n",
    "    conf.load_instaML_model_file()\n",
    "\n",
    "    bert_config = { 'root': conf.model_dir, 'name': conf.model_info['name'], 'classes': conf.model_info['classes'], 'device': device }\n",
    "    bert_config.update(conf.model_info['bert'])\n",
    "    model = BertClassifier(bert_config)\n",
    "    model.load_bert_for_inference()\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(model.model_file_path, map_location=torch.device(device)))\n",
    "    model.l1.myname = 'vbert'\n",
    "    return model\n",
    "# end\n",
    "\n",
    "def load_model(env_config, dir_model, device):\n",
    "    modelDir = dir_model\n",
    "    if not os.path.exists(modelDir):\n",
    "        return None, \"Model does not exist: \" + modelDir\n",
    "\n",
    "    print(\"Importing model from\", modelDir)\n",
    "    try:\n",
    "        conf = InstaMLConfig(modelDir + \"/.model.json\")\n",
    "        model = load_model_readymlbert(conf, device)\n",
    "        return model, None\n",
    "    except Exception as e:\n",
    "        err = \"Cannot load model: \" + str(e)\n",
    "        return None, err\n",
    "    # end\n",
    "# end\n",
    "\n",
    "def predict(model, text):\n",
    "    result = model.classify_text(text)\n",
    "    return result\n",
    "# end\n",
    "\n",
    "def main_load_model(path_config='config.yaml', dir_model='vBERT-base-20', device='cuda'):\n",
    "    env_config = get_config(path_config)\n",
    "    model, err = load_model(env_config, dir_model, device)\n",
    "    if not err:\n",
    "        return model\n",
    "    else:\n",
    "        raise Exception(err)\n",
    "    # end\n",
    "# end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d32ef3-2a4e-453a-9682-04c21ca406b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.yaml\n",
      "Importing model from vBERT-base-20\n",
      "Loading Tokenizer from: vBERT-base-20\n",
      "Loading vBERT config from: vBERT-base-20/bert_config.json\n",
      "Adding 768x16 classification layer\n"
     ]
    }
   ],
   "source": [
    "# jinyuj = SimpleBertClassifier('vBERT-base-20')\n",
    "# jinyuj.load()\n",
    "# model_jinyuj = jinyuj.l1\n",
    "# model_jinyuj.myname = 'jinyuj'\n",
    "\n",
    "\n",
    "vbert = main_load_model()\n",
    "model_vbert = vbert.l1\n",
    "model_vbert.myname = 'vbert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf967c0-75c6-44ad-8d37-88973076089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_jinyuj = jinyuj.factory_encoder.tokenizer\n",
    "# tokenizer_jinyuj.myname = 'jinyuj'\n",
    "\n",
    "tokenizer_vbert = vbert.tokenizer\n",
    "tokenizer_vbert.myname = 'vbert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50fe2f07-7ace-4f50-8cb6-7e771787d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bad(sentence, tokenizer, device='cuda'):\n",
    "    sentence = \" \".join(sentence.split())\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "    # end\n",
    "    \n",
    "    return inputs\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670923d8-e310-4580-a764-da0aed38d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bad2(sentence, tokenizer, model, device='cuda'):\n",
    "    dataset = pd.DataFrame.from_dict({ 'row': [ 0, sentence ]}, orient='index')\n",
    "    prepared_set = Prepare(dataset, tokenizer, 128)\n",
    "    params = {'batch_size': 1,'shuffle': False,'num_workers': 0}\n",
    "    loader = DataLoader(prepared_set, **params)\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            input_ids = data['ids'].to(device)\n",
    "            attention_mask = data['mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "        # for\n",
    "    # with\n",
    "    return outputs\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9accbc-7ffe-4c4f-9984-7f361b14dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_bad3(txt, tokenizer, model, device='cuda'):\n",
    "    dataset = pd.DataFrame.from_dict({ 'row': [ 0, txt ]}, orient='index')\n",
    "    prepared_set = Prepare(dataset, tokenizer, 128)\n",
    "    params = {'batch_size': 1,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "            }\n",
    "    data_loader = DataLoader(prepared_set, **params)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_ids = data['ids'].to(device)\n",
    "            attention_mask = data['mask'].to(device)\n",
    "\n",
    "            outputs_raw = model.l1(input_ids, attention_mask)  # model here is l1\n",
    "        # end\n",
    "    # end\n",
    "\n",
    "    return outputs_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26453066-8e54-49d7-87dd-1c89a9163fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8497, -0.8521,  0.2835,  ...,  0.2763,  0.0888,  0.3067],\n",
      "         [-0.1570,  1.1183, -0.7084,  ..., -0.6026, -0.2311,  0.3412],\n",
      "         [ 0.0698,  0.1330, -0.3425,  ..., -0.5510,  0.0676, -0.7215],\n",
      "         ...,\n",
      "         [-0.0167,  0.1184,  0.0677,  ..., -0.2849, -0.1566,  0.6619],\n",
      "         [-0.1870,  0.1368, -0.0331,  ..., -0.2738, -0.1185,  0.5610],\n",
      "         [-0.0327,  0.1657, -0.2367,  ...,  0.0413,  0.0374,  0.6371]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sentence = 'timestamp failed at play deploy vm efi nvme vmxnet number timestamp task upload local file to esxi data store task path home worker workspace dw rhel number arm ansible vsphere gos validation common esxi upload data store file yml number exception in vsphere copy python when main in request python when http error default fatal localhost failed http error number not found timestamp task testing exit due to failure task path home worker workspace dw rhel number arm ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi nvme vmxnet number'\n",
    "outputs = encode_bad3(sentence, vbert.tokenizer, vbert)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f2bd1e-e52e-4b46-bd91-c4868768b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sentence = 'timestamp failed at play deploy vm efi nvme vmxnet number timestamp task upload local file to esxi data store task path home worker workspace dw rhel number arm ansible vsphere gos validation common esxi upload data store file yml number exception in vsphere copy python when main in request python when http error default fatal localhost failed http error number not found timestamp task testing exit due to failure task path home worker workspace dw rhel number arm ansible vsphere gos validation common test rescue yml number fatal localhost failed exit testing when exit testing when fail is set to true in test case deploy vm efi nvme vmxnet number'\n",
    "# item_encoded = encode_bad(sentence, tokenizer_vbert)\n",
    "# with torch.no_grad():\n",
    "#     output = model_vbert(**item_encoded)\n",
    "# # end\n",
    "# output\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
