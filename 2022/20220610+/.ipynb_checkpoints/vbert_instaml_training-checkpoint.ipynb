{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d78fbbf-fd8b-4086-a662-60bee5554ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, errno\n",
    "import json\n",
    "import yaml\n",
    "import argparse\n",
    "import sys\n",
    "import traceback\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# default constants\n",
    "\n",
    "VBERT_MODEL = \"vBERT-2020-Base\"\n",
    "CLASSIFICATION_LAYER_WIDTH= 768 if \"Base\" in VBERT_MODEL else 1024\n",
    "MAX_STRING_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a91dd4-04b1-4bd3-9d7d-92ce39d275e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_base = 'data'\n",
    "name_testset = 'importance_detection_testset_200_12000.csv'\n",
    "path_testset = os.path.join(path_data_base, name_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a083aa-e8d1-485b-963f-f59454c8c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_base = 'models-instaml-finetune'\n",
    "version_model = 'v1216'\n",
    "name_file_config = '.model.json'\n",
    "path_model = os.path.join(path_model_base, version_model)\n",
    "path_config = os.path.join(path_model, name_file_config)\n",
    "\n",
    "with open(path_config, 'r') as file:\n",
    "    config_model = json.load(file)\n",
    "# end\n",
    "\n",
    "for k, v in config_model.get('bert').items():\n",
    "    config_model[k] = v\n",
    "# end\n",
    "\n",
    "config_model['root'] = path_model\n",
    "\n",
    "del config_model['allmetrics']\n",
    "del config_model['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7ddd177-3c91-405f-b2a5-9ce159119fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13d6203e-e775-42bf-ae6a-b8310e822b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.config = config\n",
    "        self.model_dir = config.get(\"root\", \".\")\n",
    "        self.model_name = config.get(\"name\", VBERT_MODEL)\n",
    "        self.classification_layer_width = config.get(\"input_size\", CLASSIFICATION_LAYER_WIDTH)\n",
    "        self.classes = config.get(\"classes\", [\"0\", \"1\"])\n",
    "        self.num_classes = config.get(\"output_size\", len(self.classes))\n",
    "        self.max_string_length = config.get(\"max_length\", MAX_STRING_LEN)\n",
    "        self.bert_config_file = self.model_dir + \"/bert_config.json\"\n",
    "        self.model_file_path = self.model_dir + \"/model.pt\"\n",
    "        self.device = config.get(\"device\", \"cpu\")\n",
    "#         self.device = 'cuda'\n",
    "        self.metrics = [ \"accuracy\", \"precision\", \"recall\", \"F-score\" ]\n",
    "\n",
    "        # Load the vBERT vocabulary into the tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_dir)\n",
    "    # end\n",
    "\n",
    "    def load_bert_for_training(self):\n",
    "        print('Loading vBERT model: ' + self.model_name)\n",
    "        self.l1 = BertModel.from_pretrained(pretrained_model_name_or_path=self.config.bert_model_dir)\n",
    "        print(\"Adding {}x{} classification layer\".format(self.classification_layer_width, self.num_classes))\n",
    "        self.classifier = torch.nn.Linear(self.classification_layer_width, self.num_classes)\n",
    "\n",
    "    def load_bert_for_inference(self):\n",
    "        print('Loading vBERT config')\n",
    "        self.l1 = BertModel(BertConfig.from_pretrained(self.bert_config_file))\n",
    "        print(\"Adding {}x{} classification layer\".format(self.classification_layer_width, self.num_classes))\n",
    "        self.classifier = torch.nn.Linear(self.classification_layer_width, self.num_classes)\n",
    "\n",
    "    # Encode the input with vBERT, read output from the last layer of vBERT, and pass to the classification layer\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         print(output_1.keys())\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0,:]\n",
    "#         pooler = hidden_state[:, 0]\n",
    "#         print(pooler.numpy()[0][:5])\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "#     def do_inference(self, data_loader, test_output=None):\n",
    "#         model = self.eval()\n",
    "#         predictions = []\n",
    "#         real_values = []\n",
    "#         with torch.no_grad():\n",
    "#             for data in data_loader:\n",
    "#                 input_ids = data['ids'].to(self.device)\n",
    "#                 attention_mask = data['mask'].to(self.device)\n",
    "#                 targets = data['targets'].to(self.device)\n",
    "#                 outputs = model(input_ids, attention_mask)\n",
    "#                 _, preds = torch.max(outputs, dim=1)\n",
    "#                 predictions.extend(preds)\n",
    "#                 real_values.extend(targets)\n",
    "#                 if test_output != None:\n",
    "#                     test_output.extend(outputs)\n",
    "#         predictions = torch.stack(predictions).cpu()\n",
    "#         real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "#         test_accu = 100 * accuracy_score(real_values, predictions)\n",
    "#         test_precision, test_recall, test_fscore, ignore = precision_recall_fscore_support(real_values, predictions, average='macro')\n",
    "#         test_precision *= 100\n",
    "#         test_recall *= 100\n",
    "#         test_fscore *= 100\n",
    "#         metrics = [ test_accu, test_precision, test_recall, test_fscore ]\n",
    "#         return predictions, real_values, metrics\n",
    "#     # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e9fc264-f7e4-4e2e-9041-0270fade5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prepare(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, classes):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.classes = classes\n",
    "        \n",
    "        self.index_class_label = {klass:index for index, klass in enumerate(self.classes)}\n",
    "    # end\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data['log'][index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        label = self.index_class_label[self.data['pcc'][index]]\n",
    "\n",
    "#         target = torch.tensor(label, dtype=torch.long)\n",
    "        target = label\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long).to(device),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long).to(device),\n",
    "            'targets': label\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    # end\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a591be59-7104-4cd4-b26b-d4d65c96b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# def classify_text(model, txt, conf=None):\n",
    "#     tokenizer = model.tokenizer\n",
    "#     max_string_length = model.max_string_length\n",
    "    \n",
    "#     dataset = pd.DataFrame.from_csv(path_testset)\n",
    "#     prepared_set = Prepare(dataset, tokenizer, max_string_length)\n",
    "#     params = {'batch_size': 1,\n",
    "#                 'shuffle': False,\n",
    "#                 'num_workers': 0\n",
    "#             }\n",
    "#     loader = DataLoader(prepared_set, **params)\n",
    "#     outputs = []\n",
    "#     predictions, real_values, metrics = self.do_inference(loader, outputs)\n",
    "#     classes = model.classes\n",
    "#     result = { 'classes': [ ], 'text': txt, 'top_class': classes[int(predictions[0])], 'top_class_index': int(predictions[0]) }\n",
    "#     xi = 0\n",
    "#     for x in classes:\n",
    "#         result['classes'].append({ 'class_name': x, 'confidence': float(outputs[0][xi]) })\n",
    "#         xi += 1\n",
    "#     return result\n",
    "\n",
    "\n",
    "#     dataset_eval = Dataset(train_texts + valid_texts, train_labels + valid_labels)\n",
    "#     dataload_eval = torch.utils.data.DataLoader(dataset_eval, batch_size=4, shuffle=True)\n",
    "\n",
    "#     y_hats_all = []\n",
    "#     y_true_all = []\n",
    "#     for _valid_texts, _valid_labels in tqdm(dataload_eval):\n",
    "#         outputs = predict_minibatch(model, _valid_texts)\n",
    "#         y_hats = outputs['y_hats'].tolist()\n",
    "#         y_true = _valid_labels.detach().cpu().numpy().tolist()   \n",
    "#         y_hats_all += y_hats\n",
    "#         y_true_all += y_true\n",
    "#     # end\n",
    "\n",
    "#     print('precision_score: {}'.format(precision_score(y_true_all, y_hats_all)))\n",
    "#     print('recall_score: {}'.format(recall_score(y_true_all, y_hats_all)))\n",
    "#     print('f1_score: {}'.format(f1_score(y_true_all, y_hats_all)))\n",
    "# # end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c9ec5ea-cc2f-4e86-a684-c1ff1e0a7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(path_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09423524-04b5-440d-8246-715668d1318f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc</th>\n",
       "      <th>pcc</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment</td>\n",
       "      <td>comment</td>\n",
       "      <td>timestamp 5776059 admission failure in path ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>comment</td>\n",
       "      <td>timestamp cpu 2 2156005 os fs os fs get mount ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>comment</td>\n",
       "      <td>timestamp hex id debug id hex id vsan ctl get ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pc      pcc                                                log\n",
       "0  comment  comment  timestamp 5776059 admission failure in path ho...\n",
       "1  comment  comment  timestamp cpu 2 2156005 os fs os fs get mount ...\n",
       "2  comment  comment  timestamp hex id debug id hex id vsan ctl get ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7894b731-e804-430a-9377-93b7ae573f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vBERT config\n",
      "Adding 768x2 classification layer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertClassifier(config_model)\n",
    "model.load_bert_for_inference()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(model.model_file_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2bc1af9-8920-42a1-88ab-49ba95965cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(path_testset)\n",
    "dataset = dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8724997-d8d0-480d-a14c-31c74a2af0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 26.88it/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = model.max_string_length\n",
    "classes = model.classes\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "prepared_set = Prepare(dataset, tokenizer, max_length, classes)\n",
    "params_dataloader = {\n",
    "    'batch_size': 2,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "}\n",
    "data_loader = DataLoader(prepared_set, **params_dataloader)\n",
    "y_hats_all = []\n",
    "y_true_all = []\n",
    "ids_top3 = []\n",
    "outputs_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(data_loader):\n",
    "#         ids_top3.append(data['ids'].cpu().numpy()[0][:3])\n",
    "        input_ids = data['ids'].to(device)\n",
    "        attention_mask = data['mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        outputs_all.append(outputs)\n",
    "#         print(outputs)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        y_hats_all.append(preds.cpu().numpy()[0])\n",
    "        y_true_all.append(data['targets'][0])\n",
    "    # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01e25fc2-c44f-49eb-9d2b-ced6f495a705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8095,  1.9613],\n",
       "        [ 2.8513, -2.6571]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86703195-d593-4777-96c3-5f3cb2e49bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.8094643,  1.9612609],\n",
       "       [ 2.851346 , -2.6570942]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_all[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5a4dcca-5869-428d-97d0-2c7e3b3975ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.8094643354415894, 1.9612609148025513],\n",
       " [2.851346015930176, -2.6570942401885986],\n",
       " [1.0022659301757812, -0.46435776352882385],\n",
       " [0.9951595664024353, -0.7257012128829956],\n",
       " [-0.6877875924110413, 0.6506273746490479],\n",
       " [1.5641168355941772, -1.6831454038619995],\n",
       " [1.7741361856460571, -1.6320698261260986],\n",
       " [1.0426920652389526, -0.3897593915462494],\n",
       " [0.8489823937416077, -0.6762673258781433],\n",
       " [2.16194486618042, -1.6360777616500854]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "vectors_all = []\n",
    "for outputs in outputs_all:\n",
    "    for vector_predict in outputs.cpu().numpy():\n",
    "        vectors_all.append(vector_predict.tolist())\n",
    "    # end\n",
    "# end\n",
    "vectors_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f69cde0-6cfd-4d50-b5ec-6c6d7a5fbe9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['safe', 'comment']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_set.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd122533-ff39-4cdf-8b81-9f301e675ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12546/12546 [04:13<00:00, 49.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.9350390562729157\n",
      "precision_score: 0.07934336525307797\n",
      "recall_score: 0.29\n",
      "f1_score: 0.12459720730397421\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score: {}'.format(accuracy_score(y_true_all, y_hats_all)))\n",
    "print('precision_score: {}'.format(precision_score(y_true_all, y_hats_all, zero_division=1)))\n",
    "print('recall_score: {}'.format(recall_score(y_true_all, y_hats_all, zero_division=1)))\n",
    "print('f1_score: {}'.format(f1_score(y_true_all, y_hats_all, zero_division=1)))\n",
    "# print(ids_top3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a31b660-d014-4326-b7ce-aaf8221aeb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_hats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da22ae9-e276-4c5f-b32a-1af5befcd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_hats_all.txt', 'w+') as file:\n",
    "    file.write(json.dumps([int(x) for x in y_hats_all]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8f1a755-bf68-44f8-94db-21e252e9783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y_true_all.txt', 'w+') as file:\n",
    "    file.write(json.dumps([int(x) for x in y_true_all]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44d18938-2a77-479c-a7c3-8d080df51190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.9350390562729157\n",
      "precision_score: 0.5336623724276393\n",
      "recall_score: 0.6177442086505751\n",
      "f1_score: 0.5454325798946923\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score: {}'.format(accuracy_score(y_true_all, y_hats_all)))\n",
    "print('precision_score: {}'.format(precision_score(y_true_all, y_hats_all, zero_division=1, average='macro')))\n",
    "print('recall_score: {}'.format(recall_score(y_true_all, y_hats_all, zero_division=1, average='macro')))\n",
    "print('f1_score: {}'.format(f1_score(y_true_all, y_hats_all, zero_division=1, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975025e-8087-4805-9cfc-4cd29957f785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
