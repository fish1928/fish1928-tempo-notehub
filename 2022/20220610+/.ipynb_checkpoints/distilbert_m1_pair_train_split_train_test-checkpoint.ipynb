{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a6d8e0-5800-4e55-ae2e-dab30c423b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you get all pretrained model name here\n",
    "# https://huggingface.co/transformers/pretrained_models.html\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "# from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, EarlyStoppingCallback\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2adce6ab-61dd-4ce6-8a19-ae6a376de4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model we gonna train, base uncased BERT\n",
    "# check text classification models here: https://huggingface.co/models?filter=text-classification\n",
    "# model_name = \"bert-base-uncased\"\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_dir = 'model_esxdeploy_filter_distilbert_1'\n",
    "# max sequence length for each document/sentence sample\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef49596-da0b-4ba2-9157-afa279f76193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d4a022-6660-4130-9b11-c320f27cdefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_passages(path_data_train, path_data_eval):\n",
    "    df_train = pd.read_csv(path_data_train)\n",
    "    anchers_train = df_train['ancher'].to_list()\n",
    "    documents_train = df_train['log'].to_list()\n",
    "    labels_str_train = df_train['label'].to_list()\n",
    "    \n",
    "    df_eval = pd.read_csv(path_data_eval)\n",
    "    anchers_eval = df_eval['ancher'].to_list()\n",
    "    documents_eval = df_eval['log'].to_list()\n",
    "    labels_str_eval = df_eval['label'].to_list()\n",
    "    \n",
    "\n",
    "    samples_train = [(ancher, document) for ancher, document in zip(anchers_train,documents_train)]\n",
    "    samples_eval = [(ancher, document) for ancher, document in zip(anchers_eval,documents_eval)]\n",
    "    \n",
    "    labels_index = sorted(list(set(labels_str_train + labels_str_eval)))\n",
    "    labels_all = {l:idx for idx, l in enumerate(labels_index)}\n",
    "    \n",
    "    labels_train = [labels_all[label_str] for label_str in labels_str_train]\n",
    "    labels_eval = [labels_all[label_str] for label_str in labels_str_eval]\n",
    "\n",
    "    samples_eval = samples_eval[:len(samples_eval)/2]\n",
    "    labels_eval = labels_eval[:len(samples_eval)/2]\n",
    "    return samples_train, samples_eval, labels_train, labels_eval, labels_index\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ccf01d-49ea-4f8b-8570-7f4e3ac20888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "dir_data = 'datasource'\n",
    "filename_train = 'esxdeploy_20220512_ancher_sample_nonumber.csv'\n",
    "filename_eval = 'esxdeploy_20220512_ari_nonumber_m1_testset.csv'\n",
    "path_file_train = os.path.join(dir_data, filename_train)\n",
    "path_file_eval = os.path.join(dir_data, filename_eval)\n",
    "\n",
    "train_samples, valid_samples, train_labels, valid_labels, target_names = read_passages(path_file_train, path_file_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4482838-28d7-4fc4-b4d4-52d861463d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_samples[837]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "536e29f8-269f-4de8-adb3-a05a16636adc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextInputSequence must be str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cee233fc3ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvalid_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2684\u001b[0m         )\n\u001b[1;32m   2685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2686\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2687\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2688\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TextInputSequence must be str"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer.batch_encode_plus(train_samples, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "valid_encodings = tokenizer.batch_encode_plus(valid_samples, truncation=True, padding=True, max_length=max_length, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c635d7-7eb0-4513-b10d-b3ea53217e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c1278-aa9d-456a-9329-2c5a315d14c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SimpleDataset(train_encodings, train_labels)\n",
    "valid_dataset = SimpleDataset(valid_encodings, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea18b82-385c-4897-bcb4-9a4805e8704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(target_names))\n",
    "if os.path.exists(model_dir) and len(os.listdir(model_dir) > 0):\n",
    "    print('load model from local')\n",
    "    model_info = model_dir\n",
    "else:\n",
    "    print('load model from official')\n",
    "    model_info = model_name\n",
    "    \n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_info, num_labels=len(target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b1702c-ced3-4170-a078-1aff7d98598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids.reshape(-1)\n",
    "    # pred = np.argmax(pred, axis=1)\n",
    "    preds = pred.predictions.argmax(-1).reshape(-1)\n",
    "\n",
    "    # print('labels: {}'.format(labels))\n",
    "    # print('pred: {}'.format(preds))\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=preds)\n",
    "    precision = precision_score(y_true=labels, y_pred=preds, zero_division=1, average='macro')\n",
    "    recall = recall_score(y_true=labels, y_pred=preds, zero_division=1, average='macro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, zero_division=1, average='macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b592a11-8722-40e8-8ad6-ef9f31f3455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=20,              # total number of training epochs\n",
    "    per_device_train_batch_size=2,  # batch size per device during training\n",
    "    per_device_eval_batch_size=2,   # batch size for evaluation\n",
    "    warmup_steps=0,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "    logging_steps=1,               # log & save weights each logging_steps\n",
    "    evaluation_strategy=\"epoch\",     # evaluate each `logging_steps`\n",
    "    learning_rate=2e-5,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    metric_for_best_model='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb769a-0bc7-4d6b-a34c-6e57e6760c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,                         # the instantiated Transformers model to be trained\n",
    "#     args=training_args,                  # training arguments, defined above\n",
    "#     train_dataset=train_dataset,         # training dataset\n",
    "#     compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "# )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6a7f1-2255-4c98-9cee-2ed5214b69a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed2a8c-34d2-4871-8a3a-0e533f3d09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013c187-af0a-4aa0-b119-122dcc9c4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be7187-23eb-4970-bbcd-f9bda226e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('model_priority_distilbert_1/labels.json', 'w+') as file:\n",
    "    file.write(json.dumps(target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
