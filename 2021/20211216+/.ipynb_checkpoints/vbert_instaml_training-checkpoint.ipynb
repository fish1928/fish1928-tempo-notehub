{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d78fbbf-fd8b-4086-a662-60bee5554ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, errno\n",
    "import json\n",
    "import yaml\n",
    "import argparse\n",
    "import sys\n",
    "import traceback\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# default constants\n",
    "\n",
    "VBERT_MODEL = \"vBERT-2020-Base\"\n",
    "CLASSIFICATION_LAYER_WIDTH = 768 if \"Base\" in VBERT_MODEL else 1024\n",
    "MAX_STRING_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a91dd4-04b1-4bd3-9d7d-92ce39d275e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_base = 'data'\n",
    "name_testset = 'importance_detection_testset_200_12000.csv'\n",
    "path_testset = os.path.join(path_data_base, name_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a083aa-e8d1-485b-963f-f59454c8c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_base = 'models-instaml-finetune'\n",
    "version_model = 'v11'\n",
    "name_file_config = '.model.json'\n",
    "path_model = os.path.join(path_model_base, version_model)\n",
    "path_config = os.path.join(path_model, name_file_config)\n",
    "\n",
    "with open(path_config, 'r') as file:\n",
    "    config_model = json.load(file)\n",
    "# end\n",
    "\n",
    "for k, v in config_model.get('bert').items():\n",
    "    config_model[k] = v\n",
    "# end\n",
    "\n",
    "config_model['root'] = path_model\n",
    "\n",
    "del config_model['allmetrics']\n",
    "del config_model['metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ddd177-3c91-405f-b2a5-9ce159119fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'importance_detection_30_12000_rand0_0-vBERT-2020-Base-20-512-2-20',\n",
       " 'user': 'jinyuj@vmware.com',\n",
       " 'description': 'Training model importance_detection_30_12000_rand0_0-vBERT-2020-Base-20-512-2-20',\n",
       " 'parentLM': 'vBERT-2020-Base',\n",
       " 'uuid': 'c4a54cde-e6a3-421b-ae48-1ffc593506e5',\n",
       " 'arch': 'bert',\n",
       " 'isSubtask': False,\n",
       " 'lm': 'clas_2.h5',\n",
       " 'vocab': 'itos.pkl',\n",
       " 'type': 'Classification',\n",
       " 'version': '1.0',\n",
       " 'bert': {'max_length': 512, 'input_size': 768, 'output_size': 2},\n",
       " 'classes': ['safe', 'comment'],\n",
       " 'best_epoch': 3,\n",
       " 'best_Fscore': 0.9614337933366612,\n",
       " 'max_length': 512,\n",
       " 'input_size': 768,\n",
       " 'output_size': 2,\n",
       " 'root': 'models-instaml-finetune/v11'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13d6203e-e775-42bf-ae6a-b8310e822b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.config = config\n",
    "        self.model_dir = config.get(\"root\", \".\")\n",
    "        self.model_name = config.get(\"name\", VBERT_MODEL)\n",
    "        self.classification_layer_width = config.get(\"input_size\", CLASSIFICATION_LAYER_WIDTH)\n",
    "        self.classes = config.get(\"classes\", [\"0\", \"1\"])\n",
    "        self.num_classes = config.get(\"output_size\", len(self.classes))\n",
    "        self.max_string_length = config.get(\"max_length\", MAX_STRING_LEN)\n",
    "        self.bert_config_file = self.model_dir + \"/bert_config.json\"\n",
    "        self.model_file_path = self.model_dir + \"/model.pt\"\n",
    "        self.device = config.get(\"device\", \"cpu\")\n",
    "#         self.device = 'cuda'\n",
    "        self.metrics = [ \"accuracy\", \"precision\", \"recall\", \"F-score\" ]\n",
    "\n",
    "        # Load the vBERT vocabulary into the tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_dir)\n",
    "    # end\n",
    "\n",
    "    def load_bert_for_training(self):\n",
    "        print('Loading vBERT model: ' + self.model_name)\n",
    "        self.l1 = BertModel.from_pretrained(pretrained_model_name_or_path=self.config.bert_model_dir)\n",
    "        print(\"Adding {}x{} classification layer\".format(self.classification_layer_width, self.num_classes))\n",
    "        self.classifier = torch.nn.Linear(self.classification_layer_width, self.num_classes)\n",
    "\n",
    "    def load_bert_for_inference(self):\n",
    "        print('Loading vBERT config')\n",
    "        self.l1 = BertModel(BertConfig.from_pretrained(self.bert_config_file))\n",
    "        print(\"Adding {}x{} classification layer\".format(self.classification_layer_width, self.num_classes))\n",
    "        self.classifier = torch.nn.Linear(self.classification_layer_width, self.num_classes)\n",
    "\n",
    "    # Encode the input with vBERT, read output from the last layer of vBERT, and pass to the classification layer\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "#         print(pooler.numpy()[0][:5])\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "#     def do_inference(self, data_loader, test_output=None):\n",
    "#         model = self.eval()\n",
    "#         predictions = []\n",
    "#         real_values = []\n",
    "#         with torch.no_grad():\n",
    "#             for data in data_loader:\n",
    "#                 input_ids = data['ids'].to(self.device)\n",
    "#                 attention_mask = data['mask'].to(self.device)\n",
    "#                 targets = data['targets'].to(self.device)\n",
    "#                 outputs = model(input_ids, attention_mask)\n",
    "#                 _, preds = torch.max(outputs, dim=1)\n",
    "#                 predictions.extend(preds)\n",
    "#                 real_values.extend(targets)\n",
    "#                 if test_output != None:\n",
    "#                     test_output.extend(outputs)\n",
    "#         predictions = torch.stack(predictions).cpu()\n",
    "#         real_values = torch.stack(real_values).cpu()\n",
    "\n",
    "#         test_accu = 100 * accuracy_score(real_values, predictions)\n",
    "#         test_precision, test_recall, test_fscore, ignore = precision_recall_fscore_support(real_values, predictions, average='macro')\n",
    "#         test_precision *= 100\n",
    "#         test_recall *= 100\n",
    "#         test_fscore *= 100\n",
    "#         metrics = [ test_accu, test_precision, test_recall, test_fscore ]\n",
    "#         return predictions, real_values, metrics\n",
    "#     # end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e9fc264-f7e4-4e2e-9041-0270fade5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prepare(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, classes):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.classes = classes\n",
    "        \n",
    "        self.index_class_label = {klass:index for index, klass in enumerate(self.classes)}\n",
    "    # end\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data['log'][index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        label = self.index_class_label[self.data['pcc'][index]]\n",
    "\n",
    "#         target = torch.tensor(label, dtype=torch.long)\n",
    "        target = label\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': label\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    # end\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a591be59-7104-4cd4-b26b-d4d65c96b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# def classify_text(model, txt, conf=None):\n",
    "#     tokenizer = model.tokenizer\n",
    "#     max_string_length = model.max_string_length\n",
    "    \n",
    "#     dataset = pd.DataFrame.from_csv(path_testset)\n",
    "#     prepared_set = Prepare(dataset, tokenizer, max_string_length)\n",
    "#     params = {'batch_size': 1,\n",
    "#                 'shuffle': False,\n",
    "#                 'num_workers': 0\n",
    "#             }\n",
    "#     loader = DataLoader(prepared_set, **params)\n",
    "#     outputs = []\n",
    "#     predictions, real_values, metrics = self.do_inference(loader, outputs)\n",
    "#     classes = model.classes\n",
    "#     result = { 'classes': [ ], 'text': txt, 'top_class': classes[int(predictions[0])], 'top_class_index': int(predictions[0]) }\n",
    "#     xi = 0\n",
    "#     for x in classes:\n",
    "#         result['classes'].append({ 'class_name': x, 'confidence': float(outputs[0][xi]) })\n",
    "#         xi += 1\n",
    "#     return result\n",
    "\n",
    "\n",
    "#     dataset_eval = Dataset(train_texts + valid_texts, train_labels + valid_labels)\n",
    "#     dataload_eval = torch.utils.data.DataLoader(dataset_eval, batch_size=4, shuffle=True)\n",
    "\n",
    "#     y_hats_all = []\n",
    "#     y_true_all = []\n",
    "#     for _valid_texts, _valid_labels in tqdm(dataload_eval):\n",
    "#         outputs = predict_minibatch(model, _valid_texts)\n",
    "#         y_hats = outputs['y_hats'].tolist()\n",
    "#         y_true = _valid_labels.detach().cpu().numpy().tolist()   \n",
    "#         y_hats_all += y_hats\n",
    "#         y_true_all += y_true\n",
    "#     # end\n",
    "\n",
    "#     print('precision_score: {}'.format(precision_score(y_true_all, y_hats_all)))\n",
    "#     print('recall_score: {}'.format(recall_score(y_true_all, y_hats_all)))\n",
    "#     print('f1_score: {}'.format(f1_score(y_true_all, y_hats_all)))\n",
    "# # end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c9ec5ea-cc2f-4e86-a684-c1ff1e0a7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09423524-04b5-440d-8246-715668d1318f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc</th>\n",
       "      <th>pcc</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comment</td>\n",
       "      <td>comment</td>\n",
       "      <td>timestamp 5776059 admission failure in path ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>comment</td>\n",
       "      <td>timestamp cpu 2 2156005 os fs os fs get mount ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>comment</td>\n",
       "      <td>timestamp hex id debug id hex id vsan ctl get ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pc      pcc                                                log\n",
       "0  comment  comment  timestamp 5776059 admission failure in path ho...\n",
       "1  comment  comment  timestamp cpu 2 2156005 os fs os fs get mount ...\n",
       "2  comment  comment  timestamp hex id debug id hex id vsan ctl get ..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7894b731-e804-430a-9377-93b7ae573f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file models-instaml-finetune/v11/config.json not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vBERT config\n",
      "Adding 768x2 classification layer\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(config_model)\n",
    "model.load_bert_for_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "675a4870-f095-4d44-baa4-c7ee7efd635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:01,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.88267845 -0.21946722  0.8123034   2.0112073   0.0971041 ]\n",
      "tensor([[0.0389, 0.8071]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.4128314  -0.11017562  0.01700808  1.9025621   1.7560191 ]\n",
      "tensor([[0.0229, 0.4654]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.555083   0.55100614 0.5655971  1.6053084  1.6196274 ]\n",
      "tensor([[0.6791, 0.8159]])\n",
      "precision_score: 1.0\n",
      "recall_score: 1.0\n",
      "f1_score: 1.0\n",
      "[array([ 101,   13, 5401]), array([  101,    13, 17368]), array([ 101,   13, 2002])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(path_testset)[:3]\n",
    "max_length = model.max_string_length\n",
    "classes = model.classes\n",
    "tokenizer = model.tokenizer\n",
    "device = 'cpu'\n",
    "prepared_set = Prepare(dataset, tokenizer, max_length, classes)\n",
    "params_dataloader = {\n",
    "    'batch_size': 1,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "}\n",
    "data_loader = DataLoader(prepared_set, **params_dataloader)\n",
    "y_hats_all = []\n",
    "y_true_all = []\n",
    "ids_top3 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(data_loader):\n",
    "        ids_top3.append(data['ids'].numpy()[0][:3])\n",
    "        input_ids = data['ids'].to(device)\n",
    "        attention_mask = data['mask'].to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        print(outputs)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        y_hats_all.append(preds.numpy()[0])\n",
    "        y_true_all.append(data['targets'][0])\n",
    "    # end\n",
    "# end\n",
    "\n",
    "print('precision_score: {}'.format(precision_score(y_true_all, y_hats_all)))\n",
    "print('recall_score: {}'.format(recall_score(y_true_all, y_hats_all)))\n",
    "print('f1_score: {}'.format(f1_score(y_true_all, y_hats_all)))\n",
    "print(ids_top3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31b660-d014-4326-b7ce-aaf8221aeb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da22ae9-e276-4c5f-b32a-1af5befcd4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
